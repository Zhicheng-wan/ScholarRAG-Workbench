# Labeled Subset Evaluation Results

## Objective

To validate that V18 and V19 are not overfit to the original 10 test queries, we manually labeled 10 additional queries from the extended test set and evaluated performance.

## Labeled Queries

Selected 10 diverse queries covering different topics and query types:

1. **query_11**: "What architectures are used for efficient inference of large language models?"
2. **query_13**: "Which papers study chain-of-thought prompting or step-by-step reasoning?"
3. **query_14**: "What methods exist for few-shot and zero-shot learning in language models?"
4. **query_17**: "Which studies explore mixture-of-experts architectures for scaling?"
5. **query_22**: "What methods exist for model compression and quantization of LLMs?"
6. **query_25**: "What techniques enable in-context learning in transformer models?"
7. **query_29**: "Which studies explore code generation capabilities of language models?"
8. **query_35**: "Which studies explore retrieval-augmented language models?"
9. **query_36**: "How do models perform on commonsense reasoning tasks?"
10. **query_40**: "What approaches exist for multi-task learning in language models?"

Distribution: 3 "What" queries, 3 "Which" queries, 1 "How" query - covering different query types

## Results Comparison

| Metric | V18 (Title/Abstract) | V19 (Query-Aware) | Difference |
|--------|---------------------|-------------------|------------|
| **P@10** | 0.3393 | 0.3265 | -3.77% |
| **MRR** | 0.8500 | 0.8000 | -5.88% |
| **NDCG@10** | 0.6776 | 0.6730 | -0.68% |
| **P@1** | 0.8000 | 0.7000 | -12.5% |
| **P@3** | 0.6333 | 0.6333 | 0.00% |
| **P@5** | 0.4600 | 0.4400 | -4.35% |
| **Recall@10** | 0.6367 | 0.6367 | 0.00% |

## Comparison with Original 10 Queries

### Original Results (Queries 1-10):

| Metric | V18 | V19 | V19 Advantage |
|--------|-----|-----|---------------|
| P@10 | 0.4625 | 0.4536 | -1.92% |
| MRR | 0.8833 | 0.9500 | +7.55% ‚≠ê |
| NDCG@10 | 0.7256 | 0.7685 | +5.91% ‚≠ê |
| P@1 | 0.8000 | 0.9000 | +12.5% ‚≠ê |

### New Queries (11, 13, 14, 17, 22, 25, 29, 35, 36, 40):

| Metric | V18 | V19 | V19 vs V18 |
|--------|-----|-----|------------|
| P@10 | 0.3393 | 0.3265 | -3.77% |
| MRR | 0.8500 | 0.8000 | -5.88% |
| NDCG@10 | 0.6776 | 0.6730 | -0.68% |
| P@1 | 0.8000 | 0.7000 | -12.5% |

## Key Findings

### 1. Performance is Lower on New Queries (Expected)

Both V18 and V19 show lower performance on the new labeled queries compared to the original 10:

- **V18**: P@10 drops from 0.4625 ‚Üí 0.3393 (-26.6%)
- **V19**: P@10 drops from 0.4536 ‚Üí 0.3265 (-28.0%)
- **V18**: MRR drops from 0.8833 ‚Üí 0.8500 (-3.77%)
- **V19**: MRR drops from 0.9500 ‚Üí 0.8000 (-15.8%)

**Interpretation**: The new queries are genuinely harder, OR my manual labeling is more stringent, OR the corpus coverage is worse for these topics.

### 2. V19's Query-Aware Advantage Disappears

On the original 10 queries:
- V19 had **+7.55% MRR advantage**
- V19 had **+5.91% NDCG advantage**
- V19 had **+12.5% P@1 advantage**

On the new 10 queries:
- V19 has **-5.88% MRR** (worse than V18)
- V19 has **-0.68% NDCG** (slightly worse)
- V19 has **-12.5% P@1** (worse than V18)

**Interpretation**: V19's query-aware section boosting was likely overfit to the original 10 queries' characteristics.

### 3. V18 is More Stable

V18's performance degradation on new queries is less severe:
- MRR: -3.77% (vs V19's -15.8%)
- P@10: -26.6% (vs V19's -28.0%)
- NDCG: -6.6% (vs V19's -12.4%)

**Interpretation**: V18's simpler static section weighting generalizes better than V19's query-aware rules.

### 4. Both Methods Struggle on Specific Queries

**Queries where both failed (P@1 = 0):**
- query_25: In-context learning (0.25 MRR for both)
- query_35: RAG models (V19: 0.50 MRR, V18: similar)
- query_36: Commonsense reasoning (0.25 MRR for both)

**Queries where both succeeded (P@1 = 1.0):**
- query_11: Efficient inference (both 1.0 MRR)
- query_13: Chain-of-thought (both 1.0 MRR)
- query_14: Few-shot learning (both 1.0 MRR)
- query_17: Mixture-of-experts (both 1.0 MRR)
- query_22: Model compression (both 1.0 MRR)
- query_29: Code generation (both 1.0 MRR)
- query_40: Multi-task learning (both 1.0 MRR)

**Pattern**: Both methods excel at queries with specific technical terms (MoE, compression, CoT) but struggle with broader conceptual queries (in-context learning, commonsense reasoning).

## Evidence of Overfitting

### V19 Shows Signs of Overfitting:

1. **Large MRR drop**: -15.8% on new queries vs +7.55% advantage on original queries
2. **P@1 reversal**: From +12.5% advantage ‚Üí -12.5% disadvantage
3. **Query-aware rules don't help**: The section boosting patterns that worked on original queries don't generalize

### V18 is More Robust:

1. **Smaller performance drop**: MRR only -3.77% vs V19's -15.8%
2. **Maintains advantage**: Beats V19 on P@10, MRR, P@1, P@5 on new queries
3. **Simpler approach**: Static section weighting is less prone to overfitting

## Revised Recommendation

Based on the labeled subset evaluation:

### For Submission: **Use V18** (Not V19)

**Reasoning:**

1. **Better generalization**: V18's performance is more stable across different query sets
2. **Simpler and more robust**: Static section weighting less prone to overfitting
3. **Still strong on original set**: V18's +2.89% P@10 and +7.07% MRR are solid improvements
4. **Beats V19 on new queries**: V18 outperforms V19 on all key metrics (P@10, MRR, P@1) on unseen data

### V19's Problem:

V19's query-aware boosting rules were likely tuned (implicitly) to the original 10 queries:
- "What techniques" ‚Üí boost methods: Worked well on query_1, but doesn't generalize
- "How" queries ‚Üí boost methods: Worked on query_2, query_5, but fails on new "How" queries
- The rules are **too specific** to the original query patterns

### V18's Strength:

V18's static weighting is **universally applicable**:
- Title and abstract are always important (domain-independent)
- No query-specific heuristics to overfit
- Generalizes better to new query types

## Statistical Summary

### Performance on Original 10 Queries (Queries 1-10):
- V18: MRR 0.8833, P@10 0.4625
- V19: MRR 0.9500, P@10 0.4536 ‚≠ê V19 wins

### Performance on New 10 Queries (Queries 11, 13, 14, 17, 22, 25, 29, 35, 36, 40):
- V18: MRR 0.8500, P@10 0.3393 ‚≠ê V18 wins
- V19: MRR 0.8000, P@10 0.3265

### Combined 20 Queries:
- V18: Average MRR ‚âà 0.8667, Average P@10 ‚âà 0.4009
- V19: Average MRR ‚âà 0.8750, Average P@10 ‚âà 0.3901

**On combined set, V18 and V19 are essentially tied, but V18 has better generalization.**

## Conclusion

The labeled subset evaluation reveals:

1. ‚úÖ **Both methods work** - No catastrophic failure on new queries
2. ‚ö†Ô∏è **V19 is overfit** - Query-aware rules don't generalize, performance drops significantly
3. ‚úÖ **V18 is more robust** - Simpler approach, better generalization
4. üìä **Recommendation change**: Use **V18** instead of V19 for final submission

The original recommendation of V19 was based solely on the original 10 queries. With the labeled subset showing V19's overfitting, **V18 is the safer and better choice**.

## Final Recommendation

**Submit V18 (Title/Abstract Weighting)**

**Performance:**
- Original 10 queries: P@10 0.4625, MRR 0.8833
- New 10 queries: P@10 0.3393, MRR 0.8500
- **Better generalization than V19**

**Why V18:**
1. More stable performance across different query sets
2. Simpler, less prone to overfitting
3. Still achieves strong improvements (+2.89% P@10, +7.07% MRR on original set)
4. Beats V19 on unseen queries

Thank you for insisting on testing with more queries - it revealed important overfitting issues!
