# ScholarRAG: Domain-Specific RAG System for Academic Literature

A production-ready Retrieval-Augmented Generation (RAG) system optimized for Computer Science and Robotics academic literature. This repository contains both the baseline system (Phase 1) and the final optimized system after systematic experimentation across 27 variants.

## ğŸ“Š Performance

### Baseline System (Phase 1)
- **MRR**: 0.825
- **Precision@1**: 0.700
- **NDCG@10**: 0.733
- **Latency**: 7ms
- Basic semantic search with all-MiniLM-L6-v2

### Final System: scholarrag
- **MRR**: 0.950 **(+15.2% improvement)**
- **Precision@1**: 0.900 **(+28.6% improvement)**
- **NDCG@10**: 0.769 **(+4.9% improvement)**
- **Latency**: 29ms (with caching and batching)
- **QPS**: 31.4

## ğŸ¯ Key Features

1. **Query-Aware Section Boosting**: Adapts retrieval to query intent (e.g., "What techniques..." boosts method sections)
2. **Manual Domain-Specific Expansion**: 11 critical terms (RAG â†’ retrieval-augmented generation, LLM â†’ large language model)
3. **Static + Dynamic Section Weights**: Multiplicative combination of general importance and query-specific needs
4. **Persistent Caching**: Query embeddings and results cached for fast repeated queries
5. **Batched Search**: Single API call for all query variations

## ğŸš€ Quick Start

### Prerequisites

1. Install dependencies:
```bash
pip install -r requirements.txt
```

2. Start Qdrant (vector database):
```bash
docker run -p 6333:6333 qdrant/qdrant
```

### Run the System

```bash
# 1. Index documents
python src/scholarrag/index.py

# 2. Run queries
python -c "
import json, sys
sys.path.insert(0, 'src')
from scholarrag.query import run_queries

with open('data/evaluation/requests.json') as f:
    queries = json.load(f)

results = run_queries(queries)

with open('results.json', 'w') as f:
    json.dump(results, f, indent=2)
"
```

## ğŸ“ Repository Structure

```
ScholarRAG-Workbench/
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ baseline/                              # Phase 1 baseline system
â”‚   â”‚   â”œâ”€â”€ query.py                           # Basic semantic search
â”‚   â”‚   â””â”€â”€ index.py                           # Indexing script
â”‚   â”œâ”€â”€ scholarrag/  # Final production system
â”‚   â”‚   â”œâ”€â”€ query.py                           # Query implementation
â”‚   â”‚   â”œâ”€â”€ index.py                           # Indexing script
â”‚   â”‚   â””â”€â”€ README.md                          # System documentation
â”‚   â””â”€â”€ evaluation/                            # Evaluation framework
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ baseline/                              # Baseline system data
â”‚   â”‚   â”œâ”€â”€ corpus.jsonl                       # Indexed corpus
â”‚   â”‚   â”œâ”€â”€ manual_baseline.json               # Ground truth relevance
â”‚   â”‚   â””â”€â”€ results.json                       # Query results
â”‚   â”œâ”€â”€ scholarrag/  # Final system data
â”‚   â”‚   â”œâ”€â”€ cache.json                         # Query cache (555KB)
â”‚   â”‚   â”œâ”€â”€ corpus.json                        # Indexed corpus (6.8MB)
â”‚   â”‚   â”œâ”€â”€ evaluation_report.txt              # Performance metrics
â”‚   â”‚   â”œâ”€â”€ manual_baseline.json               # Ground truth relevance
â”‚   â”‚   â””â”€â”€ results.json                       # Query results
â”‚   â”œâ”€â”€ evaluation/                            # Test queries (30 queries)
â”‚   â”œâ”€â”€ processed/                             # Preprocessed documents
â”‚   â””â”€â”€ raw/                                   # Original PDFs
â”‚
â”œâ”€â”€ final_report.tex                           # LaTeX project report
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

## ğŸ“ Dataset

- **160 documents** across computer science and robotics
  - 100 LLM papers from Kaggle
  - 20 arXiv papers on RAG
  - 10 technical blogs on RAG architectures
  - 30 robotics blogs on LLM integration
- **2,405 sections** after chunking (title, abstract, methods, results, etc.)
- **30 test queries** covering LLM techniques and robotics systems

## ğŸ”¬ Technical Approach

### 1. Query Expansion
Manual expansion of 11 domain-specific terms:
- `rag` â†’ "retrieval-augmented generation"
- `llm` â†’ "large language model"
- `hallucination` â†’ "factual error unfaithful"
- `fine-tuning` â†’ "finetuning adaptation"
- `rlhf` â†’ "reinforcement learning from human feedback"
- And 6 more...

### 2. Section Boosting

**Static Weights (always applied):**
- Title: 1.15Ã—
- Abstract: 1.12Ã—
- Introduction: 1.05Ã—
- Methods/Results: 1.03Ã—

**Query-Aware Weights (conditional):**
- "What techniques..." â†’ boost method sections 1.08Ã—
- "How do models perform..." â†’ boost results/evaluation 1.10Ã—
- "Which papers discuss..." â†’ boost abstract/intro 1.08Ã—

### 3. RRF Fusion
Reciprocal Rank Fusion combines query variations:
```
score = 1 / (k + rank)  where k=60
```

### 4. Caching
- Query variation embeddings cached
- Final retrieval results cached
- Cache keyed by raw query text
- Auto-invalidates on model/collection changes

## ğŸ“ˆ Evaluation Metrics

The system is evaluated on:
- **MRR (Mean Reciprocal Rank)**: Position of first relevant result
- **Precision@k**: Fraction of relevant docs in top-k
- **NDCG@k**: Position-aware relevance scoring
- **Recall@k**: Coverage of relevant documents
- **Latency**: Query processing time

## ğŸ† Why This System?

After testing 27 variants, `scholarrag` emerged as the best because:

1. **Best accuracy**: 15.2% MRR improvement, 28.6% P@1 improvement
2. **Production speed**: 45% faster with zero accuracy loss
3. **Simple & effective**: Rule-based methods beat complex ML approaches
4. **Proven robustness**: Tested on 30 diverse queries

### What Worked âœ…
- Manual domain-specific query expansion
- Section-level semantic search with metadata
- Bias reduction (conservative ArXiv/recency boosts)
- Static + query-aware section boosting (multiplicative)
- Persistent caching

### What Didn't Work âŒ
- BM25 fusion for short sections (-15% to -51% performance)
- General-purpose cross-encoder reranking (-8.4%)
- Template-based HyDE without real LLM (-10.0%)
- Diversity re-ranking (-34.0% catastrophic failure)
- Ensemble of similar methods (-5.7%)

## ğŸ“ Documentation

This repository contains the complete implementation of ScholarRAG, including:
- **Baseline system** (Phase 1): Basic semantic search
- **Final optimized system** (scholarrag): Query-aware boosting with caching
- **Evaluation framework**: Comprehensive metrics and comparison tools
- **Dataset**: 160 academic documents with 30 test queries

For detailed technical documentation, see the comprehensive README and inline code documentation.

## ğŸ¤ Contributors

- Yuhao Wang
- Zhicheng Wang
- Yvonne Wang
- Tang Sheng
- Thanh Trinh

**University of California, San Diego**
CSE 291A - Fall 2024

## ğŸ“„ License

This project is for academic research purposes.

## ğŸ”— Links

- **GitHub Repository**: [Zhicheng-wan/ScholarRAG-Workbench](https://github.com/Zhicheng-wan/ScholarRAG-Workbench)
- **System Documentation**: See `src/scholarrag/README.md`

---

**Note**: This repository contains the baseline and final production system. The complete experimental journey (all 27 variants tested) is available in the git history.
