{
  "query_11": {
    "relevant_docs": [
      "arxiv:2020.aacl-main.88#abstract",
      "arxiv:2211.10438#related-work",
      "arxiv:2304.01373#abstract:part-4",
      "arxiv:2301.00774#introduction:part-1",
      "arxiv:NIPS-1989-optimal-brain-damage-Paper#introduction",
      "arxiv:2211.10438#abstract:part-2"
    ],
    "relevance_scores": {
      "arxiv:2020.aacl-main.88#abstract": 1.0,
      "arxiv:2211.10438#related-work": 0.5,
      "arxiv:2304.01373#abstract:part-4": 0.5,
      "arxiv:2301.00774#introduction:part-1": 1.0,
      "arxiv:NIPS-1989-optimal-brain-damage-Paper#introduction": 1.0,
      "arxiv:2211.10438#abstract:part-2": 0.5
    },
    "notes": "Query asks about architectures for efficient inference of LLMs. The most relevant papers discuss distillation, quantization, and sparse models for reducing inference costs. The 2020.aacl paper discusses efficient multi-task models. The 2301.00774 and NIPS-1989 papers cover neural network pruning and efficiency. Papers with score 1.0 directly address inference efficiency architectures, while 0.5 papers discuss LLM architectures more generally or mention efficiency in passing."
  },
  "query_13": {
    "relevant_docs": [
      "arxiv:2212.03551#introduction:part-15",
      "arxiv:2307.10169#methods:part-1",
      "arxiv:2303.18223#approach:part-19",
      "arxiv:2212.03551#approach:part-1",
      "arxiv:2212.03551#introduction:part-1"
    ],
    "relevance_scores": {
      "arxiv:2212.03551#introduction:part-15": 1.0,
      "arxiv:2307.10169#methods:part-1": 1.0,
      "arxiv:2303.18223#approach:part-19": 1.0,
      "arxiv:2212.03551#approach:part-1": 1.0,
      "arxiv:2212.03551#introduction:part-1": 0.5
    },
    "notes": "Query asks about chain-of-thought prompting. The 2212.03551 paper appears to be directly about CoT prompting (multiple sections highly relevant). The 2307.10169 paper discusses prompt chaining methods. The 2303.18223 paper discusses graph-based reasoning (ToT - Tree of Thoughts). All top papers (1.0) directly propose or analyze step-by-step reasoning methods."
  },
  "query_14": {
    "relevant_docs": [
      "arxiv:20-074#abstract:part-10",
      "arxiv:language_models_are_unsupervised_multitask_learners#abstract:part-3",
      "arxiv:2005.14165#approach:part-2",
      "arxiv:20-074#introduction:part-1",
      "arxiv:2005.14165#abstract:part-1"
    ],
    "relevance_scores": {
      "arxiv:20-074#abstract:part-10": 1.0,
      "arxiv:language_models_are_unsupervised_multitask_learners#abstract:part-3": 1.0,
      "arxiv:2005.14165#approach:part-2": 1.0,
      "arxiv:20-074#introduction:part-1": 0.5,
      "arxiv:2005.14165#abstract:part-1": 0.5
    },
    "notes": "Query asks about few-shot and zero-shot learning methods. The GPT-2 paper (language_models_are_unsupervised_multitask_learners) demonstrates zero-shot transfer. The GPT-3 paper (2005.14165) is the canonical few-shot learning work. The 20-074 paper discusses Natural Language Decathlon and multi-task zero-shot evaluation. Papers with 1.0 directly demonstrate or analyze few/zero-shot capabilities."
  },
  "query_17": {
    "relevant_docs": [
      "arxiv:Adaptive-mixtures-of-local-experts#abstract:part-8",
      "arxiv:1701.06538#abstract:part-6",
      "arxiv:1701.06538#abstract:part-4",
      "arxiv:1701.06538#model:part-1",
      "arxiv:Adaptive-mixtures-of-local-experts#introduction"
    ],
    "relevance_scores": {
      "arxiv:Adaptive-mixtures-of-local-experts#abstract:part-8": 1.0,
      "arxiv:1701.06538#abstract:part-6": 1.0,
      "arxiv:1701.06538#abstract:part-4": 1.0,
      "arxiv:1701.06538#model:part-1": 1.0,
      "arxiv:Adaptive-mixtures-of-local-experts#introduction": 0.5
    },
    "notes": "Query asks about mixture-of-experts architectures for scaling. The Adaptive-mixtures paper is the foundational MoE work. The 1701.06538 paper (Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer) is a key modern MoE paper for scaling. All 1.0 papers directly describe MoE architectures and their scaling properties."
  },
  "query_22": {
    "relevant_docs": [
      "arxiv:2305.11627#methods:part-1",
      "arxiv:2305.19268#introduction:part-1",
      "arxiv:2306.11695#introduction:part-1",
      "arxiv:2305.11627#abstract",
      "arxiv:NIPS-1989-optimal-brain-damage-Paper#model",
      "arxiv:NIPS-1992-second-order-derivatives-for-network-pruning-optimal-brain-surgeon-Paper#introduction"
    ],
    "relevance_scores": {
      "arxiv:2305.11627#methods:part-1": 1.0,
      "arxiv:2305.19268#introduction:part-1": 1.0,
      "arxiv:2306.11695#introduction:part-1": 1.0,
      "arxiv:2305.11627#abstract": 1.0,
      "arxiv:NIPS-1989-optimal-brain-damage-Paper#model": 0.5,
      "arxiv:NIPS-1992-second-order-derivatives-for-network-pruning-optimal-brain-surgeon-Paper#introduction": 0.5
    },
    "notes": "Query asks about model compression and quantization. The 2305.11627 paper is LLM-Pruner. The 2305.19268 and 2306.11695 papers discuss compression for large language models. The NIPS papers are foundational pruning works. Papers with 1.0 directly address LLM compression/quantization, while 0.5 papers cover general neural network compression."
  },
  "query_25": {
    "relevant_docs": [
      "arxiv:2005.14165#abstract:part-1",
      "arxiv:2005.14165#approach:part-2",
      "arxiv:20-074#abstract:part-10",
      "arxiv:2301.00774#model:part-3",
      "arxiv:language_models_are_unsupervised_multitask_learners#abstract:part-3"
    ],
    "relevance_scores": {
      "arxiv:2005.14165#abstract:part-1": 1.0,
      "arxiv:2005.14165#approach:part-2": 1.0,
      "arxiv:20-074#abstract:part-10": 0.5,
      "arxiv:2301.00774#model:part-3": 0.5,
      "arxiv:language_models_are_unsupervised_multitask_learners#abstract:part-3": 1.0
    },
    "notes": "Query asks about techniques enabling in-context learning. GPT-3 (2005.14165) demonstrates and analyzes in-context learning extensively. GPT-2 (language_models_are_unsupervised_multitask_learners) shows early in-context capabilities. Papers with 1.0 directly demonstrate or analyze in-context learning mechanisms."
  },
  "query_29": {
    "relevant_docs": [
      "arxiv:2310.05492#abstract",
      "arxiv:2211.05100#model:part-1",
      "arxiv:2303.18223#result:part-15",
      "arxiv:2307.10169#introduction:part-1",
      "arxiv:2310.05492#introduction:part-1"
    ],
    "relevance_scores": {
      "arxiv:2310.05492#abstract": 1.0,
      "arxiv:2211.05100#model:part-1": 1.0,
      "arxiv:2303.18223#result:part-15": 0.5,
      "arxiv:2307.10169#introduction:part-1": 0.5,
      "arxiv:2310.05492#introduction:part-1": 1.0
    },
    "notes": "Query asks about code generation capabilities. The 2310.05492 paper appears to focus on code generation. The 2211.05100 paper (BLOOM) discusses code capabilities. Papers with 1.0 directly evaluate or demonstrate code generation, while 0.5 papers mention code tasks in broader LLM evaluations."
  },
  "query_35": {
    "relevant_docs": [
      "arxiv:2307.10169#methods:part-3",
      "arxiv:2307.10169#introduction:part-1",
      "arxiv:2305.03047#introduction:part-1",
      "arxiv:2007.14966#abstract",
      "arxiv:2007.14966#model:part-1",
      "arxiv:2305.03047#abstract"
    ],
    "relevance_scores": {
      "arxiv:2307.10169#methods:part-3": 1.0,
      "arxiv:2307.10169#introduction:part-1": 1.0,
      "arxiv:2305.03047#introduction:part-1": 1.0,
      "arxiv:2007.14966#abstract": 1.0,
      "arxiv:2007.14966#model:part-1": 1.0,
      "arxiv:2305.03047#abstract": 0.5
    },
    "notes": "Query asks about retrieval-augmented language models. The 2307.10169 paper discusses RAG systems. The 2007.14966 paper is REALM (Retrieval-Augmented Language Model Pre-Training). The 2305.03047 paper discusses retrieval-augmented generation. Papers with 1.0 directly propose or analyze retrieval-augmented methods."
  },
  "query_36": {
    "relevant_docs": [
      "arxiv:2303.18223#result:part-14",
      "arxiv:2305.14314#evaluation:part-1",
      "arxiv:palm2techreport#evaluation:part-7",
      "arxiv:2303.18223#result:part-20",
      "arxiv:20-074#abstract:part-10"
    ],
    "relevance_scores": {
      "arxiv:2303.18223#result:part-14": 1.0,
      "arxiv:2305.14314#evaluation:part-1": 0.5,
      "arxiv:palm2techreport#evaluation:part-7": 0.5,
      "arxiv:2303.18223#result:part-20": 1.0,
      "arxiv:20-074#abstract:part-10": 0.5
    },
    "notes": "Query asks about performance on commonsense reasoning tasks. The 2303.18223 results sections likely evaluate commonsense benchmarks. The 2305.14314 and PaLM 2 papers include commonsense evaluations. Papers with 1.0 directly report commonsense reasoning performance with specific benchmarks."
  },
  "query_40": {
    "relevant_docs": [
      "arxiv:20-074#abstract:part-10",
      "arxiv:2020.aacl-main.88#abstract",
      "arxiv:2301.00774#introduction:part-1",
      "arxiv:20-074#introduction:part-1",
      "arxiv:2020.aacl-main.88#model:part-1"
    ],
    "relevance_scores": {
      "arxiv:20-074#abstract:part-10": 1.0,
      "arxiv:2020.aacl-main.88#abstract": 1.0,
      "arxiv:2301.00774#introduction:part-1": 0.5,
      "arxiv:20-074#introduction:part-1": 1.0,
      "arxiv:2020.aacl-main.88#model:part-1": 1.0
    },
    "notes": "Query asks about multi-task learning approaches. The 20-074 paper (T5) demonstrates multi-task training. The 2020.aacl paper discusses efficient multi-task language models. Papers with 1.0 directly propose or analyze multi-task learning methods for language models."
  }
}
