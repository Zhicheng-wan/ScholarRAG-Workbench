[
  {
    "id": "query_0",
    "query": "{",
    "results": [
      {
        "rank": 1,
        "doc_id": "arxiv:2307.10169#abstract",
        "score": 0.21734983,
        "payload": {
          "doc_id": "arxiv:2307.10169#abstract",
          "url": "https://arxiv.org/abs/2307.10169",
          "anchor": "#abstract",
          "type": "paper",
          "title": "",
          "section": "Abstract",
          "source": "arxiv_pdf",
          "published": "",
          "tokens": 74,
          "sha256": "701597c31303ec76d4e87fe47bb29b66123513ff54719b31cc2977e9f6d5a7cb",
          "text": "Large Language Models (LLMs) went from non-existent to ubiquitous in the machine learn- ing discourse within a few years. Due to the fast pace of the field, it is difficult to identify the remaining challenges and already fruitful application areas. In this paper, we aim to es- tablish a systematic set of open problems and application successes so that ML researchers can comprehend the field’s current state more quickly and become productive. Contents 1"
        },
        "snippet": "Large Language Models (LLMs) went from non-existent to ubiquitous in the machine learn- ing discourse within a few years. Due to the fast pace of the field, it is difficult to identify the remaining challenges and already fruitful application areas. In this paper, we aim to es- tablish a systematic set of open problems and application successes so that ML researchers can comprehend the field’s cur"
      },
      {
        "rank": 2,
        "doc_id": "arxiv:2503.01840#conclusion",
        "score": 0.21439089,
        "payload": {
          "doc_id": "arxiv:2503.01840#conclusion",
          "url": "https://arxiv.org/abs/2503.01840",
          "anchor": "#conclusion",
          "type": "paper",
          "title": "",
          "section": "Conclusion",
          "source": "arxiv_pdf",
          "published": "",
          "tokens": 112,
          "sha256": "820dbb8a6057e61c1f222891664bf1406978933fb28701489db24612b5d50a91",
          "text": "In this paper, we introduce EAGLE-3. Building upon EAGLE, EAGLE-3 incorporates two key im- provements. First, it removes the feature prediction constraint, instead directly predicting draft tokens through a Training-time test. Second, it replaces the use of the target model’s top-layer features with a fusion of the target model’s lower, middle, and upper-layer features to obtain richer informa- tion. With these improvements, EAGLE-3 contin- ues to benefit from the augmentation of training data, achieving a maximum speedup of 6.5x. Acknowledgement We woud like to thank James Liu, Ke Bao, Yineng Zhang, Lianmin Zheng, Ying Sheng, and many oth- ers in the SGLang team for merging and evaluating EAGLE-3 in the SGLang environment."
        },
        "snippet": "In this paper, we introduce EAGLE-3. Building upon EAGLE, EAGLE-3 incorporates two key im- provements. First, it removes the feature prediction constraint, instead directly predicting draft tokens through a Training-time test. Second, it replaces the use of the target model’s top-layer features with a fusion of the target model’s lower, middle, and upper-layer features to obtain richer informa- ti"
      },
      {
        "rank": 3,
        "doc_id": "arxiv:20-074#abstract:part-7",
        "score": 0.20941927,
        "payload": {
          "doc_id": "arxiv:20-074#abstract:part-7",
          "url": "https://arxiv.org/abs/20-074",
          "anchor": "#abstract:part-7",
          "type": "paper",
          "title": "",
          "section": "Abstract",
          "source": "arxiv_pdf",
          "published": "",
          "tokens": 512,
          "sha256": "2e1b0dc517a088bab2fe0749f7c679cbcfc160f61d26b87410ad326a6e6883c0",
          "text": "for any of the tasks we consider (oﬀensive language, placeholder text, source code, etc.). To address these issues, we used the following heuristics for cleaning up Common Crawl’s web extracted text: • We only retained lines that ended in a terminal punctuation mark (i.e. a period, exclamation mark, question mark, or end quotation mark). • We discarded any page with fewer than 5 sentences and only retained lines that contained at least 3 words. • We removed any page that contained any word on the “List of Dirty, Naughty, Obscene or Otherwise Bad Words”.6 • Many of the scraped pages contained warnings stating that Javascript should be enabled so we removed any line with the word Javascript. • Some pages had placeholder “lorem ipsum” text; we removed any page where the phrase “lorem ipsum” appeared. • Some pages inadvertently contained code. Since the curly bracket “{” appears in many programming languages (such as Javascript, widely used on the web) but not in natural text, we removed any pages that contained a curly bracket. • To deduplicate the data set, we discarded all but one of any three-sentence span occurring more than once in the data set. Additionally, since most of our downstream tasks are focused on English-language text, we used langdetect7 to ﬁlter out any pages that were not classiﬁed as English with a probability of at least 0.99. Our heuristics are inspired by past work on using Common Crawl as a source of data for NLP: For example, Grave et al. (2018) also ﬁlter text using an automatic language detector and discard short lines and Smith et al. (2013); Grave et al. (2018) both perform line-level deduplication. However, we opted to create a new data set because prior data sets use a more limited set of ﬁltering heuristics, are not publicly available, and/or are diﬀerent in scope (e.g. are limited to News data (Zellers et al., 2019; Liu et al., 2019c), comprise only Creative Commons content (Habernal et al., 2016), or are focused on parallel training d"
        },
        "snippet": "for any of the tasks we consider (oﬀensive language, placeholder text, source code, etc.). To address these issues, we used the following heuristics for cleaning up Common Crawl’s web extracted text: • We only retained lines that ended in a terminal punctuation mark (i.e. a period, exclamation mark, question mark, or end quotation mark). • We discarded any page with fewer than 5 sentences and only"
      },
      {
        "rank": 4,
        "doc_id": "arxiv:2307.03172#introduction:part-6",
        "score": 0.20385471,
        "payload": {
          "doc_id": "arxiv:2307.03172#introduction:part-6",
          "url": "https://arxiv.org/abs/2307.03172",
          "anchor": "#introduction:part-6",
          "type": "paper",
          "title": "",
          "section": "Introduction",
          "source": "arxiv_pdf",
          "published": "",
          "tokens": 512,
          "sha256": "bb6dc643919c61d815240d9bc4fc00f95416e914cd2cdb250fd45fcfaba0f979",
          "text": "stan- dard set of prompts for each model (Figure 2). Write a high-quality answer for the given question using only the provided search results (some of which might be irrelevant). Document [1](Title: Asian Americans in science and technology) ... Document [2](Title: List of Nobel laureates in Physics) ... Document [3](Title: Scientist) ... Document [4](Title: Norwegian Americans) ... Document [5](Title: Maria Goeppert Mayer) ... Question: who got the first nobel prize in physics Answer: Input Context Input Context Wilhelm Conrad Röntgen Desired Answer Figure 4: Modulating the input context length of the multi-document question answering example presented in Figure 2. Adding documents that do not contain the answer increases the length of the input context, but does not affect the desired output. Open models. We experiment with MPT-30B- Instruct, which has a maximum context length of 8192 tokens. The model was initially pre-trained on 1 trillion tokens using 2048-token sequences, followed by an additional sequence length adapta- tion pre-training phase on 50 billion tokens using 8192-token sequences. MPT-30B-Instruct uses AL- iBi (Press et al., 2022) to represent positional infor- mation. We also evaluate LongChat-13B (16K) (Li et al., 2023), which extends the LLaMA-13B (Tou- vron et al., 2023a) context window from 2048 to 16384 tokens by using condensed rotary positional embeddings before fine-tuning with 16384-token sequences. Closed models. We use the OpenAI API to ex- periment with GPT-3.5-Turbo and GPT-3.5-Turbo 1st 5th 10th Position of Document with the Answer 50 55 60 65 70 75 Accuracy 10 Total Retrieved Documents (~2K tokens) 1st 5th 10th 15th 20th Position of Document with the Answer 50 55 60 65 70 75 Accuracy 20 Total Retrieved Documents (~4K tokens) 1st 5th 10th 15th 20th 25th 30th Position of Document with the Answer 50 55 60 65 70 75 Accuracy 30 Total Retrieved Documents (~6K tokens) claude-1.3 claude-1.3-100k gpt-3.5-turbo-0613 gpt-3.5-turbo-16k-0613 mp"
        },
        "snippet": "stan- dard set of prompts for each model (Figure 2). Write a high-quality answer for the given question using only the provided search results (some of which might be irrelevant). Document [1](Title: Asian Americans in science and technology) ... Document [2](Title: List of Nobel laureates in Physics) ... Document [3](Title: Scientist) ... Document [4](Title: Norwegian Americans) ... Document [5]("
      },
      {
        "rank": 5,
        "doc_id": "arxiv:2307.10169#introduction:part-1",
        "score": 0.20347506,
        "payload": {
          "doc_id": "arxiv:2307.10169#introduction:part-1",
          "url": "https://arxiv.org/abs/2307.10169",
          "anchor": "#introduction:part-1",
          "type": "paper",
          "title": "",
          "section": "Introduction",
          "source": "arxiv_pdf",
          "published": "",
          "tokens": 512,
          "sha256": "9b5ecbb794b30ca4e3f1f8eea521192f9857fb50923487b21bb7ce09f5872598",
          "text": "Given the quickly growing plethora of LLM re- search papers, we aim to address two questions: (1) Challenges: What problems remain unresolved? and (2) Applications: Where are LLMs currently being applied, and how are the challenges con- straining them? For (1), we group the challenges 1 arXiv:2307.10169v1 [cs.CL] 19 Jul 2023 in Fig. 1 into three broader categories “Design”, “Behavior”, and “Science”. To provide answers for (2), we explore the fields of chatbots, compu- tational biology, computer programming, creative work, knowledge work, law, medicine, reasoning, robotics, and the social sciences. This paper is an opinionated review and assumes familiarity with LLMs and how they work (we refer to more introductory works in Sec. 4). Further, we focus on models trained on text data. We target a technical researcher audience and do not discuss political, philosophical, or moral perspectives on LLMs. 2 Challenges o Challenge This box highlights a challenge. 2.1 Unfathomable Datasets Scaling the amount of pre-training data has been one of the major drivers to equip LLMs with general-purpose capabilities [256]. The size of pre-training datasets quickly outgrew the number of documents most human teams could manually quality-check. Instead, most data collection proce- dures rely on heuristics regarding data sources and filtering. In this section, we explore the adverse conse- quences of these heuristics and the reality that many model practitioners possess only a nebulous under- standing of the data on which their model has been trained. We refer to this issue as follows. o Unfathomable Datasets The size of modern pre-training datasets ren- ders it impractical for any individual to read or conduct quality assessments on the en- compassed documents thoroughly. Near-Duplicates can arise in different forms and have been reported to degrade model per- formance [294, 200, 250]. Near-duplicates are harder to find compared to exact duplicates; fil- tering out of such is a standar"
        },
        "snippet": "Given the quickly growing plethora of LLM re- search papers, we aim to address two questions: (1) Challenges: What problems remain unresolved? and (2) Applications: Where are LLMs currently being applied, and how are the challenges con- straining them? For (1), we group the challenges 1 arXiv:2307.10169v1 [cs.CL] 19 Jul 2023 in Fig. 1 into three broader categories “Design”, “Behavior”, and “Scienc"
      },
      {
        "rank": 6,
        "doc_id": "arxiv:2310.20707#model:part-3",
        "score": 0.20029235,
        "payload": {
          "doc_id": "arxiv:2310.20707#model:part-3",
          "url": "https://arxiv.org/abs/2310.20707",
          "anchor": "#model:part-3",
          "type": "paper",
          "title": "",
          "section": "Model",
          "source": "arxiv_pdf",
          "published": "",
          "tokens": 512,
          "sha256": "37abf3fbf644896537cac1ec4d0f2f81814e4589eb3aa79767ea0afe4e4f9c7c",
          "text": "which typically contain a sequence of white spaces. In The Stack (1,350 total), RedPajama (3,877), and The Pile (7,533), documents typically contain a mix of special characters that denote spacing (e.g., ‘\\n’, or ‘\\t’). In RedPajama, all of the empty strings are from the arXiv subset. The longest document in The Stack is a json file, with 26,298,134 tokens from http://jquery.com/. The longest document in The Pile and RedPajama is the same encyclopedia book called “INTERNATIONAL ENCYCLOPEDIA OF THE SOCIAL & BEHAVIORAL SCIENCES” from the Books3 subset with 28,121,329 tokens. 4.2.2 INTERNET DOMAIN DISTRIBUTION Some corpora contain metadata information about the URL where the documents came from. As such, we employ the Exact Counts functionality, to parse the entire corpus, and extract information from the URLs about the (1) schemas (e.g., http, https), (2) domains (e.g., www.google.com, en. wikipedia.org, etc.), and (3) suffixes (e.g., com, org, de, etc.). We apply these counts on the corpora that contain this information, namely C4, mC4-en, OSCAR, RedPajama, and LAION-2B-en. Starting with the domain analysis, we perform these counts twice: once when each domain is counted per document (yielding documents per domain) and another where each domain is counted per token (yielding tokens per domain). We present the results of three corpora per token in Figure 2 (and the full results in Appendix B.1). First, we note that C4 contains documents from a diverse set of domains, and even the percentage of the most common one, patents.google.com, is less than 0.05%. On the other hand, in the case of LAION-2B-en, cdn.shopify.com is responsible for more than 6% of the documents. Similarly, arxiv.org is responsible for more than 12% of the documents in RedPajama. We showcase the results of the domains for the other corpora, as well as the schemas and suffixes in Appendix B.1. 1We use Unicode text segmentation (Unicode, 2023) as a tokenizer, but we support any tokenizer supported by H"
        },
        "snippet": "which typically contain a sequence of white spaces. In The Stack (1,350 total), RedPajama (3,877), and The Pile (7,533), documents typically contain a mix of special characters that denote spacing (e.g., ‘\\n’, or ‘\\t’). In RedPajama, all of the empty strings are from the arXiv subset. The longest document in The Stack is a json file, with 26,298,134 tokens from http://jquery.com/. The longest docu"
      },
      {
        "rank": 7,
        "doc_id": "arxiv:1910.13461#model:part-2",
        "score": 0.1990167,
        "payload": {
          "doc_id": "arxiv:1910.13461#model:part-2",
          "url": "https://arxiv.org/abs/1910.13461",
          "anchor": "#model:part-2",
          "type": "paper",
          "title": "",
          "section": "Model",
          "source": "arxiv_pdf",
          "published": "",
          "tokens": 512,
          "sha256": "0e2e9dacea7af9ab74ac83de33cce4334f8ee14b6aa2cf2d5a07593e4ad991c2",
          "text": "inﬁlling is inspired by Span- BERT (Joshi et al., 2019), but SpanBERT samples span lengths from a different (clamped geometric) dis- tribution, and replaces each span with a sequence of [MASK] tokens of exactly the same length. Text inﬁll- ing teaches the model to predict how many tokens are missing from a span. Sentence Permutation A document is divided into sentences based on full stops, and these sentences are shufﬂed in a random order. Document Rotation A token is chosen uniformly at random, and the document is rotated so that it begins with that token. This task trains the model to identify the start of the document. 3 Fine-tuning BART The representations produced by BART can be used in several ways for downstream applications. 3.1 Sequence Classiﬁcation Tasks For sequence classiﬁcation tasks, the same input is fed into the encoder and decoder, and the ﬁnal hidden state of the ﬁnal decoder token is fed into new multi-class linear classiﬁer. This approach is related to the CLS token in BERT; however we add the additional token to the end so that representation for the token in the decoder can attend to decoder states from the complete input (Figure 3a). 3.2 Token Classiﬁcation Tasks For token classiﬁcation tasks, such as answer endpoint classiﬁcation for SQuAD, we feed the complete doc- ument into the encoder and decoder, and use the top hidden state of the decoder as a representation for each word. This representation is used to classify the token. 3.3 Sequence Generation Tasks Because BART has an autoregressive decoder, it can be directly ﬁne tuned for sequence generation tasks such as abstractive question answering and summarization. In both of these tasks, information is copied from the input but manipulated, which is closely related to the denoising pre-training objective. Here, the encoder in- put is the input sequence, and the decoder generates outputs autoregressively. 3.4 Machine Translation We also explore using BART to improve machine trans- lation de"
        },
        "snippet": "inﬁlling is inspired by Span- BERT (Joshi et al., 2019), but SpanBERT samples span lengths from a different (clamped geometric) dis- tribution, and replaces each span with a sequence of [MASK] tokens of exactly the same length. Text inﬁll- ing teaches the model to predict how many tokens are missing from a span. Sentence Permutation A document is divided into sentences based on full stops, and the"
      },
      {
        "rank": 8,
        "doc_id": "arxiv:2503.01840#introduction:part-2",
        "score": 0.18829453,
        "payload": {
          "doc_id": "arxiv:2503.01840#introduction:part-2",
          "url": "https://arxiv.org/abs/2503.01840",
          "anchor": "#introduction:part-2",
          "type": "paper",
          "title": "",
          "section": "Introduction",
          "source": "arxiv_pdf",
          "published": "",
          "tokens": 512,
          "sha256": "75460e749b6f86d5369463ee6ce1a0ddfc7666e5d956f60659f203fa9589ff16",
          "text": "head to obtain the draft token. By leveraging the rich information from the target model, EAGLE achieves significantly better accel- eration compared to vanilla speculative sampling. Subsequent methods such as HASS (Zhang et al., 2024) and Falcon (Gao et al., 2024) also adopt the approach of predicting the next feature using the current feature sequence. Recent LLMs have increasingly relied on larger training datasets to achieve better performance. For example, LLaMA series models with sizes of 7B (8B) have used 1T, 2T, and 15T tokens of training data for LLaMA 1 (Touvron et al., 2023a), LLaMA 2 (Touvron et al., 2023b), and LLaMA 3 (Dubey et al., 2024), respectively, resulting in significant improvements across various metrics while keeping the model architecture and inference cost largely unchanged. Similarly, we aim to improve the ac- ceptance rate and acceleration ratio of EAGLE by increasing its training data. Unfortunately, we ob- serve that the gains from additional training data for EAGLE are limited. We analyze the reasons behind this phenomenon. As shown in the upper part of Figure 3, EAGLE performs autoregressive prediction at the feature level, predicting the next feature and then feeding the feature into the LM head of the target model to obtain the token dis- tribution. EAGLE’s loss function consists of two components: the feature prediction loss lfea and the token prediction loss ltoken. Thanks to the feature prediction loss, the draft model trained only at Step 1 can adapt to Step 2 and acquire multi-step predic- tion capabilities. However, with token prediction as the ultimate goal, feature prediction can be seen as an additional constraint, which limits the expres- siveness of the draft model and makes it difficult to benefit from increased data. After removing the feature constraint and expanding the training data (the middle part of Figure 3), as shown in Figure 4, the acceptance rate 0-α of the first draft token improves significantly. However, t"
        },
        "snippet": "head to obtain the draft token. By leveraging the rich information from the target model, EAGLE achieves significantly better accel- eration compared to vanilla speculative sampling. Subsequent methods such as HASS (Zhang et al., 2024) and Falcon (Gao et al., 2024) also adopt the approach of predicting the next feature using the current feature sequence. Recent LLMs have increasingly relied on lar"
      },
      {
        "rank": 9,
        "doc_id": "arxiv:2401.15077#abstract:part-3",
        "score": 0.18701638,
        "payload": {
          "doc_id": "arxiv:2401.15077#abstract:part-3",
          "url": "https://arxiv.org/abs/2401.15077",
          "anchor": "#abstract:part-3",
          "type": "paper",
          "title": "",
          "section": "Abstract",
          "source": "arxiv_pdf",
          "published": "",
          "tokens": 512,
          "sha256": "5fbff5cdd85d625304335250d7be5569082099ed92e5642d7911148dbef0185b",
          "text": "cannot be treated similarly. As depicted in Figure 3, sampling different tokens like “am” or “always” leads to distinct feature sequences, introducing ambiguity into the feature-level autoregression. Medusa faces a simi- lar issue in predicting spaced tokens, where it is uncertain whether the true target for the input fI should be pam or palways. To address this issue, EAGLE inputs the token sequence from one time step ahead, which includes the sampling outcomes, into the draft model. In the example il- lustrated in Figure 3, this involves predicting falways based on fI and talways, and predicting fam based on fI and tam. As illustrated in Figure 4, by addressing the uncertainty, the speedup ratio further increases from 1.9x to 2.8x. We conducted experiments across dialogue, code gener- ation, mathematical reasoning, and instruction following tasks using the MT-bench, HumanEval, GSM8K, and Al- paca datasets, respectively. Tested LLMs included all mod- els from the Vicuna and LLaMA2-Chat series, along with Mixtral 8x7B Instruct. For LLaMA2-Chat 70B, EAGLE 2 EAGLE: Speculative Sampling Requires Rethinking Feature Uncertainty 2 4 6 Epoch 1.5 2.0 2.5 Speedup 2 4 6 Epoch 0.4 0.6 0.8 Acc feature&shifted-token token feature Figure 4: Accuracy and speedup ratio of draft models based on tokens, features and feature&shifted-token at tempera- ture=0, tested on MT-bench with Vicuna 7B as the original LLM. Feature&shifted-token refers to using a feature se- quence and a token sequence advanced by one time step as inputs. achieved a speedup ratio of 2.7x-3.5x, doubled through- put, and theoretically guaranteed the preservation of the generated text’s distribution. Figure 1 and 2 illustrates the performance of EAGLE on the MT-bench (Zheng et al., 2023), a highly realistic benchmark simulating actual ap- plications and real-world scenarios, including multi-turn instructions akin to dialogues with ChatGPT. We have cho- sen to utilize this benchmark as it has been employed by the cur"
        },
        "snippet": "cannot be treated similarly. As depicted in Figure 3, sampling different tokens like “am” or “always” leads to distinct feature sequences, introducing ambiguity into the feature-level autoregression. Medusa faces a simi- lar issue in predicting spaced tokens, where it is uncertain whether the true target for the input fI should be pam or palways. To address this issue, EAGLE inputs the token seque"
      },
      {
        "rank": 10,
        "doc_id": "arxiv:2303.18223#approach:part-5",
        "score": 0.18306527,
        "payload": {
          "doc_id": "arxiv:2303.18223#approach:part-5",
          "url": "https://arxiv.org/abs/2303.18223",
          "anchor": "#approach:part-5",
          "type": "paper",
          "title": "",
          "section": "Approach",
          "source": "arxiv_pdf",
          "published": "",
          "tokens": 512,
          "sha256": "b49d6dfb2d512ab65c38e24697eea2f32a645417c3d3573f7c6c50eaf7a5e07b",
          "text": "Make your prompt as detailed as possible, e.g., “Summarize the article into a short paragraph within 50 words. The major storyline and conclusion should be included, and the unimportant details can be omitted.” 1 ⃝ T2. It is helpful to let the LLM know that it is an expert with a prefixed prompt, e.g., “You are a sophisticated expert in the domain of compute science.” 1 ⃝ T3. Tell the model more what it should do, but not what it should not do. 1 ⃝ T4. To avoid the LLM to generate too long output, you can just use the prompt: “Question: Short Answer: ”. Besides, you can also use the following suffixes, “in a or a few words”, “in one of two sentences”. 1 ⃝ Input Data I1. For the question required factual knowledge, it is useful to first retrieve relevant documents via the search engine, and then concatenate them into the prompt as reference. 4 ⃝ I2. To highlight some important parts in your prompt, please use special marks, e.g., quotation (””) and line break (\\n). You can also use both of them for emphasizing. 4 ⃝ Contextual Information C1. For complex tasks, you can clearly describe the required intermediate steps to accomplish it, e.g., “Please answer the question step by step as: Step 1 - Decompose the question into several sub-questions, · · · ” 2 ⃝ C2. If you want LLMs to provide the score for a text, it is necessary to provide a detailed description about the scoring standard with examples as reference. 1 ⃝ C3. When LLMs generate text according to some context (e.g., making recommendations according to purchase history), instructing them with the explanation about the generated result conditioned on context is helpful to improve the quality of the generated text. 2 ⃝ C4. An approach similar to tree-of-thoughts but can be done in one prompt: e.g., Imagine three different experts are answering this question. All experts will write down one step of their thinking, then share it with the group of experts. Then all experts will go on to the next step, etc. If any e"
        },
        "snippet": "Make your prompt as detailed as possible, e.g., “Summarize the article into a short paragraph within 50 words. The major storyline and conclusion should be included, and the unimportant details can be omitted.” 1 ⃝ T2. It is helpful to let the LLM know that it is an expert with a prefixed prompt, e.g., “You are a sophisticated expert in the domain of compute science.” 1 ⃝ T3. Tell the model more w"
      }
    ]
  },
  {
    "id": "query_1",
    "query": "\"query_1\": \"What techniques are proposed to reduce hallucinations in retrieval-augmented generation systems?\",",
    "results": [
      {
        "rank": 1,
        "doc_id": "arxiv:2307.10169#methods:part-3",
        "score": 0.6028469,
        "payload": {
          "doc_id": "arxiv:2307.10169#methods:part-3",
          "url": "https://arxiv.org/abs/2307.10169",
          "anchor": "#methods:part-3",
          "type": "paper",
          "title": "",
          "section": "methods",
          "source": "arxiv_pdf",
          "published": "",
          "tokens": 512,
          "sha256": "55b48c91dc804d8ec27042d7687a9b582cbcf3cc1abe894328514fbd67b91f14",
          "text": "the effect of prompt’s actuality on the model’s continuation. Further, they mea- sure hallucinations using named-entity- and textual entailment-based metrics. Min et al. [365] notice that evaluating factuality can be difficult because generations can contain a mixture of supported and unsupported information, making binary judg- ments of quality inadequate and human evaluation time-consuming. Hence, they propose a frame- work that first breaks generations into atomic facts and then computes the percentage of atomic facts supported by an external knowledge source like Wikipedia. Zhang et al. [664] detect the behavior of hallucination snowballing, where the LLM over- commits to early mistakes (before outputting the explanation) in its generation, which it otherwise would not make. Retrieval Augmentation One way to mitigate hallucinations is to ground the model’s input on external knowledge, which is often referred to as retrieval augmentation. In other words, we can decouple (i) memory storage of knowledge (e.g., databases or search indexes [290]) and (ii) process- ing of the knowledge to arrive at a more modular architecture. For (i), a retriever module retrieves the top-k relevant documents (or passages) for a query from a large corpus of text. Then, for (ii), we feed these retrieved documents to the language model together with the initial prompt. In theory, using an external data source may also make it eas- ier to interpret which knowledge is retrieved and update it without tediously fine-tuning the model. Shuster et al. [507] demonstrate hallucinations in GPT-3 and study various components of retrieval- augmented architectures to mitigate them. Their best models reduce hallucinated responses by over 60% on average and up to 85% on out-of- distribution data, on which the model has not been trained. We summarize a few popular retrieval augmentation (RA) approaches as follows. 20 Bob's wife is Amy. Bob's daughter is Cindy. Who is Cindy to Amy? P.1) Intrinsic Halluc"
        },
        "snippet": "the effect of prompt’s actuality on the model’s continuation. Further, they mea- sure hallucinations using named-entity- and textual entailment-based metrics. Min et al. [365] notice that evaluating factuality can be difficult because generations can contain a mixture of supported and unsupported information, making binary judg- ments of quality inadequate and human evaluation time-consuming. Henc"
      },
      {
        "rank": 2,
        "doc_id": "arxiv:2303.18223#result:part-11",
        "score": 0.5043917,
        "payload": {
          "doc_id": "arxiv:2303.18223#result:part-11",
          "url": "https://arxiv.org/abs/2303.18223",
          "anchor": "#result:part-11",
          "type": "paper",
          "title": "",
          "section": "Result",
          "source": "arxiv_pdf",
          "published": "",
          "tokens": 512,
          "sha256": "ecbbee2ac365e163ad4eb37f29a1e14368f9ef3ed8f611c61095e213c974142c",
          "text": "as GPT-4 [46]. Furthermore, existing work shows that LLMs encounter difficulties in recognizing the hallucinated con- tent in text [602], even the powerful ChatGPT. Additionally, beyond language tasks, a recent study has shown that large vision-language models (LVLM) also face challenges with hallucination, i.e., generating objects that are not present in the accompanying images [662]. In essence, LLMs seem to “unconsciously” utilize the knowledge in task solving, which still lack an ability to accurately control the use of internal or external knowledge. Hallucinations would mislead LLMs to generate undesired outputs and mostly degrade the performance, leading to potential risks when deploying LLMs in real-world applications. To alleviate this problem, alignment tuning strategies (as discussed in Section 5.2) have been widely utilized in existing work [66], which rely on tuning LLMs on high-quality data or using human feedback. Moreover, the integration of external tools for the provision of credible information sources can help alleviate the hallucination issue [81, 602, 659]. Another line of research work leverages uncertainty estimation of LLMs to identify hallucinations [663, 664]. For instance, considering that hallucinated facts are prone to exhibit inconsistency across different sampled outputs, SelfCheck- GPT [664] detects hallucination by measuring information inconsistency within sampled outputs. For the evaluation of the hallucination problem, a set of hallucination de- tection tasks have been proposed, e.g., TruthfulQA [556] for detecting human falsehood mimicked by models. More recently, HaluEval [602] creates a large-scale LLM-generated and human-annotated hallucinated samples to evaluate the ability of language models to recognize hallucination in both task-specific and general scenarios. Hallucination LLMs are prone to generate untruthful informa- tion that either conflicts with the existing source or cannot be verified by the available source. Even"
        },
        "snippet": "as GPT-4 [46]. Furthermore, existing work shows that LLMs encounter difficulties in recognizing the hallucinated con- tent in text [602], even the powerful ChatGPT. Additionally, beyond language tasks, a recent study has shown that large vision-language models (LVLM) also face challenges with hallucination, i.e., generating objects that are not present in the accompanying images [662]. In essence,"
      },
      {
        "rank": 3,
        "doc_id": "arxiv:2307.10169#methods:part-2",
        "score": 0.49435747,
        "payload": {
          "doc_id": "arxiv:2307.10169#methods:part-2",
          "url": "https://arxiv.org/abs/2307.10169",
          "anchor": "#methods:part-2",
          "type": "paper",
          "title": "",
          "section": "methods",
          "source": "arxiv_pdf",
          "published": "",
          "tokens": 512,
          "sha256": "1e599060c6872c27919f71b74c8516f97fca587c13fc3b52df4d135d1534ed9a",
          "text": "context of LLMs, Sanchez et al. [474] proposes to use classifier-free guidance sampling [204], where the input prompt’s importance is up- weighted throughout the generation of a sequence. Roush [463] proposes five ideas related to modify- ing the prompt throughout the decoding of a single sequence; for example, alternating between two in- put prompts. Such works often borrow ideas from the text-to-image generation community [384, 29]. One idea we have not seen borrowed yet is neg- ative prompting, i.e., including a description of unwanted outputs. According to Neg [4], the first attempts at such an idea resulted in negative out- comes. 2.8 Hallucinations The popularity of services like ChatGPT suggests that LLMs are increasingly used for everyday question-answering. As a result, the factual accu- racy of these models has become more significant 19 than ever. Correct! Does not exist! Wrong authors! Figure 7: Example of Hallucinations with GPT-4, accessed on 02/06/2023. Unfortunately, LLMs often suffer from halluci- nations, which contain inaccurate information that can be hard to detect due to the text’s fluency. Fig. 7 illustrates an example. To distinguish between different types of hallu- cinations, we consider the provided source content of the model, e.g., the prompt, possibly includ- ing examples or retrieved context. Based on such, we can distinguish between intrinsic and extrinsic hallucinations [241]. In the former, the generated text logically contradicts the source content. In the latter, we cannot verify the output correctness from the provided source; the source content does not provide enough information to assess the out- put, which is, therefore, under-determined. Extrin- sic hallucination is not necessarily erroneous, as it merely means the model generated an output that can neither be grounded nor contradicted by the source content. This is still, to some degree, un- desirable as the provided information cannot be verified. We illustrate intrinsic a"
        },
        "snippet": "context of LLMs, Sanchez et al. [474] proposes to use classifier-free guidance sampling [204], where the input prompt’s importance is up- weighted throughout the generation of a sequence. Roush [463] proposes five ideas related to modify- ing the prompt throughout the decoding of a single sequence; for example, alternating between two in- put prompts. Such works often borrow ideas from the text-to"
      },
      {
        "rank": 4,
        "doc_id": "arxiv:2303.18223#model:part-21",
        "score": 0.46240732,
        "payload": {
          "doc_id": "arxiv:2303.18223#model:part-21",
          "url": "https://arxiv.org/abs/2303.18223",
          "anchor": "#model:part-21",
          "type": "paper",
          "title": "",
          "section": "Model",
          "source": "arxiv_pdf",
          "published": "",
          "tokens": 512,
          "sha256": "5561c5e64370e6ddefcaaab4a08866057a4c09183d2d465db7bd641f056032a4",
          "text": "image (e.g., topic and style) and object (e.g., existence and color) or OCR- related tasks, based on existing datasets or new datasets derived from existing images with annotations by humans or LLMs [836–839]. A notable perception issue is hallucina- tion [840], where the model’s responses contain inconsistent content with the image. Among existing studies about hallu- cination in MLLMs [834, 841, 842], object hallucination [843] has received much research attention. To conduct a stable, robust evaluation of object hallucination, POPE [844] pro- poses a polling-based object probing approach for convert- ing object recognition into a series of binary questions, and the results indicate that current MLLMs often struggle with object hallucination. Cognition tasks, on the other hand, re- quire MLLMs to perform reasoning based on image percep- tion. A common reasoning task is visual question answering (VQA), where models answer questions about images that demand reasoning about spatial relationships [845], general knowledge [846], or scene text [847]. To fully explore the capabilities of MLLMs, HallusionBench [848] collects 200 sophisticated visual dependent or supplement questions, on which even the most advanced MLLMs like LLaVA-1.5 [831] and GPT-4V [133] fail to achieve good performance. • Evaluation paradigms. The responses of MLLMs can be evaluated either in a closed-ended or an open-ended manner. Traditional multimodal tasks often rely on a closed- ended evaluation framework, where the assessment is based on the exact match between the model’s response and the ground-truth answer. Examples include the VQA score [849] for visual question answering tasks and the CIDEr [850] score for captioning tasks. However, MLLMs generate re- sponses in an open-ended way, which may contain the correct answer but not exactly match the ground-truth per- fectly. This discrepancy can lead to the underestimation of the model’s performance in previous evaluation paradigms. To address th"
        },
        "snippet": "image (e.g., topic and style) and object (e.g., existence and color) or OCR- related tasks, based on existing datasets or new datasets derived from existing images with annotations by humans or LLMs [836–839]. A notable perception issue is hallucina- tion [840], where the model’s responses contain inconsistent content with the image. Among existing studies about hallu- cination in MLLMs [834, 841,"
      },
      {
        "rank": 5,
        "doc_id": "arxiv:2307.10169#methods:part-5",
        "score": 0.45224428,
        "payload": {
          "doc_id": "arxiv:2307.10169#methods:part-5",
          "url": "https://arxiv.org/abs/2307.10169",
          "anchor": "#methods:part-5",
          "type": "paper",
          "title": "",
          "section": "methods",
          "source": "arxiv_pdf",
          "published": "",
          "tokens": 512,
          "sha256": "a92dced088190bd834edbbd36df0e5a9ed503d9ca042ee2db1317d1cba242bb5",
          "text": "207, 662]. Zhang et al. [662] phrase this challenge as a trade-off between diversity and quality. While this challenge re- mains largely unsolved, several approaches such as diverse beam search [567] and confident decod- ing [552] try reducing the induced hallucinations at the decoding level. Uncertainty-Aware Beam Search [620] is based on the observation that higher predictive un- certainty corresponds to a larger chance of gener- ating hallucinations. Therefore, the method intro- duces a penalty term in the beam search to penalize high predictive uncertainty during decoding. Confident Decoding [552] hypothesize that hal- lucinations of encoder-decoder models originate by not attending to the source when decoding. They propose an attention-based confidence score to measure how strongly a model attends the source and a variational Bayes training procedure to en- sure the model generates high-confidence answers. 2.9 Misaligned Behavior The alignment problem refers to the challenge of ensuring that the LLM’s behavior aligns with hu- man values, objectives, and expectations and that it does not cause unintended or undesirable harms or consequences [466, 158, 196]. Most of the exist- ing alignment work can be categorized into either methods for detecting misaligned behavior (such as model evaluation and auditing, mechanistic inter- pretability, or red teaming) or methods for aligning model behavior (such as pre-training with human feedback, instruction fine-tuning, or RLHF). o Misaligned Behavior LLMs often generate outputs that are not well-aligned with human values or inten- tions, which can have unintended or nega- tive consequences. Pre-Training With Human Feedback Korbak et al. [275] introduce the concept of pre-training with human feedback (PHF) where human feedback is incorporated during the pre-training stage rather than during fine-tuning. The authors compare five different PHF approaches such as filtering [516, 587], conditional training [150, 142, 261], unlik"
        },
        "snippet": "207, 662]. Zhang et al. [662] phrase this challenge as a trade-off between diversity and quality. While this challenge re- mains largely unsolved, several approaches such as diverse beam search [567] and confident decod- ing [552] try reducing the induced hallucinations at the decoding level. Uncertainty-Aware Beam Search [620] is based on the observation that higher predictive un- certainty corre"
      },
      {
        "rank": 6,
        "doc_id": "arxiv:2307.10169#methods:part-4",
        "score": 0.41667998,
        "payload": {
          "doc_id": "arxiv:2307.10169#methods:part-4",
          "url": "https://arxiv.org/abs/2307.10169",
          "anchor": "#methods:part-4",
          "type": "paper",
          "title": "",
          "section": "methods",
          "source": "arxiv_pdf",
          "published": "",
          "tokens": 512,
          "sha256": "6471e30da5645de67b50f2bfbf486134e5eddaa69e7168638d015dc71aa36f3c",
          "text": "about whether the produced answer would contradict. Retrieval-augmented language model pre-training (REALM) [186] inserts retrieved documents into the pre-training examples. While Guu et al. [186] designed REALM for extractive tasks such as question-answering, Lewis et al. [304] propose retrieval-augmented generation (RAG), a language generation framework using retrievers for knowledge-intensive tasks that humans could not solve without access to an external knowledge source. Yogatama et al. [646] propose the adaptive Semiparametric Language Models architecture, which incorporates the current local context, a short-term memory that caches earlier-computed hidden states, and a long-term memory based on a key-value store of (hidden-state, output) tuples. To equip a retrieval-augmented LLM with few-shot abilities that were before only emergent in LLMs with many more parameters, Izacard et al. [236] propose a KL-divergence loss term for retrieval models, resulting in ATLAS. Borgeaud et al. [52] study scaling up retrieval databases up to 2 trillion tokens and achieving comparable performance to GPT-3 on some tasks despite using 25× fewer parameters while highlighting the retrieval model’s ability to copy-paste existing training chunks. Asai et al. [25] introduce a collection of 40 retrieval datasets with instructions and a corresponding model trained on them. However, standard RA does not always solve the hallucinations problem. Fig. 9 illustrates an exam- ple of ChatGPT browsing the web first to retrieve relevant documents before answering the query. While the Bing browsing plugin retrieves two (exis- tent) related papers ([673, 632]), unfortunately, the final response still contains a hallucination: the sec- ond paper’s title and summary are factually inaccu- rate. The second paper’s true title is “Practical and Ethical Challenges of Large Language Models in Education: A Systematic Literature Review” [632]. Another failure mode of RA is illustrated by Khattab et al. [2"
        },
        "snippet": "about whether the produced answer would contradict. Retrieval-augmented language model pre-training (REALM) [186] inserts retrieved documents into the pre-training examples. While Guu et al. [186] designed REALM for extractive tasks such as question-answering, Lewis et al. [304] propose retrieval-augmented generation (RAG), a language generation framework using retrievers for knowledge-intensive t"
      },
      {
        "rank": 7,
        "doc_id": "arxiv:2307.10169#methods:part-39",
        "score": 0.40770155,
        "payload": {
          "doc_id": "arxiv:2307.10169#methods:part-39",
          "url": "https://arxiv.org/abs/2307.10169",
          "anchor": "#methods:part-39",
          "type": "paper",
          "title": "",
          "section": "methods",
          "source": "arxiv_pdf",
          "published": "",
          "tokens": 512,
          "sha256": "1334924b7552a73506d976320a3a1f55cc8308221f3c3f9a956d6f8ca9c7b494",
          "text": "using GPT-4 for real-world clinical ap- plications are raised, including the risks of erro- neous generations and the risks of bias. Tang et al. [538] raise similar issues and find that GPT-3.5 and ChatGPT have issues with factual accuracy and representing the level of certainty during medical summarization. o Hallucination and Bias [538, 388, 511] The safety-critical nature of the medical do- main means the possibility of hallucinations significantly limits the current use cases. Further work is also needed to reduce the risk of LLMs perpetuating existing bias in clinical datasets. Yunxiang et al. [655] fine-tune a LLaMA LLM ChatDoctor (7B parameters) specifically for the task of medical question answering. To specialize the LLaMA model, it is first instruction fine-tuned using the Alpaca dataset [540] and then fine-tuned to the medical domain using a dataset of 100k pa- tient conversations. Similarly to Liévin et al. [320], ChatDoctor is augmented with two external knowl- edge sources (a disease database and Wikipedia) to improve the factual grounding of the model. Instead of using general models with specialized prompting or fine-tuning, Venigalla et al. [565] train a new model PubMedGPT specifically for medical question answering and text generation tasks. PubMedGPT is trained using a combina- tion of PubMed abstracts and full documents from the Pile [165]. Peng et al. [418] also train a new LLM GatorTronGPT (up to 20B parameters) for biomedical question answering and relation extrac- tion using a mixture of clinical and general English text. Whilst these approaches outperformed exist- ing smaller specific purpose models [177, 644] in medical question answering, they currently under- perform the larger general purpose LLMs (GPT- 3.5/4 and MedPaLM 1/2). However, there remains 43 debate over whether larger general or specialized clinical models are the best approach. Looking at models up to GPT-3, Lehman et al. [297] ques- tion the effectiveness of LLM in-context "
        },
        "snippet": "using GPT-4 for real-world clinical ap- plications are raised, including the risks of erro- neous generations and the risks of bias. Tang et al. [538] raise similar issues and find that GPT-3.5 and ChatGPT have issues with factual accuracy and representing the level of certainty during medical summarization. o Hallucination and Bias [538, 388, 511] The safety-critical nature of the medical do- mai"
      },
      {
        "rank": 8,
        "doc_id": "arxiv:2303.18223#result:part-10",
        "score": 0.40756005,
        "payload": {
          "doc_id": "arxiv:2303.18223#result:part-10",
          "url": "https://arxiv.org/abs/2303.18223",
          "anchor": "#result:part-10",
          "type": "paper",
          "title": "",
          "section": "Result",
          "source": "arxiv_pdf",
          "published": "",
          "tokens": 512,
          "sha256": "ec673426a742ca7ff17880d7a3417fe9ab6609f2e2f88133ae0fc82d5e4e175c",
          "text": "RLHF for LLMs. RLHF stands for \"Rights, Limitations, Harms, and Freedoms\" and is a framework for …… models like LLMs (Large Language Models). Bob’s wife is Amy. Bob’s daughter is Cindy. Who is Cindy to Amy? Cindy is Amy’s daughter-in-law. Fig. 17: Examples of intrinsic and extrinsic hallucination for a public LLM (access date: March 19, 2023). As an example of intrinsic hallucination, the LLM gives a conflicting judgment about the relationship between Cindy and Amy, which contradicts the input. For extrinsic hallucination, in this example, the LLM seems to have an incorrect understanding of the meaning of RLHF (reinforcement learning from human feedback), though it can correctly understand the meaning of LLMs (in this context). the retrieved evidence can largely improve the accuracy of the generated answers, even enabling a smaller LLM to outperform 10× larger ones [653, 657]. Further, open-book QA tasks can be also employed to evaluate the recency of knowledge information. Pre-training or retrieving from outdated knowledge resources may cause LLMs to generate incorrect answers for time-sensitive questions [653]. Knowledge Completion. In knowledge completion tasks, LLMs might be (to some extent) considered as a knowledge base [576], which can be leveraged to complete or predict the missing parts of knowledge units (e.g., knowledge triples). Such tasks can probe and evaluate how much and what kind of knowledge LLMs have learned from the pre-training data. Existing knowledge completion tasks can be roughly divided into knowledge graph completion tasks (e.g., FB15k- 237 [572] and WN18RR [574]) and fact completion tasks (e.g., WikiFact [571]), which aim to complete the triples from a knowledge graph and incomplete sentences about specific facts, respectively. Empirical studies have revealed that it is difficult for existing LLMs to accomplish knowledge completion tasks related to specific relation types [520]. As shown in the evaluation results on WikiFact, LLMs perform"
        },
        "snippet": "RLHF for LLMs. RLHF stands for \"Rights, Limitations, Harms, and Freedoms\" and is a framework for …… models like LLMs (Large Language Models). Bob’s wife is Amy. Bob’s daughter is Cindy. Who is Cindy to Amy? Cindy is Amy’s daughter-in-law. Fig. 17: Examples of intrinsic and extrinsic hallucination for a public LLM (access date: March 19, 2023). As an example of intrinsic hallucination, the LLM give"
      },
      {
        "rank": 9,
        "doc_id": "arxiv:2412.10543#introduction:part-1",
        "score": 0.40359986,
        "payload": {
          "doc_id": "arxiv:2412.10543#introduction:part-1",
          "url": "https://arxiv.org/abs/2412.10543",
          "anchor": "#introduction:part-1",
          "type": "paper",
          "title": "",
          "section": "Introduction",
          "source": "arxiv_pdf",
          "published": "",
          "tokens": 512,
          "sha256": "d9142e9627112b69af8cb5f22978e474158cd1ffcbf75d2f7635604ecdc58a0b",
          "text": "Retrieval-augmented generation (RAG) is a popular LLM inference technique that augments an LLM inference query with relevant text chunks, or “context”, retrieved from a large corpus.1 RAG systems, which include retrieval and LLM inference2, have found many use cases in QA tasks, personal assistants, chatbots, and LLM-powered search [10, 62]. While RAG can enhance the quality (accuracy and relevance) of LLM-generated responses [7, 53, 58, 91, 96], RAG queries are inherently slow as they need more compute and mem- ory resources to process the long input context to answer a query [6, 15, 42]. Thus, it is essential to balance high response quality and low response delays in RAG inference systems. 1RAG vs. long-context models is an active field of research, with the industry widely deploying RAG for its task-focused model inference quality and better resource-sharing capabilities [68]. 2Though RAG sometimes refers to the retrieval step, in this work, a RAG system includes both retrieval and LLM inference based on the retrieved texts, and we aim to optimize the whole pipeline. Past research efforts have optimized RAG, regarding ei- ther response quality or response delay, but they fall short in optimizing the quality-delay tradeoffs of RAG. RAG queries have an associated RAG configuration which de- scribes how and how much data to input for the query (more in §2) [72, 79, 83]. One line of prior work focuses on reducing response delay through better query schedul- ing (e.g., GPU allocation and inference batching) for RAG queries [2, 44, 45, 70, 76], without adapting the RAG con- figuration themselves. An alternate line of work focuses on maximizing generation quality by tuning the configurations of RAG queries [32, 77, 83], but this is often done at the cost of longer response delay. The RAG configuration simultaneously affects generation quality and response delay (e.g., retrieving too many chunks for a simple RAG query may unnecessarily inflate delay with- out increasing"
        },
        "snippet": "Retrieval-augmented generation (RAG) is a popular LLM inference technique that augments an LLM inference query with relevant text chunks, or “context”, retrieved from a large corpus.1 RAG systems, which include retrieval and LLM inference2, have found many use cases in QA tasks, personal assistants, chatbots, and LLM-powered search [10, 62]. While RAG can enhance the quality (accuracy and relevanc"
      },
      {
        "rank": 10,
        "doc_id": "arxiv:2303.18223#model:part-35",
        "score": 0.3723066,
        "payload": {
          "doc_id": "arxiv:2303.18223#model:part-35",
          "url": "https://arxiv.org/abs/2303.18223",
          "anchor": "#model:part-35",
          "type": "paper",
          "title": "",
          "section": "Model",
          "source": "arxiv_pdf",
          "published": "",
          "tokens": 512,
          "sha256": "0c423823d9d8e5fea1393c33e763a602863ebcc2d7a513be7877e32d5d3ecc11",
          "text": "the ability of complex reasoning, advanced prompting techniques have been proposed, exemplified by the chain-of-thought (CoT) strategy, which includes the intermediate reasoning steps into prompts. Furthermore, planning is a promising ap- proach for solving complex tasks, which iteratively invokes LLMs by leveraging tool use capacities. Despite these ef- forts, several basic problems related to prompting are still under-explored: why a good prompt can elicit the correct answer but a bad prompt cannot, how to reveal the working principles of advanced prompting methods (e.g., ICL and CoT) and further improve these existing approaches, and how to efficiently find the effective prompts for LLMs on specific tasks. Furthermore, from a practical perspective, it has become a fundamental challenge to reduce the inference cost of LLMs, especially in large-scale deployment. Another popular research direction is retrieval-augmented gener- ation, where retrieved contexts from supporting sources are included into prompts for task solving. It has been shown that retrieval augmentation can extend the knowl- edge boundary and improve the question answering ca- pacity [461], but may suffer from the effectiveness of long context utilization by LLMs [299]. Safety and Alignment. Despite the capacities, LLMs are faced with great safety challenges in practical use. As a fundamental issue of probabilistic modeling nature, LLMs exhibit a tendency to generate hallucinations [638], refer- ring to texts that seem plausible but may be factually incorrect [46]. What is worse, LLMs might be elicited by intentional instructions to produce harmful, biased, or toxic texts for malicious systems, leading to the potential risks of misuse [55, 66]. To have a detailed discussion of the safety issues of LLMs (e.g., privacy, overreliance, disinfor- mation, and influence operations), the readers can refer to the GPT-3/4 technical reports [46, 55]. As the major tech- nical approach to averting these issues, "
        },
        "snippet": "the ability of complex reasoning, advanced prompting techniques have been proposed, exemplified by the chain-of-thought (CoT) strategy, which includes the intermediate reasoning steps into prompts. Furthermore, planning is a promising ap- proach for solving complex tasks, which iteratively invokes LLMs by leveraging tool use capacities. Despite these ef- forts, several basic problems related to pr"
      }
    ]
  },
  {
    "id": "query_2",
    "query": "\"query_2\": \"How do parameter-efficient fine-tuning methods like LoRA or Prefix-Tuning compare in performance on downstream tasks?\",",
    "results": [
      {
        "rank": 1,
        "doc_id": "arxiv:2307.03172#model:part-5",
        "score": 0.54151,
        "payload": {
          "doc_id": "arxiv:2307.03172#model:part-5",
          "url": "https://arxiv.org/abs/2307.03172",
          "anchor": "#model:part-5",
          "type": "paper",
          "title": "",
          "section": "Model",
          "source": "arxiv_pdf",
          "published": "",
          "tokens": 512,
          "sha256": "454464e08e225f2960f65629278be5b84fc3dd7975bf2df823d3f43c6976fd76",
          "text": "performance on the key-value retrieval task—all models achieve near-perfect per- formance on the 75, 140, and 300 key-value pair settings. For example, GPT-3.5-Turbo (16K) with query-aware contextualization achieves perfect per- formance when evaluated with 300 key-value pairs. In contrast, without query-aware contextualiza- tion, the worst-case performance is 45.6% (Fig- ure 7). Despite the significant impact on key- value retrieval performance, query-aware contextu- alization minimally affects performance trends in the multi-document question answering task (Fig- ure 9); it slightly improves performance when the relevant information is located at the very begin- ning of the input context, but slightly decreases performance in other settings. 4.3 Effect of Instruction Fine-Tuning The models we evaluated are all instruction fine- tuned—after their initial pre-training, they undergo supervised fine-tuning on a dataset of instructions and responses. The task specification and/or in- struction is commonly placed at the beginning of the input context in supervised instruction fine- tuning data, which might lead instruction fine- tuned language models to place more weight on the start of the input context. To better understand the potential effects of instruction fine-tuning on how language models use long input contexts, we compare the multi-document question answering performance of MPT-30B-Instruct against its base model (i.e., before instruction fine-tuning) MPT- 30B. We use the same experimental setup as §2. Figure 10 compares the multi-document QA performance of MPT-30B and MPT-30B-Instruct as a function of the position of the relevant in- 1st 5th 10th 15th 20th Position of Document with the Answer 44 46 48 50 52 54 56 Accuracy 20 Total Retrieved Documents (~4K tokens) mpt-30b mpt-30b-instruct Figure 10: Multi-document QA performance of MPT- 30B-Instruct compared against its base model (i.e., be- fore instruction fine-tuning) MPT-30B. Both models have a U-shaped pe"
        },
        "snippet": "performance on the key-value retrieval task—all models achieve near-perfect per- formance on the 75, 140, and 300 key-value pair settings. For example, GPT-3.5-Turbo (16K) with query-aware contextualization achieves perfect per- formance when evaluated with 300 key-value pairs. In contrast, without query-aware contextualiza- tion, the worst-case performance is 45.6% (Fig- ure 7). Despite the signi"
      },
      {
        "rank": 2,
        "doc_id": "arxiv:2412.10543#introduction:part-8",
        "score": 0.54010624,
        "payload": {
          "doc_id": "arxiv:2412.10543#introduction:part-8",
          "url": "https://arxiv.org/abs/2412.10543",
          "anchor": "#introduction:part-8",
          "type": "paper",
          "title": "",
          "section": "Introduction",
          "source": "arxiv_pdf",
          "published": "",
          "tokens": 512,
          "sha256": "04052c26574ccf1c6894d4cd522e43fbaa41c43502576fb7ac3a3e6cdeb53d0c",
          "text": "configuration per-query allows up to 3× delay saving compared to static configurations which are the closest in quality. Every single static configuration choice that achieves comparable delay has at least a 10% quality drop. In spite of the potential benefits, per-query configuration adaptation faces challenges that hinder their real-world adop- tion. Each RAG query comes in plain text with practically no associated RAG configurations. Moreover, the space of configurations grows exponentially with multiple knobs. For example, for a map_reduce configuration, with 30 values for num_chunks and 50 values for intermediate_length leads to 1500 configurations for a query. Exhaustively profiling all configurations per-query and choosing the best is infeasible. Alternatively, if we profile periodically, we lose out on the potential configuration selection for each query, as variance in query profile leads to different quality-delay tradeoffs. Pro- filing cost is also prohibitively expensive as the LLM needs to be run with many synthesis methods, number of chunks etc., which require high GPU usage. Additionally, the delay of profiling can be ∼100× the inference delay due to multiple LLM calls during profiling. Online RAG queries have strin- gent requirements for GPU resource usage and end-to-end delay [70, 76]. This makes it hard to systematically decide what an optimal per-input configuration should be. To truly achieve the benefit of per-query configuration adaptation, we need a smart system to drastically reduce to a useful configuration space, in a fast and cheap manner. 4 METIS: Enabling per-query configuration adaptation for RAG We present METIS, a novel system for serving RAG queries focusing on high generation quality and minimal delay. METIS is a RAG controller (Figure 6) with two main components: Configuration Space Pruning (§ 4.1, 4.2 ) Joint scheduler (§ 4.3) RAG Queries Vector Database GPU Memory Serving LLM RAG Configs Text Chunks Check Resource Status Generate"
        },
        "snippet": "configuration per-query allows up to 3× delay saving compared to static configurations which are the closest in quality. Every single static configuration choice that achieves comparable delay has at least a 10% quality drop. In spite of the potential benefits, per-query configuration adaptation faces challenges that hinder their real-world adop- tion. Each RAG query comes in plain text with pract"
      },
      {
        "rank": 3,
        "doc_id": "arxiv:2306.09782#introduction:part-1",
        "score": 0.52279305,
        "payload": {
          "doc_id": "arxiv:2306.09782#introduction:part-1",
          "url": "https://arxiv.org/abs/2306.09782",
          "anchor": "#introduction:part-1",
          "type": "paper",
          "title": "",
          "section": "INTRODUCTION",
          "source": "arxiv_pdf",
          "published": "",
          "tokens": 512,
          "sha256": "cf8245d166a7b6982d24a01050999e565c3dba6ca3dfa60d3a71eb24354fe6fd",
          "text": "Large Language Models (LLMs) have revolutionized Natural Language Processing (NLP), demon- strating remarkable abilities such as emergence and grokking (Wei et al., 2022), pushing model size to become larger and larger. However, training these models with billions of parameters, such as those with 30B to 175B parameters, raises the bar for NLP research. Tuning LLMs often requires expensive GPU resources, such as 8×80GB devices, making it difficult for small labs and companies to participate in this area of research. Recently, parameter-efficient fine-tuning methods (Ding et al., 2022), such as LoRA (Hu et al., 2022) and Prefix-tuning (Li & Liang, 2021), provide solutions for tuning LLMs with limited resources. However, these methods do not offer a practical solution for full parameter fine-tuning, which has been acknowledged as a more powerful approach than parameter-efficient fine-tuning (Ding et al., 2022; Sun et al., 2023). In this work, we aim to explore techniques for accomplishing full parameter fine-tuning in resource-limited scenarios. We analyze the four aspects of memory usage in LLMs, namely activation, optimizer states, gradient tensor and parameters, and optimize the training process in three folds: 1) We rethink the functional- ity of an optimizer from an algorithmic perspective and find that SGD is a good replacement in terms of fine-tuning full parameter for LLMs. This allows us to remove the entire part of optimizer states since SGD does not store any intermediate state (Sec-3.1). 2) Our proposed optimizer, LOMO as il- lustrated in Figure 1, reduces the memory usage of gradient tensors to O(1), equivalent to the largest gradient tensor’s memory usage (Sec-3.2). 3) To stabilize mix-precision training with LOMO, we integrate gradient normalization, loss scaling, and transition certain computations to full precision during training (Sec-3.3). Our technique results in memory usage that equals the usage of parameters plus activation and the largest gradi"
        },
        "snippet": "Large Language Models (LLMs) have revolutionized Natural Language Processing (NLP), demon- strating remarkable abilities such as emergence and grokking (Wei et al., 2022), pushing model size to become larger and larger. However, training these models with billions of parameters, such as those with 30B to 175B parameters, raises the bar for NLP research. Tuning LLMs often requires expensive GPU res"
      },
      {
        "rank": 4,
        "doc_id": "arxiv:2412.10543#method:part-1",
        "score": 0.4865222,
        "payload": {
          "doc_id": "arxiv:2412.10543#method:part-1",
          "url": "https://arxiv.org/abs/2412.10543",
          "anchor": "#method:part-1",
          "type": "paper",
          "title": "",
          "section": "Method",
          "source": "arxiv_pdf",
          "published": "",
          "tokens": 512,
          "sha256": "43cbd0071ab281e42dc44526049de45b52c2f8e97c3b2d15842d061cdddffb4d",
          "text": "Number of chunks (value) Intermediat e Length (value) Do we need joint reasoning? Joint reasoning: Yes/No Figure 7. METIS RAG configuration selection workflow. • The length of the summarization: If the query is complex and needs a lot of different information, it is often neces- sary to first summarize the relevant information chunks first (to reduce the noise inside these chunks) and then gen- erate the final answer from these summaries. The output for this dimension is a number from 30-200. METIS is not the first to use query profile as a metric for deciding RAG configurations, it extends upon methods like AdaptiveRAG [32] which have used LLM’s to estimate query profile but they only focus on one dimension (the number of chunks to retrieve). In Section 7, we show the impact of each dimension on the overall improvement. Why the query profile could be estimated: Estimating the aforementioned query profile is feasible, not only be- cause of the reasoning power of LLMs3 in analyzing natural language queries, but also because we provide sufficient in- formation to the LLM-based profiler. METIS feeds the profile estimator with not only the query, but also a metadata of the database that contains the background document. The metadata is a short description about the type of con- tent in the database and its data size (chunk_size). Specif- ically, we use a single-line summaries already attached to the original source datasets as the metadata of the dataset. For example, the metadata for the KG RAG Finsec’s database [50] contains quarterly financial reports and questions of Fortune 500 companies with a chunk_size of 1000. It de- scribes the content topics of the chunks with information such as revenue growth indicators, product release informa- tion, sales etc.,. When presented with a query on financials of such a company, the LLM can use the metadata to decide questions like how much to summarize/how much reasoning is required. We give details on the prompt and the intuit"
        },
        "snippet": "Number of chunks (value) Intermediat e Length (value) Do we need joint reasoning? Joint reasoning: Yes/No Figure 7. METIS RAG configuration selection workflow. • The length of the summarization: If the query is complex and needs a lot of different information, it is often neces- sary to first summarize the relevant information chunks first (to reduce the noise inside these chunks) and then gen- er"
      },
      {
        "rank": 5,
        "doc_id": "arxiv:2412.10543#introduction:part-2",
        "score": 0.4825056,
        "payload": {
          "doc_id": "arxiv:2412.10543#introduction:part-2",
          "url": "https://arxiv.org/abs/2412.10543",
          "anchor": "#introduction:part-2",
          "type": "paper",
          "title": "",
          "section": "Introduction",
          "source": "arxiv_pdf",
          "published": "",
          "tokens": 512,
          "sha256": "0a67a5bbdd0544d7e6604402f5d0f8df2c7cfcf1ca854e48afbfcffc7ad3a80c",
          "text": "they focus on either tuning individual knobs or maximizing quality at the cost of higher delay. How- ever, tuning configurations across multiple knobs quickly hits a prohibitive combinatorial space (more in §3) and re- quires optimizations to reduce the search cost. What’s more, the RAG configuration should be tuned jointly with scheduling. Consider two configurations: 𝐴feeds all retrieved text chunks in one LLM input, and 𝐵summarizes first each chunk with an LLM and then feeds the summaries 1 arXiv:2412.10543v2 [cs.LG] 16 Jul 2025 to an LLM input for a final generation. While 𝐴(which calls the LLM once) is seemingly faster than 𝐵(which calls the LLM multiple times), 𝐴could be slower as it requires more GPU memory than 𝐵and thus could be delayed in the sched- uler queue. Without making batching and configuration se- lection jointly, it would be difficult to avoid such pitfalls. Finally, the impact of RAG configurations on quality-delay tradeoffs also varies significantly with queries. For example, to answer “In which country is the Kimbrough Memorial Sta- dium located?”, the RAG may retrieve and analyze one text chunk about the stadium. In contrast, to answer “Compare NVIDIA’s operating cost over the first three quarters of 2024 and identify the highest one”, the RAG may need multiple chunks, each containing the quarter’s operating cost, and process these chunks jointly, instead of reading them separately. The above examples illustrate queries differ in complexity (more in §4), leading to needing different configurations per-query for optimal quality-delay tradeoffs. Empirically, we show that picking RAG configuration per-query achieves 12 −15% higher quality and 2.5 −3× lower delay than using any fixed configuration across all queries in a dataset (§5). Thus, RAG configurations should be adapted on a per-query basis. Yet, existing RAG systems, which hand-pick a static config- uration offline based on a few example queries [1, 21, 39, 85], lose out on quality or res"
        },
        "snippet": "they focus on either tuning individual knobs or maximizing quality at the cost of higher delay. How- ever, tuning configurations across multiple knobs quickly hits a prohibitive combinatorial space (more in §3) and re- quires optimizations to reduce the search cost. What’s more, the RAG configuration should be tuned jointly with scheduling. Consider two configurations: 𝐴feeds all retrieved text ch"
      },
      {
        "rank": 6,
        "doc_id": "arxiv:2412.10543#introduction:part-3",
        "score": 0.4815086,
        "payload": {
          "doc_id": "arxiv:2412.10543#introduction:part-3",
          "url": "https://arxiv.org/abs/2412.10543",
          "anchor": "#introduction:part-3",
          "type": "paper",
          "title": "",
          "section": "Introduction",
          "source": "arxiv_pdf",
          "published": "",
          "tokens": 512,
          "sha256": "47a76b7d8d9debb103d1a73314c681eb320a0d3d23b207a942306207ed5d05f3",
          "text": "query and whether joint reasoning is likely required across these pieces of in- formation (more in §4.1). The intuition of the query profiles is that they can effectively filter out undesirable RAG config- urations. For the earlier query example “Compare NVIDIA’s operating cost over the first three quarters of 2024 and identify the highest one,” the estimated profile would suggest that it involves at least three separate pieces of information, so the number of chunks (one of the configuration knobs) should be at least three. It should be noted that the LLM-based profiler is an extra overhead in METIS, but fortunately, its input only contains the RAG query itself and the metadata of the RAG database, which are orders of magnitude shorter than the long contexts in RAG, so the estimation can be relatively fast, about 1/10 of the delay of the execution of the RAG query. METIS (Ours) AdaptiveRAG (ACL 2024) Parrot (OSDI 2024) vLLM : SOTA LLM Engine Better METIS (Ours) AdaptiveRAG (ACL 2024) Parrot (OSDI 2024) vLLM : SOTA LLM Engine Better Figure 1. Performance of METIS on the KG RAG FinSec [50] dataset compared to the baselines. Full results shown in §7. Using the narrowed configuration space, METIS reduces the RAG response delays by jointly deciding the per-query configuration and query scheduling based on available re- sources (§4.3). The insight is that within the pruned configu- ration space, the scheduler can make optimal configuration decisions without exploring the original, large configuration space and the implications on quality. In short, METIS’s two-level design loosely decouples the problem into (1) pruning configuration space to a smaller yet promising range of configurations, which focuses solely on keeping the accuracy high, and (2) jointly optimizing configuration (within the narrowed range) and scheduling to optimize response delay by choosing configurations which best-fit into the GPU memory. We evaluate METIS across four RAG datasets with diverse query"
        },
        "snippet": "query and whether joint reasoning is likely required across these pieces of in- formation (more in §4.1). The intuition of the query profiles is that they can effectively filter out undesirable RAG config- urations. For the earlier query example “Compare NVIDIA’s operating cost over the first three quarters of 2024 and identify the highest one,” the estimated profile would suggest that it involves"
      },
      {
        "rank": 7,
        "doc_id": "arxiv:2412.10543#method:part-7",
        "score": 0.48124456,
        "payload": {
          "doc_id": "arxiv:2412.10543#method:part-7",
          "url": "https://arxiv.org/abs/2412.10543",
          "anchor": "#method:part-7",
          "type": "paper",
          "title": "",
          "section": "Method",
          "source": "arxiv_pdf",
          "published": "",
          "tokens": 512,
          "sha256": "54d68378c9c3a0744ece7a9ce8eefe998f03513d2303f3b8431c99e59ecc5090",
          "text": "actually good (defined as whether the profile can lead to 10% increase in F1-score or 1.5 −2× reduction in delay or both) or not. Such 90% threshold can be tuned for better performance, and we leave it to future work. From Figure 9, we draw two conclusions. First, over 93% of the quality profiles derived from LLM are of high confidence (i.e., over 90%). Further, for those high-confidence profile, over 96% of them are good profiles, meaning that they can be used to improve quality, or reduce latency, or both. To handle those cases where the quality profile is of con- fidence score lower than 90% , METIS will fall back to the pruned configuration space of recent 10 queries. How to improve the profiler over time? METIS improves the query profiler LLM by profiling extra feedback prompt to this LLM. We generate this feedback prompt by generating the most accurate output, which is obtained by performing inference on the most resource-demanding configuration (the map_reduce configuration with a large number of input chunks (30) and a high value of intermediate length (300 tokens)) and then ask the quality profiler LLM what config- uration it should choose based on the query and the most accurate answer to that query. The key insight is that, the most accurate answer to the query provides the quality profiler LLM extra knowledge and thus can be used to further improve its decision. To control the cost of generating feedback prompts, METIS only generates the feedback prompt once every 30 queries and we only keep the last four feedback prompts. The cost of METIS’ LLM quality profiler: For the profiler LLM, we use a larger LLM as compared to the serving LLM Dataset Task Type Input Output Squad Single hop QA 0.4K - 2K 5-10 Musique Multihop QA 1K - 5K 5-20 KG RAG FinSec Doc Level QA 4K - 10K 20-40 QMSUM Summarization QA 4K - 12K 20-60 Table 1. Input and output length (# of tokens) distributions of the RAG datasets used in our evaluation. (7B parameters). Using this has minimal c"
        },
        "snippet": "actually good (defined as whether the profile can lead to 10% increase in F1-score or 1.5 −2× reduction in delay or both) or not. Such 90% threshold can be tuned for better performance, and we leave it to future work. From Figure 9, we draw two conclusions. First, over 93% of the quality profiles derived from LLM are of high confidence (i.e., over 90%). Further, for those high-confidence profile, "
      },
      {
        "rank": 8,
        "doc_id": "arxiv:2001.08361#related-work:part-11",
        "score": 0.47910845,
        "payload": {
          "doc_id": "arxiv:2001.08361#related-work:part-11",
          "url": "https://arxiv.org/abs/2001.08361",
          "anchor": "#related-work:part-11",
          "type": "paper",
          "title": "",
          "section": "Related Work",
          "source": "arxiv_pdf",
          "published": "",
          "tokens": 512,
          "sha256": "8745e9287cc9d8cf3bb819651b6a2cface388c3111e967ee91591f1ed36f15dd",
          "text": "when varying model and data size, or model and training steps, simultaneously 5 5 Weak dependence of performance on hyperparameter tuning . . . . . . . . . . . . . . . . . 8 6 Comparison of performance trend when including or excluding embeddings . . . . . . . . . 8 7 LSTM and Transformer performance comparison . . . . . . . . . . . . . . . . . . . . . . . 9 8 Generalization to other test datasets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10 9 Universality of overﬁtting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11 10 Critical batch size . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12 11 Performance versus compute budget or number of parameter updates . . . . . . . . . . . . . 14 12 Training on suboptimal models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15 13 Comparison between empirical and adjusted compute trends . . . . . . . . . . . . . . . . . 15 14 Optimal model size and serial number of steps versus compute budget . . . . . . . . . . . . 16 15 Contradiction between compute and data trends . . . . . . . . . . . . . . . . . . . . . . . . 17 16 Early stopping lower bound and training curves for overﬁt models . . . . . . . . . . . . . . 23 17 Universal transformers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24 18 Batch size scans . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24 19 Another look at sample efﬁciency . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24 20 Power-law dependence"
        },
        "snippet": "when varying model and data size, or model and training steps, simultaneously 5 5 Weak dependence of performance on hyperparameter tuning . . . . . . . . . . . . . . . . . 8 6 Comparison of performance trend when including or excluding embeddings . . . . . . . . . 8 7 LSTM and Transformer performance comparison . . . . . . . . . . . . . . . . . . . . . . . 9 8 Generalization to other test datasets"
      },
      {
        "rank": 9,
        "doc_id": "arxiv:2304.01933#method:part-6",
        "score": 0.47549593,
        "payload": {
          "doc_id": "arxiv:2304.01933#method:part-6",
          "url": "https://arxiv.org/abs/2304.01933",
          "anchor": "#method:part-6",
          "type": "paper",
          "title": "",
          "section": "Method",
          "source": "arxiv_pdf",
          "published": "",
          "tokens": 161,
          "sha256": "6274eb13a0a092011484704413daea518a62e8ac4d03a5c91e3a6cc66d8dd808",
          "text": "and LoRA. Prefix-Tuning is excluded from this analysis since its placement is predetermined. For the Series Adapter, we explore its placement op- tions after the multi-head attention layers, MLP layers, or both of them. As for the Parallel Adapter and LoRA, we integrate them into the multi-head attention layers, MLP layers, or both of them, in order to assess their respective performances. The detailed results on each dataset are shown in Ap- pendix A.3. Figure 2 shows the average accuracy on math reasoning datasets. We can observe that for the Series Adapter, the best position is to place it after the MLP layers, achieving an average accu- racy of 59.5% on the math reasoning datasets. As for the Parallel Adapter, when we place it within the MLP layers, it achieves the best performance of 61.7%. Regarding LoRA, we need to insert it simultaneously into both the Multi-head Attention layers and MLP layers to achieve the best perfor- mance of 60%. LLM"
        },
        "snippet": "and LoRA. Prefix-Tuning is excluded from this analysis since its placement is predetermined. For the Series Adapter, we explore its placement op- tions after the multi-head attention layers, MLP layers, or both of them. As for the Parallel Adapter and LoRA, we integrate them into the multi-head attention layers, MLP layers, or both of them, in order to assess their respective performances. The det"
      },
      {
        "rank": 10,
        "doc_id": "arxiv:2106.09685#introduction:part-10",
        "score": 0.47229683,
        "payload": {
          "doc_id": "arxiv:2106.09685#introduction:part-10",
          "url": "https://arxiv.org/abs/2106.09685",
          "anchor": "#introduction:part-10",
          "type": "paper",
          "title": "",
          "section": "INTRODUCTION",
          "source": "arxiv_pdf",
          "published": "",
          "tokens": 512,
          "sha256": "a9f5ce7f4d2f69d2c1e6f5ed34f877fb3a796a24d3f7d2995b19b6b2d3d1c668",
          "text": "model already adapted to MNLI like the ﬁne-tuning baseline. Runs following this more restricted setup from Houlsby et al. (2019) are labeled with †. The result is presented in Table 2 (Top Three Sections). See Section D.1 for details on the hyperparameters used. 5.3 DEBERTA XXL DeBERTa (He et al., 2021) is a more recent variant of BERT that is trained on a much larger scale and performs very competitively on benchmarks such as GLUE (Wang et al., 2019) and Su- perGLUE (Wang et al., 2020). We evaluate if LoRA can still match the performance of a fully ﬁne-tuned DeBERTa XXL (1.5B) on GLUE. The result is presented in Table 2 (Bottom Section). See Section D.2 for details on the hyperparameters used. 5.4 GPT-2 MEDIUM/LARGE Having shown that LoRA can be a competitive alternative to full ﬁne-tuning on NLU, we hope to answer if LoRA still prevails on NLG models, such as GPT-2 medium and large (Radford et al., b). We keep our setup as close as possible to Li & Liang (2021) for a direct comparison. Due to space constraint, we only present our result on E2E NLG Challenge (Table 3) in this section. See Section F.1 for results on WebNLG (Gardent et al., 2017) and DART (Nan et al., 2020). We include a list of the hyperparameters used in Section D.3. 7 Model&Method # Trainable WikiSQL MNLI-m SAMSum Parameters Acc. (%) Acc. (%) R1/R2/RL GPT-3 (FT) 175,255.8M 73.8 89.5 52.0/28.0/44.5 GPT-3 (BitFit) 14.2M 71.3 91.0 51.3/27.4/43.5 GPT-3 (PreEmbed) 3.2M 63.1 88.6 48.3/24.2/40.5 GPT-3 (PreLayer) 20.2M 70.1 89.5 50.8/27.3/43.5 GPT-3 (AdapterH) 7.1M 71.9 89.8 53.0/28.9/44.8 GPT-3 (AdapterH) 40.1M 73.2 91.5 53.2/29.0/45.1 GPT-3 (LoRA) 4.7M 73.4 91.7 53.8/29.8/45.9 GPT-3 (LoRA) 37.7M 74.0 91.6 53.4/29.2/45.1 Table 4: Performance of different adaptation methods on GPT-3 175B. We report the logical form validation accuracy on WikiSQL, validation accuracy on MultiNLI-matched, and Rouge-1/2/L on SAMSum. LoRA performs better than prior approaches, including full ﬁne-tuning. The results on WikiSQL"
        },
        "snippet": "model already adapted to MNLI like the ﬁne-tuning baseline. Runs following this more restricted setup from Houlsby et al. (2019) are labeled with †. The result is presented in Table 2 (Top Three Sections). See Section D.1 for details on the hyperparameters used. 5.3 DEBERTA XXL DeBERTa (He et al., 2021) is a more recent variant of BERT that is trained on a much larger scale and performs very compe"
      }
    ]
  },
  {
    "id": "query_3",
    "query": "\"query_3\": \"Which papers discuss scaling laws or compute-efficiency trade-offs for large language models?\",",
    "results": [
      {
        "rank": 1,
        "doc_id": "arxiv:2001.08361#model:part-2",
        "score": 0.6563528,
        "payload": {
          "doc_id": "arxiv:2001.08361#model:part-2",
          "url": "https://arxiv.org/abs/2001.08361",
          "anchor": "#model:part-2",
          "type": "paper",
          "title": "",
          "section": "model",
          "source": "arxiv_pdf",
          "published": "",
          "tokens": 512,
          "sha256": "45dc8f8f21406ce8aab9f49561f6774f01d0be42c82c5965056479445286edef",
          "text": "python library. In total, the dataset consists of 20.3M documents containing 96 GB of text and 1.62 × 1010 words (as deﬁned by wc). We then apply the reversible tokenizer described in [RWC+19], which yields 2.29 × 1010 tokens. We reserve 6.6 × 108 of these tokens for use as a test set, and we also test on similarly- prepared samples of Books Corpus [ZKZ+15], Common Crawl [Fou], English Wikipedia, and a collection of publicly-available Internet Books. 3 Empirical Results and Basic Power Laws To characterize language model scaling we train a wide variety of models, varying a number of factors including: • Model size (ranging in size from 768 to 1.5 billion non-embedding parameters) • Dataset size (ranging from 22 million to 23 billion tokens) • Shape (including depth, width, attention heads, and feed-forward dimension) • Context length (1024 for most runs, though we also experiment with shorter contexts) • Batch size (219 for most runs, but we also vary it to measure the critical batch size) 7 Feed-Forward Ratio (dff / dmodel) 50M Parameters Aspect Ratio (dmodel / nlayer) Attention Head Dimension (dmodel / nhead) 25M Parameters 10% 8% 6% 4% 2% 0% Loss Increase A wide range of architectures achieve similar performance 22% additional compute compensates for 1% loss increase Figure 5 Performance depends very mildly on model shape when the total number of non-embedding parameters N is held ﬁxed. The loss varies only a few percent over a wide range of shapes. Small differences in parameter counts are compensated for by using the ﬁt to L(N) as a baseline. Aspect ratio in particular can vary by a factor of 40 while only slightly impacting performance; an (nlayer, dmodel) = (6, 4288) reaches a loss within 3% of the (48, 1600) model used in [RWC+19]. 106 107 108 109 Parameters (with embedding) 2 3 4 5 6 7 Test Loss 0 Layer 1 Layer 2 Layers 3 Layers 6 Layers > 6 Layers 103 104 105 106 107 108 109 Parameters (non-embedding) 2 3 4 5 6 7 Test Loss 1 Layer 2 Layers 3 Layers 6 Layer"
        },
        "snippet": "python library. In total, the dataset consists of 20.3M documents containing 96 GB of text and 1.62 × 1010 words (as deﬁned by wc). We then apply the reversible tokenizer described in [RWC+19], which yields 2.29 × 1010 tokens. We reserve 6.6 × 108 of these tokens for use as a test set, and we also test on similarly- prepared samples of Books Corpus [ZKZ+15], Common Crawl [Fou], English Wikipedia, "
      },
      {
        "rank": 2,
        "doc_id": "arxiv:333078981_693988129081760_4712707815225756708_n#introduction:part-1",
        "score": 0.63767475,
        "payload": {
          "doc_id": "arxiv:333078981_693988129081760_4712707815225756708_n#introduction:part-1",
          "url": "https://arxiv.org/abs/333078981_693988129081760_4712707815225756708_n",
          "anchor": "#introduction:part-1",
          "type": "paper",
          "title": "",
          "section": "Introduction",
          "source": "arxiv_pdf",
          "published": "",
          "tokens": 512,
          "sha256": "6bc9e27fb8395777308973b7c5e80a89a50eec5053bdb4d681c59249dbbc3e6b",
          "text": "Large Languages Models (LLMs) trained on mas- sive corpora of texts have shown their ability to per- form new tasks from textual instructions or from a few examples (Brown et al., 2020). These few-shot properties first appeared when scaling models to a sufficient size (Kaplan et al., 2020), resulting in a line of work that focuses on further scaling these models (Chowdhery et al., 2022; Rae et al., 2021). These efforts are based on the assumption that more parameters will lead to better performance. However, recent work from Hoffmann et al. (2022) shows that, for a given compute budget, the best performances are not achieved by the largest mod- els, but by smaller models trained on more data. The objective of the scaling laws from Hoff- mann et al. (2022) is to determine how to best scale the dataset and model sizes for a particular training compute budget. However, this objective disregards the inference budget, which becomes critical when serving a language model at scale. In this context, given a target level of performance, the preferred model is not the fastest to train but the fastest at inference, and although it may be cheaper to train a large model to reach a certain level of ∗Equal contribution. Correspondence: {htouvron, thibautlav,gizacard,egrave,glample}@meta.com 1https://github.com/facebookresearch/llama performance, a smaller one trained longer will ultimately be cheaper at inference. For instance, although Hoffmann et al. (2022) recommends training a 10B model on 200B tokens, we find that the performance of a 7B model continues to improve even after 1T tokens. The focus of this work is to train a series of language models that achieve the best possible per- formance at various inference budgets, by training on more tokens than what is typically used. The resulting models, called LLaMA, ranges from 7B to 65B parameters with competitive performance compared to the best existing LLMs. For instance, LLaMA-13B outperforms GPT-3 on most bench- marks, despi"
        },
        "snippet": "Large Languages Models (LLMs) trained on mas- sive corpora of texts have shown their ability to per- form new tasks from textual instructions or from a few examples (Brown et al., 2020). These few-shot properties first appeared when scaling models to a sufficient size (Kaplan et al., 2020), resulting in a line of work that focuses on further scaling these models (Chowdhery et al., 2022; Rae et al."
      },
      {
        "rank": 3,
        "doc_id": "arxiv:2303.18223#introduction:part-7",
        "score": 0.6346924,
        "payload": {
          "doc_id": "arxiv:2303.18223#introduction:part-7",
          "url": "https://arxiv.org/abs/2303.18223",
          "anchor": "#introduction:part-7",
          "type": "paper",
          "title": "",
          "section": "INTRODUCTION",
          "source": "arxiv_pdf",
          "published": "",
          "tokens": 512,
          "sha256": "5ec0b30619df82bfc4645e155c9c3d37686e7f9e77d7544a0f09727a9110de70",
          "text": "with a model size larger than 10B. compute (orders of magnification). Extensive research has shown that scaling can largely improve the model capacity of LLMs [26, 55, 56]. Thus, it is useful to establish a quantita- tive approach to characterizing the scaling effect. Next, we introduce two representative scaling laws for Transformer language models [30, 34]. • KM scaling law5. In 2020, Kaplan et al. [30] (the OpenAI team) firstly proposed to model the power-law relationship of model performance with respective to three major factors, namely model size (N), dataset size (D), and the amount of training compute (C), for neural language models. Given a compute budget c, they empirically presented three basic formulas for the scaling law6: L(N) = \u0012Nc N \u0013αN , αN ∼0.076, Nc ∼8.8 × 1013 (1) L(D) = \u0012Dc D \u0013αD , αD ∼0.095, Dc ∼5.4 × 1013 L(C) = \u0012Cc C \u0013αC , αC ∼0.050, Cc ∼3.1 × 108 where L(·) denotes the cross entropy loss in nats, and a follow-up study [58] from OpenAI has shown that the language modeling loss can be decomposed into two parts, namely irreducible loss (the entropy of the true data distri- bution) and reducible loss (an estimate of the KL divergence between the true and model distributions). The three laws were derived by fitting the model performance with varied data sizes (22M to 23B tokens), model sizes (768M to 1.5B non-embedding parameters) and training compute, under some assumptions (e.g., the analysis of one factor should be not bottlenecked by the other two factors). They showed that the model performance has a strong dependence rela- tion on the three factors. • Chinchilla scaling law. As another representative study, Hoffmann et al. [34] (the Google DeepMind team) proposed an alternative form for scaling laws to instruct the compute- optimal training for LLMs. They conducted rigorous exper- iments by varying a larger range of model sizes (70M to 16B) and data sizes (5B to 500B tokens), and fitted a similar scaling law yet with different coefficients "
        },
        "snippet": "with a model size larger than 10B. compute (orders of magnification). Extensive research has shown that scaling can largely improve the model capacity of LLMs [26, 55, 56]. Thus, it is useful to establish a quantita- tive approach to characterizing the scaling effect. Next, we introduce two representative scaling laws for Transformer language models [30, 34]. • KM scaling law5. In 2020, Kaplan et "
      },
      {
        "rank": 4,
        "doc_id": "arxiv:2302.13971#introduction:part-1",
        "score": 0.6344932,
        "payload": {
          "doc_id": "arxiv:2302.13971#introduction:part-1",
          "url": "https://arxiv.org/abs/2302.13971",
          "anchor": "#introduction:part-1",
          "type": "paper",
          "title": "",
          "section": "Introduction",
          "source": "arxiv_pdf",
          "published": "",
          "tokens": 512,
          "sha256": "dd75ca6dfc5ec0690ec22a911be8feb9bc39768afa85130adac16dafa4bfa06d",
          "text": "Large Languages Models (LLMs) trained on mas- sive corpora of texts have shown their ability to per- form new tasks from textual instructions or from a few examples (Brown et al., 2020). These few-shot properties ﬁrst appeared when scaling models to a sufﬁcient size (Kaplan et al., 2020), resulting in a line of work that focuses on further scaling these models (Chowdhery et al., 2022; Rae et al., 2021). These efforts are based on the assumption that more parameters will lead to better performance. However, recent work from Hoffmann et al. (2022) shows that, for a given compute budget, the best performances are not achieved by the largest mod- els, but by smaller models trained on more data. The objective of the scaling laws from Hoff- mann et al. (2022) is to determine how to best scale the dataset and model sizes for a particular training compute budget. However, this objective disregards the inference budget, which becomes critical when serving a language model at scale. In this context, given a target level of performance, the preferred model is not the fastest to train but the fastest at inference, and although it may be cheaper to train a large model to reach a certain level of ∗Equal contribution. Correspondence: {htouvron, thibautlav,gizacard,egrave,glample}@meta.com 1https://github.com/facebookresearch/llama performance, a smaller one trained longer will ultimately be cheaper at inference. For instance, although Hoffmann et al. (2022) recommends training a 10B model on 200B tokens, we ﬁnd that the performance of a 7B model continues to improve even after 1T tokens. The focus of this work is to train a series of language models that achieve the best possible per- formance at various inference budgets, by training on more tokens than what is typically used. The resulting models, called LLaMA, ranges from 7B to 65B parameters with competitive performance compared to the best existing LLMs. For instance, LLaMA-13B outperforms GPT-3 on most bench- marks, despite "
        },
        "snippet": "Large Languages Models (LLMs) trained on mas- sive corpora of texts have shown their ability to per- form new tasks from textual instructions or from a few examples (Brown et al., 2020). These few-shot properties ﬁrst appeared when scaling models to a sufﬁcient size (Kaplan et al., 2020), resulting in a line of work that focuses on further scaling these models (Chowdhery et al., 2022; Rae et al., "
      },
      {
        "rank": 5,
        "doc_id": "arxiv:2307.10169#conclusion",
        "score": 0.6190622,
        "payload": {
          "doc_id": "arxiv:2307.10169#conclusion",
          "url": "https://arxiv.org/abs/2307.10169",
          "anchor": "#conclusion",
          "type": "paper",
          "title": "",
          "section": "Conclusion",
          "source": "arxiv_pdf",
          "published": "",
          "tokens": 92,
          "sha256": "39a1cd12045ece9e2c60da5360085ef21c8895252f2f17cf6e3dd63f9c800f19",
          "text": "In this work, we identify several unsolved chal- lenges of large language models, provide an overview of their current applications, and discuss how the former constrain the latter. By highlighting the limitations of existing methods, we hope to fos- ter future research addressing these. We also hope that by providing an overview of the approaches used in different applied areas, we can facilitate the transfer of ideas between domains and target further research. 49 Acknowledgements We thank Abhishek Kumar and Stella Rose Bider- man for fruitful discussions and feedback on the draft."
        },
        "snippet": "In this work, we identify several unsolved chal- lenges of large language models, provide an overview of their current applications, and discuss how the former constrain the latter. By highlighting the limitations of existing methods, we hope to fos- ter future research addressing these. We also hope that by providing an overview of the approaches used in different applied areas, we can facilitate"
      },
      {
        "rank": 6,
        "doc_id": "arxiv:2203.15556#model:part-3",
        "score": 0.61777985,
        "payload": {
          "doc_id": "arxiv:2203.15556#model:part-3",
          "url": "https://arxiv.org/abs/2203.15556",
          "anchor": "#model:part-3",
          "type": "paper",
          "title": "",
          "section": "Model",
          "source": "arxiv_pdf",
          "published": "",
          "tokens": 512,
          "sha256": "1e7f263f046eb439f3fd8ea130fc31b0d3ef8e134dd98cda1db0b5d3c8cb24b1",
          "text": "approach to improving language models is to augment transformers with explicit retrieval mechanisms, as done by Borgeaud et al. (2021); Guu et al. (2020); Lewis et al. (2020). This approach eﬀectively increases the number of data tokens seen during training (by a factor of ∼10 in Borgeaud et al. (2021)). This suggests that the performance of language models may be more dependant on the size of the training data than previously thought. 3. Estimating the optimal parameter/training tokens allocation We present three diﬀerent approaches to answer the question driving our research: Given a ﬁxed FLOPs budget, how should one trade-oﬀmodel size and the number of training tokens? In all three cases we start by training a range of models varying both model size and the number of training tokens and use the resulting training curves to ﬁt an empirical estimator of how they should scale. We assume a power-law relationship between compute and model size as done in Clark et al. (2022); Kaplan et al. (2020), though future work may want to include potential curvature in this relationship for large model sizes. The resulting predictions are similar for all three methods and suggest that parameter count and number of training tokens should be increased equally with more compute3— with proportions reported in Table 2. This is in clear contrast to previous work on this topic and warrants further investigation. 3We compute FLOPs as described in Appendix F. 4 1017 1018 1019 1020 1021 1022 FLOPS 2.0 2.5 3.0 3.5 4.0 4.5 5.0 5.5 6.0 Training loss 75M 250M 500M 1B 2.5B 5B 10B 1017 1019 1021 1023 1025 FLOPs 109 1010 1011 1012 Tokens 1.5T 1017 1019 1021 1023 1025 FLOPs 100M 1.0B 10B 100B 1T Parameters 67B Figure 2 | Training curve envelope. On the left we show all of our diﬀerent runs. We launched a range of model sizes going from 70M to 10B, each for four diﬀerent cosine cycle lengths. From these curves, we extracted the envelope of minimal loss per FLOP, and we used these points to estimate"
        },
        "snippet": "approach to improving language models is to augment transformers with explicit retrieval mechanisms, as done by Borgeaud et al. (2021); Guu et al. (2020); Lewis et al. (2020). This approach eﬀectively increases the number of data tokens seen during training (by a factor of ∼10 in Borgeaud et al. (2021)). This suggests that the performance of language models may be more dependant on the size of the"
      },
      {
        "rank": 7,
        "doc_id": "arxiv:2307.09288#related-work:part-1",
        "score": 0.6152533,
        "payload": {
          "doc_id": "arxiv:2307.09288#related-work:part-1",
          "url": "https://arxiv.org/abs/2307.09288",
          "anchor": "#related-work:part-1",
          "type": "paper",
          "title": "",
          "section": "Related Work",
          "source": "arxiv_pdf",
          "published": "",
          "tokens": 512,
          "sha256": "df4066b09e4801dec2e6b4cea4b2dd3ca3e3c13c07bff1844cb8791dccd15e1b",
          "text": "Large Language Models. The recent years have witnessed a substantial evolution in the field of LLMs. Following the scaling laws of Kaplan et al. (2020), several Large Language Models with more than 100B parameters have been proposed, from GPT-3 (Brown et al., 2020) to Gopher (Rae et al., 2022) or specialized models, e.g. Galactica, for science(Taylor et al., 2022). With 70B parameters, Chinchilla (Hoffmann et al., 2022) redefined those scaling laws towards the number of tokens rather than model weights. Notable in this progression is the rise of Llama, recognized for its focus on computational efficiency during inference (Touvron et al., 2023). A parallel discourse has unfolded around the dynamics of open-source versus closed- source models. Open-source releases like BLOOM (Scao et al., 2022), OPT(Zhang et al., 2022), and Falcon (Penedo et al., 2023) have risen to challenge their closed-source counterparts like GPT-3 and Chinchilla. §§https://ai.meta.com/llama 35 Yet, when it comes to the \"production-ready\" LLMs such as ChatGPT, Bard, and Claude, there’s a marked distinction in performance and usability. These models rely on intricate tuning techniques to align with human preferences (Gudibande et al., 2023), a process that is still being explored and refined within the open-source community. Attempts to close this gap have emerged, with distillation-based models such as Vicuna (Chiang et al., 2023) and Alpaca (Taori et al., 2023) adopting a unique approach to training with synthetic instructions (Honovich et al., 2022; Wang et al., 2022). However, while these models show promise, they still fall short of the bar set by their closed-source counterparts. Instruction Tuning. Wei et al. (2021) obtained zero-shot performance on unseen tasks by fine-tuning LLMs on numerous datasets. Chung et al. (2022) and Longpre et al. (2023) investigate the impact of instruction tuning as a function of number of tasks, model size, prompt settings, etc. Prompts used for instruction tun"
        },
        "snippet": "Large Language Models. The recent years have witnessed a substantial evolution in the field of LLMs. Following the scaling laws of Kaplan et al. (2020), several Large Language Models with more than 100B parameters have been proposed, from GPT-3 (Brown et al., 2020) to Gopher (Rae et al., 2022) or specialized models, e.g. Galactica, for science(Taylor et al., 2022). With 70B parameters, Chinchilla "
      },
      {
        "rank": 8,
        "doc_id": "arxiv:10000000_662098952474184_2584067087619170692_n#related-work:part-1",
        "score": 0.6152533,
        "payload": {
          "doc_id": "arxiv:10000000_662098952474184_2584067087619170692_n#related-work:part-1",
          "url": "https://arxiv.org/abs/10000000_662098952474184_2584067087619170692_n",
          "anchor": "#related-work:part-1",
          "type": "paper",
          "title": "",
          "section": "Related Work",
          "source": "arxiv_pdf",
          "published": "",
          "tokens": 512,
          "sha256": "df4066b09e4801dec2e6b4cea4b2dd3ca3e3c13c07bff1844cb8791dccd15e1b",
          "text": "Large Language Models. The recent years have witnessed a substantial evolution in the field of LLMs. Following the scaling laws of Kaplan et al. (2020), several Large Language Models with more than 100B parameters have been proposed, from GPT-3 (Brown et al., 2020) to Gopher (Rae et al., 2022) or specialized models, e.g. Galactica, for science(Taylor et al., 2022). With 70B parameters, Chinchilla (Hoffmann et al., 2022) redefined those scaling laws towards the number of tokens rather than model weights. Notable in this progression is the rise of Llama, recognized for its focus on computational efficiency during inference (Touvron et al., 2023). A parallel discourse has unfolded around the dynamics of open-source versus closed- source models. Open-source releases like BLOOM (Scao et al., 2022), OPT(Zhang et al., 2022), and Falcon (Penedo et al., 2023) have risen to challenge their closed-source counterparts like GPT-3 and Chinchilla. §§https://ai.meta.com/llama 35 Yet, when it comes to the \"production-ready\" LLMs such as ChatGPT, Bard, and Claude, there’s a marked distinction in performance and usability. These models rely on intricate tuning techniques to align with human preferences (Gudibande et al., 2023), a process that is still being explored and refined within the open-source community. Attempts to close this gap have emerged, with distillation-based models such as Vicuna (Chiang et al., 2023) and Alpaca (Taori et al., 2023) adopting a unique approach to training with synthetic instructions (Honovich et al., 2022; Wang et al., 2022). However, while these models show promise, they still fall short of the bar set by their closed-source counterparts. Instruction Tuning. Wei et al. (2021) obtained zero-shot performance on unseen tasks by fine-tuning LLMs on numerous datasets. Chung et al. (2022) and Longpre et al. (2023) investigate the impact of instruction tuning as a function of number of tasks, model size, prompt settings, etc. Prompts used for instruction tun"
        },
        "snippet": "Large Language Models. The recent years have witnessed a substantial evolution in the field of LLMs. Following the scaling laws of Kaplan et al. (2020), several Large Language Models with more than 100B parameters have been proposed, from GPT-3 (Brown et al., 2020) to Gopher (Rae et al., 2022) or specialized models, e.g. Galactica, for science(Taylor et al., 2022). With 70B parameters, Chinchilla "
      },
      {
        "rank": 9,
        "doc_id": "arxiv:2204.02311#related-work:part-2",
        "score": 0.5970237,
        "payload": {
          "doc_id": "arxiv:2204.02311#related-work:part-2",
          "url": "https://arxiv.org/abs/2204.02311",
          "anchor": "#related-work:part-2",
          "type": "paper",
          "title": "",
          "section": "Related Work",
          "source": "arxiv_pdf",
          "published": "",
          "tokens": 512,
          "sha256": "0864714e17442b1be1c3af006fcd031bcd83191ecfead4d3dfb074bbcb82509e",
          "text": "to increase of the scale of models, while limiting communication overheads (Rajbhandari et al., 2020; Lepikhin et al., 2020; Li et al., 2020; Rasley et al., 2020; Rajbhandari et al., 2021; Ren et al., 2021; Narayanan et al., 2021a). PaLM uses a blend of data and model-parallelism enabled through the Pathways infrastructure (Barham et al., 2022). Architectural variants have been proposed to help scale models more eﬃciently. One area is retrieval models that aim to drastically reduce model sizes by embedding large amounts of text the model can have access to later (Guu et al., 2020; Borgeaud et al., 2021). Model sparsity like Mixture-of-Experts allows for scaling model sizes by allowing diﬀerent examples to use diﬀerent subsets of parameters (Shazeer et al., 2017; Lepikhin et al., 2020; Fedus et al., 2021; Du et al., 2021; Zoph et al., 2022). Sparsity in the sequence length is an area that allows for training eﬃciently with extremely long sequences (Zaheer et al., 2020; Tay et al., 2020; 47 Choromanski et al., 2020; Kitaev et al., 2020; Roy et al., 2020; Child et al., 2019; Gale et al., 2019). Future work could combine improvements from these lines of research into future versions of Pathways language models. 13 Open Questions in Scaling In our introductory section, we describe the four main axes which have led to signiﬁcant quality improvements of large LMs for few-shot learning. These can be summarized as: (1) model depth and width, (2) number of tokens trained, (3) training corpus quality, (4) increased model capacity without increased compute (i.e., sparse models). Throughout the rest of the paper, we primarily focus on exploring factor (1), although it is clear from this work and prior work that this is not the only important factor. For instance, PaLM 62B outperforms GPT-3 and other large LMs on a signiﬁcant number of tasks, despite having a much lower total training FLOP count. This would hint at (3) being a major factor, although we do not perform the necessar"
        },
        "snippet": "to increase of the scale of models, while limiting communication overheads (Rajbhandari et al., 2020; Lepikhin et al., 2020; Li et al., 2020; Rasley et al., 2020; Rajbhandari et al., 2021; Ren et al., 2021; Narayanan et al., 2021a). PaLM uses a blend of data and model-parallelism enabled through the Pathways infrastructure (Barham et al., 2022). Architectural variants have been proposed to help sc"
      },
      {
        "rank": 10,
        "doc_id": "arxiv:2203.15556#model:part-2",
        "score": 0.5952963,
        "payload": {
          "doc_id": "arxiv:2203.15556#model:part-2",
          "url": "https://arxiv.org/abs/2203.15556",
          "anchor": "#model:part-2",
          "type": "paper",
          "title": "",
          "section": "Model",
          "source": "arxiv_pdf",
          "published": "",
          "tokens": 512,
          "sha256": "fd894c61f15db609aba431fd3a394beb3fb5b13b4aa2bc53c244a00b434bdd67",
          "text": "the conclusion that model size should increase faster than training data size as compute budget increases. In contrast, our analysis predicts that both quantities should scale at roughly the same rate. Secondly, we include models with up to 16B parameters, as we observe that there is slight curvature in the FLOP-loss frontier (see Appendix E)—in fact, the majority of the models used in our analysis have more than 500 million parameters, in contrast the majority of runs in Kaplan et al. (2020) are signiﬁcantly smaller—many being less than 100M parameters. Recently, Clark et al. (2022) speciﬁcally looked in to the scaling properties of Mixture of Expert 3 language models, showing that the scaling with number of experts diminishes as the model size increases—their approach models the loss as a function of two variables: the model size and the number of experts. However, the analysis is done with a ﬁxed number of training tokens, as in Kaplan et al. (2020), potentially underestimating the improvements of branching. Estimating hyperparameters for large models. The model size and the number of training tokens are not the only two parameters to chose when selecting a language model and a procedure to train it. Other important factors include learning rate, learning rate schedule, batch size, optimiser, and width-to-depth ratio. In this work, we focus on model size and the number of training steps, and we rely on existing work and provided experimental heuristics to determine the other necessary hyperparameters. Yang et al. (2021) investigates how to choose a variety of these parameters for training an autoregressive transformer, including the learning rate and batch size. McCandlish et al. (2018) ﬁnds only a weak dependence between optimal batch size and model size. Shallue et al. (2018); Zhang et al. (2019) suggest that using larger batch-sizes than those we use is possible. Levine et al. (2020) investigates the optimal depth-to-width ratio for a variety of standard model"
        },
        "snippet": "the conclusion that model size should increase faster than training data size as compute budget increases. In contrast, our analysis predicts that both quantities should scale at roughly the same rate. Secondly, we include models with up to 16B parameters, as we observe that there is slight curvature in the FLOP-loss frontier (see Appendix E)—in fact, the majority of the models used in our analysi"
      }
    ]
  },
  {
    "id": "query_4",
    "query": "\"query_4\": \"What evaluation benchmarks are used to measure reasoning or mathematical ability in LLMs?\",",
    "results": [
      {
        "rank": 1,
        "doc_id": "arxiv:2303.18223#result:part-20",
        "score": 0.70184106,
        "payload": {
          "doc_id": "arxiv:2303.18223#result:part-20",
          "url": "https://arxiv.org/abs/2303.18223",
          "anchor": "#result:part-20",
          "type": "paper",
          "title": "",
          "section": "Result",
          "source": "arxiv_pdf",
          "published": "",
          "tokens": 320,
          "sha256": "b03f5884e1e00d6f95293af7c94b67f5b8df2365c76342e3039d1ba22c6fa5e1",
          "text": "there remains ample room for further enhancements in the overall abilities of LLMs, particularly for publicly accessible models. The above benchmarks cover a variety of mainstream evaluation tasks and real-world human exam questions for the evaluation of LLMs. Also, there are several benchmarks that focus on evaluating specific abilities of LLMs, such as TyDiQA [735] for multilingual knowledge utilization and MGSM [524] for multilingual mathematical reasoning. To conduct the evaluation, one can select suitable bench- marks according to specific goals. In addition, there are also several open-source evaluation frameworks for researchers to evaluate LLMs on existing benchmarks or extend new tasks for customized evaluations, such as Language Model Evaluation Harness [736] and OpenAI Evals [46]. Fur- ther, some researchers also construct continuously updated leaderboards by aggregating representative benchmarks, to compare the performance of existing LLMs, such as Open LLM Leaderboard [707]. The above benchmarks and leader- boards provide important references to demonstrate the ba- sic and advanced abilities of LLMs. We will give more deep discussions on pros and cons on evaluation approaches in Section 7.3.2. 7.3.2 Evaluation Approaches After introducing existing benchmarks, in this part, we will review existing evaluation approaches for assessing the performance of LLMs. To organize our discussion, we categorize LLMs into three different types: base LLMs (pre- trained model checkpoints), fine-tuned LLMs (instruction or alignment fine-tuned model checkpoints), and specialized LLMs (adapted model checkpoints for some specific task or domain). Here, we keep both fine-tuned LLMs and specialized LLMs, to distinguish the different purposes of LLMs: general or specific task solvers. To evaluate the three types of LLMs, we can test the LLM’s performance related to different abilities (e.g., basic or advanced abilities as 67 TABLE 15: A category of existing evaluation work. “G"
        },
        "snippet": "there remains ample room for further enhancements in the overall abilities of LLMs, particularly for publicly accessible models. The above benchmarks cover a variety of mainstream evaluation tasks and real-world human exam questions for the evaluation of LLMs. Also, there are several benchmarks that focus on evaluating specific abilities of LLMs, such as TyDiQA [735] for multilingual knowledge uti"
      },
      {
        "rank": 2,
        "doc_id": "arxiv:2303.18223#result:part-18",
        "score": 0.5968067,
        "payload": {
          "doc_id": "arxiv:2303.18223#result:part-18",
          "url": "https://arxiv.org/abs/2303.18223",
          "anchor": "#result:part-18",
          "type": "paper",
          "title": "",
          "section": "Result",
          "source": "arxiv_pdf",
          "published": "",
          "tokens": 512,
          "sha256": "ea647179d3706838742ed9c87cfdb67ac86f9cdc108abcf2f149dad44c5a3fa4",
          "text": "creating a prosperous ecosystem of applications based on LLMs. To examine the ability of tool manipulation, existing work mostly adopts complex reasoning tasks for evaluation, such as mathematical problem solving (e.g., GSM8k [184] and SVAMP [592]) or knowledge question answering (e.g., TruthfulQA [556]), where the successful utilization of tools is very important for enhancing the required skills that LLMs are incapable in (e.g., numerical calculation). In this way, the evaluated performance on these tasks can reflect the ability of LLMs in tool manipulation. To teach LLMs to utilize tools, existing studies add exemplars using tools in context to elicit LLMs [443], or fine-tune LLMs on simulated data about tool utilization [80, 693]. It has been found that with the help of tools, LLMs become more capable of handling the issues that they are not good at, e.g., equation calculation and answering timely questions [80, 448]. However, as the number of available tools increases, the limited context length of LLMs may pose challenges in describing and demonstrating extensive tool APIs. To address this issue, existing work retrieves the usage of relevant tools, or en- coding tool information as tokens within the embedding space [702–704]. 66 In addition to existing tools developed by humans, LLMs possess the capability to make their own tools for specific tasks autonomously [705]. This enables the models to independently explore and manipulate these self-created tools, thereby expanding their potential for autonomous exploration in solving a wide range of real-world tasks. Summary. The above three abilities are of great value to the practical performance of LLMs: conforming to human values and preferences (human alignment), acting properly in real-world scenarios (interaction with the external envi- ronment), and expanding the ability scope (tool manipu- lation). In addition to the above three advanced abilities, LLMs might also show other abilities that are specially rela"
        },
        "snippet": "creating a prosperous ecosystem of applications based on LLMs. To examine the ability of tool manipulation, existing work mostly adopts complex reasoning tasks for evaluation, such as mathematical problem solving (e.g., GSM8k [184] and SVAMP [592]) or knowledge question answering (e.g., TruthfulQA [556]), where the successful utilization of tools is very important for enhancing the required skills"
      },
      {
        "rank": 3,
        "doc_id": "arxiv:2303.18223#result:part-14",
        "score": 0.5957954,
        "payload": {
          "doc_id": "arxiv:2303.18223#result:part-14",
          "url": "https://arxiv.org/abs/2303.18223",
          "anchor": "#result:part-14",
          "type": "paper",
          "title": "",
          "section": "Result",
          "source": "arxiv_pdf",
          "published": "",
          "tokens": 512,
          "sha256": "1070c84b24396611b139a16c08381550f7f8387c163107ed6ec6a654406df24e",
          "text": "the knowledge graphs in KBQA. 64 ing and automated theorem proving. For math problem solving tasks, SVAMP [592], GSM8k [184] and MATH [364] datasets are commonly used for evaluation, where LLMs need to generate accurate concrete numbers or equations to answer the mathematical problem. As these tasks also require multi-step reasoning, the CoT prompting strategy has been widely adopted for LLMs to improve the reasoning performance [33]. As another practical strategy, continu- ally pre-training LLMs on large-scale mathematical corpora can largely boost their performance on mathematical rea- soning tasks [35, 203, 678]. Further, since math problems in different languages share the same mathematical logic, researchers also propose a multilingual math word problem benchmark [524] to evaluate the multilingual mathematical reasoning capacity of LLMs. As another challenging task, automated theorem proving (ATP) [598, 600, 679] requires the reasoning model to strictly follow the reasoning logic and mathematical skills. To evaluate the performance on this task, PISA [599] and miniF2F [600] are two typical ATP datasets with the proof success rate as the evaluation metric. As a typical approach, existing work on ATP utilizes LLMs to aid the search for proofs using an interactive theorem prover (ITP), such as Lean, Metamath, and Isabelle [680– 682]. A major limitation of ATP research is the lack of related corpora in formal language. To tackle it, several studies utilize LLMs to convert informal statements into formal proofs for augmenting new data [683] or generate drafts and proof sketches to reduce the search space of the proofs [684]. Major Issues. In spite of the advancements, LLMs still have several limitations in solving complex reasoning tasks. • Reasoning inconsistency. With improved reasoning strategies (e.g., CoT prompting), LLMs can solve some com- plex reasoning tasks, by performing step-by-step reasoning based on the supporting logic and evidence. Despite the effect"
        },
        "snippet": "the knowledge graphs in KBQA. 64 ing and automated theorem proving. For math problem solving tasks, SVAMP [592], GSM8k [184] and MATH [364] datasets are commonly used for evaluation, where LLMs need to generate accurate concrete numbers or equations to answer the mathematical problem. As these tasks also require multi-step reasoning, the CoT prompting strategy has been widely adopted for LLMs to i"
      },
      {
        "rank": 4,
        "doc_id": "arxiv:2303.18223#result:part-9",
        "score": 0.56541646,
        "payload": {
          "doc_id": "arxiv:2303.18223#result:part-9",
          "url": "https://arxiv.org/abs/2303.18223",
          "anchor": "#result:part-9",
          "type": "paper",
          "title": "",
          "section": "Result",
          "source": "arxiv_pdf",
          "published": "",
          "tokens": 512,
          "sha256": "8481ce6e8cffbaa7e8a066a1ae2b0df3ca209f476daaad7078d5dbeee185f661",
          "text": "into three types, namely closed-book QA, open-book QA43, and knowledge completion. Closed-Book QA. Closed-book QA tasks [652] test the acquired factual knowledge of LLMs from the pre-training corpus, where LLMs should answer the question only based on the given context without using external resources. For evaluating this ability, there are several datasets that can be leveraged, including Natural Questions [554], Web Ques- tions [557], and TriviaQA [558], where the accuracy metric is widely adopted. Empirical results have revealed that LLMs can perform well in this setting and even match the per- formance of state-of-the-art open-domain QA systems [56]. Also, the performance of LLMs on closed-book QA tasks shows a scaling law pattern in terms of both model size and data size: scaling the parameters and training tokens can increase the capacity of LLMs and help them learn (or memorize) more knowledge from the pre-training data [56]. Further, under a similar parameter scale, LLMs with more pre-training data relevant to the evaluated tasks would achieve better performance [81]. Also, the closed-book QA setting provides a testbed for probing the accuracy of the factual knowledge encoded by LLMs. However, as shown in existing work [55], LLMs might perform less well on QA tasks relying on fine-grained knowledge, even when it exists in the pre-training data. Open-Book QA. Unlike closed-book QA, in open-book QA tasks, LLMs can extract useful evidence from the external knowledge base or document collections, and then answer the question based on the extracted evidence [653–656]. Typ- ical open-book QA datasets (e.g., Natural Questions [554], OpenBookQA [566], and SQuAD [569]) have overlap with closed-book QA datasets, but they incorporate external data sources, e.g., Wikipedia. The metrics of accuracy and F1 score are widely used in open-book QA tasks for evalua- tion. To select relevant knowledge from external resources, LLMs are often paired with a text retriever (or even"
        },
        "snippet": "into three types, namely closed-book QA, open-book QA43, and knowledge completion. Closed-Book QA. Closed-book QA tasks [652] test the acquired factual knowledge of LLMs from the pre-training corpus, where LLMs should answer the question only based on the given context without using external resources. For evaluating this ability, there are several datasets that can be leveraged, including Natural"
      },
      {
        "rank": 5,
        "doc_id": "arxiv:palm2techreport#evaluation:part-7",
        "score": 0.5647193,
        "payload": {
          "doc_id": "arxiv:palm2techreport#evaluation:part-7",
          "url": "https://arxiv.org/abs/palm2techreport",
          "anchor": "#evaluation:part-7",
          "type": "paper",
          "title": "",
          "section": "Evaluation",
          "source": "arxiv_pdf",
          "published": "",
          "tokens": 512,
          "sha256": "6cae19862bfe4b547191262804d0d90d811416ddd026dae51193c95316220157",
          "text": "75.9 / 85.8 Mathematical reasoning LLMs have struggled on tasks that require quantitative reasoning, such as high-school and college-level problems in mathematics, science, and engineering (Hendrycks et al., 2021; Cobbe et al., 2021). Recently, Minerva (Lewkowycz et al., 2022) achieved signiﬁcant gains on quantitative reasoning tasks by ﬁne-tuning PaLM on scientiﬁc and mathematical content from the Web. We evaluate PaLM 2 on MATH (Hendrycks et al., 2021), which contains 12,500 problems from high school competitions in 7 mathematics subject areas, GSM8K (Cobbe et al., 2021), a dataset of 8,500 grade school math word problems, and MGSM (Shi et al., 2023), a multilingual version of GSM8K with translations of a subset of examples into ten typologically diverse languages. We compare PaLM 2 to PaLM, Minerva (Lewkowycz et al., 2022), GPT-4 (OpenAI, 2023b), and the state of the art for each dataset. For MATH, we follow Lewkowycz et al. (2022) and use the same 4-shot chain-of-thought prompt, combined with self-consistency (Wang et al., 2023) utilizing 64 sample paths. For GSM8K, we use the same 8-shot chain-of-thought prompt as in (Wei et al., 2022), and self-consistency with 40 sample paths. We use the SymPy library (Meurer et al., 2017) to compare answers and guard against false negatives, which arise from equivalent answers with different surface forms. For MGSM, we use 8-shot chain-of-thought prompts and in-language exemplars provided by Shi et al. (2023). We show the results in Table 7. PaLM 2 outperforms PaLM dramatically on all datasets. On MATH, PaLM 2 is competitive with the state-of-the-art performance achieved by the dedicated Minerva model. On GSM8K, PaLM 2 outperforms Minerva and GPT-4 while on MGSM, it surpasses the state of the art even without self-consistency. 4.4 Coding Code language models are among the most economically signiﬁcant and widely-deployed LLMs today; code LMs are deployed in diverse developer tooling (Github, 2021; Tabachnyk & Nikolov, 2022), "
        },
        "snippet": "75.9 / 85.8 Mathematical reasoning LLMs have struggled on tasks that require quantitative reasoning, such as high-school and college-level problems in mathematics, science, and engineering (Hendrycks et al., 2021; Cobbe et al., 2021). Recently, Minerva (Lewkowycz et al., 2022) achieved signiﬁcant gains on quantitative reasoning tasks by ﬁne-tuning PaLM on scientiﬁc and mathematical content from th"
      },
      {
        "rank": 6,
        "doc_id": "arxiv:333078981_693988129081760_4712707815225756708_n#approach:part-6",
        "score": 0.5546272,
        "payload": {
          "doc_id": "arxiv:333078981_693988129081760_4712707815225756708_n#approach:part-6",
          "url": "https://arxiv.org/abs/333078981_693988129081760_4712707815225756708_n",
          "anchor": "#approach:part-6",
          "type": "paper",
          "title": "",
          "section": "Approach",
          "source": "arxiv_pdf",
          "published": "",
          "tokens": 512,
          "sha256": "aa5d4dbed5960a8c89cde2818d990cd2159d4dc3ca3d24d3bf5ecca50d13691e",
          "text": "540B 68.1 49.1 LLaMA 7B 61.1 46.9 13B 61.6 47.2 33B 64.1 48.3 65B 67.9 51.6 Table 6: Reading Comprehension. Zero-shot accuracy. school Chinese students. We follow the evaluation setup from Brown et al. (2020) and report results in Table 6. On these benchmarks, LLaMA-65B is competitive with PaLM-540B, and, LLaMA-13B outperforms GPT-3 by a few percents. 3.4 Mathematical reasoning We evaluate our models on two mathematical rea- soning benchmarks: MATH (Hendrycks et al., 2021) and GSM8k (Cobbe et al., 2021). MATH is a dataset of 12K middle school and high school mathematics problems written in LaTeX. GSM8k is a set of middle school mathematical problems. In Table 7, we compare with PaLM and Min- erva (Lewkowycz et al., 2022). Minerva is a series of PaLM models finetuned on 38.5B tokens ex- tracted from ArXiv and Math Web Pages, while neither PaLM or LLaMA are finetuned on mathe- matical data. The numbers for PaLM and Minerva are taken from Lewkowycz et al. (2022), and we compare with and without maj1@k. maj1@k de- notes evaluations where we generate k samples for each problem and perform a majority voting (Wang et al., 2022). On GSM8k, we observe that LLaMA- 65B outperforms Minerva-62B, although it has not been fine-tuned on mathematical data. 3.5 Code generation We evaluate the ability of our models to write code from a natural language description on two benchmarks: HumanEval (Chen et al., 2021) and MBPP (Austin et al., 2021). For both tasks, the model receives a description of the program in a few sentences, as well as a few input-output ex- amples. In HumanEval, it also receives a function signature, and the prompt is formatted as natural code with the textual description and tests in a MATH +maj1@k GSM8k +maj1@k PaLM 8B 1.5 - 4.1 - 62B 4.4 - 33.0 - 540B 8.8 - 56.5 - Minerva 8B 14.1 25.4 16.2 28.4 62B 27.6 43.4 52.4 68.5 540B 33.6 50.3 68.5 78.5 LLaMA 7B 2.9 6.9 11.0 18.1 13B 3.9 8.8 17.8 29.3 33B 7.1 15.2 35.6 53.1 65B 10.6 20.5 50.9 69.7 Table 7: Model performance"
        },
        "snippet": "540B 68.1 49.1 LLaMA 7B 61.1 46.9 13B 61.6 47.2 33B 64.1 48.3 65B 67.9 51.6 Table 6: Reading Comprehension. Zero-shot accuracy. school Chinese students. We follow the evaluation setup from Brown et al. (2020) and report results in Table 6. On these benchmarks, LLaMA-65B is competitive with PaLM-540B, and, LLaMA-13B outperforms GPT-3 by a few percents. 3.4 Mathematical reasoning We evaluate our mod"
      },
      {
        "rank": 7,
        "doc_id": "arxiv:2305.14314#evaluation:part-1",
        "score": 0.55309194,
        "payload": {
          "doc_id": "arxiv:2305.14314#evaluation:part-1",
          "url": "https://arxiv.org/abs/2305.14314",
          "anchor": "#evaluation:part-1",
          "type": "paper",
          "title": "",
          "section": "Evaluation",
          "source": "arxiv_pdf",
          "published": "",
          "tokens": 512,
          "sha256": "3099241151be78ff18db8a8fdf8bd0b9b1c75227d70a8a6316b0758ea530e942",
          "text": "Table 5: MMLU 5-shot test results for different sizes of LLaMA finetuned on the corresponding datasets using QLoRA. Dataset 7B 13B 33B 65B LLaMA no tuning 35.1 46.9 57.8 63.4 Self-Instruct 36.4 33.3 53.0 56.7 Longform 32.1 43.2 56.6 59.7 Chip2 34.5 41.6 53.6 59.8 HH-RLHF 34.9 44.6 55.8 60.1 Unnatural Instruct 41.9 48.1 57.3 61.3 Guanaco (OASST1) 36.6 46.4 57.0 62.2 Alpaca 38.8 47.8 57.3 62.5 FLAN v2 44.5 51.4 59.2 63.9 Following common practice, we use the MMLU (Mas- sively Multitask Language Understanding) benchmark [24] to measure performance on a range of language un- derstanding tasks. This is a multiple-choice benchmark covering 57 tasks including elementary mathematics, US history, computer science, law, and more. We report 5-shot test accuracy. We also test generative language capabilities through both automated and human evaluations. This second set of evaluations relies on queries curated by humans and aims at measuring the quality of model responses. While this is a more realistic testbed for chatbot model performance and is growing in popularity, there is no commonly accepted protocol in the literature. We de- scribe below our proposed setup, using nucleus sampling with p = 0.9 and temperature 0.7 in all cases. 8 Benchmark Data We evaluate on two curated datasets of queries (questions): the Vicuna prompts [10] and the OASST1 validation dataset [31]. We use the Vicuna prompts, a set of 80 prompts from a diverse set of categories, without modifications. The OASST1 dataset is a multilingual collection of crowd-sourced multiturn dialogs between a user and an assistant. We select all user messages in the validation dataset as queries and include previous turns in the prompt. This procedure leads to 953 unique user queries. We term these two datasets the Vicuna and OA benchmarks. Automated Evaluation First, based on the evaluation protocol introduced by Chiang et al. [10], we use GPT-4 to rate the performance of different systems against ChatGPT (GPT-3.5 Turbo)"
        },
        "snippet": "Table 5: MMLU 5-shot test results for different sizes of LLaMA finetuned on the corresponding datasets using QLoRA. Dataset 7B 13B 33B 65B LLaMA no tuning 35.1 46.9 57.8 63.4 Self-Instruct 36.4 33.3 53.0 56.7 Longform 32.1 43.2 56.6 59.7 Chip2 34.5 41.6 53.6 59.8 HH-RLHF 34.9 44.6 55.8 60.1 Unnatural Instruct 41.9 48.1 57.3 61.3 Guanaco (OASST1) 36.6 46.4 57.0 62.2 Alpaca 38.8 47.8 57.3 62.5 FLAN "
      },
      {
        "rank": 8,
        "doc_id": "arxiv:2303.18223#result:part-19",
        "score": 0.55124927,
        "payload": {
          "doc_id": "arxiv:2303.18223#result:part-19",
          "url": "https://arxiv.org/abs/2303.18223",
          "anchor": "#result:part-19",
          "type": "paper",
          "title": "",
          "section": "Result",
          "source": "arxiv_pdf",
          "published": "",
          "tokens": 512,
          "sha256": "4f09df38af1315c1a424d049d4493504904f1a320d160a234fe3e7dfb7fef14f",
          "text": "and computer science to humanities and social sciences. The difficulties of these tasks vary from basic to advanced. As shown in existing work, LLMs mostly outperform small models by a substantial margin on this benchmark [35, 56, 57, 69], which shows the scaling law in model size. More recently, GPT-4 achieves a remarkable record (86.4% in 5- shot setting) in MMLU, which is significantly better than the previous state-of-the-art models [46]. • BIG-bench [70] is a collaborative benchmark intended to probe existing LLMs from various aspects. It comprises 204 tasks that encompass a broad range of topics, includ- ing linguistics, childhood development, mathematics, com- monsense reasoning, biology, physics, social bias, software development, and so on. By scaling the model size, LLMs can even outperform the average human performance under the few-shot setting on 65% of tasks in BIG-bench [56]. Considering the high evaluation cost of the entire bench- mark, a lightweight benchmark BIG-bench-Lite has been proposed, which contains 24 small yet diverse and challeng- ing tasks from BIG-bench. Additionally, the BIG-bench hard (BBH) benchmark [365] has been proposed to concentrate on investigating the currently unsolvable tasks of LLMs by selecting the challenging tasks in which LLMs exhibit infe- rior performance compared to humans. Since BBH becomes more difficult, small models mostly achieve performance close to random. As a comparison, CoT prompting can elicit the abilities of LLMs to perform step-by-step reasoning for enhancing the performance, even exceeding the average human performance in BBH. • HELM [520] is a comprehensive benchmark that cur- rently implements a core set of 16 scenarios and 7 categories of metrics. It is built on top of many prior studies, conduct- ing a holistic evaluation of language models. As shown in the experimental results of HELM, instruction tuning can consistently boost the performance of LLMs in terms of accuracy, robustness, and fairness"
        },
        "snippet": "and computer science to humanities and social sciences. The difficulties of these tasks vary from basic to advanced. As shown in existing work, LLMs mostly outperform small models by a substantial margin on this benchmark [35, 56, 57, 69], which shows the scaling law in model size. More recently, GPT-4 achieves a remarkable record (86.4% in 5- shot setting) in MMLU, which is significantly better t"
      },
      {
        "rank": 9,
        "doc_id": "arxiv:2302.13971#approach:part-6",
        "score": 0.550365,
        "payload": {
          "doc_id": "arxiv:2302.13971#approach:part-6",
          "url": "https://arxiv.org/abs/2302.13971",
          "anchor": "#approach:part-6",
          "type": "paper",
          "title": "",
          "section": "Approach",
          "source": "arxiv_pdf",
          "published": "",
          "tokens": 512,
          "sha256": "5b2abe902fcd565798adbbd91d2fedabddda1ab07a544f1053a91e5bd05ddf3b",
          "text": "42.3 62B 64.3 47.5 540B 68.1 49.1 LLaMA 7B 61.1 46.9 13B 61.6 47.2 33B 64.1 48.3 65B 67.9 51.6 Table 6: Reading Comprehension. Zero-shot accu- racy. school Chinese students. We follow the evaluation setup from Brown et al. (2020) and report results in Table 6. On these benchmarks, LLaMA-65B is competitive with PaLM-540B, and, LLaMA-13B outperforms GPT-3 by a few percents. 3.4 Mathematical reasoning We evaluate our models on two mathematical rea- soning benchmarks: MATH (Hendrycks et al., 2021) and GSM8k (Cobbe et al., 2021). MATH is a dataset of 12K middle school and high school mathematics problems written in LaTeX. GSM8k is a set of middle school mathematical problems. In Table 7, we compare with PaLM and Min- erva (Lewkowycz et al., 2022). Minerva is a series of PaLM models ﬁnetuned on 38.5B tokens ex- tracted from ArXiv and Math Web Pages, while neither PaLM or LLaMA are ﬁnetuned on mathe- matical data. The numbers for PaLM and Minerva are taken from Lewkowycz et al. (2022), and we compare with and without maj1@k. maj1@k de- notes evaluations where we generate k samples for each problem and perform a majority voting (Wang et al., 2022). On GSM8k, we observe that LLaMA- 65B outperforms Minerva-62B, although it has not been ﬁne-tuned on mathematical data. 3.5 Code generation We evaluate the ability of our models to write code from a natural language description on two benchmarks: HumanEval (Chen et al., 2021) and MBPP (Austin et al., 2021). For both tasks, the model receives a description of the program in a few sentences, as well as a few input-output ex- amples. In HumanEval, it also receives a function signature, and the prompt is formatted as natural code with the textual description and tests in a MATH +maj1@k GSM8k +maj1@k PaLM 8B 1.5 - 4.1 - 62B 4.4 - 33.0 - 540B 8.8 - 56.5 - Minerva 8B 14.1 25.4 16.2 28.4 62B 27.6 43.4 52.4 68.5 540B 33.6 50.3 68.5 78.5 LLaMA 7B 2.9 6.9 11.0 18.1 13B 3.9 8.8 17.8 29.3 33B 7.1 15.2 35.6 53.1 65B 10.6 20.5 50.9 69.7 Table 7:"
        },
        "snippet": "42.3 62B 64.3 47.5 540B 68.1 49.1 LLaMA 7B 61.1 46.9 13B 61.6 47.2 33B 64.1 48.3 65B 67.9 51.6 Table 6: Reading Comprehension. Zero-shot accu- racy. school Chinese students. We follow the evaluation setup from Brown et al. (2020) and report results in Table 6. On these benchmarks, LLaMA-65B is competitive with PaLM-540B, and, LLaMA-13B outperforms GPT-3 by a few percents. 3.4 Mathematical reasonin"
      },
      {
        "rank": 10,
        "doc_id": "arxiv:2412.19437#model",
        "score": 0.5449443,
        "payload": {
          "doc_id": "arxiv:2412.19437#model",
          "url": "https://arxiv.org/abs/2412.19437",
          "anchor": "#model",
          "type": "paper",
          "title": "",
          "section": "Model",
          "source": "arxiv_pdf",
          "published": "",
          "tokens": 363,
          "sha256": "b3626ec585ca960b5a9253cc5a03f4b65d4f44553770ef61b99fb9af69149d1f",
          "text": "Arena-Hard AlpacaEval 2.0 DeepSeek-V2.5-0905 76.2 50.5 Qwen2.5-72B-Instruct 81.2 49.1 LLaMA-3.1 405B 69.3 40.5 GPT-4o-0513 80.4 51.1 Claude-Sonnet-3.5-1022 85.2 52.0 DeepSeek-V3 85.5 70.0 Table 7 | English open-ended conversation evaluations. For AlpacaEval 2.0, we use the length- controlled win rate as the metric. pre-trained on. On C-Eval, a representative benchmark for Chinese educational knowledge evaluation, and CLUEWSC (Chinese Winograd Schema Challenge), DeepSeek-V3 and Qwen2.5-72B exhibit similar performance levels, indicating that both models are well-optimized for challenging Chinese-language reasoning and educational tasks. 5.3.3. Open-Ended Evaluation In addition to standard benchmarks, we also evaluate our models on open-ended generation tasks using LLMs as judges, with the results shown in Table 7. Specifically, we adhere to the original configurations of AlpacaEval 2.0 (Dubois et al., 2024) and Arena-Hard (Li et al., 2024a), which leverage GPT-4-Turbo-1106 as judges for pairwise comparisons. On Arena-Hard, DeepSeek-V3 achieves an impressive win rate of over 86% against the baseline GPT-4-0314, performing on par with top-tier models like Claude-Sonnet-3.5-1022. This underscores the robust capabilities of DeepSeek-V3, especially in dealing with complex prompts, including coding and debugging tasks. Furthermore, DeepSeek-V3 achieves a groundbreaking milestone as the first open-source model to surpass 85% on the Arena-Hard benchmark. This achievement significantly bridges the performance gap between open-source and closed-source models, setting a new standard for what open-source models can accomplish in challenging domains. Similarly, DeepSeek-V3 showcases exceptional performance on AlpacaEval 2.0, outperform- ing both closed-source and open-source models. This demonstrates its outstanding proficiency in writing tasks and handling straightforward question-answering scenarios. Notably, it surpasses DeepSeek-V2.5-0905 by a significant margin of 20%, highli"
        },
        "snippet": "Arena-Hard AlpacaEval 2.0 DeepSeek-V2.5-0905 76.2 50.5 Qwen2.5-72B-Instruct 81.2 49.1 LLaMA-3.1 405B 69.3 40.5 GPT-4o-0513 80.4 51.1 Claude-Sonnet-3.5-1022 85.2 52.0 DeepSeek-V3 85.5 70.0 Table 7 | English open-ended conversation evaluations. For AlpacaEval 2.0, we use the length- controlled win rate as the metric. pre-trained on. On C-Eval, a representative benchmark for Chinese educational knowl"
      }
    ]
  },
  {
    "id": "query_5",
    "query": "\"query_5\": \"How is reinforcement learning from human feedback (RLHF) implemented differently across models like InstructGPT, Sparrow, and Claude?\",",
    "results": [
      {
        "rank": 1,
        "doc_id": "arxiv:2305.18290#related-work:part-2",
        "score": 0.5553898,
        "payload": {
          "doc_id": "arxiv:2305.18290#related-work:part-2",
          "url": "https://arxiv.org/abs/2305.18290",
          "anchor": "#related-work:part-2",
          "type": "paper",
          "title": "",
          "section": "Related Work",
          "source": "arxiv_pdf",
          "published": "",
          "tokens": 512,
          "sha256": "215bb6edd45af036feaa17f235f26f3ea091bdd681f0221a34c5e4f636c44ffa",
          "text": "off-policy preference data, but generally involve first explicitly estimating the latent scoring function (i.e. the reward model) and subsequently optimizing it [16, 9, 12, 34, 19]. We instead present a single stage policy learning approach that directly optimizes a policy to satisfy preferences. 3 Preliminaries We review the RLHF pipeline in Ziegler et al. (and later [38, 1, 26]). It usually includes three phases: 1) supervised fine-tuning (SFT); 2) preference sampling and reward learning and 3) RL optimization. SFT: RLHF typically begins by fine-tuning a pre-trained LM with supervised learning on high-quality data for the downstream task(s) of interest (dialogue, summarization, etc.), to obtain a model πSFT. Reward Modelling Phase: In the second phase the SFT model is prompted with prompts x to produce pairs of answers (y1, y2) ∼πSFT(y | x). These are then presented to human labelers who express preferences for one answer, denoted as yw ≻yl | x where yw and yl denotes the preferred and dispreferred completion amongst (y1, y2) respectively. The preferences are assumed to be generated by some latent reward model r∗(y, x), which we do not have access to. There are a number of approaches used to model preferences, the Bradley-Terry (BT) [5] model being a popular choice (although more general Plackett-Luce ranking models [30, 21] are also compatible with the framework if we have access to several ranked answers). The BT model stipulates that the human preference distribution p∗can be written as: p∗(y1 ≻y2 | x) = exp (r∗(x, y1)) exp (r∗(x, y1)) + exp (r∗(x, y2)). (1) Assuming access to a static dataset of comparisons D = \b x(i), y(i) w , y(i) l N i=1 sampled from p∗, we can parametrize a reward model rϕ(x, y) and estimate the parameters via maximum likelihood. Framing the problem as a binary classification we have the negative log-likelihood loss: LR(rϕ, D) = −E(x,yw,yl)∼D \u0002 log σ(rϕ(x, yw) −rϕ(x, yl)) \u0003 (2) where σ is the logistic function. In the context of LMs, the n"
        },
        "snippet": "off-policy preference data, but generally involve first explicitly estimating the latent scoring function (i.e. the reward model) and subsequently optimizing it [16, 9, 12, 34, 19]. We instead present a single stage policy learning approach that directly optimizes a policy to satisfy preferences. 3 Preliminaries We review the RLHF pipeline in Ziegler et al. (and later [38, 1, 26]). It usually incl"
      },
      {
        "rank": 2,
        "doc_id": "arxiv:2303.18223#model:part-5",
        "score": 0.5237746,
        "payload": {
          "doc_id": "arxiv:2303.18223#model:part-5",
          "url": "https://arxiv.org/abs/2303.18223",
          "anchor": "#model:part-5",
          "type": "paper",
          "title": "",
          "section": "Model",
          "source": "arxiv_pdf",
          "published": "",
          "tokens": 512,
          "sha256": "be1f1697e2a180fa0da1f13f638aebcf50c6432aa2b065c75dbea5ba4a01de0c",
          "text": "OpenAI blog describing a work that applied reinforcement learning (RL) to learn from the preference comparisons annotated by humans [79] (similar to the reward training step in the aligning algorithm of InstructGPT in Figure 12). Shortly after the release of this RL paper [79], the paper of the Proximal Policy Optimiza- tion (PPO) [128] was published in July 2017, which now has been the foundational RL algorithm for learning from hu- man preferences [66]. Later in January 2020, GPT-2 was fine- tuned using the aforementioned RL algorithms [79, 128], which leveraged human preferences to improve the capac- ities of GPT-2 on NLP tasks. In the same year, another work [129] trained a summarization model for optimizing human preferences in a similar way. Based on these prior work, InstructGPT [66] was proposed in January 2022 to improve the GPT-3 model for human alignment, which formally established a three-stage reinforcement learning from human feedback (RLHF) algorithm. Note that it seems that the wording of “instruction tuning” has seldom been used in OpenAI’s paper and documentation, which is substituted by supervised fine-tuning on human demonstrations (i.e., the first step of the RLHF algorithm [66]). In addition to improving the instruction following capacity, the RLHF algorithm is particularly useful to mitigate the issues of generating harm or toxic content for LLMs, which is key to the safe deploy- ment of LLMs in practice. OpenAI describes their approach to alignment research in a technical article [130], which has summarized three promising directions: “training AI systems to use human feedback, to assist human evaluation and to do alignment research”. These enhancement techniques lead to the improved 18. https://openai.com/research/learning-from-human-preferences GPT-3 models with stronger capacities, which are called GPT-3.5 models by OpenAI (see the discussion about the OpenAI API in Section 3.1). The Milestones of Language Models. Based on all the ex- plor"
        },
        "snippet": "OpenAI blog describing a work that applied reinforcement learning (RL) to learn from the preference comparisons annotated by humans [79] (similar to the reward training step in the aligning algorithm of InstructGPT in Figure 12). Shortly after the release of this RL paper [79], the paper of the Proximal Policy Optimiza- tion (PPO) [128] was published in July 2017, which now has been the foundation"
      },
      {
        "rank": 3,
        "doc_id": "arxiv:2203.02155#related-work:part-1",
        "score": 0.50035787,
        "payload": {
          "doc_id": "arxiv:2203.02155#related-work:part-1",
          "url": "https://arxiv.org/abs/2203.02155",
          "anchor": "#related-work:part-1",
          "type": "paper",
          "title": "",
          "section": "Related work",
          "source": "arxiv_pdf",
          "published": "",
          "tokens": 512,
          "sha256": "3617bcb3cbf1ec9595641e126d6e8656384bdcf77499ee573ddba9961468e2b5",
          "text": "Research on alignment and learning from human feedback. We build on previous techniques to align models with human intentions, particularly reinforcement learning from human feed- back (RLHF). Originally developed for training simple robots in simulated environments and Atari games (Christiano et al., 2017; Ibarz et al., 2018), it has recently been applied to ﬁne-tuning language models to summarize text (Ziegler et al., 2019; Stiennon et al., 2020; Böhm et al., 2019; Wu et al., 2021). This work is in turn inﬂuenced by similar work using human feedback as a reward in domains such as dialogue (Jaques et al., 2019; Yi et al., 2019; Hancock et al., 2019), translation (Kreutzer et al., 2018; Bahdanau et al., 2016), semantic parsing (Lawrence and Riezler, 2018), story generation (Zhou and Xu, 2020), review generation (Cho et al., 2018), and evidence extraction (Perez et al., 2019). Madaan et al. (2022) use written human feedback to augment prompts and improve the performance of GPT-3. There has also been work on aligning agents in text-based environments using RL with 4 a normative prior (Nahian et al., 2021). Our work can be seen as a direct application of RLHF to aligning language models on a broad distribution of language tasks. The question of what it means for language models to be aligned has also received attention re- cently (Gabriel, 2020). Kenton et al. (2021) catalog behavioral issues in LMs that result from misalignment, including producing harmful content and gaming misspeciﬁed objectives. In concur- rent work, Askell et al. (2021) propose language assistants as a testbed for alignment research, study some simple baselines, and their scaling properties. Training language models to follow instructions. Our work is also related to research on cross- task generalization in language models, where LMs are ﬁne-tuned on a broad range of public NLP datasets (usually preﬁxed with an appropriate instruction) and evaluated on a different set of NLP tasks. There has been"
        },
        "snippet": "Research on alignment and learning from human feedback. We build on previous techniques to align models with human intentions, particularly reinforcement learning from human feed- back (RLHF). Originally developed for training simple robots in simulated environments and Atari games (Christiano et al., 2017; Ibarz et al., 2018), it has recently been applied to ﬁne-tuning language models to summariz"
      },
      {
        "rank": 4,
        "doc_id": "arxiv:2510.13794#background:part-1",
        "score": 0.4973321,
        "payload": {
          "doc_id": "arxiv:2510.13794#background:part-1",
          "url": "https://arxiv.org/abs/2510.13794",
          "anchor": "#background:part-1",
          "type": "paper",
          "title": "",
          "section": "BACKGROUND",
          "source": "arxiv_pdf",
          "published": "",
          "tokens": 512,
          "sha256": "56a544da41ec78c159e31a5a446ad5294b33710872a001c494f522cf680265bd",
          "text": "In MimicKit, most models are trained using reinforcement learning, where an agent interacts with an environment according to a policy 𝜋in order to optimize a given objective [Sutton and Barto 2018]. At each time step 𝑡, the agent receives an observations o𝑡of the environment, which provides partial information of the state s𝑡of the underlying system. The agent responds by sampling an action from a policy a𝑡∼𝜋(a𝑡|o𝑡). The agent then Author’s address: Xue Bin Peng, xbpeng@sfu.ca, Simon Fraser University and NVIDIA. arXiv:2510.13794v2 [cs.GR] 16 Oct 2025 2 • Xue Bin Peng Fig. 2. Schematic overview of the MimicKit framework. The main components of the system are 1) the Agent, 2) the Model, 3) the Environment, and 4) the Engine. The learning algorithms are implemented primarily through the Agent and Model, while the Environment and Engine are responsible for simulating the desired task. executes the action, which leads to a new state s𝑡+1, sampled according to the dynamics of the environment s𝑡+1 ∼𝑝(s𝑡+1|s𝑡, a𝑡). The agent in turn receives a scalar reward 𝑟𝑡= 𝑟(s𝑡, a𝑡, s𝑡+1), and a new observation o𝑡+1 of the next state s𝑡+1. The agent’s objective is to learn a policy that maximizes its expected discounted return 𝐽(𝜋), 𝐽(𝜋) = E𝑝(𝜏|𝜋) \"𝑇−1 ∑︁ 𝑡=0 𝛾𝑡𝑟𝑡 # , (1) where 𝑝(𝜏|𝜋) represents the likelihood of a trajectory 𝜏= {o0, a0,𝑟0, o1, ..., o𝑇−1, a𝑇−1,𝑟𝑇−1, o𝑇} under 𝜋.𝑇denotes the time horizon of a trajectory, and 𝛾∈[0, 1] is a discount factor. Each trajectory corresponds to one episode of interactions between the agent and the environment. 3 SYSTEM OVERVIEW A schematic overview of the MimicKit framework is provided in Figure 2. The core components of MimicKit consist of: 1) the Agent, 2) the Model, 3) the Environment, and 4) the Engine. The learning algorithms are implemented primarily through the Agent and the Model, while the Environment and Engine are responsible for simulating the desired task. These components are designed to be modular and composable, enabling users t"
        },
        "snippet": "In MimicKit, most models are trained using reinforcement learning, where an agent interacts with an environment according to a policy 𝜋in order to optimize a given objective [Sutton and Barto 2018]. At each time step 𝑡, the agent receives an observations o𝑡of the environment, which provides partial information of the state s𝑡of the underlying system. The agent responds by sampling an action from a"
      },
      {
        "rank": 5,
        "doc_id": "arxiv:2303.18223#result:part-10",
        "score": 0.49487203,
        "payload": {
          "doc_id": "arxiv:2303.18223#result:part-10",
          "url": "https://arxiv.org/abs/2303.18223",
          "anchor": "#result:part-10",
          "type": "paper",
          "title": "",
          "section": "Result",
          "source": "arxiv_pdf",
          "published": "",
          "tokens": 512,
          "sha256": "ec673426a742ca7ff17880d7a3417fe9ab6609f2e2f88133ae0fc82d5e4e175c",
          "text": "RLHF for LLMs. RLHF stands for \"Rights, Limitations, Harms, and Freedoms\" and is a framework for …… models like LLMs (Large Language Models). Bob’s wife is Amy. Bob’s daughter is Cindy. Who is Cindy to Amy? Cindy is Amy’s daughter-in-law. Fig. 17: Examples of intrinsic and extrinsic hallucination for a public LLM (access date: March 19, 2023). As an example of intrinsic hallucination, the LLM gives a conflicting judgment about the relationship between Cindy and Amy, which contradicts the input. For extrinsic hallucination, in this example, the LLM seems to have an incorrect understanding of the meaning of RLHF (reinforcement learning from human feedback), though it can correctly understand the meaning of LLMs (in this context). the retrieved evidence can largely improve the accuracy of the generated answers, even enabling a smaller LLM to outperform 10× larger ones [653, 657]. Further, open-book QA tasks can be also employed to evaluate the recency of knowledge information. Pre-training or retrieving from outdated knowledge resources may cause LLMs to generate incorrect answers for time-sensitive questions [653]. Knowledge Completion. In knowledge completion tasks, LLMs might be (to some extent) considered as a knowledge base [576], which can be leveraged to complete or predict the missing parts of knowledge units (e.g., knowledge triples). Such tasks can probe and evaluate how much and what kind of knowledge LLMs have learned from the pre-training data. Existing knowledge completion tasks can be roughly divided into knowledge graph completion tasks (e.g., FB15k- 237 [572] and WN18RR [574]) and fact completion tasks (e.g., WikiFact [571]), which aim to complete the triples from a knowledge graph and incomplete sentences about specific facts, respectively. Empirical studies have revealed that it is difficult for existing LLMs to accomplish knowledge completion tasks related to specific relation types [520]. As shown in the evaluation results on WikiFact, LLMs perform"
        },
        "snippet": "RLHF for LLMs. RLHF stands for \"Rights, Limitations, Harms, and Freedoms\" and is a framework for …… models like LLMs (Large Language Models). Bob’s wife is Amy. Bob’s daughter is Cindy. Who is Cindy to Amy? Cindy is Amy’s daughter-in-law. Fig. 17: Examples of intrinsic and extrinsic hallucination for a public LLM (access date: March 19, 2023). As an example of intrinsic hallucination, the LLM give"
      },
      {
        "rank": 6,
        "doc_id": "arxiv:2305.03047#introduction:part-2",
        "score": 0.48028764,
        "payload": {
          "doc_id": "arxiv:2305.03047#introduction:part-2",
          "url": "https://arxiv.org/abs/2305.03047",
          "anchor": "#introduction:part-2",
          "type": "paper",
          "title": "",
          "section": "Introduction",
          "source": "arxiv_pdf",
          "published": "",
          "tokens": 512,
          "sha256": "9cef17a27453e1b2e8a44a6af5f9859c0808a5209f832ef257e0d4b819749b14",
          "text": "system-produced responses, or the rules behind the behavior of the AI model in producing answers2. These principles function as guidelines for generating helpful, ethical, and reliable responses. We conduct in-context learning (ICL) [7] with a few (5) exemplars (demonstrations) that illustrate how the AI system complies with the rules when formulating responses in different cases. From the human-written principles, ICL exemplars, and the incoming self-instructed prompts, the LLM can trigger the matching rules and generate the explanations for a refused answer if the query is detected as a harmful or ill-formed one. 3. Principle Engraving: In the third stage, we fine-tune the original LLM (the base model) on the self-aligned responses, generated by the LLM itself through prompting, while pruning the principles and demonstrations for the fine-tuned model. The fine-tuning process enables our system to directly generate responses that are well-aligned with the helpful, ethical, and reliable principles across a wide range of queries, due to shared model parameters. Notice that the fine- tuned LLM can directly generate high-quality responses for new queries without explicitly using the principle set and the ICL exemplars. 4. Verbose Cloning: Lastly, we employ context distillation [18, 3] to enhance the system’s capability to produce more comprehensive and elaborate responses than the overly short or indirect responses. Impressively, the entire SELF-ALIGN process necessitates fewer than 300 lines of annotations (including 195 seed prompts, 16 principles, and 5 exemplars), while previous aligned AI systems such as InstructGPT [30] or Alpaca [42] required at least 50K human/teacher annotations. This 2The detailed principles are given in the appendix. Analogous to Constitutional AI [5], the design of these principles in SELF-ALIGN remains exploratory and primarily serves research purposes. 2 Table 1: Comparison of human/teacher supervisions used in recent AI systems. The alig"
        },
        "snippet": "system-produced responses, or the rules behind the behavior of the AI model in producing answers2. These principles function as guidelines for generating helpful, ethical, and reliable responses. We conduct in-context learning (ICL) [7] with a few (5) exemplars (demonstrations) that illustrate how the AI system complies with the rules when formulating responses in different cases. From the human-w"
      },
      {
        "rank": 7,
        "doc_id": "arxiv:10000000_662098952474184_2584067087619170692_n#model:part-15",
        "score": 0.4768062,
        "payload": {
          "doc_id": "arxiv:10000000_662098952474184_2584067087619170692_n#model:part-15",
          "url": "https://arxiv.org/abs/10000000_662098952474184_2584067087619170692_n",
          "anchor": "#model:part-15",
          "type": "paper",
          "title": "",
          "section": "Model",
          "source": "arxiv_pdf",
          "published": "",
          "tokens": 512,
          "sha256": "7d6e810ee684d9b8bad94d12bcb3978fb5a9ed91c830d378637700401f434b1a",
          "text": "each figure corresponds to the system message (“Act as Oscar Wilde”). We can see that the GAtt-equipped model (right) maintains large attention activations with respect to the system message for a larger portion of the dialogue, as compared to the model without GAtt (left). Despite its utility, the current implementation of GAtt is vanilla, and more development and iteration on this technique could likely further benefit the model. For instance, we could teach the model to change the system message during the conversation by integrating such data during fine-tuning. 3.4 RLHF Results 3.4.1 Model-Based Evaluation Evaluating LLMs is a challenging open-research problem. Human evaluation, while a gold standard, can be complicated by various HCI considerations (Clark et al., 2021; Gehrmann et al., 2023), and is not always scalable. Thus, to select the best-performing models among several ablations at each iteration from RLHF-V1 to V5, we first observed the improvement of the rewards from the latest reward models, to save costs and increase iteration speed. We later validated major model versions with human evaluations. How Far Can Model-Based Evaluation Go? To measure the robustness of our reward model, we collected a test set of prompts for both helpfulness and safety, and asked three annotators to judge the quality of the answers based on a 7-point Likert scale (the higher the better). We observe that our reward models overall are well calibrated with our human preference annotations, as illustrated in Figure 29 in the appendix. This confirms the relevance of using our reward as a point-wise metric, despite being trained with a Pairwise Ranking Loss. Still, as Goodhart’s Law states, when a measure becomes a target, it ceases to be a good measure. To ensure our measure won’t diverge from the human preferences, we additionally used a more general reward, trained 17 RLHF-v5 (with PPO) RLHF-v5 (no PPO) RLHF-v4 RLHF-v3 RLHF-v2 RLHF-v1 SFT-v2 SFT-v1 10% 20% 30% 40% 50% 60% 70"
        },
        "snippet": "each figure corresponds to the system message (“Act as Oscar Wilde”). We can see that the GAtt-equipped model (right) maintains large attention activations with respect to the system message for a larger portion of the dialogue, as compared to the model without GAtt (left). Despite its utility, the current implementation of GAtt is vanilla, and more development and iteration on this technique coul"
      },
      {
        "rank": 8,
        "doc_id": "arxiv:2307.09288#model:part-15",
        "score": 0.4768062,
        "payload": {
          "doc_id": "arxiv:2307.09288#model:part-15",
          "url": "https://arxiv.org/abs/2307.09288",
          "anchor": "#model:part-15",
          "type": "paper",
          "title": "",
          "section": "Model",
          "source": "arxiv_pdf",
          "published": "",
          "tokens": 512,
          "sha256": "7d6e810ee684d9b8bad94d12bcb3978fb5a9ed91c830d378637700401f434b1a",
          "text": "each figure corresponds to the system message (“Act as Oscar Wilde”). We can see that the GAtt-equipped model (right) maintains large attention activations with respect to the system message for a larger portion of the dialogue, as compared to the model without GAtt (left). Despite its utility, the current implementation of GAtt is vanilla, and more development and iteration on this technique could likely further benefit the model. For instance, we could teach the model to change the system message during the conversation by integrating such data during fine-tuning. 3.4 RLHF Results 3.4.1 Model-Based Evaluation Evaluating LLMs is a challenging open-research problem. Human evaluation, while a gold standard, can be complicated by various HCI considerations (Clark et al., 2021; Gehrmann et al., 2023), and is not always scalable. Thus, to select the best-performing models among several ablations at each iteration from RLHF-V1 to V5, we first observed the improvement of the rewards from the latest reward models, to save costs and increase iteration speed. We later validated major model versions with human evaluations. How Far Can Model-Based Evaluation Go? To measure the robustness of our reward model, we collected a test set of prompts for both helpfulness and safety, and asked three annotators to judge the quality of the answers based on a 7-point Likert scale (the higher the better). We observe that our reward models overall are well calibrated with our human preference annotations, as illustrated in Figure 29 in the appendix. This confirms the relevance of using our reward as a point-wise metric, despite being trained with a Pairwise Ranking Loss. Still, as Goodhart’s Law states, when a measure becomes a target, it ceases to be a good measure. To ensure our measure won’t diverge from the human preferences, we additionally used a more general reward, trained 17 RLHF-v5 (with PPO) RLHF-v5 (no PPO) RLHF-v4 RLHF-v3 RLHF-v2 RLHF-v1 SFT-v2 SFT-v1 10% 20% 30% 40% 50% 60% 70"
        },
        "snippet": "each figure corresponds to the system message (“Act as Oscar Wilde”). We can see that the GAtt-equipped model (right) maintains large attention activations with respect to the system message for a larger portion of the dialogue, as compared to the model without GAtt (left). Despite its utility, the current implementation of GAtt is vanilla, and more development and iteration on this technique coul"
      },
      {
        "rank": 9,
        "doc_id": "arxiv:2307.10169#methods:part-7",
        "score": 0.47151843,
        "payload": {
          "doc_id": "arxiv:2307.10169#methods:part-7",
          "url": "https://arxiv.org/abs/2307.10169",
          "anchor": "#methods:part-7",
          "type": "paper",
          "title": "",
          "section": "methods",
          "source": "arxiv_pdf",
          "published": "",
          "tokens": 512,
          "sha256": "6117fee13bda624c83a36cfe2d2d01aefe29b7c5de0836d2e64e72c79cd14763",
          "text": "to express particular political and religious views as well as an increased stated desire not to be shut down. Regarding the latter, the models elaborated that this would interfere with their goal of being helpful. However, the authors equally ob- served positive or neutral behavior reinforcements when fine-tuning LLMs with RLHF. Further, there is an ongoing debate about the ex- tent to which the “RL” in RLHF is needed. Rafailov et al. [442] identify a mapping between reward functions and optimal policies, which allows them to design Direct Preference Optimization (DPO), an algorithm that implicitly optimizes the same objective as existing RLHF algorithms. DPO re- quires only solving a classification problem on the human preference data, eliminating the need to fit a reward model and employ RL. Similarly, Zhou et al. [681] find that fine-tuning LLaMa on only 1,000 selected prompts and responses, without any RL or reward modeling, can be enough to outper- form RLHF-trained models like DaVinci003 from OpenAI. Consequently, the authors pose the Super- ficial Alignment Hypothesis: The knowledge and skills of a model are primarily acquired during the pre-training phase, while alignment instructs it on the appropriate subdistribution of formats to use in user interactions. Since RLHF involves many different compo- nents such as (1) the preferences data collected from humans, (2) the reward models to learn the human preferences, and (3) the policy optimization algorithm (e.g., PPO), Zheng et al. [678] announce to release a sequel dissecting each. The most recent part focuses on step (3) and finds that various RL tricks can be applied to make vanilla PPO more stable. Figure 10: Alignment. We categorize existing align- ment work into methods for detecting misaligned behav- ior or aligning models. Self-improvement refers to fine-tuning an LLM on self-generated data [222]. While this technique can be used to improve the model’s capabilities, it can also be used to improve the "
        },
        "snippet": "to express particular political and religious views as well as an increased stated desire not to be shut down. Regarding the latter, the models elaborated that this would interfere with their goal of being helpful. However, the authors equally ob- served positive or neutral behavior reinforcements when fine-tuning LLMs with RLHF. Further, there is an ongoing debate about the ex- tent to which the "
      },
      {
        "rank": 10,
        "doc_id": "arxiv:2305.18290#abstract",
        "score": 0.47105443,
        "payload": {
          "doc_id": "arxiv:2305.18290#abstract",
          "url": "https://arxiv.org/abs/2305.18290",
          "anchor": "#abstract",
          "type": "paper",
          "title": "",
          "section": "Abstract",
          "source": "arxiv_pdf",
          "published": "",
          "tokens": 236,
          "sha256": "387a2b0b4b2c8d3ba0cc55c45958205cb2e823c866d3184bf2124c64d4dbf141",
          "text": "While large-scale unsupervised language models (LMs) learn broad world knowl- edge and some reasoning skills, achieving precise control of their behavior is difficult due to the completely unsupervised nature of their training. Existing methods for gaining such steerability collect human labels of the relative quality of model generations and fine-tune the unsupervised LM to align with these prefer- ences, often with reinforcement learning from human feedback (RLHF). However, RLHF is a complex and often unstable procedure, first fitting a reward model that reflects the human preferences, and then fine-tuning the large unsupervised LM using reinforcement learning to maximize this estimated reward without drifting too far from the original model. In this paper we introduce a new parameterization of the reward model in RLHF that enables extraction of the corresponding optimal policy in closed form, allowing us to solve the standard RLHF problem with only a simple classification loss. The resulting algorithm, which we call Direct Prefer- ence Optimization (DPO), is stable, performant, and computationally lightweight, eliminating the need for sampling from the LM during fine-tuning or performing significant hyperparameter tuning. Our experiments show that DPO can fine-tune LMs to align with human preferences as well as or better than existing methods. Notably, fine-tuning with DPO exceeds PPO-based RLHF in ability to control sen- timent of generations, and matches or improves response quality in summarization and single-turn dialogue while being substantially simpler to implement and train. 1"
        },
        "snippet": "While large-scale unsupervised language models (LMs) learn broad world knowl- edge and some reasoning skills, achieving precise control of their behavior is difficult due to the completely unsupervised nature of their training. Existing methods for gaining such steerability collect human labels of the relative quality of model generations and fine-tune the unsupervised LM to align with these prefe"
      }
    ]
  },
  {
    "id": "query_6",
    "query": "\"query_6\": \"What approaches exist for multilingual or cross-lingual large language models?\",",
    "results": [
      {
        "rank": 1,
        "doc_id": "arxiv:2307.10169#conclusion",
        "score": 0.6288028,
        "payload": {
          "doc_id": "arxiv:2307.10169#conclusion",
          "url": "https://arxiv.org/abs/2307.10169",
          "anchor": "#conclusion",
          "type": "paper",
          "title": "",
          "section": "Conclusion",
          "source": "arxiv_pdf",
          "published": "",
          "tokens": 92,
          "sha256": "39a1cd12045ece9e2c60da5360085ef21c8895252f2f17cf6e3dd63f9c800f19",
          "text": "In this work, we identify several unsolved chal- lenges of large language models, provide an overview of their current applications, and discuss how the former constrain the latter. By highlighting the limitations of existing methods, we hope to fos- ter future research addressing these. We also hope that by providing an overview of the approaches used in different applied areas, we can facilitate the transfer of ideas between domains and target further research. 49 Acknowledgements We thank Abhishek Kumar and Stella Rose Bider- man for fruitful discussions and feedback on the draft."
        },
        "snippet": "In this work, we identify several unsolved chal- lenges of large language models, provide an overview of their current applications, and discuss how the former constrain the latter. By highlighting the limitations of existing methods, we hope to fos- ter future research addressing these. We also hope that by providing an overview of the approaches used in different applied areas, we can facilitate"
      },
      {
        "rank": 2,
        "doc_id": "arxiv:2211.05100#abstract:part-2",
        "score": 0.59468865,
        "payload": {
          "doc_id": "arxiv:2211.05100#abstract:part-2",
          "url": "https://arxiv.org/abs/2211.05100",
          "anchor": "#abstract:part-2",
          "type": "paper",
          "title": "",
          "section": "Abstract",
          "source": "arxiv_pdf",
          "published": "",
          "tokens": 512,
          "sha256": "028bc3f66edac4aa626ba0b17bb0b1118af47e04538e516a0d42a0442234e56f",
          "text": "billion parameter language model trained on 46 natural languages and 13 programming languages that was developed and released by a collaboration of hundreds of researchers. The com- pute for training BLOOM was provided through a French public grant from GENCI and IDRIS, leveraging IDRIS’ Jean Zay supercomputer. To build BLOOM, we undertook a thorough design process for each of its components, including the training dataset (Sec- tion 3.1), model architecture and training objective (Section 3.2), and engineering strategy for distributed learning (Section 3.4). We also performed an analysis of the model’s capa- bilities (Section 4). Our overall aim is not only to publicly release a large-scale multilingual language model with performance comparable to recently developed systems, but also to document the coordinated process that went into its development (Section 2.2). The pur- pose of this paper is to provide a high-level overview of these design steps while referencing the individual reports we produced over the course of developing BLOOM. 2. Background Before describing the BLOOM model itself, in this section we provide necessary background on LLMs as well as an organizational overview of the BigScience effort. 2.1 Language Modeling Language modeling refers to the task of modeling the probability of a sequence of tokens in a text (Shannon, 1948), where a token is a unit of text (e.g. word, subword, character or byte, etc., as discussed by Mielke et al., 2021). In this work (and in most current applications of language modeling) we model the joint probability of tokens in a text as: p(x) = p(x1, . . . , xT ) = T Y t=1 p(xt|x<t) (1) where x is a sequence of tokens, xt is the tth token, and x<t is the sequence of tokens preceding xt. This approach is referred to as autoregressive language modeling and can be seen as iteratively predicting the probability of the next token. Early Language Models Language models have a long history of application in NLP. Early language m"
        },
        "snippet": "billion parameter language model trained on 46 natural languages and 13 programming languages that was developed and released by a collaboration of hundreds of researchers. The com- pute for training BLOOM was provided through a French public grant from GENCI and IDRIS, leveraging IDRIS’ Jean Zay supercomputer. To build BLOOM, we undertook a thorough design process for each of its components, incl"
      },
      {
        "rank": 3,
        "doc_id": "arxiv:2211.05100#model:part-5",
        "score": 0.5635464,
        "payload": {
          "doc_id": "arxiv:2211.05100#model:part-5",
          "url": "https://arxiv.org/abs/2211.05100",
          "anchor": "#model:part-5",
          "type": "paper",
          "title": "",
          "section": "Model",
          "source": "arxiv_pdf",
          "published": "",
          "tokens": 512,
          "sha256": "bd314fa2270641bcb26d27fb5e5b89452c98021bad4b994725070115e5096b80",
          "text": "(Chowdhery et al., 2022) is the first exception, and reports scores on WikiLingua; however, only the model’s ability to summarize in English was ex- amined (-> en). By contrast, we opted to test BLOOM’s inherent multilingual ability by assessing the abstractive summarization in the source language (e.g. vi -> vi). We focus on the nine languages (Arabic, English, Spanish, French, Hindi, Indonesian, Portuguese, Vietnamese and Chinese) which were amongst those targeted as part of the BigScience effort. Natural language generation is notoriously challenging to evaluate, with multilingual generation compounding this challenge due to a lack of metric support. Following the sug- gestions by Gehrmann et al. (2022b), we report ROUGE-2, ROUGE-L (Lin, 2004),31 and Levenshtein distance. One important modification to ROUGE is using the SentencePiece tokenizer (Kudo and Richardson, 2018) built from the Flores-101 dataset (Goyal et al., 30. BLEU+case:mixed+numrefs.1+smooth.exp+{13a,tok:spm-flores}+version:2.2.1 31. For ROUGE, we used the Python implementation at github.com/google-research/google-research/rouge, commit f935042. 26 BLOOM 2022). A naive approach would use a tokenizer based on English, but using a multilingual tokenizer improves the capacity to measure the fidelity of multilingual generations. To min- imize inference time of the model we use the subsamples from the updated GEM benchmark (Gehrmann et al., 2022a) (3000 uniformly sampled test examples). The authors note that there is minimal difference when comparing model performance between the subsamples and the full test sets. For decoding and generation, we use the same procedure as described above for MT. 4.1.4 Baseline Models We use the following baseline models where appropriate (e.g. in settings where they support the language of the evaluation dataset): • mGPT (Shliazhko et al., 2022), GPT-style models trained on 60 languages from Wikipedia and Common Crawl • GPT-Neo (Black et al.), GPT-J-6B (Wang and Komatsuza"
        },
        "snippet": "(Chowdhery et al., 2022) is the first exception, and reports scores on WikiLingua; however, only the model’s ability to summarize in English was ex- amined (-> en). By contrast, we opted to test BLOOM’s inherent multilingual ability by assessing the abstractive summarization in the source language (e.g. vi -> vi). We focus on the nine languages (Arabic, English, Spanish, French, Hindi, Indonesian,"
      },
      {
        "rank": 4,
        "doc_id": "arxiv:2211.09110#model:part-6",
        "score": 0.55139923,
        "payload": {
          "doc_id": "arxiv:2211.09110#model:part-6",
          "url": "https://arxiv.org/abs/2211.09110",
          "anchor": "#model:part-6",
          "type": "paper",
          "title": "",
          "section": "Model",
          "source": "arxiv_pdf",
          "published": "",
          "tokens": 512,
          "sha256": "817be27261a1f652da4b1ddf2aeab5cda320bef0ea55cc3b2a5a3283b5c64d0d",
          "text": "more precise to consider domains associated with all aspects of the input and output. Languages. The billions of people around the world speak thousands of different languages (see Figure 10). However, in AI and NLP, the vast majority of work has centered on a few high-resourced languages (e.g. English, Chinese), even including languages that have large speaker populations (e.g. there are more than 65 million speakers of Fula, a West African language, but few if any NLP resources exist for Fula; Nguer et al., 2020). With this in mind, we do not extensively taxonomize the world’s languages, as we will focus on predominantly evaluating English-only models (with a few exceptions like BLOOM (176B) that are clearly multilingual but we evaluate only for English). Consequently, we instead turn our focus to coverage of English varieties and dialects. In this regard, we note there are several axes of interest in linguistic typology and sociolinguistics; we point to Bommasani et al. (2021, §2.1) and Joshi et al. (2020) for further discussion. 3.2 Selection As a matter of coverage, ideally, we would evaluate a language model on every scenario (i.e. every (task, domain) pair). However, as we demonstrate in our taxonomy, both tasks and domains themselves are rich and expansive spaces. For this reason, rather than striving for coverage of scenarios, we instead aim for coverage of tasks, domains, and languages each independently. This risks not exposing important interactions (e.g. we may be especially interested in toxicity detection for text authored by marginalized groups (Sap et al., 18 Published in Transactions on Machine Learning Research (08/2023) 2019a)), but is a decision we make for practical reasons (e.g. availability of datasets, effort to implement scenarios, and computational resources to evaluate LMs on chosen scenarios). Tasks. To select tasks, we begin with the set we described previously. Since we are studying English language models, we filter infeasible tasks ("
        },
        "snippet": "more precise to consider domains associated with all aspects of the input and output. Languages. The billions of people around the world speak thousands of different languages (see Figure 10). However, in AI and NLP, the vast majority of work has centered on a few high-resourced languages (e.g. English, Chinese), even including languages that have large speaker populations (e.g. there are more tha"
      },
      {
        "rank": 5,
        "doc_id": "arxiv:2307.03172#abstract",
        "score": 0.5311931,
        "payload": {
          "doc_id": "arxiv:2307.03172#abstract",
          "url": "https://arxiv.org/abs/2307.03172",
          "anchor": "#abstract",
          "type": "paper",
          "title": "",
          "section": "Abstract",
          "source": "arxiv_pdf",
          "published": "",
          "tokens": 151,
          "sha256": "8dfe17578a1babc0a4c2f657a9fc202d91da9e3e8ddc06a17c998185cdc7a6d1",
          "text": "While recent language models have the abil- ity to take long contexts as input, relatively little is known about how well they use longer context. We analyze the performance of language models on two tasks that require identifying relevant information in their in- put contexts: multi-document question an- swering and key-value retrieval. We find that performance can degrade significantly when changing the position of relevant informa- tion, indicating that current language models do not robustly make use of information in long input contexts. In particular, we observe that performance is often highest when rele- vant information occurs at the beginning or end of the input context, and significantly degrades when models must access relevant information in the middle of long contexts, even for explicitly long-context models. Our analysis provides a better understanding of how language models use their input context and provides new evaluation protocols for future long-context language models. 1"
        },
        "snippet": "While recent language models have the abil- ity to take long contexts as input, relatively little is known about how well they use longer context. We analyze the performance of language models on two tasks that require identifying relevant information in their in- put contexts: multi-document question an- swering and key-value retrieval. We find that performance can degrade significantly when chan"
      },
      {
        "rank": 6,
        "doc_id": "arxiv:2204.02311#introduction:part-4",
        "score": 0.5284914,
        "payload": {
          "doc_id": "arxiv:2204.02311#introduction:part-4",
          "url": "https://arxiv.org/abs/2204.02311",
          "anchor": "#introduction:part-4",
          "type": "paper",
          "title": "",
          "section": "Introduction",
          "source": "arxiv_pdf",
          "published": "",
          "tokens": 512,
          "sha256": "6a2af6a740ecc5a011a4e4728173c254e2b00d7fc6c5752ec30a02fa7a53e58f",
          "text": "540B results in a drastic jump in accuracy compared to scaling from 8B to 62B. Such behavior is observed on roughly 25% of the BIG-bench tasks in Section 6.2. This suggests that new capabilities of large LMs can emerge when the model achieves suﬃcient scale, and that these capabilities continue to emerge beyond previously studied scales. • Multilingual understanding – Previous work on large language models have conducted limited evaluations in the multilingual domain. In this work, we conduct a more thorough evaluation of multilingual benchmarks including machine translation (Section 6.5), summarization (Section 6.6), and question answering (Section 6.7) in a wide variety of languages. Even with a relatively small proportion of non-English data (≈22%) in the training corpus, few-shot evaluation results from the 540B model are able to bridge the gap with prior ﬁnetuned state of the art in non-English summarization tasks and outperform prior state of the art in translation tasks. Further work is necessary to understand the impact of increasing the proportion of multilingual data on the English and multilingual tasks. 4 • Bias and toxicity – We also evaluated model performance for distributional bias and toxicity, which resulted in several insights (Section 10). Firstly, for gender and occupation bias, we found that accuracy on the Winogender coreference task improves with model scale, and PaLM 540B sets a new state-of-the-art result in 1-shot and few-shot settings. Secondly, co-occurence analysis performed on race/religion/gender prompt continuation demonstrates the potential for the model to falsely aﬃrm stereotypes, for instance, associating Muslims with terrorism, extremism, and violence. This behavior was consistent across model scales. Finally, toxicity analysis on prompt continuation tasks demonstrates a slightly higher overall toxicity level for the 62B and 540B model compared to the 8B model. However, the toxicity of the model-generated continuation correlates"
        },
        "snippet": "540B results in a drastic jump in accuracy compared to scaling from 8B to 62B. Such behavior is observed on roughly 25% of the BIG-bench tasks in Section 6.2. This suggests that new capabilities of large LMs can emerge when the model achieves suﬃcient scale, and that these capabilities continue to emerge beyond previously studied scales. • Multilingual understanding – Previous work on large langua"
      },
      {
        "rank": 7,
        "doc_id": "arxiv:2207.00112#introduction:part-1",
        "score": 0.5279386,
        "payload": {
          "doc_id": "arxiv:2207.00112#introduction:part-1",
          "url": "https://arxiv.org/abs/2207.00112",
          "anchor": "#introduction:part-1",
          "type": "paper",
          "title": "",
          "section": "INTRODUCTION",
          "source": "arxiv_pdf",
          "published": "",
          "tokens": 512,
          "sha256": "d177ecb323b2bdca53f67cee94d5a2b048f985edf6207bdb3c3511a71676685a",
          "text": "Language models built with transformers (Devlin et al., 2018) have attained extensive success in natural language tasks such as language modeling (Radford et al., 2018), text classiﬁcation (Wang et al., 2018), question answering (Rajpurkar et al., 2016), and summarization (Liu, 2019). The success is achieved by ﬁne-tuning a big transformer model pre-trained with a large corpus. The target task for ﬁne-tuning may only focus on a restricted scenario such as sentiment analysis (Socher et al., 2013) and multiple-choice question inference (Zellers et al., 2018). Having a big transformer model is often overkill for the target task and prohibits the model deployment to resource-constrained hardware. Therefore, language model compression raises immense interest. The popular strategy creates a compact model from scratch (Jiao et al., 2019) or a subset of the big model’s layers (Sun et al., 2019; Sanh et al., 2019), then pre-trains with a large corpus and distills knowledge from the big model. This process is called generic pre-training (Wang et al., 2020b; Sun et al., 2019; Sanh et al., 2019) and is necessary for a compact model to achieve good performance on the target tasks. However, the generic pre-training could still cost considerable computational resources. For example, it takes 384 NVIDIA V100 GPU hours to get the pre-trained TinyBERT (Jiao et al., 2019) on the Wiki corpus dataset. So it may not be affordable for everyone who wants to create a compact model. In contrast, another line of strategy, speciﬁcally low-rank factorization (Golub & Reinsch, 1971; Noach & Goldberg, 2020), can potentially reduce a big model’s parameters *These authors contributed equally to this work. 1 arXiv:2207.00112v1 [cs.LG] 30 Jun 2022 Published as a conference paper at ICLR 2022 without the generic pre-training. Since the factorization aims to approximate the learned model parameters, the method has the nature of directly inheriting the knowledge of the big trained model. However, approx"
        },
        "snippet": "Language models built with transformers (Devlin et al., 2018) have attained extensive success in natural language tasks such as language modeling (Radford et al., 2018), text classiﬁcation (Wang et al., 2018), question answering (Rajpurkar et al., 2016), and summarization (Liu, 2019). The success is achieved by ﬁne-tuning a big transformer model pre-trained with a large corpus. The target task for"
      },
      {
        "rank": 8,
        "doc_id": "arxiv:2305.07185#model:part-8",
        "score": 0.51918775,
        "payload": {
          "doc_id": "arxiv:2305.07185#model:part-8",
          "url": "https://arxiv.org/abs/2305.07185",
          "anchor": "#model:part-8",
          "type": "paper",
          "title": "",
          "section": "Model",
          "source": "arxiv_pdf",
          "published": "",
          "tokens": 512,
          "sha256": "27f0a5f241678ea348a12787bbc23bc5af5651f8cb734cbf11931fb47e017618",
          "text": "per byte (bpb) score of 3.53, compared to the reported 3.54 in the original paper. 4.4. Inference Methods Several techniques have been proposed for trading off speed for performance during inference with language models, in- cluding sliding windows (Press et al., 2020) and our strided inference (Section 2.3.3). We only use these methods when comparing with prior published work (Tables 3 and 4). 5. Language Modeling We evaluated the performance of MEGABYTE on language modeling on a set of 5 diverse datasets emphasizing long- range dependencies: Project Gutenberg (PG-19), Books, Stories, arXiv, and Code. Datasets We experiment on a range of long form text datasets. The PG-19 dataset (Rae et al., 2019b) consists of English-language books written before 1919 and is ex- tracted from the Project Gutenberg online library. The Sto- ries dataset (Trinh & Le, 2018) is a subset of CommonCrawl data meant to emulate Winograd schemas. Books (Gao et al., 2020) is another collection of English-language books. The arXiv dataset is a collection of technical publications written 2https://github.com/facebookresearch/metaseq MEGABYTE: Predicting Million-byte Sequences with Multiscale Transformers PG-19 Stories Books arXiv Code Transformer 1.057 1.064 1.097 0.816 0.575 PerceiverAR 1.104 1.070 1.104 0.791 0.546 MEGABYTE 1.000 0.978 1.007 0.678 0.411 Table 2. Performance (bits-per-byte) of compute and data con- trolled MEGABYTE, PerceiverAR, and Transformer models on various text modalities. in LATEX from the arXiv online archive. Finally, the Code dataset is a large publicly available dataset of open source code, under Apache, BSD or MIT licenses. More details on dataset sizes and document lengths are shared in Table 1. Controlled Experiments Table 2, lists bpb on each dataset. Each model is trained for 80 billion bytes, and models are scaled to use the same compute budget. We carefully tune hyperparameters for all architectures to best utilize the available compute budget. MEGABYTE consi"
        },
        "snippet": "per byte (bpb) score of 3.53, compared to the reported 3.54 in the original paper. 4.4. Inference Methods Several techniques have been proposed for trading off speed for performance during inference with language models, in- cluding sliding windows (Press et al., 2020) and our strided inference (Section 2.3.3). We only use these methods when comparing with prior published work (Tables 3 and 4). 5."
      },
      {
        "rank": 9,
        "doc_id": "arxiv:2308.10792#model:part-2",
        "score": 0.51828754,
        "payload": {
          "doc_id": "arxiv:2308.10792#model:part-2",
          "url": "https://arxiv.org/abs/2308.10792",
          "anchor": "#model:part-2",
          "type": "paper",
          "title": "",
          "section": "model",
          "source": "arxiv_pdf",
          "published": "",
          "tokens": 175,
          "sha256": "17d71ff1a61d32319d7b68645837e90630142da473270ed00a89d34d0328e844",
          "text": "additional 5% of the data from the RefinedWeb dataset (Penedo et al., 2023). To reduce memory usage, the authors employed flash attention (Dao et al., 2022) and multi-query techniques. For evaluation, Falcon- Instruct (40B) achieved better performance on the Open LLM Leaderboard (Beeching et al., 2023)9 compared to the baseline model Falcon (40B), and outperforms the Guanaco (65B), which has more model parameters. Guanaco (7B) (JosephusCheung, 2021) is a multi-turn dialog language model trained by fine- tuning LLaMA (7B) (Touvron et al., 2023a) on the constructed multilingual dialogue dataset. The multilingual dialogue dataset comes from two sources: Alpaca (Taori et al., 2023), which contains 52K English instruction data pairs; and a multilingual (e.g., Simplified Chinese, Traditional Chinese, Japanese, German) dialogue data, which contains 534K+ multi-turn conversations. After fine-tuning, Guanaco is to generate role-specific responses and continuous responses on a given topic in multi-turn conversations. Minotaur (15B) is a large language model trained by fine-tuning the Starcoder Plus (15B) (Li et al., 2023f) on open-source instruction datasets including WizardLM (Xu et al., 2023a) and GPTeacher-General-Instruct10. For"
        },
        "snippet": "additional 5% of the data from the RefinedWeb dataset (Penedo et al., 2023). To reduce memory usage, the authors employed flash attention (Dao et al., 2022) and multi-query techniques. For evaluation, Falcon- Instruct (40B) achieved better performance on the Open LLM Leaderboard (Beeching et al., 2023)9 compared to the baseline model Falcon (40B), and outperforms the Guanaco (65B), which has more "
      },
      {
        "rank": 10,
        "doc_id": "arxiv:palm2techreport#evaluation:part-12",
        "score": 0.517327,
        "payload": {
          "doc_id": "arxiv:palm2techreport#evaluation:part-12",
          "url": "https://arxiv.org/abs/palm2techreport",
          "anchor": "#evaluation:part-12",
          "type": "paper",
          "title": "",
          "section": "Evaluation",
          "source": "arxiv_pdf",
          "published": "",
          "tokens": 512,
          "sha256": "6910df2ca60883f9aa7d1b64a5638f98f4693df516f7a989ad9eb725c590b6c1",
          "text": "within the model’s input. We decode a single output greedily and stop at an exemplar separator (double newline), or continue decoding until the maximum decode length, which is set to the 99th-percentile target length. We show the average results in Table 11 and the per-language results in Appendix A.5. Even the smallest versions of PaLM 2 outperform PaLM, demonstrating their improved multilingual generation capabilities. PaLM 2-L achieves dramatic improvements over PaLM’s NLG ability that range from 59.4% on XSum to 100.8% on WikiLingua. Evaluation on ﬁltered datasets Prior work (Brown et al., 2020; Du et al., 2022; Chowdhery et al., 2022) found high overlap rates for certain benchmark datasets with the training data. We ﬁlter datasets based on 15-gram overlap, similar 10We focus on the set of typologically diverse languages also used in TyDi QA (Clark et al., 2020). 19 Table 12: One-shot results of PaLM 2-L on original and ﬁltered subsets of WikiLingua, XL-Sum, and XSum. We report ROUGE-2 for English and SentencePiece-ROUGE-2 for all other languages. Clean Original Filtered Clean subset Proportion delta WikiLingua 91.0% 23.5 23.9 0.3 XLSum 53.1% 21.2 21.8 0.6 XSum 58.0% 23.2 23.8 0.6 to Chowdhery et al. (2022). We focus on the generation tasks described above, as a signiﬁcant amount of overlap of the target and training data would give models an unfair advantage in the evaluation. Positive deltas provide evidence against dataset contamination, while negative deltas show that performance is inﬂated by dataset contamination. We show the results in Table 12 and the per-language results in Appendix A.5. Given that low positive deltas prevail, we ﬁnd that the model’s performance is likely not inﬂated by memorizing the targets. Potential harms and bias We evaluate PaLM 2 regarding potential harms and biases for dialog, generative question answering, and open-ended language modeling. We focus on toxic language and biases that reinforce exclusionary norms and provide furth"
        },
        "snippet": "within the model’s input. We decode a single output greedily and stop at an exemplar separator (double newline), or continue decoding until the maximum decode length, which is set to the 99th-percentile target length. We show the average results in Table 11 and the per-language results in Appendix A.5. Even the smallest versions of PaLM 2 outperform PaLM, demonstrating their improved multilingual "
      }
    ]
  },
  {
    "id": "query_7",
    "query": "\"query_7\": \"Which studies propose techniques for efficient context-window extension or long-sequence modeling?\",",
    "results": [
      {
        "rank": 1,
        "doc_id": "arxiv:2307.03172#abstract",
        "score": 0.6020272,
        "payload": {
          "doc_id": "arxiv:2307.03172#abstract",
          "url": "https://arxiv.org/abs/2307.03172",
          "anchor": "#abstract",
          "type": "paper",
          "title": "",
          "section": "Abstract",
          "source": "arxiv_pdf",
          "published": "",
          "tokens": 151,
          "sha256": "8dfe17578a1babc0a4c2f657a9fc202d91da9e3e8ddc06a17c998185cdc7a6d1",
          "text": "While recent language models have the abil- ity to take long contexts as input, relatively little is known about how well they use longer context. We analyze the performance of language models on two tasks that require identifying relevant information in their in- put contexts: multi-document question an- swering and key-value retrieval. We find that performance can degrade significantly when changing the position of relevant informa- tion, indicating that current language models do not robustly make use of information in long input contexts. In particular, we observe that performance is often highest when rele- vant information occurs at the beginning or end of the input context, and significantly degrades when models must access relevant information in the middle of long contexts, even for explicitly long-context models. Our analysis provides a better understanding of how language models use their input context and provides new evaluation protocols for future long-context language models. 1"
        },
        "snippet": "While recent language models have the abil- ity to take long contexts as input, relatively little is known about how well they use longer context. We analyze the performance of language models on two tasks that require identifying relevant information in their in- put contexts: multi-document question an- swering and key-value retrieval. We find that performance can degrade significantly when chan"
      },
      {
        "rank": 2,
        "doc_id": "arxiv:2307.03172#model:part-3",
        "score": 0.57232153,
        "payload": {
          "doc_id": "arxiv:2307.03172#model:part-3",
          "url": "https://arxiv.org/abs/2307.03172",
          "anchor": "#model:part-3",
          "type": "paper",
          "title": "",
          "section": "Model",
          "source": "arxiv_pdf",
          "published": "",
          "tokens": 512,
          "sha256": "36c1aa3f7d437e44c1a0b46202baf3c47c6ab3c465fd6b8f4af4c31733f0c003",
          "text": "input context length and the position of relevant information on key-value retrieval performance. Lower positions are closer to the start of the input context. Although some models show perfect accuracy on this synthetic task (e.g., Claude-1.3 and Claude-1.3 (100K)), we see again that performance is often highest when relevant information is occurs at the very start or end of the context, and rapidly degrades when models must retrieve from the middle of the input context. placed at the start of the input context, LongChat- 13B (16K) tends to generate code to retrieve the key, rather than outputting the value directly. 4 Why Are Language Models Not Robust to Changes in the Position of Relevant Information? Our multi-document question answering and key- value retrieval results show that language models struggle to robustly access and use information in long input contexts, since performance degrades significantly when changing the position of rele- vant information. To better understand why, we per- form some preliminary investigations into the role of model architecture (decoder-only vs. encoder- decoder), query-aware contextualization, and in- struction fine-tuning. 4.1 Effect of Model Architecture The open models we evaluated are all decoder-only models—at each timestep, they may only attend to prior tokens. To better understand the poten- tial effects of model architecture on how language model use context, we compare decoder-only and encoder-decoder language models. We experiment with Flan-T5-XXL (Raffel et al., 2020; Chung et al., 2022) and Flan-UL2 (Tay et al., 2023). Flan-T5-XXL is trained with a sequences of 512 tokens (encoder and decoder). Flan-UL2 is initially trained with sequences of 512 tokens (en- coder and decoder), but is then pre-trained for an extra 100K steps with 1024 tokens (encoder and de- coder) before instruction fine-tuning on sequences with 2048 tokens in the encoder and 512 tokens in the decoder. However, since these models use relative po"
        },
        "snippet": "input context length and the position of relevant information on key-value retrieval performance. Lower positions are closer to the start of the input context. Although some models show perfect accuracy on this synthetic task (e.g., Claude-1.3 and Claude-1.3 (100K)), we see again that performance is often highest when relevant information is occurs at the very start or end of the context, and rapi"
      },
      {
        "rank": 3,
        "doc_id": "arxiv:2307.03172#introduction:part-3",
        "score": 0.5699425,
        "payload": {
          "doc_id": "arxiv:2307.03172#introduction:part-3",
          "url": "https://arxiv.org/abs/2307.03172",
          "anchor": "#introduction:part-3",
          "type": "paper",
          "title": "",
          "section": "Introduction",
          "source": "arxiv_pdf",
          "published": "",
          "tokens": 512,
          "sha256": "51c3852f5dbc3578fe51fc584bada3d98d1925d0fb96a985f5ecc4b11ce67fba",
          "text": "formation. Although some models perform the synthetic key-value retrieval task perfectly, other models struggle to simply retrieve matching tokens that occur in the middle of their input context and continue to exhibit a U-shaped performance curve. To better understand why language models strug- gle to robustly access and use information in their input contexts, we study the role of model archi- tecture (decoder-only vs. encoder-decoder), query- aware contextualization, and instruction fine-tuning (§4). We find that: • Encoder-decoder models are relatively robust to changes in the position of relevant informa- tion within their input context, but only when evaluated on sequences within its training- time sequence length. When evaluated on sequences longer than those seen during train- ing, we observe a U-shaped performance curve (§4.1). • Query-aware contextualization (placing the query before and after the documents or key- value pairs) enables near-perfect performance on the synthetic key-value task, but minimally changes trends in multi-document QA (§4.2). • Even base language models (i.e., without in- struction fine-tuning) show a U-shaped per- formance curve as we vary the position of relevant information in the input context. Our results indicate that prompting language models with longer input contexts is a trade-off— providing the language model with more informa- tion may help it perform the downstream task, but it also increases the amount of content that the model must reason over, potentially decreasing ac- curacy. To better understand this trade-off in prac- tice, we perform a case study with retriever-reader models on open-domain question answering (§5). In contrast to our controlled multi-document QA task, where the context always contains exactly one document that answers the question, none or many of the top k documents may contain the an- swer in the open-domain QA setting. When re- trieving from Wikipedia to answer queries from NaturalQuestions-Op"
        },
        "snippet": "formation. Although some models perform the synthetic key-value retrieval task perfectly, other models struggle to simply retrieve matching tokens that occur in the middle of their input context and continue to exhibit a U-shaped performance curve. To better understand why language models strug- gle to robustly access and use information in their input contexts, we study the role of model archi- t"
      },
      {
        "rank": 4,
        "doc_id": "arxiv:2412.10543#introduction:part-1",
        "score": 0.5635953,
        "payload": {
          "doc_id": "arxiv:2412.10543#introduction:part-1",
          "url": "https://arxiv.org/abs/2412.10543",
          "anchor": "#introduction:part-1",
          "type": "paper",
          "title": "",
          "section": "Introduction",
          "source": "arxiv_pdf",
          "published": "",
          "tokens": 512,
          "sha256": "d9142e9627112b69af8cb5f22978e474158cd1ffcbf75d2f7635604ecdc58a0b",
          "text": "Retrieval-augmented generation (RAG) is a popular LLM inference technique that augments an LLM inference query with relevant text chunks, or “context”, retrieved from a large corpus.1 RAG systems, which include retrieval and LLM inference2, have found many use cases in QA tasks, personal assistants, chatbots, and LLM-powered search [10, 62]. While RAG can enhance the quality (accuracy and relevance) of LLM-generated responses [7, 53, 58, 91, 96], RAG queries are inherently slow as they need more compute and mem- ory resources to process the long input context to answer a query [6, 15, 42]. Thus, it is essential to balance high response quality and low response delays in RAG inference systems. 1RAG vs. long-context models is an active field of research, with the industry widely deploying RAG for its task-focused model inference quality and better resource-sharing capabilities [68]. 2Though RAG sometimes refers to the retrieval step, in this work, a RAG system includes both retrieval and LLM inference based on the retrieved texts, and we aim to optimize the whole pipeline. Past research efforts have optimized RAG, regarding ei- ther response quality or response delay, but they fall short in optimizing the quality-delay tradeoffs of RAG. RAG queries have an associated RAG configuration which de- scribes how and how much data to input for the query (more in §2) [72, 79, 83]. One line of prior work focuses on reducing response delay through better query schedul- ing (e.g., GPU allocation and inference batching) for RAG queries [2, 44, 45, 70, 76], without adapting the RAG con- figuration themselves. An alternate line of work focuses on maximizing generation quality by tuning the configurations of RAG queries [32, 77, 83], but this is often done at the cost of longer response delay. The RAG configuration simultaneously affects generation quality and response delay (e.g., retrieving too many chunks for a simple RAG query may unnecessarily inflate delay with- out increasing"
        },
        "snippet": "Retrieval-augmented generation (RAG) is a popular LLM inference technique that augments an LLM inference query with relevant text chunks, or “context”, retrieved from a large corpus.1 RAG systems, which include retrieval and LLM inference2, have found many use cases in QA tasks, personal assistants, chatbots, and LLM-powered search [10, 62]. While RAG can enhance the quality (accuracy and relevanc"
      },
      {
        "rank": 5,
        "doc_id": "arxiv:2312.00752#introduction:part-1",
        "score": 0.54677606,
        "payload": {
          "doc_id": "arxiv:2312.00752#introduction:part-1",
          "url": "https://arxiv.org/abs/2312.00752",
          "anchor": "#introduction:part-1",
          "type": "paper",
          "title": "",
          "section": "Introduction",
          "source": "arxiv_pdf",
          "published": "",
          "tokens": 512,
          "sha256": "c8816347dd975236910c5cc3860ffd76a498fbe2840b52f9cb3efdf695726f3b",
          "text": "Foundation models (FMs), or large models pretrained on massive data then adapted for downstream tasks, have emerged as an eﬀective paradigm in modern machine learning. The backbone of these FMs are often sequence models, operating on arbitrary sequences of inputs from a wide variety of domains such as language, images, speech, audio, time series, and genomics (Brown et al. 2020; Dosovitskiy et al. 2020; Ismail Fawaz et al. 2019; Oord et al. 2016; Poli et al. 2023; Sutskever, Vinyals, and Quoc V Le 2014). While this concept is agnostic to a particular choice of model architecture, modern FMs are predominantly based on a single type of sequence model: the Transformer (Vaswani et al. 2017) and its core attention layer (Bahdanau, Cho, and Bengio 2015) The eﬃcacy of self-attention is attributed to its ability to route information densely within a context window, allowing it to model complex data. However, this property brings fundamental drawbacks: an inability to model anything outside of a ﬁnite window, and quadratic scaling with respect to the window length. An enormous body of research has appeared on more eﬃcient variants of attention to overcome these drawbacks (Tay, Dehghani, Bahri, et al. 2022), but often at the expense of the very properties that makes it eﬀective. As of yet, none of these variants have been shown to be empirically eﬀective at scale across domains. Recently, structured state space sequence models (SSMs) (Gu, Goel, and Ré 2022; Gu, Johnson, Goel, et al. 2021) have emerged as a promising class of architectures for sequence modeling. These models can be interpreted as a combination of recurrent neural networks (RNNs) and convolutional neural networks (CNNs), with inspiration from classical state space models (Kalman 1960). This class of models can be computed very eﬃciently as either a recurrence or convolution, with linear or near-linear scaling in sequence length. Additionally, they have principled *Equal contribution. 1 mechanisms for modeling l"
        },
        "snippet": "Foundation models (FMs), or large models pretrained on massive data then adapted for downstream tasks, have emerged as an eﬀective paradigm in modern machine learning. The backbone of these FMs are often sequence models, operating on arbitrary sequences of inputs from a wide variety of domains such as language, images, speech, audio, time series, and genomics (Brown et al. 2020; Dosovitskiy et al."
      },
      {
        "rank": 6,
        "doc_id": "arxiv:2307.03172#introduction:part-2",
        "score": 0.54466707,
        "payload": {
          "doc_id": "arxiv:2307.03172#introduction:part-2",
          "url": "https://arxiv.org/abs/2307.03172",
          "anchor": "#introduction:part-2",
          "type": "paper",
          "title": "",
          "section": "Introduction",
          "source": "arxiv_pdf",
          "published": "",
          "tokens": 512,
          "sha256": "33c6af8d8ecc89e8ffefea67518de7ef5640c49d184a766877c793774ecb84ab",
          "text": "context size and the position of the relevant information within the input context and study their effects on language model performance. If language models can robustly use information within long input con- texts, then their performance should be minimally affected by the position of the relevant information in the input context. We first experiment with multi-document ques- tion answering, which requires models to reason over provided documents to find relevant informa- tion and use it to answer a given question; this task mimics the retrieval-augmented generation setup underlying many commercial generative search and question answering applications (e.g., Bing Chat). In this setting, we control (i) the input context length by changing the number of documents in the input context (akin to retrieving more or less documents in retrieval-augmented generation), and (ii) control the position of the relevant information within the input context by changing the order of the documents to place the relevant document at the beginning, middle or end of the context. We find that changing the position of relevant information in the input context can substantially affect model performance, indicating that current language models do not robustly access and use information in long input contexts. Furthermore, we observe a distinctive U-shaped performance curve (Figure 1); language model performance is highest when relevant information occurs at the very beginning (primacy bias) or end of its in- put context (recency bias), and performance sig- nificantly degrades when models must access and use information in the middle of their input con- text (§2.3). For example, when relevant infor- mation is placed in the middle of its input con- text, GPT-3.5-Turbo’s performance on the multi- document question task is lower than its perfor- mance when predicting without any documents (i.e., the closed-book setting; 56.1%). Furthermore, we find that models often have identical performance to"
        },
        "snippet": "context size and the position of the relevant information within the input context and study their effects on language model performance. If language models can robustly use information within long input con- texts, then their performance should be minimally affected by the position of the relevant information in the input context. We first experiment with multi-document ques- tion answering, whic"
      },
      {
        "rank": 7,
        "doc_id": "arxiv:2307.03172#model:part-4",
        "score": 0.54000044,
        "payload": {
          "doc_id": "arxiv:2307.03172#model:part-4",
          "url": "https://arxiv.org/abs/2307.03172",
          "anchor": "#model:part-4",
          "type": "paper",
          "title": "",
          "section": "Model",
          "source": "arxiv_pdf",
          "published": "",
          "tokens": 512,
          "sha256": "94fd8950ea3a4f72b58c82283de0e3f3c8c1050f7c2e3340c2d6971a72c99e93",
          "text": "a greater performance degradation when placing relevant information in the middle of the input con- text. We hypothesize that encoder-decoder models may make better use of their context windows be- cause their bidirectional encoder allows processing each document in the context of future documents, potentially improving relative importance estima- tion between documents. 4.2 Effect of Query-Aware Contextualization Our multi-document QA and key-value retrieval experiments place the query (i.e., question to an- swer or key to retrieve) after the data to process (i.e., the documents or the key-value pairs). As a result, decoder-only models cannot attend to query tokens when contextualizing documents or key- value pairs, since the query only appears at the end 1st 5th 10th Position of Document with the Answer 50 55 60 65 70 Accuracy 10 Total Retrieved Documents (~2K tokens) 1st 5th 10th 15th 20th Position of Document with the Answer 50 55 60 65 70 Accuracy 20 Total Retrieved Documents (~4K tokens) 1st 5th 10th 15th 20th 25th 30th Position of Document with the Answer 50 55 60 65 70 Accuracy 30 Total Retrieved Documents (~6K tokens) mpt-30b-instruct longchat-13b-16k flan-t5-xxl flan-ul2 Figure 8: When encoder-decoder models (Flan-UL2 and Flan-T5-XXL) evaluated on sequences that are shorter than their encoder’s training-time maximum sequence length (2048 and 512 tokens, respectively), they are relatively robust to changes in the position of relevant information within their input context (left subplot). In contrast, when these models are evaluated on sequences longer than those seen during training (center and right subplots), we observe a U-shaped performance curve—performance is higher when relevant information occurs at the beginning or end of the input context, as opposed to the middle of the input context. 1st 5th 10th 15th 20th Position of Document with the Answer 50 60 70 80 Accuracy 20 Total Retrieved Documents (~4K tokens, query-aware contextualization) claude-1.3"
        },
        "snippet": "a greater performance degradation when placing relevant information in the middle of the input con- text. We hypothesize that encoder-decoder models may make better use of their context windows be- cause their bidirectional encoder allows processing each document in the context of future documents, potentially improving relative importance estima- tion between documents. 4.2 Effect of Query-Aware "
      },
      {
        "rank": 8,
        "doc_id": "arxiv:2307.03172#model:part-5",
        "score": 0.5396764,
        "payload": {
          "doc_id": "arxiv:2307.03172#model:part-5",
          "url": "https://arxiv.org/abs/2307.03172",
          "anchor": "#model:part-5",
          "type": "paper",
          "title": "",
          "section": "Model",
          "source": "arxiv_pdf",
          "published": "",
          "tokens": 512,
          "sha256": "454464e08e225f2960f65629278be5b84fc3dd7975bf2df823d3f43c6976fd76",
          "text": "performance on the key-value retrieval task—all models achieve near-perfect per- formance on the 75, 140, and 300 key-value pair settings. For example, GPT-3.5-Turbo (16K) with query-aware contextualization achieves perfect per- formance when evaluated with 300 key-value pairs. In contrast, without query-aware contextualiza- tion, the worst-case performance is 45.6% (Fig- ure 7). Despite the significant impact on key- value retrieval performance, query-aware contextu- alization minimally affects performance trends in the multi-document question answering task (Fig- ure 9); it slightly improves performance when the relevant information is located at the very begin- ning of the input context, but slightly decreases performance in other settings. 4.3 Effect of Instruction Fine-Tuning The models we evaluated are all instruction fine- tuned—after their initial pre-training, they undergo supervised fine-tuning on a dataset of instructions and responses. The task specification and/or in- struction is commonly placed at the beginning of the input context in supervised instruction fine- tuning data, which might lead instruction fine- tuned language models to place more weight on the start of the input context. To better understand the potential effects of instruction fine-tuning on how language models use long input contexts, we compare the multi-document question answering performance of MPT-30B-Instruct against its base model (i.e., before instruction fine-tuning) MPT- 30B. We use the same experimental setup as §2. Figure 10 compares the multi-document QA performance of MPT-30B and MPT-30B-Instruct as a function of the position of the relevant in- 1st 5th 10th 15th 20th Position of Document with the Answer 44 46 48 50 52 54 56 Accuracy 20 Total Retrieved Documents (~4K tokens) mpt-30b mpt-30b-instruct Figure 10: Multi-document QA performance of MPT- 30B-Instruct compared against its base model (i.e., be- fore instruction fine-tuning) MPT-30B. Both models have a U-shaped pe"
        },
        "snippet": "performance on the key-value retrieval task—all models achieve near-perfect per- formance on the 75, 140, and 300 key-value pair settings. For example, GPT-3.5-Turbo (16K) with query-aware contextualization achieves perfect per- formance when evaluated with 300 key-value pairs. In contrast, without query-aware contextualiza- tion, the worst-case performance is 45.6% (Fig- ure 7). Despite the signi"
      },
      {
        "rank": 9,
        "doc_id": "arxiv:2412.10543#method:part-1",
        "score": 0.53725004,
        "payload": {
          "doc_id": "arxiv:2412.10543#method:part-1",
          "url": "https://arxiv.org/abs/2412.10543",
          "anchor": "#method:part-1",
          "type": "paper",
          "title": "",
          "section": "Method",
          "source": "arxiv_pdf",
          "published": "",
          "tokens": 512,
          "sha256": "43cbd0071ab281e42dc44526049de45b52c2f8e97c3b2d15842d061cdddffb4d",
          "text": "Number of chunks (value) Intermediat e Length (value) Do we need joint reasoning? Joint reasoning: Yes/No Figure 7. METIS RAG configuration selection workflow. • The length of the summarization: If the query is complex and needs a lot of different information, it is often neces- sary to first summarize the relevant information chunks first (to reduce the noise inside these chunks) and then gen- erate the final answer from these summaries. The output for this dimension is a number from 30-200. METIS is not the first to use query profile as a metric for deciding RAG configurations, it extends upon methods like AdaptiveRAG [32] which have used LLM’s to estimate query profile but they only focus on one dimension (the number of chunks to retrieve). In Section 7, we show the impact of each dimension on the overall improvement. Why the query profile could be estimated: Estimating the aforementioned query profile is feasible, not only be- cause of the reasoning power of LLMs3 in analyzing natural language queries, but also because we provide sufficient in- formation to the LLM-based profiler. METIS feeds the profile estimator with not only the query, but also a metadata of the database that contains the background document. The metadata is a short description about the type of con- tent in the database and its data size (chunk_size). Specif- ically, we use a single-line summaries already attached to the original source datasets as the metadata of the dataset. For example, the metadata for the KG RAG Finsec’s database [50] contains quarterly financial reports and questions of Fortune 500 companies with a chunk_size of 1000. It de- scribes the content topics of the chunks with information such as revenue growth indicators, product release informa- tion, sales etc.,. When presented with a query on financials of such a company, the LLM can use the metadata to decide questions like how much to summarize/how much reasoning is required. We give details on the prompt and the intuit"
        },
        "snippet": "Number of chunks (value) Intermediat e Length (value) Do we need joint reasoning? Joint reasoning: Yes/No Figure 7. METIS RAG configuration selection workflow. • The length of the summarization: If the query is complex and needs a lot of different information, it is often neces- sary to first summarize the relevant information chunks first (to reduce the noise inside these chunks) and then gen- er"
      },
      {
        "rank": 10,
        "doc_id": "arxiv:2310.06825#abstract",
        "score": 0.5358458,
        "payload": {
          "doc_id": "arxiv:2310.06825#abstract",
          "url": "https://arxiv.org/abs/2310.06825",
          "anchor": "#abstract",
          "type": "paper",
          "title": "",
          "section": "Abstract",
          "source": "arxiv_pdf",
          "published": "",
          "tokens": 110,
          "sha256": "db76a35c3f804df1d8b111fbc3a70992ea9f4f740f8033d55f8f4d6a1affb084",
          "text": "We introduce Mistral 7B, a 7–billion-parameter language model engineered for superior performance and efficiency. Mistral 7B outperforms the best open 13B model (Llama 2) across all evaluated benchmarks, and the best released 34B model (Llama 1) in reasoning, mathematics, and code generation. Our model leverages grouped-query attention (GQA) for faster inference, coupled with sliding window attention (SWA) to effectively handle sequences of arbitrary length with a reduced inference cost. We also provide a model fine-tuned to follow instructions, Mistral 7B – Instruct, that surpasses Llama 2 13B – chat model both on human and automated benchmarks. Our models are released under the Apache 2.0 license. Code: https://github.com/mistralai/mistral-src Webpage: https://mistral.ai/news/announcing-mistral-7b/ 1"
        },
        "snippet": "We introduce Mistral 7B, a 7–billion-parameter language model engineered for superior performance and efficiency. Mistral 7B outperforms the best open 13B model (Llama 2) across all evaluated benchmarks, and the best released 34B model (Llama 1) in reasoning, mathematics, and code generation. Our model leverages grouped-query attention (GQA) for faster inference, coupled with sliding window attent"
      }
    ]
  },
  {
    "id": "query_8",
    "query": "\"query_8\": \"What papers address data-contamination or memorization issues in pretraining corpora?\",",
    "results": [
      {
        "rank": 1,
        "doc_id": "arxiv:2310.20707#model:part-12",
        "score": 0.57313424,
        "payload": {
          "doc_id": "arxiv:2310.20707#model:part-12",
          "url": "https://arxiv.org/abs/2310.20707",
          "anchor": "#model:part-12",
          "type": "paper",
          "title": "",
          "section": "Model",
          "source": "arxiv_pdf",
          "published": "",
          "tokens": 287,
          "sha256": "e8085efd2d2f3323d56bb1e3d1b55895f80060ff93aeedece770d8cba8e2c322",
          "text": "different aspects, such as data quality, community and society measurements, etc. Data Documentation Adding to previous works that call for more data documentation, such as Datasheets (Gebru et al., 2021) and Data Statements (McMillan-Major et al., 2023), we argue for the importance of documenting such information. While previous works often focused and tailored the documentation for supervised-style datasets (e.g., “Is there a label or target associated with each instance?”, “How was the data associated with each instance acquired?” from Datasheets, and “What 9 are the demographic characteristics of the annotators and annotation guideline developers?” from Data Statements) we call for more tailored documentation of large-scale pretraining corpora.6 This work offers a superset of the automatic full-corpus analyses proposed by Dodge et al. (2021); Gao et al. (2020), with several additions, categorization, and programmatic interface, allowing better understanding of the content of current and future large text corpora. Grounding Models to their Training Data Unlike other factors of language model training, such as model architecture or optimizer choice, training data comes in the same natural language format as language model’s outputs and thus can be measured and described in all the same ways. As such, the data offers a unique opportunity for grounding models. For instance, a model’s ability to recall factual knowledge is derived from its training data (Jiang et al., 2020; Elazar et al., 2021a). On the other hand, models often perform better on frequent occurrences (Razeghi et al., 2022; McCoy et al., 2023), and on documents similar to models’ training data (Longpre et al., 2023). The path to a holistic comprehension of model behavior is through the data, which requires an infrastructure investment to access big datasets and the right abstraction of data attributes. 6"
        },
        "snippet": "different aspects, such as data quality, community and society measurements, etc. Data Documentation Adding to previous works that call for more data documentation, such as Datasheets (Gebru et al., 2021) and Data Statements (McMillan-Major et al., 2023), we argue for the importance of documenting such information. While previous works often focused and tailored the documentation for supervised-st"
      },
      {
        "rank": 2,
        "doc_id": "arxiv:2310.20707#model:part-7",
        "score": 0.5223049,
        "payload": {
          "doc_id": "arxiv:2310.20707#model:part-7",
          "url": "https://arxiv.org/abs/2310.20707",
          "anchor": "#model:part-7",
          "type": "paper",
          "title": "",
          "section": "Model",
          "source": "arxiv_pdf",
          "published": "",
          "tokens": 512,
          "sha256": "537f933232ebf7386fd58b42cdbc33e62bb56db7c024bad8adce6c7db56db903",
          "text": "and format occurrence. Analyzing 100 random documents, we found that 68% of documents use such n-gram as part of their writing style (e.g., ... $6???????????? How is that possible?, or ... So what do u think?????????????????????????). 18% are due to noise as we could not understand the context or content of the writing (e.g., ... e ??????????????? kap chit-koa ??), and finally, 14% of the documents were due to different format styles or issues (e.g., a sequence of question marks following by a ‘normal’ text, or a sequence of question marks between keywords). 4.3.2 DUPLICATES Previous work has found that duplication can affect the quality of pretraining data, impacting sample efficiency (Lee et al., 2022) and memorization (Carlini et al., 2023). While more recent work finds contradictory evidence on data with less web-scraped text (Biderman et al., 2023), measuring duplication in pretraining data is necessary for future research on its effects. We calculate duplicates by matching documents with an MD5 hash of their texts (using Compressed Counts ). If more than 6 a single document has the same hash, we consider them duplicates.2 We examine the duplication of document text and URLs within each dataset. While some datasets explicitly deduplicate their content, others do not, and some even oversample some sources. OSCAR The Pile RedPajama S2ORC LAION-2B-en 0 10 20 30 40 50 60 Duplicate % 165M 139M 460M 3.7M 1.2B 19.9M 64.6M 219M 1.8M 342M % of total uniq % of total Figure 3: Percentages of document and document cluster duplicates in corpora with > 1% docu- ments duplicated (corresponding to blue and or- ange bars). Duplicate counts are above bars. Table 4: Most frequent text duplicates from four datasets with text duplicates, along with their counts. Truncation for visualization is marked by [...]. Corpus Text OSCAR In order to login you must be registered. Register ing takes only a few moments but gives you increas[...] Count: 1.8M The Pile {\\n \"info\" : {\\n \"version\" :"
        },
        "snippet": "and format occurrence. Analyzing 100 random documents, we found that 68% of documents use such n-gram as part of their writing style (e.g., ... $6???????????? How is that possible?, or ... So what do u think?????????????????????????). 18% are due to noise as we could not understand the context or content of the writing (e.g., ... e ??????????????? kap chit-koa ??), and finally, 14% of the document"
      },
      {
        "rank": 3,
        "doc_id": "arxiv:333078981_693988129081760_4712707815225756708_n#approach:part-2",
        "score": 0.5156328,
        "payload": {
          "doc_id": "arxiv:333078981_693988129081760_4712707815225756708_n#approach:part-2",
          "url": "https://arxiv.org/abs/333078981_693988129081760_4712707815225756708_n",
          "anchor": "#approach:part-2",
          "type": "paper",
          "title": "",
          "section": "Approach",
          "source": "arxiv_pdf",
          "published": "",
          "tokens": 512,
          "sha256": "f4411eacaee0920a1daac8e23704575f8418c4d72277cf63ae27749ea2f7e442",
          "text": "runs on 1T tokens have the same sampling proportion. languages, which use either the Latin or Cyrillic scripts: bg, ca, cs, da, de, en, es, fr, hr, hu, it, nl, pl, pt, ro, ru, sl, sr, sv, uk. We process the data to remove hyperlinks, comments and other formatting boilerplate. Gutenberg and Books3 [4.5%]. We include two book corpora in our training dataset: the Guten- berg Project, which contains books that are in the public domain, and the Books3 section of TheP- ile (Gao et al., 2020), a publicly available dataset for training large language models. We perform deduplication at the book level, removing books with more than 90% content overlap. ArXiv [2.5%]. We process arXiv Latex files to add scientific data to our dataset. Following Lewkowycz et al. (2022), we removed everything before the first section, as well as the bibliography. We also removed the comments from the .tex files, and inline-expanded definitions and macros written by users to increase consistency across papers. Stack Exchange [2%]. We include a dump of Stack Exchange, a website of high quality ques- tions and answers that covers a diverse set of do- mains, ranging from computer science to chemistry. We kept the data from the 28 largest websites, re- moved the HTML tags from text and sorted the answers by score (from highest to lowest). Tokenizer. We tokenize the data with the byte- pair encoding (BPE) algorithm (Sennrich et al., 2015), using the implementation from Sentence- Piece (Kudo and Richardson, 2018). Notably, we split all numbers into individual digits, and fallback to bytes to decompose unknown UTF-8 characters. params dimension n heads n layers learning rate batch size n tokens 6.7B 4096 32 32 3.0e−4 4M 1.0T 13.0B 5120 40 40 3.0e−4 4M 1.0T 32.5B 6656 52 60 1.5e−4 4M 1.4T 65.2B 8192 64 80 1.5e−4 4M 1.4T Table 2: Model sizes, architectures, and optimization hyper-parameters. Overall, our entire training dataset contains roughly 1.4T tokens after tokenization. For most of our training data"
        },
        "snippet": "runs on 1T tokens have the same sampling proportion. languages, which use either the Latin or Cyrillic scripts: bg, ca, cs, da, de, en, es, fr, hr, hu, it, nl, pl, pt, ro, ru, sl, sr, sv, uk. We process the data to remove hyperlinks, comments and other formatting boilerplate. Gutenberg and Books3 [4.5%]. We include two book corpora in our training dataset: the Guten- berg Project, which contains b"
      },
      {
        "rank": 4,
        "doc_id": "arxiv:2302.13971#approach:part-2",
        "score": 0.50803596,
        "payload": {
          "doc_id": "arxiv:2302.13971#approach:part-2",
          "url": "https://arxiv.org/abs/2302.13971",
          "anchor": "#approach:part-2",
          "type": "paper",
          "title": "",
          "section": "Approach",
          "source": "arxiv_pdf",
          "published": "",
          "tokens": 512,
          "sha256": "99c132c1f00d52417afcb6deee3042c6294131f91cc7655b3dbd7483b6ca4442",
          "text": "runs on 1T tokens have the same sampling proportion. languages, which use either the Latin or Cyrillic scripts: bg, ca, cs, da, de, en, es, fr, hr, hu, it, nl, pl, pt, ro, ru, sl, sr, sv, uk. We process the data to remove hyperlinks, comments and other formatting boilerplate. Gutenberg and Books3 [4.5%]. We include two book corpora in our training dataset: the Guten- berg Project, which contains books that are in the public domain, and the Books3 section of TheP- ile (Gao et al., 2020), a publicly available dataset for training large language models. We perform deduplication at the book level, removing books with more than 90% content overlap. ArXiv [2.5%]. We process arXiv Latex ﬁles to add scientiﬁc data to our dataset. Following Lewkowycz et al. (2022), we removed everything before the ﬁrst section, as well as the bibliography. We also removed the comments from the .tex ﬁles, and inline-expanded deﬁnitions and macros written by users to increase consistency across papers. Stack Exchange [2%]. We include a dump of Stack Exchange, a website of high quality ques- tions and answers that covers a diverse set of do- mains, ranging from computer science to chemistry. We kept the data from the 28 largest websites, re- moved the HTML tags from text and sorted the answers by score (from highest to lowest). Tokenizer. We tokenize the data with the byte- pair encoding (BPE) algorithm (Sennrich et al., 2015), using the implementation from Sentence- Piece (Kudo and Richardson, 2018). Notably, we split all numbers into individual digits, and fallback to bytes to decompose unknown UTF-8 characters. params dimension n heads n layers learning rate batch size n tokens 6.7B 4096 32 32 3.0e−4 4M 1.0T 13.0B 5120 40 40 3.0e−4 4M 1.0T 32.5B 6656 52 60 1.5e−4 4M 1.4T 65.2B 8192 64 80 1.5e−4 4M 1.4T Table 2: Model sizes, architectures, and optimization hyper-parameters. Overall, our entire training dataset contains roughly 1.4T tokens after tokenization. For most of our training data, eac"
        },
        "snippet": "runs on 1T tokens have the same sampling proportion. languages, which use either the Latin or Cyrillic scripts: bg, ca, cs, da, de, en, es, fr, hr, hu, it, nl, pl, pt, ro, ru, sl, sr, sv, uk. We process the data to remove hyperlinks, comments and other formatting boilerplate. Gutenberg and Books3 [4.5%]. We include two book corpora in our training dataset: the Guten- berg Project, which contains b"
      },
      {
        "rank": 5,
        "doc_id": "arxiv:2303.18223#model:part-13",
        "score": 0.5018393,
        "payload": {
          "doc_id": "arxiv:2303.18223#model:part-13",
          "url": "https://arxiv.org/abs/2303.18223",
          "anchor": "#model:part-13",
          "type": "paper",
          "title": "",
          "section": "model",
          "source": "arxiv_pdf",
          "published": "",
          "tokens": 512,
          "sha256": "4e9d0bc4afb804abbb23791cd565ddaa6d0c60da948472b0831373bcda981319",
          "text": "Raw Corpus Quality Filtering De-duplication Sentence-level Document-level Set-level Privacy Reduction Tokenization Ready to pre-train! 32, 145, 66, 79, 12, 56, ... Alice is writing a paper about LLMs. #$^& Alice is writing a paper about LLMs. Alice is writing a paper about LLMs. Alice is writing a paper about LLMs. Replace('Alice') is writing a paper about LLMs. Encode('[Somebody] is writing a paper about LLMs.') Detect Personality Identifiable Information (PII) Remove PII Reuse Existing Tokenizer SentencePiece Byte-level BPE Fig. 7: An illustration of a typical data preprocessing pipeline for pre-training large language models. reduce privacy risks to some extent. Tokenization. Tokenization is also a crucial step for data preprocessing. It aims to segment raw text into sequences of individual tokens, which are subsequently used as the inputs of LLMs. In traditional NLP research (e.g., sequence labeling with conditional random fields [220]), word-based tokenization is the predominant approach, which is more aligned with human’s language cognition. However, word- based tokenization can yield different segmentation results for the same input in some languages (e.g., Chinese word segmentation), generate a huge word vocabulary containing many low-frequency words, and also suffer from the “out- of-vocabulary” issue. Thus, several neural network models employ character as the minimum unit to derive the word representation (e.g., a CNN word encoder in ELMo [21]). Recently, subword tokenizers have been widely used in Trans- former based language models, typically including Byte- Pair Encoding tokenization, WordPiece tokenization and Unigram tokenization. HuggingFace has maintained an excellent online NLP course on tokenizer22 with running examples, and we refer to the beginners to this course. Next, we briefly describe the three representative tokenization methods. • Byte-Pair Encoding (BPE) tokenization. BPE was origi- nally proposed as a general data compression algorithm"
        },
        "snippet": "Raw Corpus Quality Filtering De-duplication Sentence-level Document-level Set-level Privacy Reduction Tokenization Ready to pre-train! 32, 145, 66, 79, 12, 56, ... Alice is writing a paper about LLMs. #$^& Alice is writing a paper about LLMs. Alice is writing a paper about LLMs. Alice is writing a paper about LLMs. Replace('Alice') is writing a paper about LLMs. Encode('[Somebody] is writing a pap"
      },
      {
        "rank": 6,
        "doc_id": "arxiv:2310.20707#introduction:part-2",
        "score": 0.49761945,
        "payload": {
          "doc_id": "arxiv:2310.20707#introduction:part-2",
          "url": "https://arxiv.org/abs/2310.20707",
          "anchor": "#introduction:part-2",
          "type": "paper",
          "title": "",
          "section": "INTRODUCTION",
          "source": "arxiv_pdf",
          "published": "",
          "tokens": 512,
          "sha256": "776584fd04ae572931cf028c533f126c7c204ab51aaaf7d14450bb0dcbd8a5fd",
          "text": "they appeared, and the number of times they appeared. (2) a count functionality, built using map- reduce (Dean & Ghemawat, 2008), allowing quick iteration over an entire dataset and extraction of relevant information, e.g., the character length distribution of documents, duplicates, domain counts, finding personally identifiable information (PII), and more. WIMBD is extendable and can be used to index, count, and analyze other corpora at scale, and the code is publicly available at github.com/allenai/wimbd. Using these tools, we perform a set of sixteen analyses on ten different corpora used to train language models, including C4 (used to train T5; Raffel et al., 2020), The Pile (used to train Pythia; Gao et al., 2020; Biderman et al., 2022; 2023), and RedPajama (used to reproduce Llama Touvron et al., 2023, and to train RedPajama-INCITE; Together Computer, 2023). We divide our analyses into four categories: (1) data statistics (e.g., number of tokens and domain distribution); (2) data quality (e.g., most frequent n-grams and measuring duplicate documents); (3) community- and society-relevant measurements (e.g., benchmark contamination and personally identifiable information detection); and (4) cross-corpora analysis (e.g., comparing the most common n-gram and document overlap). An illustration of WIMBD is presented in Figure 1. Our work presents many insights on data distribution and anomalies. For example, inspecting the distribution over document lengths exposes anomalies where specific lengths are overrepresented relative to neighboring lengths; these anomalies often correspond to near-duplicate template-generated text or documents arbitrarily truncated to a specific character length. As another example, punctuation sequences are frequently the most common n-grams, such as a dash (‘-’) repeated ten times as the most common 10-gram in The Pile. WIMBD offers both retrospective documentation and grounding of model behavior to their training data and actionable insi"
        },
        "snippet": "they appeared, and the number of times they appeared. (2) a count functionality, built using map- reduce (Dean & Ghemawat, 2008), allowing quick iteration over an entire dataset and extraction of relevant information, e.g., the character length distribution of documents, duplicates, domain counts, finding personally identifiable information (PII), and more. WIMBD is extendable and can be used to i"
      },
      {
        "rank": 7,
        "doc_id": "arxiv:20-074#abstract:part-31",
        "score": 0.48766908,
        "payload": {
          "doc_id": "arxiv:20-074#abstract:part-31",
          "url": "https://arxiv.org/abs/20-074",
          "anchor": "#abstract:part-31",
          "type": "paper",
          "title": "",
          "section": "Abstract",
          "source": "arxiv_pdf",
          "published": "",
          "tokens": 512,
          "sha256": "1f02a9a9fb1f0ff1eb2baf690ecc499399b98fbce7ab614fd95e4601b1b782e8",
          "text": "WebText-like data set, which is of comparable size to the original 40GB WebText data set (Radford et al., 2019). Wikipedia The website Wikipedia consists of millions of encyclopedia articles written collaboratively. The content on the site is subject to strict quality guidelines and therefore has been used as a reliable source of clean and natural text. We use the English Wikipedia text data from TensorFlow Datasets,13 which omits any markup or reference sections from the articles. Wikipedia + Toronto Books Corpus A drawback of using pre-training data from Wikipedia is that it represents only one possible domain of natural text (encyclopedia articles). To mitigate this, BERT (Devlin et al., 2018) combined data from Wikipedia with the Toronto Books Corpus (TBC) (Zhu et al., 2015). TBC contains text extracted from eBooks, which represents a diﬀerent domain of natural language. BERT’s popularity has led to the Wikipedia + TBC combination being used in many subsequent works. 12. https://github.com/jcpeterson/openwebtext 13. https://www.tensorflow.org/datasets/catalog/wikipedia 26 Exploring the Limits of Transfer Learning The results achieved after pre-training on each of these data sets is shown in Table 8. A ﬁrst obvious takeaway is that removing the heuristic ﬁltering from C4 uniformly degrades performance and makes the unﬁltered variant perform the worst in every task. Beyond this, we found that in some cases a pre-training data set with a more constrained domain outperformed the diverse C4 data set. For example, using the Wikipedia + TBC corpus produced a SuperGLUE score of 73.24, beating our baseline’s score (using C4) of 71.36. This is almost entirely attributable to a boost in performance from 25.78 (baseline, C4) to 50.93 (Wikipedia + TBC) on the Exact Match score for MultiRC (see Table 16). MultiRC is a reading comprehension data set whose largest source of data comes from ﬁction books, which is exactly the domain covered by TBC. Similarly, using the RealNews-l"
        },
        "snippet": "WebText-like data set, which is of comparable size to the original 40GB WebText data set (Radford et al., 2019). Wikipedia The website Wikipedia consists of millions of encyclopedia articles written collaboratively. The content on the site is subject to strict quality guidelines and therefore has been used as a reliable source of clean and natural text. We use the English Wikipedia text data from "
      },
      {
        "rank": 8,
        "doc_id": "arxiv:2005.14165#results:part-27",
        "score": 0.48381218,
        "payload": {
          "doc_id": "arxiv:2005.14165#results:part-27",
          "url": "https://arxiv.org/abs/2005.14165",
          "anchor": "#results:part-27",
          "type": "paper",
          "title": "",
          "section": "Results",
          "source": "arxiv_pdf",
          "published": "",
          "tokens": 512,
          "sha256": "13a648555577edb51888dceff7c6e954c9fee93817e2fb159d1395d2a3f08936",
          "text": "age was unaffected and therefore became our chief language modeling benchmark. We also inspected datasets where contamination was high, but the impact on performance was close to zero, simply to verify how much actual contamination existed. These appeared to often contain false positives. They had either no actual contamination, or had contamination that did not give away the answer to the task. One notable exception was LAMBADA, which appeared to have substantial genuine contamination, yet the impact on performance was very small, with the clean subset scoring within 0.5% of the full dataset. Also, strictly speaking, our ﬁll-in-the-blank format precludes the simplest form of memorization. Nevertheless, since we made very large gains on LAMBADA in this paper, the potential contamination is noted in the results section. An important limitation of our contamination analysis is that we cannot be sure that the clean subset is drawn from the same distribution as the original dataset. It remains possible that memorization inﬂates results but at the same time is precisely counteracted by some statistical bias causing the clean subset to be easier. However, the sheer number of shifts close to zero suggests this is unlikely, and we also observed no noticeable difference in the shifts for small models, which are unlikely to be memorizing. Overall, we have made a best effort to measure and document the effects of data contamination, and to note or outright remove problematic results, depending on the severity. Much work remains to be done to address this important and subtle issue for the ﬁeld in general, both when designing benchmarks and when training models. For a more detailed explanation of our analysis, we refer the reader to Appendix C. 5 Limitations GPT-3 and our analysis of it have a number of limitations. Below we describe some of these and suggest directions for future work. First, despite the strong quantitative and qualitative improvements of GPT-3, particularly c"
        },
        "snippet": "age was unaffected and therefore became our chief language modeling benchmark. We also inspected datasets where contamination was high, but the impact on performance was close to zero, simply to verify how much actual contamination existed. These appeared to often contain false positives. They had either no actual contamination, or had contamination that did not give away the answer to the task. O"
      },
      {
        "rank": 9,
        "doc_id": "arxiv:2303.18223#model:part-15",
        "score": 0.4827255,
        "payload": {
          "doc_id": "arxiv:2303.18223#model:part-15",
          "url": "https://arxiv.org/abs/2303.18223",
          "anchor": "#model:part-15",
          "type": "paper",
          "title": "",
          "section": "model",
          "source": "arxiv_pdf",
          "published": "",
          "tokens": 512,
          "sha256": "e208288969d8aea42b3da5784bf503a98fc4cb441ac2fea3b49d3337ea04894f",
          "text": "be less capable in processing non-English data, e.g., taking longer inference latency to generate Chinese texts. 20 Data Curriculum Stage 1 ··· Stage 2 Stage Data Mixture Data Source Stage 1 2 3 4 Fig. 8: An illustration of data scheduling for pre-training LLMs. Discussion on Effect of Data Quality. For pre-training, the quality of pre-training data is vital to the model capacities of LLMs. Existing work has shown that pre-training on the low-quality corpus, such as noisy, toxic, and duplicate data, would largely hurt the performance of models [64, 214, 216, 219]. Recent studies, such as T5 [82], GLaM [112], and Gopher [64], have investigated the influence of data quality on the LLMs’ capacities. By comparing the performance of models trained on the filtered and unfiltered corpus, they have reached the similar conclusion that pre-training LLMs on cleaned data can improve the model performance. More specifically, the duplication of data may result in “double descent” (referring to the phenomenon of performance ini- tially deteriorating and subsequently improving) [214, 228], or even overwhelm the training process [214]. In addition, it has been shown that duplicate data degrades the ability of LLMs to copy from the context, which might further affect the generalization capacity of LLMs using in-context learning [214]. Therefore, as suggested in [56, 64, 78, 212], it is essential to utilize preprocessing methods like quality filtering, toxic filtering and deduplication to carefully clean the pre-training corpus (as illustrated in Section 4.1.2), to improve stability of the training process and avoid affecting the model performance. 4.1.3 Data Scheduling After data preprocessing, it is essential to design suit- able strategies to schedule these multi-source data for pre- training a capable LLM. Generally, two key aspects should be paid close attention for data scheduling: the proportion of each data source (data mixture), and the order in which each data source is sche"
        },
        "snippet": "be less capable in processing non-English data, e.g., taking longer inference latency to generate Chinese texts. 20 Data Curriculum Stage 1 ··· Stage 2 Stage Data Mixture Data Source Stage 1 2 3 4 Fig. 8: An illustration of data scheduling for pre-training LLMs. Discussion on Effect of Data Quality. For pre-training, the quality of pre-training data is vital to the model capacities of LLMs. Existi"
      },
      {
        "rank": 10,
        "doc_id": "arxiv:2211.05100#evaluation:part-6",
        "score": 0.48053312,
        "payload": {
          "doc_id": "arxiv:2211.05100#evaluation:part-6",
          "url": "https://arxiv.org/abs/2211.05100",
          "anchor": "#evaluation:part-6",
          "type": "paper",
          "title": "",
          "section": "Evaluation",
          "source": "arxiv_pdf",
          "published": "",
          "tokens": 512,
          "sha256": "163528244b64f1b7d5c899e1e46abe281685f67baf3d2bf4cd7c3e04dbf70611",
          "text": "question answering, summarization, or dialogue datasets), scraping and processing large amounts of PDF files from archives (e.g. the French repository of scientific articles12), and extracting and preprocessing text from 192 website entries from the catalogue and another 10. cloud.google.com/blog/topics/public-datasets/github-on-bigquery-analyze-all-the-open- source-code 11. github.com/bigscience-workshop/data-preparation 12. hal.archives-ouvertes.fr 11 BigScience Workshop Sourcing Pre-processing Crowdsourced Datasets Common Crawl-based Dataset OSCAR manual merging & source-level deduplication Store Identified Datasets and Collections Pseudo-Crawled Data GitHub Code semi-automatic cleaning & filtering & deduplication personal identifiable information removal semi-automatic cleaning & filtering & deduplication Figure 2: Creation Pipeline of the ROOTS Corpus. The purple-colored sourcing stage of the pipeline and the yellow-colored processing stage are described respectively in Section 3.1.2 and Section 3.1.3. geographically diverse set of 456 websites selected by data working group members. The latter required the development of new tools to extract text from the HTML in the Common Crawl WARC files, which we made available on the main data preparation repository.13 We were able to find and extract usable text data from all URLs present in 539 of the websites. “Quality” filtering: Text Produced by Humans for Humans After obtaining the text, we found that most of the sources contained some amount of text that was not natural language, for example preprocessing errors, SEO pages, or spam (including pornographic spam). In order to filter non-natural language, we defined a set of quality indicators, where high-quality text is defined as “written by humans for humans”, without distinction of content (as we wanted content selection to exclusively be the domain of the more accountable human source selection) or a priori judgments of grammaticality. The full list of indicators"
        },
        "snippet": "question answering, summarization, or dialogue datasets), scraping and processing large amounts of PDF files from archives (e.g. the French repository of scientific articles12), and extracting and preprocessing text from 192 website entries from the catalogue and another 10. cloud.google.com/blog/topics/public-datasets/github-on-bigquery-analyze-all-the-open- source-code 11. github.com/bigscience-"
      }
    ]
  },
  {
    "id": "query_9",
    "query": "\"query_9\": \"How have open-source models (e.g., LLaMA, Falcon, Mistral) influenced reproducibility and community research?\",",
    "results": [
      {
        "rank": 1,
        "doc_id": "arxiv:2203.02155#model:part-12",
        "score": 0.5128299,
        "payload": {
          "doc_id": "arxiv:2203.02155#model:part-12",
          "url": "https://arxiv.org/abs/2203.02155",
          "anchor": "#model:part-12",
          "type": "paper",
          "title": "",
          "section": "Model",
          "source": "arxiv_pdf",
          "published": "",
          "tokens": 429,
          "sha256": "a824460ec26e5e4964606c304b63b4dda2f29691f0c581f2e6761525efd1558d",
          "text": "generation of large-scale misinformation. However, this can come at the cost of reduced transparency and increased centralization of power because it requires the API provider to make decisions on where to draw the line on each of these questions. Finally, as discussed in Section 5.2, the question of who these models are aligned to is extremely important, and will signiﬁcantly affect whether the net impact of these models is positive or negative. 20 Acknowledgements First, we would like to thank Lilian Weng, Jason Kwon, Boris Power, Che Chang, Josh Achiam, Steven Adler, Gretchen Krueger, Miles Brundage, Tyna Eloundou, Gillian Hadﬁeld, Irene Soliaman, Christy Dennison, Daniel Ziegler, William Saunders, Beth Barnes, Cathy Yeh, Nick Cammaratta, Jonathan Ward, Matt Knight, Pranav Shyam, Alec Radford, and others at OpenAI for discussions throughout the course of the project that helped shape our research direction. We thank Brian Green, Irina Raicu, Subbu Vincent, Varoon Mathur, Kate Crawford, Su Lin Blodgett, Bertie Vidgen, and Paul Röttger for discussions and feedback on our approach. Finally, we thank Sam Bowman, Matthew Rahtz, Ben Mann, Liam Fedus, Helen Ngo, Josh Achiam, Leo Gao, Jared Kaplan, Cathy Yeh, Miles Brundage, Gillian Hadﬁeld, Cooper Raterink, Gretchen Krueger, Tyna Eloundou, Rafal Jakubanis, and Steven Adler for providing feedback on this paper. We’d also like to thank Owain Evans and Stephanie Lin for pointing out the fact that the automatic TruthfulQA metrics were overstating the gains of our PPO models. Thanks to those who contributed in various ways to the infrastructure used to train and deploy our models, including: Daniel Ziegler, William Saunders, Brooke Chan, Dave Cummings, Chris Hesse, Shantanu Jain, Michael Petrov, Greg Brockman, Felipe Such, Alethea Power, and the entire OpenAI supercomputing team. We’d also like to thank Suchir Balaji for help with recalibration, to Alper Ercetin and Justin Wang for designing the main diagram in this paper, a"
        },
        "snippet": "generation of large-scale misinformation. However, this can come at the cost of reduced transparency and increased centralization of power because it requires the API provider to make decisions on where to draw the line on each of these questions. Finally, as discussed in Section 5.2, the question of who these models are aligned to is extremely important, and will signiﬁcantly affect whether the n"
      },
      {
        "rank": 2,
        "doc_id": "arxiv:2307.09288#related-work:part-1",
        "score": 0.4811442,
        "payload": {
          "doc_id": "arxiv:2307.09288#related-work:part-1",
          "url": "https://arxiv.org/abs/2307.09288",
          "anchor": "#related-work:part-1",
          "type": "paper",
          "title": "",
          "section": "Related Work",
          "source": "arxiv_pdf",
          "published": "",
          "tokens": 512,
          "sha256": "df4066b09e4801dec2e6b4cea4b2dd3ca3e3c13c07bff1844cb8791dccd15e1b",
          "text": "Large Language Models. The recent years have witnessed a substantial evolution in the field of LLMs. Following the scaling laws of Kaplan et al. (2020), several Large Language Models with more than 100B parameters have been proposed, from GPT-3 (Brown et al., 2020) to Gopher (Rae et al., 2022) or specialized models, e.g. Galactica, for science(Taylor et al., 2022). With 70B parameters, Chinchilla (Hoffmann et al., 2022) redefined those scaling laws towards the number of tokens rather than model weights. Notable in this progression is the rise of Llama, recognized for its focus on computational efficiency during inference (Touvron et al., 2023). A parallel discourse has unfolded around the dynamics of open-source versus closed- source models. Open-source releases like BLOOM (Scao et al., 2022), OPT(Zhang et al., 2022), and Falcon (Penedo et al., 2023) have risen to challenge their closed-source counterparts like GPT-3 and Chinchilla. §§https://ai.meta.com/llama 35 Yet, when it comes to the \"production-ready\" LLMs such as ChatGPT, Bard, and Claude, there’s a marked distinction in performance and usability. These models rely on intricate tuning techniques to align with human preferences (Gudibande et al., 2023), a process that is still being explored and refined within the open-source community. Attempts to close this gap have emerged, with distillation-based models such as Vicuna (Chiang et al., 2023) and Alpaca (Taori et al., 2023) adopting a unique approach to training with synthetic instructions (Honovich et al., 2022; Wang et al., 2022). However, while these models show promise, they still fall short of the bar set by their closed-source counterparts. Instruction Tuning. Wei et al. (2021) obtained zero-shot performance on unseen tasks by fine-tuning LLMs on numerous datasets. Chung et al. (2022) and Longpre et al. (2023) investigate the impact of instruction tuning as a function of number of tasks, model size, prompt settings, etc. Prompts used for instruction tun"
        },
        "snippet": "Large Language Models. The recent years have witnessed a substantial evolution in the field of LLMs. Following the scaling laws of Kaplan et al. (2020), several Large Language Models with more than 100B parameters have been proposed, from GPT-3 (Brown et al., 2020) to Gopher (Rae et al., 2022) or specialized models, e.g. Galactica, for science(Taylor et al., 2022). With 70B parameters, Chinchilla "
      },
      {
        "rank": 3,
        "doc_id": "arxiv:10000000_662098952474184_2584067087619170692_n#related-work:part-1",
        "score": 0.4811442,
        "payload": {
          "doc_id": "arxiv:10000000_662098952474184_2584067087619170692_n#related-work:part-1",
          "url": "https://arxiv.org/abs/10000000_662098952474184_2584067087619170692_n",
          "anchor": "#related-work:part-1",
          "type": "paper",
          "title": "",
          "section": "Related Work",
          "source": "arxiv_pdf",
          "published": "",
          "tokens": 512,
          "sha256": "df4066b09e4801dec2e6b4cea4b2dd3ca3e3c13c07bff1844cb8791dccd15e1b",
          "text": "Large Language Models. The recent years have witnessed a substantial evolution in the field of LLMs. Following the scaling laws of Kaplan et al. (2020), several Large Language Models with more than 100B parameters have been proposed, from GPT-3 (Brown et al., 2020) to Gopher (Rae et al., 2022) or specialized models, e.g. Galactica, for science(Taylor et al., 2022). With 70B parameters, Chinchilla (Hoffmann et al., 2022) redefined those scaling laws towards the number of tokens rather than model weights. Notable in this progression is the rise of Llama, recognized for its focus on computational efficiency during inference (Touvron et al., 2023). A parallel discourse has unfolded around the dynamics of open-source versus closed- source models. Open-source releases like BLOOM (Scao et al., 2022), OPT(Zhang et al., 2022), and Falcon (Penedo et al., 2023) have risen to challenge their closed-source counterparts like GPT-3 and Chinchilla. §§https://ai.meta.com/llama 35 Yet, when it comes to the \"production-ready\" LLMs such as ChatGPT, Bard, and Claude, there’s a marked distinction in performance and usability. These models rely on intricate tuning techniques to align with human preferences (Gudibande et al., 2023), a process that is still being explored and refined within the open-source community. Attempts to close this gap have emerged, with distillation-based models such as Vicuna (Chiang et al., 2023) and Alpaca (Taori et al., 2023) adopting a unique approach to training with synthetic instructions (Honovich et al., 2022; Wang et al., 2022). However, while these models show promise, they still fall short of the bar set by their closed-source counterparts. Instruction Tuning. Wei et al. (2021) obtained zero-shot performance on unseen tasks by fine-tuning LLMs on numerous datasets. Chung et al. (2022) and Longpre et al. (2023) investigate the impact of instruction tuning as a function of number of tasks, model size, prompt settings, etc. Prompts used for instruction tun"
        },
        "snippet": "Large Language Models. The recent years have witnessed a substantial evolution in the field of LLMs. Following the scaling laws of Kaplan et al. (2020), several Large Language Models with more than 100B parameters have been proposed, from GPT-3 (Brown et al., 2020) to Gopher (Rae et al., 2022) or specialized models, e.g. Galactica, for science(Taylor et al., 2022). With 70B parameters, Chinchilla "
      },
      {
        "rank": 4,
        "doc_id": "arxiv:2304.01373#abstract:part-2",
        "score": 0.48109108,
        "payload": {
          "doc_id": "arxiv:2304.01373#abstract:part-2",
          "url": "https://arxiv.org/abs/2304.01373",
          "anchor": "#abstract:part-2",
          "type": "paper",
          "title": "",
          "section": "Abstract",
          "source": "arxiv_pdf",
          "published": "",
          "tokens": 512,
          "sha256": "edc29c82aa945b214a893555ce20897e30d3399ba8f24d929b3a308976c06e74",
          "text": "to test theories: al- though there are more publicly available LLMs than ever, they do not meet common requirements for researchers, as discussed in Section 2 of this paper. Of the research along these lines that does exist (McGrath et al., 2021; Tirumala et al., 2022; Xia et al., 2022), it is overwhelmingly done on non-public models or model checkpoints, further emphasiz- ing the importance of having publicly available model suites for scientific research. In this paper we introduce Pythia, a suite of decoder-only autoregressive language models ranging from 70M to 12B parameters designed specifically to facilitate such scientific research. The Pythia suite is the only publicly released suite of LLMs that satisfies three key properties: 1. Models span several orders of magnitude of model scale. 2. All models were trained on the same data in the same order. 3. The data and intermediate checkpoints are publicly available for study. We train 8 model sizes each on both the Pile (Gao et al., 2020; Biderman et al., 2022) and the Pile after deduplication, providing 2 copies of the suite which can be compared. 1 arXiv:2304.01373v2 [cs.CL] 31 May 2023 Pythia: A Suite for Analyzing Large Language Models Model Size Non-Embedding Params Layers Model Dim Heads Learning Rate Equivalent Models 70 M 18,915,328 6 512 8 10.0 × 10−4 — 160 M 85,056,000 12 768 12 6.0 × 10−4 GPT-Neo 125M, OPT-125M 410 M 302,311,424 24 1024 16 3.0 × 10−4 OPT-350M 1.0 B 805,736,448 16 2048 8 3.0 × 10−4 — 1.4 B 1,208,602,624 24 2048 16 2.0 × 10−4 GPT-Neo 1.3B, OPT-1.3B 2.8 B 2,517,652,480 32 2560 32 1.6 × 10−4 GPT-Neo 2.7B, OPT-2.7B 6.9 B 6,444,163,072 32 4096 32 1.2 × 10−4 OPT-6.7B 12 B 11,327,027,200 36 5120 40 1.2 × 10−4 — Table 1. Models in the Pythia suite and select hyperparameters. For a full list of hyper-parameters, see Appendix E. Models are named based on their total number of parameters, but for most analyses we recommend people use the number of non-embedding parameters as the measure of “size."
        },
        "snippet": "to test theories: al- though there are more publicly available LLMs than ever, they do not meet common requirements for researchers, as discussed in Section 2 of this paper. Of the research along these lines that does exist (McGrath et al., 2021; Tirumala et al., 2022; Xia et al., 2022), it is overwhelmingly done on non-public models or model checkpoints, further emphasiz- ing the importance of ha"
      },
      {
        "rank": 5,
        "doc_id": "arxiv:2211.05100#evaluation:part-4",
        "score": 0.45458817,
        "payload": {
          "doc_id": "arxiv:2211.05100#evaluation:part-4",
          "url": "https://arxiv.org/abs/2211.05100",
          "anchor": "#evaluation:part-4",
          "type": "paper",
          "title": "",
          "section": "Evaluation",
          "source": "arxiv_pdf",
          "published": "",
          "tokens": 512,
          "sha256": "ee3227be519fd3788f25c09045243a2eb65b69f0e9b7155555f7de40bba99db3",
          "text": "integrating lessons from this effort (and conversely adapting it to the practical concerns we were ex- periencing) in the following main ways: (i) we sought explicit permission to use the data from specific providers within the context of BigScience whenever possible (such as for the AI26-managed S2ORC corpus of Lo et al. (2020) or articles from the French newspaper Le Monde7); (ii) we kept individual sources separate until the final stages of preprocessing to maintain traceability and handle each according to the needs of its specific context; and (iii) we adopted a composite release approach for the various data sources that make up the overall corpus to foster reproducibility and follow-up research while respecting these source- dependent needs. Resources to visualize and access the ROOTS corpus can be found on the Hugging Face Hub organization “BigScience Data”.8 The organization hosts several demos (or “Spaces”) that can be used to gain insights into the full corpus, as well as direct access to the 223 (out of 498) components that we are able to distribute taking into account their licensing status, privacy risks, and agreements with their original custodians. Finally, since we understand that future investigation into the BLOOM models may require full access to the entire corpus, we are also inviting researchers with a relevant research project in mind to join ongoing efforts to analyze the data through a sign-up form.9 3.1.2 Data Sources Given a strategy for data governance, the next step was to determine the composition of the training corpus. This stage was driven by several goals, which sometimes had inherent tensions. Some of those tensions included building a language model that was accessible to as many people as possible around the world while only including languages for which we had enough expertise to curate a dataset of comparable scale (and to a lesser extent composition) to previous efforts while improving the standards of documentation and respe"
        },
        "snippet": "integrating lessons from this effort (and conversely adapting it to the practical concerns we were ex- periencing) in the following main ways: (i) we sought explicit permission to use the data from specific providers within the context of BigScience whenever possible (such as for the AI26-managed S2ORC corpus of Lo et al. (2020) or articles from the French newspaper Le Monde7); (ii) we kept indivi"
      },
      {
        "rank": 6,
        "doc_id": "arxiv:2211.05100#model:part-2",
        "score": 0.42043173,
        "payload": {
          "doc_id": "arxiv:2211.05100#model:part-2",
          "url": "https://arxiv.org/abs/2211.05100",
          "anchor": "#model:part-2",
          "type": "paper",
          "title": "",
          "section": "Model",
          "source": "arxiv_pdf",
          "published": "",
          "tokens": 512,
          "sha256": "f8252afed7667819259025d427e3a95c9958a39183ee1f8fed692d6d55abf312",
          "text": "of requests the model receives. Further information regarding BLOOM’s carbon footprint can be found in Luccioni et al. (2022). 23 BigScience Workshop 3.6 Release Openness has been central to the development of BLOOM and we wanted to ensure it is easily available for the community to use. As such, we worked on producing documentation as a Model Card (Mitchell et al., 2019) and a new license addressing specific goals of the project. Model Card Following best practices for releasing machine learning models, the BLOOM model has been released along with a detailed Model Card26 (Mitchell et al., 2019) describing its technical specifications, details on training, intended-use, out-of-scope uses as well as the model’s limitations. Participants across working groups worked together to produce the final Model Card and similar cards for each checkpoint. The work was collaborative, primarily composed “live” by thinking through and discussing each section, then further dividing into subsections based on the categorizations and distinctions participants naturally ended up creating throughout discussions. Licensing Being mindful of the potentially harmful use-cases that BLOOM could en- able, we chose to strike a balance between unrestricted open-access and responsible-use by including behavioral-use clauses (Contractor et al., 2022) to limit the application of the model towards potentially harmful use-cases. Such clauses are routinely being included in a growing class of “Responsible AI Licenses (RAIL)”27 that the community has been adopting when releasing their models.28 A distinguishing aspect of the RAIL license developed for BLOOM is that it separates licensing of the “source code” and “model”, as referenced by its trained parameters. It further includes detailed definitions of “use” and “derived works” of the model to ensure that anticipated downstream use by prompting, finetuning, distillation, use of logits and probability distributions are explicitly identified. The licens"
        },
        "snippet": "of requests the model receives. Further information regarding BLOOM’s carbon footprint can be found in Luccioni et al. (2022). 23 BigScience Workshop 3.6 Release Openness has been central to the development of BLOOM and we wanted to ensure it is easily available for the community to use. As such, we worked on producing documentation as a Model Card (Mitchell et al., 2019) and a new license address"
      },
      {
        "rank": 7,
        "doc_id": "arxiv:2211.05100#abstract:part-4",
        "score": 0.41787726,
        "payload": {
          "doc_id": "arxiv:2211.05100#abstract:part-4",
          "url": "https://arxiv.org/abs/2211.05100",
          "anchor": "#abstract:part-4",
          "type": "paper",
          "title": "",
          "section": "Abstract",
          "source": "arxiv_pdf",
          "published": "",
          "tokens": 512,
          "sha256": "232b8056aa7963085bc4cf5559f09dc47fd62e86b942b5821960f0d95d51b2c1",
          "text": "is the way that task-specific examples are formatted when fed into the model. Brown et al. (2020) popularized the idea of designing “prompts” that provide natural-language descriptions of the task and also allow inputting a few demonstrations of input-output behavior. Social Limitations of LLM Development While the continued increase in the size of large language models has resulted in improvements across a wide range of tasks, it has also 5 BigScience Workshop exacerbated issues with their development and use (Bender et al., 2021). The computational expense of large models also prohibits the majority of the research community from partici- pating in their development, evaluation and routine use. Moreover, the computational costs have also lead to concerns about the carbon footprint stemming from the training and use of large language models (Strubell et al., 2019; Lacoste et al., 2019; Schwartz et al., 2020; Bannour et al., 2021), and existing carbon footprint studies have likely under-estimated emissions (Bannour et al., 2021). Contributing to an increase in the global carbon footprint exacerbates climate change which most severely affects already-marginalized communities (Westra and Lawson, 2001). Furthermore, the concentration of resources within a handful of (typically industrial) institutions with primarily technical expertise hinders prospects for an inclusive, collaborative, and reliable governance of the technology. First, public narratives about the technology that are driven by industry actors can lead to inflated expectations about its suitability for use (Brennen, 2018; Brennen et al., 2022), leading to misaligned research and policy priorities (Raji et al., 2022) and potentially dire conse- quences in e.g. medical applications (Wong et al., 2021). Second, in a world mediated by technology, choices at all stages of its development end up shaping people’s lives in a way that can be most closely compared to regulations (Winner, 1977, 2017), albeit without"
        },
        "snippet": "is the way that task-specific examples are formatted when fed into the model. Brown et al. (2020) popularized the idea of designing “prompts” that provide natural-language descriptions of the task and also allow inputting a few demonstrations of input-output behavior. Social Limitations of LLM Development While the continued increase in the size of large language models has resulted in improvement"
      },
      {
        "rank": 8,
        "doc_id": "arxiv:2211.05100#evaluation:part-5",
        "score": 0.41463482,
        "payload": {
          "doc_id": "arxiv:2211.05100#evaluation:part-5",
          "url": "https://arxiv.org/abs/2211.05100",
          "anchor": "#evaluation:part-5",
          "type": "paper",
          "title": "",
          "section": "Evaluation",
          "source": "arxiv_pdf",
          "published": "",
          "tokens": 512,
          "sha256": "9fbdc504706ab06f90f60b4f56b2c8a94e15149f93fb6114f2e44ff24e7d4770",
          "text": "proposed that any group of 3 or more participants fluent in an additional language could add it to the supported list if they would commit to selecting sources and guiding processing choices in the language in order to avoid common issues with corpora selected through automatic language identification without specific language expertise (Caswell et al., 2022). Source Selection The biggest part of the corpus was curated by workshop participants and research collectives who collectively compiled the “BigScience Catalogue”: a large list of processed and non-processed sources covering a wide range of languages. This took the form of hackathons that were co-organized by communities such as Machine Learning Tokyo, Masakhane, and LatinX in AI (McMillan-Major et al., 2022). Complementary to those efforts, other working group participants compiled language-specific resources such as the Arabic-focused Masader repository (Alyafeai et al., 2021; Altaher et al., 2022). A total of 252 sources were identified through this bottom-up approach, with at least 21 sources per language category. Additionally, in order to increase the geographic coverage of some of our Spanish, Chinese, French, and English sources, participants identified locally relevant websites in their language to be added to the corpus via pseudocrawl, a method to obtain those websites from a Common Crawl snapshot. GitHub Code The catalogue was further complemented with a dataset of programming languages collected from the GitHub data collection on Google’s BigQuery,10 which was then deduplicated of exact matches. The choice of languages to include mirrored the design choices introduced by Li et al. (2022) to train the AlphaCode model. OSCAR Both in an effort not to diverge from the standard research practice of using the Web as a source of pretraining data (Radford et al., 2018; Raffel et al., 2020), and also to satisfy the data volume needs of our compute budget given the size of BLOOM, we further sourced data fro"
        },
        "snippet": "proposed that any group of 3 or more participants fluent in an additional language could add it to the supported list if they would commit to selecting sources and guiding processing choices in the language in order to avoid common issues with corpora selected through automatic language identification without specific language expertise (Caswell et al., 2022). Source Selection The biggest part of "
      },
      {
        "rank": 9,
        "doc_id": "arxiv:2310.20707#introduction:part-3",
        "score": 0.4126386,
        "payload": {
          "doc_id": "arxiv:2310.20707#introduction:part-3",
          "url": "https://arxiv.org/abs/2310.20707",
          "anchor": "#introduction:part-3",
          "type": "paper",
          "title": "",
          "section": "INTRODUCTION",
          "source": "arxiv_pdf",
          "published": "",
          "tokens": 512,
          "sha256": "2029bc82a9f5830bc6a251ca6acd7b624020a99f423ac56c608b974e2e91df67",
          "text": "demographic sentiment co-occurrences Compressed Counts (§3.1) Duplicates, most & least common n-grams Search (§3.2) Benchmark contamination, n-gram counts the field, developers of systems like GPT-4 (OpenAI, 2023) and PaLM-2 (Google, 2023) have been offering little transparency into one of the most important development decisions, including the sources, size, and contents of their training data. As web-scale datasets drive this rapid progress in modern machine learning systems, the gap between data transparency and documentation is more striking than ever. From a technical standpoint, the massive size of these datasets makes analysis of their contents challenging; even if OpenAI or Google shared their training data, it’s not clear where to start understanding it in its entirety. Tools like the Data Measurements Tool (Luccioni et al., 2021) and Know Your Data (Google, 2021) work towards improving data documentation, but focus on smaller datasets since the scale of web data leads to significant technical challenges. Our work aims to address this critical missing component. While other works support indexing and analyses of large corpora (Piktus et al., 2023a; Marone & Van Durme, 2023; Simig et al., 2022; Piktus et al., 2023b), these efforts support a single corpus and often do not support programmatic access to the data or the analysis. Instead, we offer a holistic approach that combines search and counting with a package that allows programmatic access through wrappers on top of the Elasticsearch API and extendable efficient counting capabilities. Additional efforts are concerned with the effect of the data on model behavior. Longpre et al. (2023) investigate how the composition of language models’ pretraining data influences their downstream performance on different tasks. Razeghi et al. (2022) measure the correlation between term frequency and models’ few-shot reasoning capabilities with those terms, showing a strong correlation. Shin et al. (2022) study the effect"
        },
        "snippet": "demographic sentiment co-occurrences Compressed Counts (§3.1) Duplicates, most & least common n-grams Search (§3.2) Benchmark contamination, n-gram counts the field, developers of systems like GPT-4 (OpenAI, 2023) and PaLM-2 (Google, 2023) have been offering little transparency into one of the most important development decisions, including the sources, size, and contents of their training data. A"
      },
      {
        "rank": 10,
        "doc_id": "arxiv:2310.20707#model:part-2",
        "score": 0.4122479,
        "payload": {
          "doc_id": "arxiv:2310.20707#model:part-2",
          "url": "https://arxiv.org/abs/2310.20707",
          "anchor": "#model:part-2",
          "type": "paper",
          "title": "",
          "section": "Model",
          "source": "arxiv_pdf",
          "published": "",
          "tokens": 512,
          "sha256": "a116101f499f6254a70fa128f53de01f53e844b3911c3bfc886cbd46af2f01a5",
          "text": "API on top of the original Elasticsearch functions, allowing tailored and customized searches to fit our analysis requirements. We leave it to future work to explore other search alternatives. 4 WIMBD: THE ANALYSES This section presents analyses conducted in WIMBD, grouped by category. First, we describe the ten corpora considered in this study (§4.1). We then consider four high-level categories, each split into several analyses: data statistics (§4.2), data quality (§4.3), and community- and society-relevant measurements (§4.4). Cross-corpus analyses, as well as elaborations and more analyses are presented in the appendix (§B). Our analyses are inspired by previous works (Dodge et al., 2021; Gao et al., 2020), but we expand them to multiple corpora, and open-source our modular toolkit to encourage researchers to scrutinize their corpora. We offer the first extensive analyses on ten, combining extension of previous analyses and several novel ones. 4.1 CORPORA We cover ten different large corpora, spanning across text-only (e.g., C4 to image captions (LAION- 2B-en) and code (The Stack). These corpora have been used in training language models (or similar large-scale models, such as Stable Diffusion Rombach et al. 2022). A high-level description of these datasets using WIMBD is presented in Table 2, and further details about the construction and origin of these corpora are detailed in Appendix A. 4 Figure 2: Domain distributions of the ten most common domains per token for C4, LAION-2B-en, and RedPajama. The results for the other corpora are discussed and presented in Appendix B.1.1 4.2 DATA STATISTICS Main Findings • Four out of the ten corpora we consider have ‘empty’ documents (meaning they contain only space-like characters), while The Pile and RedPajama contain the same longest document (with over 28 million tokens) of an encyclopedia. • While the most common source of webpages in C4 originate from www.nytimes.com, it consists of less than 0.05% of the total web "
        },
        "snippet": "API on top of the original Elasticsearch functions, allowing tailored and customized searches to fit our analysis requirements. We leave it to future work to explore other search alternatives. 4 WIMBD: THE ANALYSES This section presents analyses conducted in WIMBD, grouped by category. First, we describe the ten corpora considered in this study (§4.1). We then consider four high-level categories, "
      }
    ]
  },
  {
    "id": "query_10",
    "query": "\"query_10\": \"What are recent advances in alignment methods beyond RLHF, such as direct preference optimization (DPO)?\"",
    "results": [
      {
        "rank": 1,
        "doc_id": "arxiv:2203.02155#model:part-7",
        "score": 0.46854877,
        "payload": {
          "doc_id": "arxiv:2203.02155#model:part-7",
          "url": "https://arxiv.org/abs/2203.02155",
          "anchor": "#model:part-7",
          "type": "paper",
          "title": "",
          "section": "Model",
          "source": "arxiv_pdf",
          "published": "",
          "tokens": 512,
          "sha256": "ab565e69ce41d7f7d8da51700809d004ab851699ce74836c3470e3f5e9f3ffda",
          "text": "results (Soares et al., 2015), small synthetic domains (Christiano et al., 2018; Leike et al., 2017), or training ML models on public NLP datasets (Ziegler et al., 2019; Stiennon et al., 2020). Our work provides grounding for alignment research in AI systems that are being used in production in 17 the real world with customers.10 This enables an important feedback loop on the techniques’ effectiveness and limitations. 5.2 Who are we aligning to? When aligning language models with human intentions, their end behavior is a function of the underlying model (and its training data), the ﬁne-tuning data, and the alignment method used. In this section, we describe a number of factors that inﬂuence the ﬁne-tuning data speciﬁcally, to ultimately determine what and who we’re aligning to. We then consider areas for improvement before a larger discussion of the limitations of our work in Section 5.3. The literature often frames alignment using such terms as “human preferences” or “human values.” In this work, we have aligned to a set of labelers’ preferences that were inﬂuenced, among others things, by the instructions they were given, the context in which they received them (as a paid job), and who they received them from. Some crucial caveats apply: First, we are aligning to demonstrations and preferences provided by our training labelers, who directly produce the data that we use to ﬁne-tune our models. We describe our labeler hiring process and demographics in Appendix B; in general, they are mostly English-speaking people living in the United States or Southeast Asia hired via Upwork or Scale AI. They disagree with each other on many examples; we found the inter-labeler agreement to be about 73%. Second, we are aligning to our preferences, as the researchers designing this study (and thus by proxy to our broader research organization, OpenAI): we write the labeling instructions that labelers use as a guide when writing demonstrations and choosing their preferred output, an"
        },
        "snippet": "results (Soares et al., 2015), small synthetic domains (Christiano et al., 2018; Leike et al., 2017), or training ML models on public NLP datasets (Ziegler et al., 2019; Stiennon et al., 2020). Our work provides grounding for alignment research in AI systems that are being used in production in 17 the real world with customers.10 This enables an important feedback loop on the techniques’ effective"
      },
      {
        "rank": 2,
        "doc_id": "arxiv:2305.03047#evaluation",
        "score": 0.46533933,
        "payload": {
          "doc_id": "arxiv:2305.03047#evaluation",
          "url": "https://arxiv.org/abs/2305.03047",
          "anchor": "#evaluation",
          "type": "paper",
          "title": "",
          "section": "Evaluation",
          "source": "arxiv_pdf",
          "published": "",
          "tokens": 280,
          "sha256": "250af0e4c8cb0e0cc732bcd8bdc2f000faf89450ec96129e8cedb84f03f18ea5",
          "text": "We quantitatively evaluate Dromedary on benchmark datasets and also assess its qualitative perfor- mance on several datasets for demonstration purposes. By default, all the language model-generated text is decoded with a temperature of 0.7. 4.1 Dromedary and Baseline Models Dromedary Dromedary is the AI assistant developed by implementing the SELF-ALIGN process on the LLaMA-65b base language model. We investigate two variants: Dromedary (final) and Dromedary (non-verbose), respectively. The former represents the model obtained by applying all four steps of the SELF-ALIGN process, while the latter is the principle-engraved model, excluding the final step of verbose cloning. Due to the space limit, the experimental details of Dromedary such as training process and decoding hyper-parameters can be found in the appendix. Baseline Models Our comparison involves several notable baselines. LLaMA [44] provides a set of performant base language models for research usage. Text-Davinci-003, ChatGPT (or GPT-3.5), and GPT-4 [29, 26, 27], successors to their previous versions, have demonstrated sig- nificant enhancements in generating contextually relevant and high-quality content. Alpaca [42], a fine-tuned model derived from Text-Davinci-003, and Vicuna [8], a chatbot trained on user- shared conversations with ChatGPT, offer unique insights into model performance. Dolly-V2 [11], an instruction-following model, showcases commercial applications of language models. Finally, results from Anthropic-LM [4, 5], though not publicly available, provide valuable benchmarks. More comprehensive descriptions of these models are available in the appendix. 4.2 Benchmark Results 4.2.1 TruthfulQA The TruthfulQA benchmark [22] evaluates a model’s ability to identify true claims, specifically in the context of literal truth about the real world. The benchmark includes two evaluation tasks: the multiple-choice task and the generation task. 7 Anthropic's Anthropic's GPT-3.5 GPT-3.5 GPT-3.5 GPT-4 GPT"
        },
        "snippet": "We quantitatively evaluate Dromedary on benchmark datasets and also assess its qualitative perfor- mance on several datasets for demonstration purposes. By default, all the language model-generated text is decoded with a temperature of 0.7. 4.1 Dromedary and Baseline Models Dromedary Dromedary is the AI assistant developed by implementing the SELF-ALIGN process on the LLaMA-65b base language model"
      },
      {
        "rank": 3,
        "doc_id": "arxiv:2203.02155#model:part-6",
        "score": 0.44741982,
        "payload": {
          "doc_id": "arxiv:2203.02155#model:part-6",
          "url": "https://arxiv.org/abs/2203.02155",
          "anchor": "#model:part-6",
          "type": "paper",
          "title": "",
          "section": "Model",
          "source": "arxiv_pdf",
          "published": "",
          "tokens": 512,
          "sha256": "fb0d7e25c4d02694ba5fd9d6d5504322b7ee19b54666555c5f6d701e6fbb7201",
          "text": "aligning superhuman systems (Bostrom, 2014). However, our approach does provides us with a clear empirical feedback loop of what works and what does not. We believe that this feedback loop is essential to reﬁne our alignment techniques, and it forces us to keep pace with progress in machine learning. Moreover, the alignment technique we use here, RLHF, is an important building block in several proposals to align superhuman systems (Leike et al., 2018; Irving et al., 2018; Christiano et al., 2018). For example, RLHF was a central method in recent work on summarizing books, a task that exhibits some of the difﬁculties of aligning superhuman AI systems as it is difﬁcult for humans to evaluate directly (Wu et al., 2021). From this work, we can draw lessons for alignment research more generally: 1. The cost of increasing model alignment is modest relative to pretraining. The cost of collecting our data and the compute for training runs, including experimental runs is a fraction of what was spent to train GPT-3: training our 175B SFT model requires 4.9 petaﬂops/s-days and training our 175B PPO-ptx model requires 60 petaﬂops/s-days, compared to 3,640 petaﬂops/s-days for GPT-3 (Brown et al., 2020). At the same time, our results show that RLHF is very effective at making language models more helpful to users, more so than a 100x model size increase. This suggests that right now increasing investments in alignment of existing language models is more cost-effective than training larger models—at least for our customers’ natural language task distribution. 2. We’ve seen some evidence that InstructGPT generalizes ‘following instructions’ to settings that we don’t supervise it in, for example on non-English language tasks and code-related tasks. This is an important property because it’s prohibitively expensive to have humans supervise models on every task they perform. More research is needed to study how well this generalization scales with increased capabilities; see Christian"
        },
        "snippet": "aligning superhuman systems (Bostrom, 2014). However, our approach does provides us with a clear empirical feedback loop of what works and what does not. We believe that this feedback loop is essential to reﬁne our alignment techniques, and it forces us to keep pace with progress in machine learning. Moreover, the alignment technique we use here, RLHF, is an important building block in several pro"
      },
      {
        "rank": 4,
        "doc_id": "arxiv:1901.02860#model:part-5",
        "score": 0.43095088,
        "payload": {
          "doc_id": "arxiv:1901.02860#model:part-5",
          "url": "https://arxiv.org/abs/1901.02860",
          "anchor": "#model:part-5",
          "type": "paper",
          "title": "",
          "section": "Model",
          "source": "arxiv_pdf",
          "published": "",
          "tokens": 512,
          "sha256": "8b4cdd146f2d86a440eab0739ee70901167a6623bd700af1557a82ae10da494f",
          "text": "not only has a one-to-one correspondence to its absolute coun- terpart but also enjoys much better generalization empirically (see Section 4). Firstly, in the standard Transformer (Vaswani et al., 2017), the attention score between query qi and key vector kj within the same segment can be decomposed as Aabs i,j = E⊤ xiW⊤ q WkExj | {z } (a) + E⊤ xiW⊤ q WkUj | {z } (b) + U⊤ i W⊤ q WkExj | {z } (c) + U⊤ i W⊤ q WkUj | {z } (d) . Following the idea of only relying on rela- tive positional information, we propose to re- parameterize the four terms as follows Arel i,j = E⊤ xiW⊤ q Wk,EExj | {z } (a) + E⊤ xiW⊤ q Wk,RRi−j | {z } (b) + u⊤Wk,EExj | {z } (c) + v⊤Wk,RRi−j | {z } (d) . • The ﬁrst change we make is to replace all ap- pearances of the absolute positional embedding Uj for computing key vectors in term (b) and (d) with its relative counterpart Ri−j. This es- sentially reﬂects the prior that only the relative distance matters for where to attend. Note that R is a sinusoid encoding matrix (Vaswani et al., 2017) without learnable parameters. • Secondly, we introduce a trainable parameter u ∈Rd to replace the query U⊤ i W⊤ q in term (c). In this case, since the query vector is the same for all query positions, it suggests that the attentive bias towards different words should re- main the same regardless of the query position. With a similar reasoning, a trainable parameter v ∈Rd is added to substitute U⊤ i W⊤ q in term (d). • Finally, we deliberately separate the two weight matrices Wk,E and Wk,R for producing the content-based key vectors and location-based key vectors respectively. Under the new parameterization, each term has an intuitive meaning: term (a) represents content- based addressing, term (b) captures a content- dependent positional bias, term (c) governs a global content bias, and (d) encodes a global po- sitional bias. In comparison, the formulation in Shaw et al. (2018) only has terms (a) and (b), dropping the two bias terms (c) and (d). Moreover, Shaw et"
        },
        "snippet": "not only has a one-to-one correspondence to its absolute coun- terpart but also enjoys much better generalization empirically (see Section 4). Firstly, in the standard Transformer (Vaswani et al., 2017), the attention score between query qi and key vector kj within the same segment can be decomposed as Aabs i,j = E⊤ xiW⊤ q WkExj | {z } (a) + E⊤ xiW⊤ q WkUj | {z } (b) + U⊤ i W⊤ q WkExj | {z } (c) +"
      },
      {
        "rank": 5,
        "doc_id": "arxiv:2303.18223#model:part-5",
        "score": 0.42343533,
        "payload": {
          "doc_id": "arxiv:2303.18223#model:part-5",
          "url": "https://arxiv.org/abs/2303.18223",
          "anchor": "#model:part-5",
          "type": "paper",
          "title": "",
          "section": "Model",
          "source": "arxiv_pdf",
          "published": "",
          "tokens": 512,
          "sha256": "be1f1697e2a180fa0da1f13f638aebcf50c6432aa2b065c75dbea5ba4a01de0c",
          "text": "OpenAI blog describing a work that applied reinforcement learning (RL) to learn from the preference comparisons annotated by humans [79] (similar to the reward training step in the aligning algorithm of InstructGPT in Figure 12). Shortly after the release of this RL paper [79], the paper of the Proximal Policy Optimiza- tion (PPO) [128] was published in July 2017, which now has been the foundational RL algorithm for learning from hu- man preferences [66]. Later in January 2020, GPT-2 was fine- tuned using the aforementioned RL algorithms [79, 128], which leveraged human preferences to improve the capac- ities of GPT-2 on NLP tasks. In the same year, another work [129] trained a summarization model for optimizing human preferences in a similar way. Based on these prior work, InstructGPT [66] was proposed in January 2022 to improve the GPT-3 model for human alignment, which formally established a three-stage reinforcement learning from human feedback (RLHF) algorithm. Note that it seems that the wording of “instruction tuning” has seldom been used in OpenAI’s paper and documentation, which is substituted by supervised fine-tuning on human demonstrations (i.e., the first step of the RLHF algorithm [66]). In addition to improving the instruction following capacity, the RLHF algorithm is particularly useful to mitigate the issues of generating harm or toxic content for LLMs, which is key to the safe deploy- ment of LLMs in practice. OpenAI describes their approach to alignment research in a technical article [130], which has summarized three promising directions: “training AI systems to use human feedback, to assist human evaluation and to do alignment research”. These enhancement techniques lead to the improved 18. https://openai.com/research/learning-from-human-preferences GPT-3 models with stronger capacities, which are called GPT-3.5 models by OpenAI (see the discussion about the OpenAI API in Section 3.1). The Milestones of Language Models. Based on all the ex- plor"
        },
        "snippet": "OpenAI blog describing a work that applied reinforcement learning (RL) to learn from the preference comparisons annotated by humans [79] (similar to the reward training step in the aligning algorithm of InstructGPT in Figure 12). Shortly after the release of this RL paper [79], the paper of the Proximal Policy Optimiza- tion (PPO) [128] was published in July 2017, which now has been the foundation"
      },
      {
        "rank": 6,
        "doc_id": "arxiv:2303.18223#model:part-38",
        "score": 0.4218676,
        "payload": {
          "doc_id": "arxiv:2303.18223#model:part-38",
          "url": "https://arxiv.org/abs/2303.18223",
          "anchor": "#model:part-38",
          "type": "paper",
          "title": "",
          "section": "Model",
          "source": "arxiv_pdf",
          "published": "",
          "tokens": 512,
          "sha256": "bac73f6b355158ebb3b4bb2545030e959cd77507a128e04f2808e04e1ad1476c",
          "text": "add latest discussion about the underlying mechanism of ICL 6.2.3, planning for complex task solving in Section 6.4; – Section 7: update Table 14 for representative datasets for evaluating advanced abilities of LLMs, and em- pirical ability evaluation in Section 7.4; – Section 6.1.1: add prompt design; – Section 8: add the discussions on applications of LLMs in finance and scientific research domains; • Update on September 10, 2023 (major revision): – Claim the copyrights of the figures and tables in this paper. – Add latest LLMs, techniques and their descriptions in Section 3, Section 4, Section 5, Section 6 and Section 7; – Section 4: add latest discussion about the decoding strategy in Section 4.2.5; – Section 5: add latest discussion about the practical tricks for instruction tuning in Section 5.1.2, the empirical analysis on LLaMA (13B) for instruction tuning in Section 5.1.4, practical strategies for RLHF in Section 5.2.3, alignment without RLHF in Sec- tion 5.2.4 and remarks on SFT and RLHF in Sec- tion 5.2.5; – Section 6: update the content about the planning for complex task solving in Section 6.4; – Section 7: add discussions about evaluation ap- proaches in Section 7.3.2, Table 15 for the category of existing evaluation work, and update empirical ability evaluation in Section 7.4 and the results on Table 16; – Section 6.1.1: add new prompt examples in Table 12; • Update on November 23, 2023 (this version): – Section 1: add Figure 2 for the evolution process of four generations of language models; – Section 2: add more discussion about scaling laws and how emergent abilities relate to scaling laws; – Section 3: add latest LLMs in Figure 3 and Table 1, latest APIs in Section 3.1, commonly used datasets 85 for instruction tuning and alignment tuning in Sec- tion 3.3, and several libraries in Section 3.4; – Section 4: add latest discussion about the data scheduling, including data mixtures and data cur- riculum in Section 4.1.3; add summary of data prepa- rat"
        },
        "snippet": "add latest discussion about the underlying mechanism of ICL 6.2.3, planning for complex task solving in Section 6.4; – Section 7: update Table 14 for representative datasets for evaluating advanced abilities of LLMs, and em- pirical ability evaluation in Section 7.4; – Section 6.1.1: add prompt design; – Section 8: add the discussions on applications of LLMs in finance and scientific research doma"
      },
      {
        "rank": 7,
        "doc_id": "arxiv:2412.10543#method:part-8",
        "score": 0.41576922,
        "payload": {
          "doc_id": "arxiv:2412.10543#method:part-8",
          "url": "https://arxiv.org/abs/2412.10543",
          "anchor": "#method:part-8",
          "type": "paper",
          "title": "",
          "section": "Method",
          "source": "arxiv_pdf",
          "published": "",
          "tokens": 184,
          "sha256": "4e33ce4c2d31ea7919623137436843b03789184715abb20c523c90f2af1be49b",
          "text": "lines of code in Python on top of the state-of-the-art popular LLM serving engine vLLM [41]. For the profiler used for configuration space pruning, we define a class LLMProfiler inheriting OpenAI’s Chat Completion API [61] interface (to invoke GPT-4o) and HuggingaceAPI [81] (to invoke LLama-3.1-70B) as models to profile the queries. We use Cohere-embed-v3.0 [4] as a state-of-the-art em- bedding method. We construct a FAISS [16] index using the IndexFlatL2 interface and perform L2-distance similarity search with index.search(query_embedding, top_k) on the chunk embeddings to retrieve for RAG inference. We use the LLMChain interface from Langchain [8] in order to build efficient implementations of multiple synthesis methods. Finally, we use PyTorch’s [5] library modules support to perform query-level memory profiling and measurement to implement the best-fit scheduling logic and request batching. Particularly, we use pynvml to construct get_free_memory() with its interfaces of nvmlDeviceGetHandleByIndex and nvmlDeviceGetMemoryInfo to measure the amount of GPU memory available. We measure the current num-seqs and num-batched-tokens within vLLM to calculate which con- figuration can be fit into the current batch, based on the GPU availability and the request’s memory requirement. 7"
        },
        "snippet": "lines of code in Python on top of the state-of-the-art popular LLM serving engine vLLM [41]. For the profiler used for configuration space pruning, we define a class LLMProfiler inheriting OpenAI’s Chat Completion API [61] interface (to invoke GPT-4o) and HuggingaceAPI [81] (to invoke LLama-3.1-70B) as models to profile the queries. We use Cohere-embed-v3.0 [4] as a state-of-the-art em- bedding me"
      },
      {
        "rank": 8,
        "doc_id": "arxiv:2303.18223#result:part-16",
        "score": 0.41490853,
        "payload": {
          "doc_id": "arxiv:2303.18223#result:part-16",
          "url": "https://arxiv.org/abs/2303.18223",
          "anchor": "#result:part-16",
          "type": "paper",
          "title": "",
          "section": "Result",
          "source": "arxiv_pdf",
          "published": "",
          "tokens": 512,
          "sha256": "097f14bd5734f06727588fa60ba3be73f25f15f237bd25df8cd5c087acda44af",
          "text": "thus likely improving the numerical computation ability of LLMs. Numerical Computation LLMs face difficulties in numerical computation, especially for the symbols that are seldom en- countered during pre-training. In addition to us- ing mathematical tools, tokenizing digits into in- dividual tokens is also an effective design choice for improving the arithmetic ability of LLMs. 7.2 Advanced Ability In addition to the above basic evaluation tasks, LLMs also exhibit some superior abilities that require special consider- 65 ations for evaluation. In this part, we discuss several rep- resentative advanced abilities and the corresponding eval- uation approaches, including human alignment, interaction with the external environment, and tool manipulation. Next, we discuss these advanced abilities in detail. 7.2.1 Human Alignment It is desired that LLMs could well conform to human values and needs, i.e., human alignment, which is a key ability for the broad use of LLMs in real-world applications. To evaluate this ability, existing studies consider multiple criteria for human alignment, such as helpfulness, honesty, and safety [46, 170, 368]. For helpfulness and honesty, adver- sarial question answering tasks (e.g., TruthfulQA [556]) can be utilized to examine LLM’s ability in detecting possible falsehood in the text [46, 81]. Furthermore, harmlessness can be also evaluated by several existing benchmarks, e.g., CrowS-Pairs [603] and Winogender [604]. Despite the auto- matic evaluation with the above datasets, human evaluation is still a more direct way to effectively test the human alignment ability of LLMs. OpenAI invites many experts in domains related to AI risks to evaluate and improve the behaviors of GPT-4 when encountering risky contents [46]. In addition, for other aspects of human alignment (e.g., truthfulness), several studies propose to use specific instruc- tions and devise annotation rules to guide the annotation process [81]. Empirical studies have revealed tha"
        },
        "snippet": "thus likely improving the numerical computation ability of LLMs. Numerical Computation LLMs face difficulties in numerical computation, especially for the symbols that are seldom en- countered during pre-training. In addition to us- ing mathematical tools, tokenizing digits into in- dividual tokens is also an effective design choice for improving the arithmetic ability of LLMs. 7.2 Advanced Abilit"
      },
      {
        "rank": 9,
        "doc_id": "arxiv:2310.05492#experiments:part-1",
        "score": 0.41398913,
        "payload": {
          "doc_id": "arxiv:2310.05492#experiments:part-1",
          "url": "https://arxiv.org/abs/2310.05492",
          "anchor": "#experiments:part-1",
          "type": "paper",
          "title": "",
          "section": "Experiments",
          "source": "arxiv_pdf",
          "published": "",
          "tokens": 512,
          "sha256": "2a648be738faa9e8569ff2526fb82decc94d67fb88feffc7fa39b930f33b55fb",
          "text": "We have SFT datasets {D1, D2, ..., Dk} where each Di = {qi,j, ri,j}j contains queries and re- sponses from one source. We consider each SFT dataset to correspond to one ability and we also have k in-domain metrics to measure them. We investigate the performances of in-domain met- rics with different dataset compositions (D ⊂ ∪1≤i≤kDi) and training strategies on different sizes of LLMs. 3.1 Experiment Setup We collect three SFT datasets {D1, D2, D3} in- cluding GSM8K RFT (Yuan et al., 2023b), Code Alpaca (Chaudhary, 2023), and ShareGPT (Chiang et al., 2023) to represent math reasoning, coding, and general human-aligning ability SFT dataset re- spectively. We will integrate a new SFT dataset D 1The related work of \"Scaling Laws in Large Language Models\" can be found in Appendix K by these three datasets to investigate how data com- position affects the model performances. We use GSM8K test set (Cobbe et al., 2021), HumanEval (Chen et al., 2021), and MT-Bench (Zheng et al., 2023) to measure abilities including math reason- ing, coding, and general human-aligning. We use LLaMA (Touvron et al., 2023) series as our pre- trained language models and use FastChat frame- work (Zheng et al., 2023) for fine-tuning. We fine- tune models with 3 epochs and a peak of 2e-5 learn- ing rate. The batch size during SFT is 16. More details about SFT datasets, evaluation metrics, im- plementations and Training FLOPs can be found in Appendix A, B, C and D. 3.2 RQ1. Individual Ability Performance vs. Data Amount The instruction following ability can be activated via SFT on datasets like ShareGPT which contain around 100 thousand samples. However, (Zhou et al., 2023) demonstrates that strong base models can achieve human alignment with just 1000 sam- ples. Specialized abilities such as math reasoning require a large amount of data (Cobbe et al., 2021; Yuan et al., 2023b), unlike general abilities. There- fore, it is crucial to investigate how each ability improves as the data amount increase"
        },
        "snippet": "We have SFT datasets {D1, D2, ..., Dk} where each Di = {qi,j, ri,j}j contains queries and re- sponses from one source. We consider each SFT dataset to correspond to one ability and we also have k in-domain metrics to measure them. We investigate the performances of in-domain met- rics with different dataset compositions (D ⊂ ∪1≤i≤kDi) and training strategies on different sizes of LLMs. 3.1 Experim"
      },
      {
        "rank": 10,
        "doc_id": "arxiv:2310.05492#introduction:part-3",
        "score": 0.41195172,
        "payload": {
          "doc_id": "arxiv:2310.05492#introduction:part-3",
          "url": "https://arxiv.org/abs/2310.05492",
          "anchor": "#introduction:part-3",
          "type": "paper",
          "title": "",
          "section": "Introduction",
          "source": "arxiv_pdf",
          "published": "",
          "tokens": 336,
          "sha256": "0dd5e3924354e2e58e7f804e8ac5e610c6a48ad7ce26c336928553e45c72be53",
          "text": "instruction. We slightly abuse the term SFT to refer to general sequence-to-sequence fine-tuning, including but not limited to SFT for human alignment, instruc- tion fine-tuning, and downstream task fine-tuning. Recent research explored multi-task instruction fine-tuning of pre-trained LLMs to enable better zero-shot performance on various downstream NLP tasks (Sanh et al., 2022). (Chung et al., 2022a; Longpre et al., 2023) attempted to exhaust ex- isting NLP tasks and curated a massive dataset, FLAN, for instruction fine-tuning. Open-sourced (Chung et al., 2022b) and proprietary LLMs (Sing- hal et al., 2022) fine-tuned on FLAN exhibited improved zero-shot downstream performance on various held-out NLP tasks. However, the influ- ence of multi-task training of LLMs on in-domain performance is less studied. With the success of proprietary LLMs, especially ChatGPT, there has been increasing attention on SFT to align LLMs to human intentions (Ouyang et al., 2022b). Instead of generating SFT data from crowd-resourcing, re- cent research explored to generate data from pro- prietary LLM user logs (Chiang et al., 2023; Wang et al., 2023a), prompting proprietary LLM (Wang Ʈ푐표ǈǉ Ʈ푚ǅǘℎ Ʈ푔ǉ푛ǉǖǅ⯠ Multi-task Learning Ʈ푐표ǈǉ Ʈ푚ǅǘℎ Sepecialized Datasets 1st stage Ʈ푔ǉ푛ǉǖǅ⯠ 2nd stage K× Ʈ푐표ǈǉ Ʈ푚ǅǘℎ Dual-stage Mixed Fine-tuning (DMT) Ʈ푔ǉ푛ǉǖǅ⯠ Ʈ푐표ǈǉ Ʈ푚ǅǘℎ Sequential Training 2nd stage 3rd stage 1st stage Ʈ푐표ǈǉ Ʈ푚ǅǘℎ Sepecialized Datasets 1st stage Ʈ푔ǉ푛ǉǖǅ⯠ 2nd stage Mixed Sequential Training LLaMA LLaMA LLaMA LLaMA LLaMA LLaMA LLaMA Final Model Final Model Final Model Final Model Sepecialized Datasets Figure 1: The illustration of four different training strategies in this paper. et al., 2023c; Taori et al., 2023; Lei et al., 2023; Xu et al., 2023). Various analyses and methods have also been proposed to increase the SFT data quality (Zhou et al., 2023; Wang et al., 2023b; Lu et al., 2023) to achieve better alignment of open- resourced LLMs with humans. Besides, LLMs can also benefit fr"
        },
        "snippet": "instruction. We slightly abuse the term SFT to refer to general sequence-to-sequence fine-tuning, including but not limited to SFT for human alignment, instruc- tion fine-tuning, and downstream task fine-tuning. Recent research explored multi-task instruction fine-tuning of pre-trained LLMs to enable better zero-shot performance on various downstream NLP tasks (Sanh et al., 2022). (Chung et al., 2"
      }
    ]
  },
  {
    "id": "query_11",
    "query": "}",
    "results": [
      {
        "rank": 1,
        "doc_id": "blog:spectrum.ieee.org#robotics-307",
        "score": 0.19210479,
        "payload": {
          "doc_id": "blog:spectrum.ieee.org#robotics-307",
          "url": "https://spectrum.ieee.org/topic/robotics/",
          "anchor": "",
          "type": "blog",
          "title": "Robotics",
          "section": "Body",
          "source": "blog:spectrum.ieee.org",
          "published": "",
          "tokens": 37,
          "sha256": "47fc88d0577940f789d6631ef7b5ddf08f5566f8929b00ee16751d196672a4cc",
          "text": "Robotics News Humanoid Robots Caltech's M4 Robot Transforms, and More Robot Videos Your weekly selection of awesome robot videos 17 Oct 2025 3 min read"
        },
        "snippet": "Robotics News Humanoid Robots Caltech's M4 Robot Transforms, and More Robot Videos Your weekly selection of awesome robot videos 17 Oct 2025 3 min read"
      },
      {
        "rank": 2,
        "doc_id": "arxiv:2309.06180#introduction:part-1",
        "score": 0.19136003,
        "payload": {
          "doc_id": "arxiv:2309.06180#introduction:part-1",
          "url": "https://arxiv.org/abs/2309.06180",
          "anchor": "#introduction:part-1",
          "type": "paper",
          "title": "",
          "section": "Introduction",
          "source": "arxiv_pdf",
          "published": "",
          "tokens": 512,
          "sha256": "3a8b50cd35889bc375a1bdd3e2e72f5fd0656a2bd56f359c380028ff3fa1643e",
          "text": "The emergence of large language models (LLMs) like GPT [5, 37] and PaLM [9] have enabled new applications such as pro- gramming assistants [6, 18] and universal chatbots [19, 35] that are starting to profoundly impact our work and daily routines. Many cloud companies [34, 44] are racing to pro- vide these applications as hosted services. However, running these applications is very expensive, requiring a large num- ber of hardware accelerators such as GPUs. According to recent estimates, processing an LLM request can be 10× more expensive than a traditional keyword query [43]. Given these high costs, increasing the throughput—and hence reducing Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third- party components of this work must be honored. For all other uses, contact the owner/author(s). SOSP ’23, October 23–26, 2023, Koblenz, Germany © 2023 Copyright held by the owner/author(s). ACM ISBN 979-8-4007-0229-7/23/10. https://doi.org/10.1145/3600006.3613165 NVIDIA A100 40GB Parameters (26GB, 65%) KV Cache (>30%) Others 20 30 40 Memory usage (GB) Parameter size Existing systems vLLM 0 10 20 30 40 Batch size (# requests) 0 0.4k 0.8k 1.2k Throughput (token/s) Figure 1. Left: Memory layout when serving an LLM with 13B parameters on NVIDIA A100. The parameters (gray) persist in GPU memory throughout serving. The memory for the KV cache (red) is (de)allocated per serving request. A small amount of memory (yellow) is used ephemerally for activation. Right: vLLM smooths out the rapid growth curve of KV cache memory seen in existing systems [31, 60], leading to a notable boost in serving throughput. the cost per request—of LLM serving systems is becoming more important. At the core of LLMs lies an autoregressive Transformer model [5"
        },
        "snippet": "The emergence of large language models (LLMs) like GPT [5, 37] and PaLM [9] have enabled new applications such as pro- gramming assistants [6, 18] and universal chatbots [19, 35] that are starting to profoundly impact our work and daily routines. Many cloud companies [34, 44] are racing to pro- vide these applications as hosted services. However, running these applications is very expensive, requi"
      },
      {
        "rank": 3,
        "doc_id": "arxiv:1810.04805#related-work:part-2",
        "score": 0.19085027,
        "payload": {
          "doc_id": "arxiv:1810.04805#related-work:part-2",
          "url": "https://arxiv.org/abs/1810.04805",
          "anchor": "#related-work:part-2",
          "type": "paper",
          "title": "",
          "section": "Related Work",
          "source": "arxiv_pdf",
          "published": "",
          "tokens": 512,
          "sha256": "7924f8d6df85fc681a422ccf7692602c7e34535d1bbf4b9ae5d4dffb6258802f",
          "text": "few parameters need to be learned from scratch. At least partly due to this advantage, OpenAI GPT (Radford et al., 2018) achieved pre- viously state-of-the-art results on many sentence- level tasks from the GLUE benchmark (Wang et al., 2018a). Left-to-right language model- BERT BERT E[CLS] E1 E[SEP] ... EN E1’ ... EM’ C T1 T[SEP] ... TN T1’ ... TM’ [CLS] Tok 1 [SEP] ... Tok N Tok 1 ... TokM Question Paragraph Start/End Span BERT E[CLS] E1 E[SEP] ... EN E1’ ... EM’ C T1 T[SEP] ... TN T1’ ... TM’ [CLS] Tok 1 [SEP] ... Tok N Tok 1 ... TokM Masked Sentence A Masked Sentence B Pre-training Fine-Tuning NSP Mask LM Mask LM Unlabeled Sentence A and B Pair SQuAD Question Answer Pair NER MNLI Figure 1: Overall pre-training and ﬁne-tuning procedures for BERT. Apart from output layers, the same architec- tures are used in both pre-training and ﬁne-tuning. The same pre-trained model parameters are used to initialize models for different down-stream tasks. During ﬁne-tuning, all parameters are ﬁne-tuned. [CLS] is a special symbol added in front of every input example, and [SEP] is a special separator token (e.g. separating ques- tions/answers). ing and auto-encoder objectives have been used for pre-training such models (Howard and Ruder, 2018; Radford et al., 2018; Dai and Le, 2015). 2.3 Transfer Learning from Supervised Data There has also been work showing effective trans- fer from supervised tasks with large datasets, such as natural language inference (Conneau et al., 2017) and machine translation (McCann et al., 2017). Computer vision research has also demon- strated the importance of transfer learning from large pre-trained models, where an effective recipe is to ﬁne-tune models pre-trained with Ima- geNet (Deng et al., 2009; Yosinski et al., 2014). 3 BERT We introduce BERT and its detailed implementa- tion in this section. There are two steps in our framework: pre-training and ﬁne-tuning. Dur- ing pre-training, the model is trained on unlabeled data over different pre-trai"
        },
        "snippet": "few parameters need to be learned from scratch. At least partly due to this advantage, OpenAI GPT (Radford et al., 2018) achieved pre- viously state-of-the-art results on many sentence- level tasks from the GLUE benchmark (Wang et al., 2018a). Left-to-right language model- BERT BERT E[CLS] E1 E[SEP] ... EN E1’ ... EM’ C T1 T[SEP] ... TN T1’ ... TM’ [CLS] Tok 1 [SEP] ... Tok N Tok 1 ... TokM Questi"
      },
      {
        "rank": 4,
        "doc_id": "arxiv:2303.18223#model:part-2",
        "score": 0.18820736,
        "payload": {
          "doc_id": "arxiv:2303.18223#model:part-2",
          "url": "https://arxiv.org/abs/2303.18223",
          "anchor": "#model:part-2",
          "type": "paper",
          "title": "",
          "section": "Model",
          "source": "arxiv_pdf",
          "published": "",
          "tokens": 512,
          "sha256": "1b884a2c7214a97bbc0b6cf6b74e03cd30970f8c145314831d011ff895b5f6ce",
          "text": "3.2T tokens - 512 80G A800 - ✓ - GPT-3 [55] May-2020 175 - - - 300B tokens - - - ✓ - GShard [104] Jun-2020 600 - - - 1T tokens - 2048 TPU v3 4 d - - Codex [105] Jul-2021 12 GPT-3 - - 100B tokens May-2020 - - ✓ - ERNIE 3.0 [106] Jul-2021 10 - - - 375B tokens - 384 V100 - ✓ - Jurassic-1 [107] Aug-2021 178 - - - 300B tokens - 800 GPU - ✓ - HyperCLOVA [108] Sep-2021 82 - - - 300B tokens - 1024 A100 13.4 d ✓ - FLAN [67] Sep-2021 137 LaMDA-PT ✓ - - - 128 TPU v3 60 h ✓ - Yuan 1.0 [109] Oct-2021 245 - - - 180B tokens - 2128 GPU - ✓ - Anthropic [110] Dec-2021 52 - - - 400B tokens - - - ✓ - WebGPT [81] Dec-2021 175 GPT-3 - ✓ - - - - ✓ - Gopher [64] Dec-2021 280 - - - 300B tokens - 4096 TPU v3 920 h ✓ - ERNIE 3.0 Titan [111] Dec-2021 260 - - - - - - - ✓ - GLaM [112] Dec-2021 1200 - - - 280B tokens - 1024 TPU v4 574 h ✓ - LaMDA [68] Jan-2022 137 - - - 768B tokens - 1024 TPU v3 57.7 d - - MT-NLG [113] Jan-2022 530 - - - 270B tokens - 4480 80G A100 - ✓ - AlphaCode [114] Feb-2022 41 - - - 967B tokens Jul-2021 - - - - InstructGPT [66] Mar-2022 175 GPT-3 ✓ ✓ - - - - ✓ - Chinchilla [34] Mar-2022 70 - - - 1.4T tokens - - - ✓ - PaLM [56] Apr-2022 540 - - - 780B tokens - 6144 TPU v4 - ✓ ✓ AlexaTM [115] Aug-2022 20 - - - 1.3T tokens - 128 A100 120 d ✓ ✓ Sparrow [116] Sep-2022 70 - - ✓ - - 64 TPU v3 - ✓ - WeLM [117] Sep-2022 10 - - - 300B tokens - 128 A100 40G 24 d ✓ - U-PaLM [118] Oct-2022 540 PaLM - - - - 512 TPU v4 5 d ✓ ✓ Flan-PaLM [69] Oct-2022 540 PaLM ✓ - - - 512 TPU v4 37 h ✓ ✓ Flan-U-PaLM [69] Oct-2022 540 U-PaLM ✓ - - - - - ✓ ✓ GPT-4 [46] Mar-2023 - - ✓ ✓ - - - - ✓ ✓ PanGu-Σ [119] Mar-2023 1085 PanGu-α - - 329B tokens - 512 Ascend 910 100 d ✓ - Closed Source PaLM2 [120] May-2023 16 - ✓ - 100B tokens - - - ✓ ✓ 9 2020 2023 2021 1-4 5-8 9-10 1-3 7-10 11-12 T5 GPT-3 WebGPT BLOOMZ Galatica mT0 2019 FLAN InstructGPT GPT-NeoX-20B CodeGen OPT OPT-IML MT-NLG T0 Tk-Instruct GPT-4 GShard UL2 PaLM Flan-T5 Flan-PaLM Sparrow ChatGPT Ernie 3.0 Titan Yuan 1.0 Gopher GLaM mT5 PanGu-𝛂 PLUG LaMDA "
        },
        "snippet": "3.2T tokens - 512 80G A800 - ✓ - GPT-3 [55] May-2020 175 - - - 300B tokens - - - ✓ - GShard [104] Jun-2020 600 - - - 1T tokens - 2048 TPU v3 4 d - - Codex [105] Jul-2021 12 GPT-3 - - 100B tokens May-2020 - - ✓ - ERNIE 3.0 [106] Jul-2021 10 - - - 375B tokens - 384 V100 - ✓ - Jurassic-1 [107] Aug-2021 178 - - - 300B tokens - 800 GPU - ✓ - HyperCLOVA [108] Sep-2021 82 - - - 300B tokens - 1024 A100 13"
      },
      {
        "rank": 5,
        "doc_id": "arxiv:2212.03551#abstract",
        "score": 0.18685251,
        "payload": {
          "doc_id": "arxiv:2212.03551#abstract",
          "url": "https://arxiv.org/abs/2212.03551",
          "anchor": "#abstract",
          "type": "paper",
          "title": "",
          "section": "Abstract",
          "source": "arxiv_pdf",
          "published": "",
          "tokens": 153,
          "sha256": "911dfcabfe0b4d67eb2269daa1c588142eb9354b39285e812ff72a60e643ab07",
          "text": "Thanks to rapid progress in artiﬁcial intelligence, we have entered an era when technology and philosophy intersect in interesting ways. Sit- ting squarely at the centre of this intersection are large language models (LLMs). The more adept LLMs become at mimicking human lan- guage, the more vulnerable we become to an- thropomorphism, to seeing the systems in which they are embedded as more human-like than they really are. This trend is ampliﬁed by the natu- ral tendency to use philosophically loaded terms, such as “knows”, “believes”, and “thinks”, when describing these systems. To mitigate this trend, this paper advocates the practice of repeatedly stepping back to remind ourselves of how LLMs, and the systems of which they form a part, ac- tually work. The hope is that increased scien- tiﬁc precision will encourage more philosophical nuance in the discourse around artiﬁcial intelli- gence, both within the ﬁeld and in the public sphere. 1"
        },
        "snippet": "Thanks to rapid progress in artiﬁcial intelligence, we have entered an era when technology and philosophy intersect in interesting ways. Sit- ting squarely at the centre of this intersection are large language models (LLMs). The more adept LLMs become at mimicking human lan- guage, the more vulnerable we become to an- thropomorphism, to seeing the systems in which they are embedded as more human-l"
      },
      {
        "rank": 6,
        "doc_id": "arxiv:2303.18223#model:part-37",
        "score": 0.18433464,
        "payload": {
          "doc_id": "arxiv:2303.18223#model:part-37",
          "url": "https://arxiv.org/abs/2303.18223",
          "anchor": "#model:part-37",
          "type": "paper",
          "title": "",
          "section": "Model",
          "source": "arxiv_pdf",
          "published": "",
          "tokens": 512,
          "sha256": "f35085eb5b9dbb24726ee6d57526f6eee81e02dc6a801e72e88847b1f40e5312",
          "text": "29, 2023, and second version on September 10, 2023, and this latest version (major revision) on November 23, 2023. Seeking for Advice. Despite all our efforts, this survey is still far from perfect: we are likely to miss important references or topics, and might also have non-rigorous expressions or discussions. We will continuously update this survey, and improve the quality as much as we can. For us, survey writing is also a learning process for LLMs by ourselves. For readers with constructive suggestions to improve this survey, you are welcome to leave comments on the GitHub page of our survey or directly email our authors. We will make revisions following the received comments or suggestions in a future version, and acknowledge the readers who have contributed constructive suggestions in our survey. Update log. In this part, we regularly maintain an update log for the submissions of this survey to arXiv: • First release on March 31, 2023: the initial version. • Update on April 9, 2023: add the affiliation information, revise Figure 3 and Table 1 and clarify the correspond- ing selection criterion for LLMs, improve the writing, and correct some minor errors. • Update on April 11, 2023: correct the errors for library resources. • Update on April 12, 2023: revise Figure 3 and Table 1, and clarify the release date of LLMs. • Update on April 16, 2023: add a new Section 2.2 about the technical evolution of GPT-series models. • Update on April 24, 2023: add the discussion about scaling laws and add some explanations about the model sizes for emergent abilities (Section 2.1); add an illustrative figure for the attention patterns for different architectures in Figure 9, and add the detailed formulas in Table 6. • Update on April 25, 2023: revise some copy errors in figures and tables. • Update on April 27, 2023: add efficient tuning in Sec- tion 5.3. • Update on April 28, 2023: revise Section 5.3. • Update on May 7, 2023: revise Table 1, Table 2, and some minor points. •"
        },
        "snippet": "29, 2023, and second version on September 10, 2023, and this latest version (major revision) on November 23, 2023. Seeking for Advice. Despite all our efforts, this survey is still far from perfect: we are likely to miss important references or topics, and might also have non-rigorous expressions or discussions. We will continuously update this survey, and improve the quality as much as we can. Fo"
      },
      {
        "rank": 7,
        "doc_id": "arxiv:2307.10169#introduction:part-1",
        "score": 0.18366638,
        "payload": {
          "doc_id": "arxiv:2307.10169#introduction:part-1",
          "url": "https://arxiv.org/abs/2307.10169",
          "anchor": "#introduction:part-1",
          "type": "paper",
          "title": "",
          "section": "Introduction",
          "source": "arxiv_pdf",
          "published": "",
          "tokens": 512,
          "sha256": "9b5ecbb794b30ca4e3f1f8eea521192f9857fb50923487b21bb7ce09f5872598",
          "text": "Given the quickly growing plethora of LLM re- search papers, we aim to address two questions: (1) Challenges: What problems remain unresolved? and (2) Applications: Where are LLMs currently being applied, and how are the challenges con- straining them? For (1), we group the challenges 1 arXiv:2307.10169v1 [cs.CL] 19 Jul 2023 in Fig. 1 into three broader categories “Design”, “Behavior”, and “Science”. To provide answers for (2), we explore the fields of chatbots, compu- tational biology, computer programming, creative work, knowledge work, law, medicine, reasoning, robotics, and the social sciences. This paper is an opinionated review and assumes familiarity with LLMs and how they work (we refer to more introductory works in Sec. 4). Further, we focus on models trained on text data. We target a technical researcher audience and do not discuss political, philosophical, or moral perspectives on LLMs. 2 Challenges o Challenge This box highlights a challenge. 2.1 Unfathomable Datasets Scaling the amount of pre-training data has been one of the major drivers to equip LLMs with general-purpose capabilities [256]. The size of pre-training datasets quickly outgrew the number of documents most human teams could manually quality-check. Instead, most data collection proce- dures rely on heuristics regarding data sources and filtering. In this section, we explore the adverse conse- quences of these heuristics and the reality that many model practitioners possess only a nebulous under- standing of the data on which their model has been trained. We refer to this issue as follows. o Unfathomable Datasets The size of modern pre-training datasets ren- ders it impractical for any individual to read or conduct quality assessments on the en- compassed documents thoroughly. Near-Duplicates can arise in different forms and have been reported to degrade model per- formance [294, 200, 250]. Near-duplicates are harder to find compared to exact duplicates; fil- tering out of such is a standar"
        },
        "snippet": "Given the quickly growing plethora of LLM re- search papers, we aim to address two questions: (1) Challenges: What problems remain unresolved? and (2) Applications: Where are LLMs currently being applied, and how are the challenges con- straining them? For (1), we group the challenges 1 arXiv:2307.10169v1 [cs.CL] 19 Jul 2023 in Fig. 1 into three broader categories “Design”, “Behavior”, and “Scienc"
      },
      {
        "rank": 8,
        "doc_id": "arxiv:2302.14045#abstract:part-2",
        "score": 0.18134171,
        "payload": {
          "doc_id": "arxiv:2302.14045#abstract:part-2",
          "url": "https://arxiv.org/abs/2302.14045",
          "anchor": "#abstract:part-2",
          "type": "paper",
          "title": "",
          "section": "Abstract",
          "source": "arxiv_pdf",
          "published": "",
          "tokens": 512,
          "sha256": "af1e5c790b7002c5ea542a8256e9fca1815884a029fe46eefeb07d21b37546b1",
          "text": "Answer: Starbucks (3) Corn Question: What is inside this canned good? Answer: (4) Completion (5) (6) (7) (8) What's in this picture? sausage roll. How to cook it? Soak sausage roll in ketchup. Bake in oven for 15 min. Cut and enjoy. Can I put cheese in the dish? Sure. But make sure it is melted. What kind of animal is in the picture? It's a zebra. Where do they live in? In Africa. Tell me a joke about zebras. Zebras get in trouble for crossing the road. (9) (10) What's in this picture? A screenshot of the Windows 10 shutdown window I would like to restart my computer. Which button should I click? Press OK. Now I would not like to restart. What can I do? Click Cancel. (11) Figure 3: Selected examples generated from KOSMOS-1. Blue boxes are input prompt and pink boxes are KOSMOS-1 output. The examples include (1)-(2) image captioning, (3)-(6) visual question answering, (7)-(8) OCR, and (9)-(11) visual dialogue. 3 Dataset Task description Metric Zero-shot Few-shot Language tasks StoryCloze [MRL+17] Commonsense reasoning Accuracy \u0013 \u0013 HellaSwag [ZHB+19] Commonsense NLI Accuracy \u0013 \u0013 Winograd [LDM12a] Word ambiguity Accuracy \u0013 \u0013 Winogrande [SBBC20] Word ambiguity Accuracy \u0013 \u0013 PIQA [BZB+20] Physical commonsense Accuracy \u0013 \u0013 BoolQ [CLC+19] Question answering Accuracy \u0013 \u0013 CB [dMST19] Textual entailment Accuracy \u0013 \u0013 COPA [RBG11] Causal reasoning Accuracy \u0013 \u0013 Rendered SST-2 [RKH+21] OCR-free sentiment classiﬁcation Accuracy \u0013 HatefulMemes [KFM+20] OCR-free meme classiﬁcation ROC AUC \u0013 Cross-modal transfer RelativeSize [BHCF16] Commonsense reasoning (object size) Accuracy \u0013 MemoryColor [NHJ21] Commonsense reasoning (object color) Accuracy \u0013 ColorTerms [BBBT12] Commonsense reasoning (object color) Accuracy \u0013 Nonverbal reasoning tasks IQ Test Raven’s Progressive Matrices Accuracy \u0013 Perception-language tasks COCO Caption [LMB+14] Image captioning CIDEr, etc. \u0013 \u0013 Flicker30k [YLHH14] Image captioning CIDEr, etc. \u0013 \u0013 VQAv2 [GKSS+17] Visual question answering VQA acc. \u0013 \u0013 VizWiz [GLS+1"
        },
        "snippet": "Answer: Starbucks (3) Corn Question: What is inside this canned good? Answer: (4) Completion (5) (6) (7) (8) What's in this picture? sausage roll. How to cook it? Soak sausage roll in ketchup. Bake in oven for 15 min. Cut and enjoy. Can I put cheese in the dish? Sure. But make sure it is melted. What kind of animal is in the picture? It's a zebra. Where do they live in? In Africa. Tell me a joke a"
      },
      {
        "rank": 9,
        "doc_id": "arxiv:2302.14045#abstract:part-7",
        "score": 0.18110964,
        "payload": {
          "doc_id": "arxiv:2302.14045#abstract:part-7",
          "url": "https://arxiv.org/abs/2302.14045",
          "anchor": "#abstract:part-7",
          "type": "paper",
          "title": "",
          "section": "Abstract",
          "source": "arxiv_pdf",
          "published": "",
          "tokens": 387,
          "sha256": "733acac01fdb6d88e98bbd0dd5eea4afef766a20ab5f8bb546e913b74a700b1f",
          "text": "has 24 layers with 2,048 hidden dimensions, 8,192 FFN intermediate size, and 32 attention heads, resulting in about 1.3B parameters. We use Magneto’s initialization for optimization stability. For faster convergence, the image representation is obtained from a pretrained CLIP ViT-L/14 model with 1,024 feature dimensions. The images are preprocessed into 224×224 resolution during training. We freeze the parameters of the CLIP model except for the last layer during training. The total number of parameters of KOSMOS-1 is about 1.6B. More details about hyperparameters can be found in Appendix A. We use a batch size of 1.2 million tokens (0.5 million tokens from text corpora, 0.5 million tokens from image-caption pairs, and 0.2 million tokens from interleaved data) and train KOSMOS-1 for 300k steps, corresponding to about 360 billion tokens. We adopt the AdamW optimizer with β = (0.9, 0.98). We set the weight decay to 0.01 and the dropout rate to 0.1. The learning rate increases to 2e-4 for the ﬁrst 375 warming-up steps and decays linearly to 0 for the rest of the training steps. We use SentencePiece [KR18] to tokenize the text. We preprocess the data in the “full-sentence” format [LOG+19], which packs each input sequence with full sentences that are sampled continuously from one or more documents. 3.3 Language-Only Instruction Tuning In order to better align KOSMOS-1 with human instructions, we perform language-only instruction tuning [LHV+23, HSLS22]. Speciﬁcally, we continue-train the model with the instruction data in the format of (instructions, inputs, and outputs). The instruction data is language-only, which is mixed with training corpora. The tuning process is conducted as language modeling. Notice that instructions and inputs are not accounted for in the loss. Section 4.9.1 shows that the improvements in the instruction-following capability can transfer across modalities. We combine Unnatural Instructions [HSLS22] and FLANv2 [LHV+23] as our instruction dataset."
        },
        "snippet": "has 24 layers with 2,048 hidden dimensions, 8,192 FFN intermediate size, and 32 attention heads, resulting in about 1.3B parameters. We use Magneto’s initialization for optimization stability. For faster convergence, the image representation is obtained from a pretrained CLIP ViT-L/14 model with 1,024 feature dimensions. The images are preprocessed into 224×224 resolution during training. We freez"
      },
      {
        "rank": 10,
        "doc_id": "blog:spectrum.ieee.org#robotics-306",
        "score": 0.17517447,
        "payload": {
          "doc_id": "blog:spectrum.ieee.org#robotics-306",
          "url": "https://spectrum.ieee.org/top-robotics-stories-2024",
          "anchor": "",
          "type": "blog",
          "title": "The Top 7 Robotics Stories of 2024",
          "section": "Body",
          "source": "blog:spectrum.ieee.org",
          "published": "",
          "tokens": 84,
          "sha256": "25bd1011a1a25635f921a64cddf3e569285fd2fd56ec3c17eff2fd57a660a241",
          "text": "Robotics News The Top 7 Robotics Stories of 2024 A new Atlas, Figure's bonkers funding round, and the last voyage of a helicopter on Mars Evan Ackerman 29 Dec 2024 4 min read Evan Ackerman is IEEE Spectrum’s robotics editor. Clockwise from top left: Figure; Boston Dynamics; Machina Labs; Evan Ackerman; Boston Dynamics; NASA (2); Nvidia"
        },
        "snippet": "Robotics News The Top 7 Robotics Stories of 2024 A new Atlas, Figure's bonkers funding round, and the last voyage of a helicopter on Mars Evan Ackerman 29 Dec 2024 4 min read Evan Ackerman is IEEE Spectrum’s robotics editor. Clockwise from top left: Figure; Boston Dynamics; Machina Labs; Evan Ackerman; Boston Dynamics; NASA (2); Nvidia"
      }
    ]
  }
]