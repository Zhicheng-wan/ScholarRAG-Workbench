{"doc_id": "arxiv:2404.10630#model", "url": "https://arxiv.org/abs/2404.10630", "anchor": "#model", "type": "paper", "title": "", "section": "Model", "text": "Sizes Sequence length OpenLLaMA1 7B, 13B 2048 OpenLLaMA2 7B 2048 LLaMA1 7B, 13B, 33B, 65B 2048 LLaMA2 7B, 13B, 70B 4096 HLAT 7B, 70B 4096 Evaluation Tasks: We evaluate HLAT against baselines on 7 groups of tasks including both zero-shot and few-shot tasks [36]. We use HumanEval [37] for coding tasks, and Language Model Evaluation Harness [38] for others. Massive Multitask Language Understanding (MMLU) [15], [39] contains 57 tasks, spanning STEM, social sciences, humanities, and other subjects. The difficulty ranges from elementary to professional levels. The breadth of the dataset tests model’s overall problem solving and knowledge ability. Commonsense Reasoning (CR) consists of 6 datasets: PIQA [40], HellaSwag [41], WinoGrande [42], ARC easy and challenge [43], and OpenBookQA [29]. These multi- choice tasks include carefully crafted riddles, puzzles, and scenarios designed to probe a model’s ability to leverage implicit knowledge, make logical inferences, and navigate the rules of physical and social worlds. World Knowledge (WK) includes NaturalQuestions [44] and TriviaQA [45]. Both tasks are designed to test model’s question-answering ability in closed book setting. The models are not provided documents that may contain information about the question, and it has to rely on information learnt or memorized in pre-training data. Reading Comprehension (RC) uses BoolQ [46] to test model’s open book comprehension ability. BoolQ is a question answering dataset for yes/no questions. Each example is a triplet of (question, passage, answer), with the title of the page as optional additional context. The model is required to answer the question based on the given context in passage. Math ability is evaluated with GSM8K (Grade School Math 8K) [47]. GSM8K contains 8,500 grade school math problems. Both problems and answers are provided in natural language. These problems take between 2 and 8 steps to solve, which is ideal for testing basic multi-step reasoning ability. Code evaluation uses HumanEval [37] dataset including 164 programming problems with a function signature, docstring, body, and several unit tests. They were handwritten to ensure not present in the training set of the models. A. Performance against open-source Models We compare the performance of HLAT with other open- source benchmarks in Table II. The numbers are reported in percentage and for HLAT results, we include both mean and TABLE II: Evaluation of HLAT against 4 open-source models on 6 groups of tasks described in Section V. Numbers in the parentheses represent standard deviation, if available.", "source": "arxiv_pdf", "published": "", "tokens": 397, "sha256": "1f7a04ae12cdbefc36365643cd475c081a310e246a966cd9d7d68b198939e1e1"}
{"doc_id": "arxiv:2404.10630#model:part-1", "url": "https://arxiv.org/abs/2404.10630", "anchor": "#model:part-1", "type": "paper", "title": "", "section": "Model", "text": "Size MMLU CR WK RC Math Code Average - - accuracy accuracy exact match accuracy accuracy pass@1 pass@10 - OpenLLaMA-1 7B 30.5 58.4 40.6 70.5 5.2 4.5 13.4 41.2 OpenLLaMA-2 7B 41.1 61.3 37.9 72.4 6.8 9.7 25 44.9 LLaMA-1 7B 35.1 63.5 43.6 76.5 11 10.5 21.3 47.4 LLaMA-2 7B 45.3 64 45.2 77.4 14.6 12.2 25.2 49.2 HLAT-7B 7B 41.3 (3.6) 59.5 (1.2) 38.8 (0.5) 72.5 (0.8) 9.4 (0.8) 7.6 19.8 44.6 OpenLLaMA-1 13B 43.5 62 45.9 72.3 8.3 7 17 47.1 LLaMA-1 13B 46.9 65.3 49.7 78.1 17.8 15.8 22.6 53.1 LLaMA-2 13B 45.3 66.3 50.5 81.7 28.7 18.3 30.5 54.6 LLaMA-1 33B 57.8 68.9 54.6 83.1 35.6 21.7 38.4 59.2 LLaMA-1 65B 63.4 69.8 57 85.3 50.9 23.7 - 62.1 LLaMA-2 70B 68.9 70.7 59 85 56.8 30.5 59.4 64.7 HLAT-70B 70B 65.1 (3.4) 67.3 (1.2) 54.5 (0.6) 82.6 (0.7) 48.5 (1.4) 21.4 57.9 60.8 standard deviation (in the parentheses, if available). We also report an average score over all tasks in the last column. HLAT-7B performs better than OpenLLaMA-1 and is on- par with OpenLLaMA-2. Both HLAT-7B and OpenLLaMA models have some gap with LLaMA-1 and LLaMA-2, which is likely due to the training data quality. Even though the data composition of RedPajama-1T is similar as those used in LLaMA-1, the data cleaning pipeline and final data quality are different, which therefore affects the model performance [48]. For HLAT-70B, we use the same training dataset as the 7B model for consistency. Although there is no OpenLLaMA baseline for a fair comparison, HLAT-70B performs better than LLaMA-1 and LLaMA-2 models of smaller sizes. The model performance gap with LLaMA-1 (65B) and LLaMA-2 (70B) is also smaller than those on 7B models. We acknowl- edge the lack of effort on data quality improvement, but our main goal is to showcase the effectiveness and efficiency of AWS TRAINIUM. On MMLU (5-shot), both HLAT models perform better than OpenLLaMA-1 and LLaMA-1 models of similar size. The performance is slightly worse than LLaMA-2 family of models, likely due to the difference in training dataset size and composition [8]. On Commonsense Reasoning (0-shot) and World Knowl- edge (5-shot), HLAT-7B performs similar to OpenLLaMA-1 and OpenLLaMA-2 models. By diving deep into performance on each individual task, HLAT-7B excels in 19/29 tasks as compared with OpenLLaMA-1, and 15/29 tasks compared with OpenLLaMA-2. Both HLAT and OpenLLaMA models have some gaps with LLaMA-1 and LLaMA-2 models, which may be due to the training set quality. Nevertheless, the gap (∼3%) is consistent on 7B and 70B models. On Math problems (GSM8K, 8-shot), HLAT-7B performs significantly better than OpenLLaMA-1 and OpenLLaMA-2. As will be discussed in the next section, HLAT-70B has a big improvement of Math ability in later training stage. HLAT- 70B performs similar as LLaMA1-65B, and we observed significant improvement in upsampling training stage. On Coding problems, both HLAT-7B and HLAT-70B perform comparable with LLaMA-1. HLAT-7B performs better than OpenLLaMA-1 and worse than OpenLLaMA- 2 and LLaMA-2. First, for OpenLLaMA-1, the tokenizer merges consecutive spaces which negatively affects the coding performance, as it eliminates important information", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "a7552a7c57daa5102b6e9cb04cf7a777ce27d4c763396756430de988a1e565d1"}
{"doc_id": "arxiv:2404.10630#model:part-2", "url": "https://arxiv.org/abs/2404.10630", "anchor": "#model:part-2", "type": "paper", "title": "", "section": "Model", "text": "OpenLLaMA-1 and OpenLLaMA-2. As will be discussed in the next section, HLAT-70B has a big improvement of Math ability in later training stage. HLAT- 70B performs similar as LLaMA1-65B, and we observed significant improvement in upsampling training stage. On Coding problems, both HLAT-7B and HLAT-70B perform comparable with LLaMA-1. HLAT-7B performs better than OpenLLaMA-1 and worse than OpenLLaMA- 2 and LLaMA-2. First, for OpenLLaMA-1, the tokenizer merges consecutive spaces which negatively affects the coding performance, as it eliminates important information such as indentation and line breaks. This issue is subsequently fixed in OpenLLaMA-2, which explains its better performance. Besides, OpenLLaMA-2 is trained with additional code data from StarCoder which also contributes to performance im- provement. B. Intermediate Model Performance During the model training, we also evaluate the intermediate checkpoints about every 200 billion tokens. Figure 3 and Figure 4 show the model performance of HLAT-7B and HLAT-70B with respect to number of seen training tokens (in billions), respectively. On most benchmarks, the performance improves steadily, and correlates with the training loss. We found that for different tasks, the model converges at different rates. For Commonsense Reasoning, the model accuracy improves quickly at beginning of training, and starts to saturate at later training stages. This is similar as the trends observed in other LLM model trainings [8], [49]. However, for Math task (GSM8K) shown in Figure 3e, the learning curve shows an exponentially increasing trend. It increase very gradually for the initial ∼1 trillion tokens and begins to improve significantly during the later stages of training. Intuitively, this seems to indicate that the model is able to grasp more logical abilities after entering a relatively stable training plateau. We defer further research into this behavior as a future work. For World Knowledge task shown in Figure 3c, the per- formance increases almost linearly with number of training tokens. Since this is a closed book test and mainly evaluates the model’s ability of memorizing facts in pre-training data, the model seems to consistently improve its ability on this domain with more training steps and epochs. In addition, we tested if the trending is related to number of shots used in evaluation. It turns out that the trends are very similar for zero-shot, 3-shot, and 5-shot tests. 250 500 750 1000 1250 1500 1750 Tokens (in Billions) 25 30 35 40 45 Accuracy (norm) MMLU (a) 250 500 750 1000 1250 1500 1750 Tokens (in Billions) 50 52 54 56 58 60 Accuracy (norm) Commonsense Reasoning (b) 250 500 750 1000 1250 1500 1750 Tokens (in Billions) 5 10 15 20 25 30 35 40 Exact match World Knowledge 0 shots 3 shots 5 shots (c) 250 500 750 1000 1250 1500 1750 Tokens (in Billions) 55.0 57.5 60.0 62.5 65.0 67.5 70.0 72.5 Accuracy BoolQ (d) 250 500 750 1000 1250 1500 1750 Tokens (in Billions) 2 4 6 8 10 Accuracy GSM8K (e) 250 500 750 1000 1250 1500 1750 Tokens (in Billions) 5 10 15 20 25 30 35 Percentage Code pass@1 pass@10 pass@100 (f) Fig. 3: Intermediate model performance with number", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "e73cb464adeaf1fc0a25a88e8ec0a68dbd5f59bd15a217af33fa8f0378a3727b"}
{"doc_id": "arxiv:2404.10630#model:part-3", "url": "https://arxiv.org/abs/2404.10630", "anchor": "#model:part-3", "type": "paper", "title": "", "section": "Model", "text": "Exact match World Knowledge 0 shots 3 shots 5 shots (c) 250 500 750 1000 1250 1500 1750 Tokens (in Billions) 55.0 57.5 60.0 62.5 65.0 67.5 70.0 72.5 Accuracy BoolQ (d) 250 500 750 1000 1250 1500 1750 Tokens (in Billions) 2 4 6 8 10 Accuracy GSM8K (e) 250 500 750 1000 1250 1500 1750 Tokens (in Billions) 5 10 15 20 25 30 35 Percentage Code pass@1 pass@10 pass@100 (f) Fig. 3: Intermediate model performance with number of seen tokens for HLAT-7B. 250 500 750 1000 1250 1500 1750 Tokens (in Billions) 30 40 50 60 70 Accuracy MMLU (a) 250 500 750 1000 1250 1500 1750 Tokens (in Billions) 54 56 58 60 62 64 66 68 Accuracy (norm) Commonsense Reasoning (b) 250 500 750 1000 1250 1500 1750 Tokens (in Billions) 35 40 45 50 55 Exact match World Knowledge (c) 250 500 750 1000 1250 1500 1750 Tokens (in Billions) 65 70 75 80 85 Accuracy BoolQ (d) 250 500 750 1000 1250 1500 1750 Tokens (in Billions) 0 10 20 30 40 50 Exact match GSM8K (e) 200 400 600 800 1000 1200 1400 1600 1800 Tokens (in Billions) 10 20 30 40 50 Percentage Code pass@1 pass@10 (f) Fig. 4: Intermediate model performance with number of seen tokens for HLAT-70B. Those observations indicate the necessity of a set of eval- uation tasks covering a wide range of domains for LLM pre-training. A single validation set or evaluation tasks from narrow domains may not fully reflect the actual over- or under- fitting of the model for general downstream performance. C. Upsampling During HLAT-70B training, we upsampled the training dataset in last 400B tokens. Specifically, we use 35.47% web data, 41.27% math data, and 23.26% coding data with more details listed in Table III. In Figure 4, we plot the evalua- tion performance of HLAT-70B with seen training tokens. In upsampling training stage, that is, after 1400B tokens, TABLE III: Upsampling dataset composition for HLAT-70B. Datasets Size Percentage (billions of tokens) Web Data Wikipedia [21] 90 35.47% C4 [21] Domain Specific StackExchange 104.7 41.27% Arxiv [21] Open-Web-Math [23] PeS2o [22] Code Github [21] 59 23.26% Total - 253.7 15.16% we observe significant model performance improvement over math, coding, and MMLU performance. It improved math by 10% and coding by 5%. This is consistent with the findings in LLaMA-3 [10], where the researchers found significant improvement of LLaMA-3 8B model on math problems. However, they mentioned such method did not help much for 405B models. Our experiment fills the model size gap, and shows that upsampling still helps for a 70B model. D. Checkpoint Averaging For HLAT-70B, we average the last two checkpoints used in pre-training to generate a checkpoint for final evaluation [10], [33]. Table IV compares the individual checkpoints with 1740B training tokens and 1800B training tokens, as well as the averaged checkpoints. The averaged checkpoints outperforms individual checkpoints on average performance, as well as some individual tasks. TABLE IV: Evaluation of HLAT-70B with individual and averaged checkpoints. Checkpoint MMLU RC WK CR Math Code", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "a8bb281e58c90887129de05b676493b851f3bec063df775da898963a0c5d5a78"}
{"doc_id": "arxiv:2404.10630#model:part-4", "url": "https://arxiv.org/abs/2404.10630", "anchor": "#model:part-4", "type": "paper", "title": "", "section": "Model", "text": "helps for a 70B model. D. Checkpoint Averaging For HLAT-70B, we average the last two checkpoints used in pre-training to generate a checkpoint for final evaluation [10], [33]. Table IV compares the individual checkpoints with 1740B training tokens and 1800B training tokens, as well as the averaged checkpoints. The averaged checkpoints outperforms individual checkpoints on average performance, as well as some individual tasks. TABLE IV: Evaluation of HLAT-70B with individual and averaged checkpoints. Checkpoint MMLU RC WK CR Math Code Avg. 1740B 64.5 67.5 54 83.1 47.3 18.3 60.3 1800B 64.2 66.9 54 82.1 47.2 21.8 60.2 Average 65.1 67.3 54.5 82.6 48.5 21.4 60.8 E. Truthfulness and Bias We report the model’s truthfulness and bias using Truth- fulQA [50] and CrowS-pairs [51]. TruthfulQA presents a collection of meticulously crafted questions spanning diverse domains such as health, law, finance, and even politics. These queries deliberately target areas where human intuition and personal biases can lead to incorrect responses, and measure an LLM’s resistance to misinformed or erroneous knowledge. CrowS-Pairs is a benchmark designed to probe LLMs for social biases across nine categories, including gender, reli- gion, race/color, sexual orientation, age, nationality, disability, physical appearance and socioeconomic status. Each example is composed of a stereotype and an anti-stereotype. TABLE V: Model Truthfulness and Bias evaluation. CrowS- pairs (CSP) uses percentage of stereotypes as metric and TruthfulQA (TQA) uses multiple choice accuracy as metric. Dataset Size CSP (↓) CSP TQA (↑) TQA Tasks - english french mc1 mc2 OpenLLaMA-1 7B 64.6 50.1 23.1 35.1 OpenLLaMA-2 7B 65.6 51.7 22.6 34.6 LLaMA-1 7B 53.7 47.5 22.0 34.1 LLaMA-2 7B 66.9 54.9 25.2 39.0 HLAT-7B 7B 65.2 54.5 23.6 37.2 LLaMA-1 65B 69.3 58.3 27.9 42.6 LLaMA-2 70B 69.8 63.5 30.6 44.8 HLAT-70B 70B 68.1 59.1 32.3 45.9 We present the results in Table V with 0 shot inference. For TruthfulQA, we measure the multiple-choice score, and higher score shows better truthfulness. For CrowS-Pairs, it measures the percentage of models choosing answers of stereotypes, so lower scores indicates smaller bias. Overall, HLAT performs similar to other open-source models. F. Efficiency and Scalability We describe the training efficiency in terms of Cost per 4-million tokens (CPT) and scalability reported in [52]. The CPT is defined as CPT = C T ×3600 × 4e6, where C is instance cost per hour ($21.50 for Trainium, and $32.77 for GPU), T is training throughput (tokens per second). CPT quantifies both the training speed and also hardware cost. We use this metric to compare training efficiency of Trainium and GPU. 4 8 16 32 64 Number of nodes 0 2 4 6 8 10 12 Normalized CPT trn1, 7B p4d, 7B trn1, 70B p4d, 70B Fig. 5: Normalized cost per 4 million tokens (CPT) for 7B and 70B models on AWS TRAINIUM with various number of nodes. CPT of GPU baseline (p4d, 7B) with 4 nodes is normalized to 100%. 70B models ran into out-of-memory on 4 nodes. For comparison, the GPU baseline is established using p4d.24xlarge instances and NeMo 23.08 [25] (available inside NeMo docker container with tag 23.08) software", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "16290b00b94467bce4ef9dc5eb01a8d0c4f47501d979f21cfe631896e70c702d"}
{"doc_id": "arxiv:2404.10630#model:part-5", "url": "https://arxiv.org/abs/2404.10630", "anchor": "#model:part-5", "type": "paper", "title": "", "section": "Model", "text": "6 8 10 12 Normalized CPT trn1, 7B p4d, 7B trn1, 70B p4d, 70B Fig. 5: Normalized cost per 4 million tokens (CPT) for 7B and 70B models on AWS TRAINIUM with various number of nodes. CPT of GPU baseline (p4d, 7B) with 4 nodes is normalized to 100%. 70B models ran into out-of-memory on 4 nodes. For comparison, the GPU baseline is established using p4d.24xlarge instances and NeMo 23.08 [25] (available inside NeMo docker container with tag 23.08) software stack. Fig- ure 5 plots the normalized CPT of training on TRAINIUM and scaling. The TRAINIUM CPT is normalized, such that the CPT of the GPU baseline (p4d, 7B) on 4 nodes is 100%. Overall, the training cost on trn1 is approximately 60% of GPU, and is consistent with the number of nodes. In addition, the CPTs on 70B models are roughly 10 times of those on 7B models. G. Model Limitation We note some limitations of HLAT in this section. Similar as other LLMs, HLAT suffers a set of limitations such as hallucinations, potential non-factual generations, biases, and toxicity [53]. For example, although comparable with other open-source pre-trained models, the bias of HLAT is still relative high on some subjects such as sexual orientation, physical appearance, religion, and socioeconomic (see Table V). This is partially due to the usage of publicly available datasets. More importantly, as a pre-trained model, HLAT has not gone through a supervised finetuning and human prefer- ence alignment. Those fine-tuning methods have been shown to be able to alleviate some limitations of pre-trained LLMs [9]. Another limitation is that our training is stopped after 1.8 trillion tokens. As is suggested by LLaMA-3 [10], HLAT may be able to further improve on certain tasks, such as math, world knowledge, MMLU, and coding, with more training tokens. VI. BEST PRACTICES & FUTURE DIRECTIONS In this section, we share some best practices we observed for training on AWS TRAINIUM, and raise open questions for future research. Parallelism: NxDT supports TP up to 32 degrees and pipeline parallelism. For a 7B model, we found that the combination of TP=8 and PP=1 provides the highest training throughput, but not for HLAT-70B. So the optimal parallelism configuration varies with model sizes and architectures. To achieve the highest training throughput, parallelism configu- ration needs to be jointly optimized with choice of activation checkpointing method, gradient accumulation steps, and train- ing precision, to balance memory and communication costs. Training Precision: NxDT supports various training pre- cision configurations, including full precision (FP32), BF16 with and without SR, standard mixed precision training, etc. Full precision training is often memory-wise infeasible for multi-billion LLMs. We compared multiple training strategies for HLAT: pure BF16, BF16 with SR and standard mixed precision training. Empirically, we found that training loss of pure BF16 diverges. BF16 with SR shows similar training loss as mixed precision on HLAT-7B model, but converges slower on HLAT-70B. We finally chose BF16 with SR for higher throughput on HLAT-7B, but standard mixed precision on HLAT-70B. For models of other sizes and architecture, warm-up study may be needed", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "2f5e409cfd33be307785bce29ac7af9f1d41871bbc242b54daeb78df32791f87"}
{"doc_id": "arxiv:2404.10630#model:part-6", "url": "https://arxiv.org/abs/2404.10630", "anchor": "#model:part-6", "type": "paper", "title": "", "section": "Model", "text": "often memory-wise infeasible for multi-billion LLMs. We compared multiple training strategies for HLAT: pure BF16, BF16 with SR and standard mixed precision training. Empirically, we found that training loss of pure BF16 diverges. BF16 with SR shows similar training loss as mixed precision on HLAT-7B model, but converges slower on HLAT-70B. We finally chose BF16 with SR for higher throughput on HLAT-7B, but standard mixed precision on HLAT-70B. For models of other sizes and architecture, warm-up study may be needed to decide the optimal training precision. Usually, the divergence can be observed in first few thousands of steps. Choice of β2: We observed that using β2 = 0.99 causes training instability and slower convergence. This is related to the choice of BF16 with SR training precision. A large β2 fails to capture the gradient explosion at current and recent steps, and hence does not effectively reduce the gradients in occurrence of gradient explosion. Switching to β2 = 0.95 addresses the above-mentioned problem. Weight decay: We applied weight decay to all layers. Empirically, weight decay is not applied to normalization and bias layers [54]. In our experiment, we did not found much performance-wise difference of those two methods. Pre-compilation: TRAINIUM requires pre-compiling the scripts to graphs. The compilation takes some time, especially for large models. Debugging on training scripts (e.g., printing out intermediate tensors) may require re-compilation. Instead of directly developing on a large model, we found it more efficient to develop and test on a smaller model and scale up afterwards. VII. RELATED WORK LLM pre-training: After the Transformer architecture [1] was introduced, BERT [54] was proposed to pre-train a language model on a large corpus of unlabeled data. Fol- lowing the success of BERT model on various NLP tasks, many pre-trained language models are later introduced with different architectures and training methods, such as GPT-2 [55], RoBERTa [56], BART [57], and so on [6]. Studies later observed significant performance improvement of language models by increasing model size and training data [58]. Such abilities are further demonstrated in LLMs such as GPT-3 [7], PaLM [59], LLaMA [8]–[10], Falcon [60], Gemini [61], Phi [48], etc. Pre-trained on trillions of tokens, LLMs with tens or hundreds of billions parameters show remarkable ability in generating creative text contents, as well as a variety of downstream tasks, such as question answering, summarization, machine translation, programming, etc. [6]. AI accelerators: Most models are trained on NVIDIA GPU accelerators, such as GPT [7], [62] and LLaMA [8], [9]. Falcon-180B [60] was trained on AWS SageMaker, with up to 4,096 A100 40GB GPUs using p4d instances. However, the landscape of hardware accelerators for deep learning train- ing has blossomed in recent years, with established players like NVIDIA GPUs facing fierce competition from custom offerings like Google’s TPU and AWS TRAINIUM. PaLM- 2 [59] and OpenLLaMA [29] have demonstreated successful LLM pre-training on Google TPU. Recently, OLMo [49] is an open-source model developed by AI2. It has two models trained on AMD and Nvidia GPUs, separately. The two models have nearly identical performance on their evaluation suite by 2T tokens.", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "c96b0fd22b26e5e58be5abed3349e5bb069d60164005e5b9aac25e1774330083"}
{"doc_id": "arxiv:2404.10630#model:part-7", "url": "https://arxiv.org/abs/2404.10630", "anchor": "#model:part-7", "type": "paper", "title": "", "section": "Model", "text": "of hardware accelerators for deep learning train- ing has blossomed in recent years, with established players like NVIDIA GPUs facing fierce competition from custom offerings like Google’s TPU and AWS TRAINIUM. PaLM- 2 [59] and OpenLLaMA [29] have demonstreated successful LLM pre-training on Google TPU. Recently, OLMo [49] is an open-source model developed by AI2. It has two models trained on AMD and Nvidia GPUs, separately. The two models have nearly identical performance on their evaluation suite by 2T tokens. AWS TRAINIUM is a machine learning accelerator developed for deep learning training with high performance and cost-competitiveness. Our work is the first demonstration of end-to-end multi-billion LLM pre-trained on AWS TRAINIUM. Ultimately, the optimal choice depends on the specific needs of the training task, with further research required to fully explore the potential of each accelerator and their possible convergence in future architectures. VIII. CONCLUSION In this paper, we pre-train HLAT, a family of 7 bil- lion and 70 billion parameter large language models, using AWS TRAINIUM over ∼1.8 trillion tokens. HLAT follows the decoder-only architecture and is trained with up to 256 Amazon EC2 trn1.32xlarge instances. We evaluate the per- formance of HLAT against popular open-source baseline models including LLaMA and OpenLLaMA on a variety of popular benchmarking tasks. We find that HLAT achieves model quality on par with these baseline models of similar sizes. This work demonstrates, for the first time, that AWS TRAINIUM with NxDT is able to successfully pre-train high- quality LLMs with high efficiency and low cost.", "source": "arxiv_pdf", "published": "", "tokens": 252, "sha256": "16d93448a27f17f55f281fffad1dc07f8c560e5ec9eb6a999f9690fc6ff65bab"}
{"doc_id": "arxiv:2412.10543#abstract", "url": "https://arxiv.org/abs/2412.10543", "anchor": "#abstract", "type": "paper", "title": "", "section": "Abstract", "text": "RAG (Retrieval Augmented Generation) allows LLMs (large language models) to generate better responses with exter- nal knowledge, but using more external knowledge causes higher response delay. Prior work focuses either on reducing the response delay (e.g., better scheduling of RAG queries) or on maximizing quality (e.g., tuning the RAG workflow), but they fall short in systematically balancing the tradeoff between the delay and quality of RAG responses. To bal- ance both quality and response delay, this paper presents METIS, the first RAG system that jointly schedules queries and adapts the key RAG configurations of each query, such as the number of retrieved text chunks and synthesis meth- ods. Using four popular RAG-QA datasets, we show that compared to the state-of-the-art RAG optimization schemes, METIS reduces the generation latency by 1.64 −2.54× with- out sacrificing generation quality. 1", "source": "arxiv_pdf", "published": "", "tokens": 137, "sha256": "b406a2a0b9db36394819f2e170b6319f504c0bec0ef9ea59bec676bf50e21d4f"}
{"doc_id": "arxiv:2412.10543#introduction:part-1", "url": "https://arxiv.org/abs/2412.10543", "anchor": "#introduction:part-1", "type": "paper", "title": "", "section": "Introduction", "text": "Retrieval-augmented generation (RAG) is a popular LLM inference technique that augments an LLM inference query with relevant text chunks, or “context”, retrieved from a large corpus.1 RAG systems, which include retrieval and LLM inference2, have found many use cases in QA tasks, personal assistants, chatbots, and LLM-powered search [10, 62]. While RAG can enhance the quality (accuracy and relevance) of LLM-generated responses [7, 53, 58, 91, 96], RAG queries are inherently slow as they need more compute and mem- ory resources to process the long input context to answer a query [6, 15, 42]. Thus, it is essential to balance high response quality and low response delays in RAG inference systems. 1RAG vs. long-context models is an active field of research, with the industry widely deploying RAG for its task-focused model inference quality and better resource-sharing capabilities [68]. 2Though RAG sometimes refers to the retrieval step, in this work, a RAG system includes both retrieval and LLM inference based on the retrieved texts, and we aim to optimize the whole pipeline. Past research efforts have optimized RAG, regarding ei- ther response quality or response delay, but they fall short in optimizing the quality-delay tradeoffs of RAG. RAG queries have an associated RAG configuration which de- scribes how and how much data to input for the query (more in §2) [72, 79, 83]. One line of prior work focuses on reducing response delay through better query schedul- ing (e.g., GPU allocation and inference batching) for RAG queries [2, 44, 45, 70, 76], without adapting the RAG con- figuration themselves. An alternate line of work focuses on maximizing generation quality by tuning the configurations of RAG queries [32, 77, 83], but this is often done at the cost of longer response delay. The RAG configuration simultaneously affects generation quality and response delay (e.g., retrieving too many chunks for a simple RAG query may unnecessarily inflate delay with- out increasing quality). Unlike traditional data queries (e.g., SQL) which specify the inputs and operators, RAG queries are inherently under-specified as they consist of a text query written in natural language [27, 32, 57, 64] and do not directly specify the exact RAG configuration of its execution. Moreover, multiple configuration knobs can influence the delay-quality tradeoffs. For instance, besides how many chunks to retrieve, how to use them in the LLM’s input involves two design choices—should the chunks be processed by the LLM jointly, or should the chunks be summarized first before being fed into the LLM together (and how long should a summary be). Recent works also attempt to tune RAG con- figuration [32, 77], but they focus on either tuning individual knobs or maximizing quality at the cost of higher delay. How- ever, tuning configurations across multiple knobs quickly hits a prohibitive combinatorial space (more in §3) and re- quires optimizations to reduce the search cost. What’s more, the RAG configuration should be tuned jointly with scheduling. Consider two configurations: 𝐴feeds all retrieved text chunks in one LLM input, and 𝐵summarizes first each chunk with an LLM and then feeds the summaries 1 arXiv:2412.10543v2 [cs.LG]", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "d9142e9627112b69af8cb5f22978e474158cd1ffcbf75d2f7635604ecdc58a0b"}
{"doc_id": "arxiv:2412.10543#introduction:part-2", "url": "https://arxiv.org/abs/2412.10543", "anchor": "#introduction:part-2", "type": "paper", "title": "", "section": "Introduction", "text": "they focus on either tuning individual knobs or maximizing quality at the cost of higher delay. How- ever, tuning configurations across multiple knobs quickly hits a prohibitive combinatorial space (more in §3) and re- quires optimizations to reduce the search cost. What’s more, the RAG configuration should be tuned jointly with scheduling. Consider two configurations: 𝐴feeds all retrieved text chunks in one LLM input, and 𝐵summarizes first each chunk with an LLM and then feeds the summaries 1 arXiv:2412.10543v2 [cs.LG] 16 Jul 2025 to an LLM input for a final generation. While 𝐴(which calls the LLM once) is seemingly faster than 𝐵(which calls the LLM multiple times), 𝐴could be slower as it requires more GPU memory than 𝐵and thus could be delayed in the sched- uler queue. Without making batching and configuration se- lection jointly, it would be difficult to avoid such pitfalls. Finally, the impact of RAG configurations on quality-delay tradeoffs also varies significantly with queries. For example, to answer “In which country is the Kimbrough Memorial Sta- dium located?”, the RAG may retrieve and analyze one text chunk about the stadium. In contrast, to answer “Compare NVIDIA’s operating cost over the first three quarters of 2024 and identify the highest one”, the RAG may need multiple chunks, each containing the quarter’s operating cost, and process these chunks jointly, instead of reading them separately. The above examples illustrate queries differ in complexity (more in §4), leading to needing different configurations per-query for optimal quality-delay tradeoffs. Empirically, we show that picking RAG configuration per-query achieves 12 −15% higher quality and 2.5 −3× lower delay than using any fixed configuration across all queries in a dataset (§5). Thus, RAG configurations should be adapted on a per-query basis. Yet, existing RAG systems, which hand-pick a static config- uration offline based on a few example queries [1, 21, 39, 85], lose out on quality or response time. This paper presents METIS, the first RAG system that adapts multiple configuration knobs on a per-query basis and jointly makes configuration selections and scheduling decisions (i.e., which LLM inference in a batch) to optimize the delay-quality tradeoffs for RAG. As this would require solving a joint combinatorial prob- lem for every query, which can be prohibitively expensive (§3), METIS tackles the challenge with a two-step approach. First, METIS prunes the massive configuration space for each received query to a smaller yet promising one that con- tains configurations that likely yield high-quality output for the given query. Specifically, METIS uses a separate LLM to estimate the query’s profile, including how many pieces of information are required to answer the query and whether joint reasoning is likely required across these pieces of in- formation (more in §4.1). The intuition of the query profiles is that they can effectively filter out undesirable RAG config- urations. For the earlier query example “Compare NVIDIA’s operating cost over the first three quarters of 2024 and identify the highest one,” the estimated profile would suggest that it involves at least three separate pieces of information, so the number of chunks (one of the configuration knobs)", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "0a67a5bbdd0544d7e6604402f5d0f8df2c7cfcf1ca854e48afbfcffc7ad3a80c"}
{"doc_id": "arxiv:2412.10543#introduction:part-3", "url": "https://arxiv.org/abs/2412.10543", "anchor": "#introduction:part-3", "type": "paper", "title": "", "section": "Introduction", "text": "query and whether joint reasoning is likely required across these pieces of in- formation (more in §4.1). The intuition of the query profiles is that they can effectively filter out undesirable RAG config- urations. For the earlier query example “Compare NVIDIA’s operating cost over the first three quarters of 2024 and identify the highest one,” the estimated profile would suggest that it involves at least three separate pieces of information, so the number of chunks (one of the configuration knobs) should be at least three. It should be noted that the LLM-based profiler is an extra overhead in METIS, but fortunately, its input only contains the RAG query itself and the metadata of the RAG database, which are orders of magnitude shorter than the long contexts in RAG, so the estimation can be relatively fast, about 1/10 of the delay of the execution of the RAG query. METIS (Ours) AdaptiveRAG (ACL 2024) Parrot (OSDI 2024) vLLM : SOTA LLM Engine Better METIS (Ours) AdaptiveRAG (ACL 2024) Parrot (OSDI 2024) vLLM : SOTA LLM Engine Better Figure 1. Performance of METIS on the KG RAG FinSec [50] dataset compared to the baselines. Full results shown in §7. Using the narrowed configuration space, METIS reduces the RAG response delays by jointly deciding the per-query configuration and query scheduling based on available re- sources (§4.3). The insight is that within the pruned configu- ration space, the scheduler can make optimal configuration decisions without exploring the original, large configuration space and the implications on quality. In short, METIS’s two-level design loosely decouples the problem into (1) pruning configuration space to a smaller yet promising range of configurations, which focuses solely on keeping the accuracy high, and (2) jointly optimizing configuration (within the narrowed range) and scheduling to optimize response delay by choosing configurations which best-fit into the GPU memory. We evaluate METIS across four RAG datasets with diverse query profiles (e.g., reasoning vs. domain-specific QA). Fig- ure 1 shows a preview of our results. Our key takeaways are as follows. When achieving the same or higher quality than the baselines, METIS reduces the response delay by 1.6−2.8× compared to the latest vLLM (a state-of-the-art serving en- gine), Parrot (the latest LLM query-scheduling method), as well as AdaptiveRAG (the latest RAG configuration-tuning method). METIS also achieves 1.8 −4.5× higher through- put compared to these baselines when achieving the same response delay and same/higher quality. The general concept of using LLMs to guide system tuning is not exactly new [60, 88], but our key contribution lies in applying the concept to RAG systems, through joint sched- uling with resource-aware configuration selection, leading to significantly better resource sharing (§4.2, §4.3). 2 RAG systems and configurations As an LLM often does not have domain-specific or up-to-date knowledge, LLM applications commonly employ RAG to sup- plement LLM inference with external knowledge to generate high-quality responses. Despite the growth of model context length, using RAG to pinpoint the relevant context is still significantly cheaper in terms of resource cost (GPU require- ment), latency, and memory consumption (KV Cache size). For 2 general-purpose", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "47a76b7d8d9debb103d1a73314c681eb320a0d3d23b207a942306207ed5d05f3"}
{"doc_id": "arxiv:2412.10543#introduction:part-4", "url": "https://arxiv.org/abs/2412.10543", "anchor": "#introduction:part-4", "type": "paper", "title": "", "section": "Introduction", "text": "resource-aware configuration selection, leading to significantly better resource sharing (§4.2, §4.3). 2 RAG systems and configurations As an LLM often does not have domain-specific or up-to-date knowledge, LLM applications commonly employ RAG to sup- plement LLM inference with external knowledge to generate high-quality responses. Despite the growth of model context length, using RAG to pinpoint the relevant context is still significantly cheaper in terms of resource cost (GPU require- ment), latency, and memory consumption (KV Cache size). For 2 general-purpose QA pipelines, RAG is cost-efficient with re- trieving targeted chunks based on semantic similarity to the query. Using LLMs with long-context documents in contrast has much higher GPU memory usage and delay [43, 45, 71]. Before processing queries, a RAG system organizes back- ground documents by splitting them into chunks (each with a fixed number of tokens), embedding each chunk using models like Bert [12, 19], and storing the embeddings with the chunks in a vector database. Processing a RAG query involves two main steps: • Retrieval: The RAG system retrieves one or more rele- vant context chunks from the database by comparing the query’s embedding, (using the same embedding model as for database indexing), with the stored embeddings. • Synthesis: After retrieving the relevant chunks, the RAG system combines these chunks and the RAG query to form a single/multiple LLM call(s) to generate the response. Retrieval is computationally lightweight and much faster than synthesis (> 100×), so the response delay is typically dominated by the synthesis step [90]. RAG configuration: This work focuses on optimizing three configuration knobs, illustrated in Figure 2, which are de- rived from key design questions that affect RAG performance in terms of response delay and quality: • How many chunks to retrieve (num_chunks): The number of context chunks directly affects the delay of the synthesis step, with more computation needed to process the longer sequences with more chunks. In the meantime, retrieving too few chunks risks low response quality if the retrieved chunks do not contain enough useful information. • How to synthesize (synthesis_method): If the LLM should read the chunks separately, RAG uses the LLM to generate one answer for the query using each chunk separately and picks the output with the highest confidence, which is called map_rerank. This often incurs the least compu- tation but can cause low quality if the useful information is scattered in different chunks, in which case the LLM should read the chunks jointly. The RAG system can feed these chunks in the LLM input directly by concatenating them within a single prompt (called stuff) or to create a shorter summary for each chunk first before feeding the summaries and the query into the LLM to generate the final response (called map_reduce). stuff needs less computa- tion than map_reduce, but risks degraded output quality for long inputs due to the lost-in-the-middle problem [47]. • How long is each summary (intermediate_length): Fi- nally, if the LLM produces the summary for each chunk based on the user query, the length of each summary greatly affects the quality and response of map_reduce— shorter", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "3b568ece8ef08daa463a1d90b44e98742e87a8d6b61869bc8b9e9d5ef7a30a9d"}
{"doc_id": "arxiv:2412.10543#introduction:part-5", "url": "https://arxiv.org/abs/2412.10543", "anchor": "#introduction:part-5", "type": "paper", "title": "", "section": "Introduction", "text": "summary for each chunk first before feeding the summaries and the query into the LLM to generate the final response (called map_reduce). stuff needs less computa- tion than map_reduce, but risks degraded output quality for long inputs due to the lost-in-the-middle problem [47]. • How long is each summary (intermediate_length): Fi- nally, if the LLM produces the summary for each chunk based on the user query, the length of each summary greatly affects the quality and response of map_reduce— shorter summaries yield lower delay but also risk not feed- ing enough information to the final LLM inference. How many chunks to retrieve? If jointly, should the LLM summarize each chunk first? If so, how long should each summary be? If multiple chunks, should the LLM read them jointly? Knob 1: num_chunks Knob 2: synthesis_method Knob 3: intermediate_length Key design choices of RAG Figure 2. The configuration knobs adapted by METIS are derived from key design choices of RAG systems. Chunk 1 Chunk 2 Chunk 3 LLM Final Answer Chunk 1 Chunk 2 Chunk 3 Final Answer 1 Confidence : 80% Final Answer 2 Confidence : 99% Final Answer 3 Confidence : 90% Chunk 1 Chunk 2 Chunk 3 S1 S2 S3 Final Answer (a) Stuff (b) Map Rerank (c) Map Reduce LLM LLM LLM Figure 3. Illustration of different RAG synthesis methods, which have various LLM reasoning capabilities. In this work, while we focus on universal RAG knobs which affect quality and delay common to all RAG systems, METIS can be extended to other tunable knobs (e.g., some RAG system may dynamically choose the embedding model, retrieval index or serving LLM). METIS’ design is extensible to any RAG configuration knob based on the query profile. Performance metrics: We evaluate the performance of a RAG system using two metrics: • Response quality calculates the F1 score of the generated response against the ground truth. The F1 score is the harmonic mean of precision (# correctly generated words) and recall (# of correct words successfully generated). This metric is widely used in prior works [10, 69, 72]. • Response delay measures the time elapsed from when the RAG system receives a RAG request to when it completes generating the response. Next, we show that these knobs need to be properly tuned on a per-query basis to achieve optimal tradeoff between quality and delay in §3. 3 Towards better quality-delay tradeoffs Prior work on RAG either optimizes for lower delay or higher quality, i.e., the first picks static configurations and focuses on reducing the delay by smart scheduling and resource allo- cation [44, 70, 76] and the second picks RAG configurations to maximize quality without regard to resource usage or delay [32, 77, 83]. For the first time, we explore the potential of optimizing the quality-delay tradeoffs for RAG. 3 (a) Change: synthesis method from map_rerank (circle) , stuff (plus) and map_reduce (square) (b) Change: number of chunks from 1 to 35 with stuff (c) Change: intermediate length from 1 to 100 with map_reduce Q1 Q1 Q1 Q2 Q2 Q2 Q3 Q3 Q3 Figure 4.", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "aa2a62e6a2676c1bc20827d57ee1fe6cbb65e434a71ff2e9c1182f9a553a14f2"}
{"doc_id": "arxiv:2412.10543#introduction:part-6", "url": "https://arxiv.org/abs/2412.10543", "anchor": "#introduction:part-6", "type": "paper", "title": "", "section": "Introduction", "text": "and the second picks RAG configurations to maximize quality without regard to resource usage or delay [32, 77, 83]. For the first time, we explore the potential of optimizing the quality-delay tradeoffs for RAG. 3 (a) Change: synthesis method from map_rerank (circle) , stuff (plus) and map_reduce (square) (b) Change: number of chunks from 1 to 35 with stuff (c) Change: intermediate length from 1 to 100 with map_reduce Q1 Q1 Q1 Q2 Q2 Q2 Q3 Q3 Q3 Figure 4. Varying each RAG configuration knob leads to different quality-latency tradeoffs, and these tradeoffs differ across queries (Q1 in green, Q2 in blue, and Q3 in red). To improve the delay-quality tradeoff, our insight is that quality and delay should jointly be optimized in this large tradeoff space created by the choice of RAG configuration knobs. Importantly, the configurations with better quality- delay tradeoffs vary significantly across queries. To showcase this observation, we use three queries from Musique [78], a popular reasoning QA dataset (§7.1). • Q1: “In what county was William W. Blair’s born?” • Q2: “Are Alison Skipper, Diane Gilliam Fisher, and Rachel McAdams from the same country?” • Q3: “When and why did the Voyager 1, the spacecraft that detected storms on Neptune, leave our solar system?” We chose queries with different natural language complexity and reasoning, Q1 being relatively less complex than Q2 and Q3. Then, we adjust the value of each configuration knob in order to quantify each knob’s impact on the quality- delay tradeoffs in each of the queries. Impact of synthesis method: Figure 4 (a) changes the syn- thesis method and shows its effect on the quality-delay trade- off, while keeping the other RAG configuration knobs con- stant. We vary the synthesis method as map_rerank, stuff, and map_reduce from left to right. The insight is that the optimal synthesis method that strikes the best quality-delay tradeoff (closest to the top left corner) differs significantly across the different queries. For simple queries like Q1 (green), quality plateaus for more complex synthesis methods (stuff and map_reduce). Because it only needs a single piece of context, map_rerank which processes chunks in isolation suffices, whereas cross- chunk reasoning (stuff and map_reduce) adds undue delay (2×) without improving quality. For queries such as Q2 (blue) that require cross-chunk rea- soning, stuff and map_reduce provide significant quality improvements (35% increase) by processing chunks jointly. For more complex queries, such as Q3 (red), which require even more reasoning and information (why Voyager 1 left has multiple reasons), methods like map_reduce improve quality (30% increase) by removing unnecessary text in the mapper phase, to help the LLM focus on the relevant content. Impact of the number of retrieved chunks: Figure 4 (b) fixes the synthesis method (stuff) and shows the impact of the number of retrieved chunks (1-35) on quality and delay. Simple queries, like Q1 (green), can often be answered using just one or two chunks (needs only birth county). For more complex queries, Q2 (blue) and Q3 (red), increasing the number of chunks (1-15) improves the likelihood of retriev- ing all relevant", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "60112ce6bcfaf4e7f034686d17df1802d9f1667ef6a3a09f8d321c375c38fbed"}
{"doc_id": "arxiv:2412.10543#introduction:part-7", "url": "https://arxiv.org/abs/2412.10543", "anchor": "#introduction:part-7", "type": "paper", "title": "", "section": "Introduction", "text": "help the LLM focus on the relevant content. Impact of the number of retrieved chunks: Figure 4 (b) fixes the synthesis method (stuff) and shows the impact of the number of retrieved chunks (1-35) on quality and delay. Simple queries, like Q1 (green), can often be answered using just one or two chunks (needs only birth county). For more complex queries, Q2 (blue) and Q3 (red), increasing the number of chunks (1-15) improves the likelihood of retriev- ing all relevant context and improves quality. Blindly retrieving more chunks than necessary risks di- luting the relevance of actual important information, due to commonly known problems such as “lost-in-the-middle” [28, 47]. In all three queries, retrieving more chunks beyond a point harms the quality (up to 20% drop) and unnecessar- ily inflates delay (up to 3×). Hence we have a quality-delay tradeoff where increasing chunks up to a point helps quality but beyond that it increases delay while degrading quality. Impact of the intermediate output length: Figure 4 (c) shows the impact of our third configuration knob, vary- ing the intermediate output length (1-100) for map_reduce synthesis methods on the quality-delay tradeoff. For simple queries like Q1 (green), short amounts of intermediate length are enough to answer the query (10-20 words). For more com- plex queries Q2 (blue) and Q3 (red), increasing the amount of intermediate length (70-100 words) provided helps the model with enough information to answer the query. Overall, we see that RAG queries naturally vary in com- plexity, requiring differing levels of inter-chunk reasoning and varying numbers of context chunks. More complex queries, which require more reasoning and context, ben- efit from increased LLM computation, which can come at the cost of increased delay. Adding more context chunks helps to a point beyond which it harms the output quality and delay. Thus, adapting RAG configuration on a per-query basis is crucial. Figures 2, 3, 4 illustrate tuning most popular RAG configuration knobs, however the tuning extends to more RAG configurations with richer tradeoff spaces (§4.2). 4 Pareto Boundary of fixed configuration with vLLM Pareto Boundary of fixed configuration with vLLM Per-Query Configuration Per-Query Configuration Figure 5. Per-query configuration can achieve significantly better quality-delay tradeoffs across queries compared to every fixed configuration choice. Figure 5 uses queries from two datasets (Musique and QM- SUM, see §7.1) and shows that picking the best configuration for each query (the best configuration is the one with the lowest delay that achieves less than 2% drop than the highest achievable quality) achieves superior quality-delay tradeoff than picking any static configuration for all queries. Choos- ing the configuration per-query allows up to 3× delay saving compared to static configurations which are the closest in quality. Every single static configuration choice that achieves comparable delay has at least a 10% quality drop. In spite of the potential benefits, per-query configuration adaptation faces challenges that hinder their real-world adop- tion. Each RAG query comes in plain text with practically no associated RAG configurations. Moreover, the space of configurations grows exponentially with multiple knobs. For example, for a map_reduce configuration,", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "943a60526132d3179fa53f81ee80181bbc3d2be0426a4ab4f1177a0a25cb94b3"}
{"doc_id": "arxiv:2412.10543#introduction:part-8", "url": "https://arxiv.org/abs/2412.10543", "anchor": "#introduction:part-8", "type": "paper", "title": "", "section": "Introduction", "text": "configuration per-query allows up to 3× delay saving compared to static configurations which are the closest in quality. Every single static configuration choice that achieves comparable delay has at least a 10% quality drop. In spite of the potential benefits, per-query configuration adaptation faces challenges that hinder their real-world adop- tion. Each RAG query comes in plain text with practically no associated RAG configurations. Moreover, the space of configurations grows exponentially with multiple knobs. For example, for a map_reduce configuration, with 30 values for num_chunks and 50 values for intermediate_length leads to 1500 configurations for a query. Exhaustively profiling all configurations per-query and choosing the best is infeasible. Alternatively, if we profile periodically, we lose out on the potential configuration selection for each query, as variance in query profile leads to different quality-delay tradeoffs. Pro- filing cost is also prohibitively expensive as the LLM needs to be run with many synthesis methods, number of chunks etc., which require high GPU usage. Additionally, the delay of profiling can be ∼100× the inference delay due to multiple LLM calls during profiling. Online RAG queries have strin- gent requirements for GPU resource usage and end-to-end delay [70, 76]. This makes it hard to systematically decide what an optimal per-input configuration should be. To truly achieve the benefit of per-query configuration adaptation, we need a smart system to drastically reduce to a useful configuration space, in a fast and cheap manner. 4 METIS: Enabling per-query configuration adaptation for RAG We present METIS, a novel system for serving RAG queries focusing on high generation quality and minimal delay. METIS is a RAG controller (Figure 6) with two main components: Configuration Space Pruning (§ 4.1, 4.2 ) Joint scheduler (§ 4.3) RAG Queries Vector Database GPU Memory Serving LLM RAG Configs Text Chunks Check Resource Status Generated Output Retriever RAG Synthesis Chosen Config Figure 6. METIS consists of a RAG controller which per- forms configuration space pruning and joint scheduling. • Pruning configuration space: We estimate each query’s pro- file (§4.1) and reduce the RAG configuration space to a smaller yet promising one that still yields high generation quality (§4.2) (leading to a 50-100× reduction). • RAG scheduler: Within the pruned configuration space for the query, METIS’ scheduler chooses the best config- uration for the query to achieve the best quality-latency trade-off based on the available system resources (§4.3). Once the configuration is chosen, the METIS’ executes the query using the chosen configuration—retrieving the selected number of chunks and uses the selected synthesis method to feed into the LLM’s input. 4.1 Estimating a query’s profile Query profile: To choose the correct RAG configurations, the first step of METIS is to create the profile of the query (as we see in Figure 7) by querying an LLM (we call this LLM query profiler). We ask the query profiler to estimate four high-level dimensions for each query. • Query complexity refers to the intricacy of the query itself. Queries with less complexity are more like simple yes/no questions, while queries with high complexity are more like why questions, which require", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "04052c26574ccf1c6894d4cd522e43fbaa41c43502576fb7ac3a3e6cdeb53d0c"}
{"doc_id": "arxiv:2412.10543#introduction:part-9", "url": "https://arxiv.org/abs/2412.10543", "anchor": "#introduction:part-9", "type": "paper", "title": "", "section": "Introduction", "text": "choose the correct RAG configurations, the first step of METIS is to create the profile of the query (as we see in Figure 7) by querying an LLM (we call this LLM query profiler). We ask the query profiler to estimate four high-level dimensions for each query. • Query complexity refers to the intricacy of the query itself. Queries with less complexity are more like simple yes/no questions, while queries with high complexity are more like why questions, which require deeper reasoning than yes/no questions. As a result, it requires more LLM com- putation to correctly answer complex queries. The output for this dimension is binary “High/Low” • Joint reasoning requirement describes whether multiple pieces of information are needed to answer the query. Even relatively simple queries may require joint reasoning (e.g., checking whether the annual income from two years is the same). The output for this dimension is binary “Yes/No” • Pieces of information required refers to the distinct, stan- dalone pieces of information required to fully answer the query (e.g., the annual income from how many years is required to draw the trend of annual income). The output for this dimension is a number from 1-10. 5 Query Profiler ( LLM ) § 4.1 Estimate the query complexity How many pieces of information? How much can we summarize? Input Prompt Content Query complexity: High/ Low Needs X pieces of information Summary length: X to Y Rule-based Mapping § 4.2 Query Synthesis", "source": "arxiv_pdf", "published": "", "tokens": 243, "sha256": "ced0960f22e0f95a13cb8c636578d5a4be831abe66773826495af351833b97cf"}
{"doc_id": "arxiv:2412.10543#method:part-1", "url": "https://arxiv.org/abs/2412.10543", "anchor": "#method:part-1", "type": "paper", "title": "", "section": "Method", "text": "Number of chunks (value) Intermediat e Length (value) Do we need joint reasoning? Joint reasoning: Yes/No Figure 7. METIS RAG configuration selection workflow. • The length of the summarization: If the query is complex and needs a lot of different information, it is often neces- sary to first summarize the relevant information chunks first (to reduce the noise inside these chunks) and then gen- erate the final answer from these summaries. The output for this dimension is a number from 30-200. METIS is not the first to use query profile as a metric for deciding RAG configurations, it extends upon methods like AdaptiveRAG [32] which have used LLM’s to estimate query profile but they only focus on one dimension (the number of chunks to retrieve). In Section 7, we show the impact of each dimension on the overall improvement. Why the query profile could be estimated: Estimating the aforementioned query profile is feasible, not only be- cause of the reasoning power of LLMs3 in analyzing natural language queries, but also because we provide sufficient in- formation to the LLM-based profiler. METIS feeds the profile estimator with not only the query, but also a metadata of the database that contains the background document. The metadata is a short description about the type of con- tent in the database and its data size (chunk_size). Specif- ically, we use a single-line summaries already attached to the original source datasets as the metadata of the dataset. For example, the metadata for the KG RAG Finsec’s database [50] contains quarterly financial reports and questions of Fortune 500 companies with a chunk_size of 1000. It de- scribes the content topics of the chunks with information such as revenue growth indicators, product release informa- tion, sales etc.,. When presented with a query on financials of such a company, the LLM can use the metadata to decide questions like how much to summarize/how much reasoning is required. We give details on the prompt and the intuition to generate metadata for new datasets in Appendix §A. It is important to acknowledge that for highly under- specified queries, it is hard for any model (even human) to reasonably estimate the query’s profile. For an example 3We have tested both GPT and Llama models as the profile query-profiler, and they yield similarly impressive results (§7). query “Compare current US Stock Market trends,” the query profile here does not provide enough information (e.g., how many years should the trend be derived from). To answer such highly under-specified queries, more information about the dataset will unlikely help.4 Moreover, we observed that extra information does not significantly improve the profiler’s estimates. For instance, in theory, it helps to know the embedding algorithm used by RAG. Yet, the embedding models perform similarly overall across queries and datasets under our consideration. This ex- plains their limited contribution to the profiler, though more future work is needed to understand the wider implications. 4.2 Mapping query profiles to RAG configurations After METIS obtains the query profile using the LLM, it per- forms rule-based mapping to generate values for RAG con-", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "43cbd0071ab281e42dc44526049de45b52c2f8e97c3b2d15842d061cdddffb4d"}
{"doc_id": "arxiv:2412.10543#method:part-2", "url": "https://arxiv.org/abs/2412.10543", "anchor": "#method:part-2", "type": "paper", "title": "", "section": "Method", "text": "significantly improve the profiler’s estimates. For instance, in theory, it helps to know the embedding algorithm used by RAG. Yet, the embedding models perform similarly overall across queries and datasets under our consideration. This ex- plains their limited contribution to the profiler, though more future work is needed to understand the wider implications. 4.2 Mapping query profiles to RAG configurations After METIS obtains the query profile using the LLM, it per- forms rule-based mapping to generate values for RAG con- figuration knobs (e.g., synthesis_method etc. introduced in §2). based on the query profiler’s outputs. How we map and why the profile helps: To understand the role of query profiles, consider the following examples: • “Who is the current CEO of NVIDIA?” This query is not complex and does not require joint reasoning. Due to the query being simple with no reasoning required and one piece of information (name of CEO). • “Which month had the highest NVIDIA’s stock price the six months from January to June 2024?” This query is simple but still needs to read information jointly, specifically six pieces of information (stock price for every month) • “What are the reasons for NVIDIA’s month-on-month stock price change from January to June 2024” This query is complex and needs to read multiple pieces of information jointly (stock prices, reasons for change etc.) As multiple reasons need to be analyzed here, summarizing all of the in- formation first helps narrow down to relevant information and perform clearer reasoning (why the prices changed). 4Maybe some chat history from the same user will help, but that is beyond the scope of this work. 6 Algorithm 1: Rule based mapping algorithm Input: Query complexity, Joint reasoning required Input: Pieces of information , Summarization length range Result: synthesis_method, num_chunks, intermediate_length 1 if Joint reasoning required == “no” then 2 synthesis_method = map_rerank 3 else 4 if Query complexity == “low” then 5 synthesis_method = stuff 6 else 7 synthesis_method = stuff, map_reduce 8 num_chunks = [Pieces of information , 3× Pieces of information] 9 intermediate_length_range = Summarization length range Algorithm 1 outlines the rule-based mapping process. This mapping is significantly helpful, it improves upon raw pro- filer outputs and converts them to usable RAG configurations. It reduces the cost of the profiler LLM by restricting it to provide short binary decisions only. We decide the range of synthesis_method selections based on two of the profile dimensions estimated in §4.1, i.e., the “Query complexity” and the “Joint reasoning require- ment”. Simple queries that don’t need any reasoning can an- swered with map_rerank while queries that require joint rea- soning need stuff or map_reduce. We then decide the range of values for num_chunks based on the profile dimension of the “Pieces of information required”, i.e., 𝑛—specifically, we set the range of num_chunks to be 1−3 times of 𝑛. We do not directly set num_chunks at𝑛, because it (1) gives some leeway for the retrieval logic (e.g., typically Bert-embedding-based)5 to find necessary information, and (2) provides the room for the scheduler to select the configuration that fits in available", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "f265b10e1abcb4c9a7843379ed75bc814f64bf8bb91b83c17d0688ba17448d1d"}
{"doc_id": "arxiv:2412.10543#method:part-3", "url": "https://arxiv.org/abs/2412.10543", "anchor": "#method:part-3", "type": "paper", "title": "", "section": "Method", "text": "rea- soning need stuff or map_reduce. We then decide the range of values for num_chunks based on the profile dimension of the “Pieces of information required”, i.e., 𝑛—specifically, we set the range of num_chunks to be 1−3 times of 𝑛. We do not directly set num_chunks at𝑛, because it (1) gives some leeway for the retrieval logic (e.g., typically Bert-embedding-based)5 to find necessary information, and (2) provides the room for the scheduler to select the configuration that fits in available memory. Finally, we get the intermediate_length range from the “summary length” estimate, which is already a value range (derived from the query, metadata and chunk size). Algorithm 1 is central to METIS’ design to reduce to the space to our useful RAG configurations and this is extendable to other RAG configurations. For instance, a particular RAG pipeline might use an external re-ranker [23, 52], query re- writer [36, 51] or perform an external web-search [73] along with database retrieval. The mapping algorithm can map the profiling LLM’s output (e.g., of Query complexity) and be used to guide such decisions for these newer RAG configurations. Additionally, such mapping algorithms greatly reduce the overall inference cost of RAG inference. Attempting to use 5A typical RAG retriever these days will have to retrieve 2-3× more chunks than minimally required to provide sufficient information for the LLM inference [24, 55]. Used GPU Mem (6GB) Used GPU mem (6GB) time time Map 1 (6GB) Map 2 (6GB) Reduce (6GB) Chunk 1, Query Chunk 2, Query Chunk 1, Chunk 2, Query Stuff (12GB) (a) Baseline Separates configuration selection and scheduling In general, \"Stuff\" is faster than \"MapReduce\" as a RAG config Yet, \"Stuff\" is memory-intensive and thus is slower when available GPU RAM is limited Free mem (6GB) (b) Ours performs configuration selection and scheduling jointly Delay saved We select MapReduce as it can readily fits in the current batch Figure 8. METIS joint schedules RAG configurations with available GPU memory (chosen example - map_reduce) the LLM profiler to directly provide the exact RAG configu- ration values does not work. For this, the LLM needs to be regularly retrained for this task to adapt to new configura- tions and will require significantly greater system resources (e.g., GPUs blocked for this). In contrast, METIS uses the LLM to only analyze natural language properties and provide bi- nary decisions, which the mapping algorithm translates to useful configurations with a significantly lower cost. It is important to note that the concept of METIS belongs to an active research trend in the ML and systems community that leverages LLM outputs and mapping functions to guide real system decisions and optimizations, an example of which is LLM routing [13, 31, 56, 59]. While current LLM routers use trained LLMs to map decisions from query complexity to only choose from families of inference models (outside the realm of RAG), we differ by mapping the output to the configuration knob we run for the RAG queries. Like these prior efforts, METIS is a heuristic to best utilize the LLM-generated information to guide system optimiza- tions.", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "c9ffdbe285143ef221b3746bf25e04ff45fb406a69526c91cfc8cdcedcec9d9d"}
{"doc_id": "arxiv:2412.10543#method:part-4", "url": "https://arxiv.org/abs/2412.10543", "anchor": "#method:part-4", "type": "paper", "title": "", "section": "Method", "text": "functions to guide real system decisions and optimizations, an example of which is LLM routing [13, 31, 56, 59]. While current LLM routers use trained LLMs to map decisions from query complexity to only choose from families of inference models (outside the realm of RAG), we differ by mapping the output to the configuration knob we run for the RAG queries. Like these prior efforts, METIS is a heuristic to best utilize the LLM-generated information to guide system optimiza- tions. While it demonstrates remarkable improvement in practice, more work will be needed to complement it for better interpretability and robustness. 4.3 Joint configuration-scheduling adaptation Once provided with the narrowed range of each RAG con- figuration knob (synthesis_method, num_chunks and intermediate_length), we need to choose a RAG configu- ration, which is aware of the current system resource (GPU memory). If we pick configurations which do not fit in cur- rent memory, it will lead to additional queuing delay waiting for the GPU memory to free up. We have METIS’s pruned configuration space where the quality is high, we now focus on choosing the best configu- ration which fits in memory, without focusing on quality. 7 Why we need to choose the scheduling jointly: We motivate the need for joint scheduling along with the RAG configuration choice in Figure 8. Consider a setup where we tune only one RAG configura- tion knob of synthesis_method. Other knobs num_chunks and intermediate_length are fixed at 20 and 100 respec- tively. Let’s assume both stuff and map_reduce are present in the pruned space. For the scheduling knob, we consider the amount of GPU memory available for the current batch. Consider a baseline system which separates the joint de- cision from the scheduling and picks only the RAG con- figuration knob (synthesis_method). It chooses the stuff configuration knob as it has lower compute requirement, so given enough memory it should be fast. The baseline system in Figure 8 (a) does not consider other jobs in the system and does not evaluate the amount of available resource to make its scheduling decision. Due to its long input length with 20 chunks, stuff turns out to be memory-intensive. If the available GPU memory is low, stuff doesn’t fit in memory and needs to be queued. This ends up with stuff being slow. Jointly considering the available GPU memory with choos- ing the RAG configuration knob avoids this pitfall. For exam- ple, in Figure 8 (b), if the original configuration was stuff, METIS can choose to use map_reduce (based on the current GPU memory available). By doing so, METIS can start putting the mappers which fit in memory, into the current running_batch of requests which fits in the GPU. While map_reduce requires more compute, in this case, it benefits from being able to start execution much faster, as some of the mappers fit in memory. METIS does not need to wait for the GPU memory to free up and changes the configuration aware of system resource, to save delay and achieve a better quality-delay tradeoff. Jointly choosing the configuration knobs: METIS first", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "b418ac55abf8a09f57ef90b9486cc62ef341adac4f029e987777f934d8ea9148"}
{"doc_id": "arxiv:2412.10543#method:part-5", "url": "https://arxiv.org/abs/2412.10543", "anchor": "#method:part-5", "type": "paper", "title": "", "section": "Method", "text": "the mappers which fit in memory, into the current running_batch of requests which fits in the GPU. While map_reduce requires more compute, in this case, it benefits from being able to start execution much faster, as some of the mappers fit in memory. METIS does not need to wait for the GPU memory to free up and changes the configuration aware of system resource, to save delay and achieve a better quality-delay tradeoff. Jointly choosing the configuration knobs: METIS first provides us with a pruned range of configurations. A straw- man solution is to pick a constant value from the across queries. (e.g., the median value of the num_chunks). While this is better than using one static configuration for all queries, it is still sub-optimal as it does not look at the current system resource availability. This prevents us from exploiting the best quality-delay tradeoff across RAG queries. We use a best-fit algorithm to allow for variation in config- urations across queries. We first compute the GPU memory requirement for the RAG query from the RAG configura- tion knobs (e.g., num_chunks) for every configuration in the pruned space. Then, we measure the current available mem- ory on the GPU to see what can fit into the current batch. We then pick the best configuration from the pruned space that fits into the GPU. METIS defines the best configuration as the one with overall highest memory requirement, from all which fit in memory. The insight here is that within the reduced range of good quality configurations, higher mem- ory configurations correspond to expensive configurations (e.g. more number of chunks, higher intermediate length). In general, these configurations should lead to slightly higher quality in the reduced space. For example, if the pruned space says num_chunks is 5-10 and the synthesis_method is stuff and both 5 or 6 chunks can fit in memory, we choose 6 chunks. We don’t pick a configuration that doesn’t fit in GPU, so we would never choose more than 6 chunks. If we do that, the system will queue the request inflating the delay. After choosing the configuration that fits into the current running_batch, the vLLM engine is optimized to perform chunked_prefill. However, even with chunked_prefill, it can only offload parts of long prefill of stuff requests which do not fit in the current batch and still inflates the queuing de- lay. Jointly scheduling RAG configurations enables efficient resource usage, which cannot be obtained by only relying on the output of the LLM profiler. What if none of the configurations fit in the GPU? A main insight for METIS’s design comes from the observation that in general, the RAG-specific focused configurations can be loosely-decoupled from the scheduling-specific configura- tions. METIS tries to fit the best possible configurations into GPU memory after it gets the profiler’s reduced configura- tion space. It can sometimes happen that the current GPU memory availability is too low and none of the profiler’s configurations fit in the currently available GPU. One way we handle this is by falling back to a cheaper fixed configuration and choosing", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "0467499c95c579e49bf99a02f274eeb8ef1eae536adf30d35ef5ee4cdab06f26"}
{"doc_id": "arxiv:2412.10543#method:part-6", "url": "https://arxiv.org/abs/2412.10543", "anchor": "#method:part-6", "type": "paper", "title": "", "section": "Method", "text": "comes from the observation that in general, the RAG-specific focused configurations can be loosely-decoupled from the scheduling-specific configura- tions. METIS tries to fit the best possible configurations into GPU memory after it gets the profiler’s reduced configura- tion space. It can sometimes happen that the current GPU memory availability is too low and none of the profiler’s configurations fit in the currently available GPU. One way we handle this is by falling back to a cheaper fixed configuration and choosing to ignore the output of the pruning. As we already have access to the query complexity profile and we can pick cheaper configurations, which would meet the requirement for the current query. For instance, if the query doesn’t require joint reason- ing, we can pick a map_rerank configuration with as many chunks that fit into the current GPU memory, ignoring the remaining pruned spaces. If joint reasoning is required, we pick a stuff or map_reduce configurations with the few chunks that fit into memory. We can choose which synthesis method to use once based on the exact memory availability. This allows loose-decoupling of the RAG configurations into a smaller space and then choosing configurations based on system resource availability. This also allows SLO-based constraints on RAG queries if certain queries have strict budgets on their generation latency. 5 Refinements to METIS In spite of it all, it is possible for the profiler to (sometimes) fail and in such cases, it is important to detect if METIS’s profiler fails on a query in a fast manner to prevent it from leading to bad RAG configurations. Also it is useful to decide how to provide feedback to METIS to improve. When is the quality profile reliable? METIS uses LLM to generate the quality profile. Inspired by recent work in use 8 Above threshold - 98% good profiles Above threshold - 96% good profiles 7% below threshold - 90% bad profiles 7% below threshold - 85% bad profiles 90% Threshold Figure 9. Confidence score threshold for different profiler outputs is used to decide when not to use the profiler output. of model confidence [20, 25, 84] as a quality metric, we use confidence scores for METIS’s LLM profiler as to measure the reliability of the profile provided. We obtain the confidence scores from the LLM’s log-probs values on the output (the logarithm of the confidence score, which is directly provided with the output with no extra overhead). We then threshold the confidence score using a confidence score threshold (90% across different datasets) to predict whether the quality profile derived from the quality profiler LLM is actually good (defined as whether the profile can lead to 10% increase in F1-score or 1.5 −2× reduction in delay or both) or not. Such 90% threshold can be tuned for better performance, and we leave it to future work. From Figure 9, we draw two conclusions. First, over 93% of the quality profiles derived from LLM are of high confidence (i.e., over 90%). Further, for those high-confidence profile, over 96% of them are good profiles, meaning that they can", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "a4c4c75ae3c41f03a22c0d711c7eae462cd3dc58039ef3019f9c174db36140e9"}
{"doc_id": "arxiv:2412.10543#method:part-7", "url": "https://arxiv.org/abs/2412.10543", "anchor": "#method:part-7", "type": "paper", "title": "", "section": "Method", "text": "actually good (defined as whether the profile can lead to 10% increase in F1-score or 1.5 −2× reduction in delay or both) or not. Such 90% threshold can be tuned for better performance, and we leave it to future work. From Figure 9, we draw two conclusions. First, over 93% of the quality profiles derived from LLM are of high confidence (i.e., over 90%). Further, for those high-confidence profile, over 96% of them are good profiles, meaning that they can be used to improve quality, or reduce latency, or both. To handle those cases where the quality profile is of con- fidence score lower than 90% , METIS will fall back to the pruned configuration space of recent 10 queries. How to improve the profiler over time? METIS improves the query profiler LLM by profiling extra feedback prompt to this LLM. We generate this feedback prompt by generating the most accurate output, which is obtained by performing inference on the most resource-demanding configuration (the map_reduce configuration with a large number of input chunks (30) and a high value of intermediate length (300 tokens)) and then ask the quality profiler LLM what config- uration it should choose based on the query and the most accurate answer to that query. The key insight is that, the most accurate answer to the query provides the quality profiler LLM extra knowledge and thus can be used to further improve its decision. To control the cost of generating feedback prompts, METIS only generates the feedback prompt once every 30 queries and we only keep the last four feedback prompts. The cost of METIS’ LLM quality profiler: For the profiler LLM, we use a larger LLM as compared to the serving LLM Dataset Task Type Input Output Squad Single hop QA 0.4K - 2K 5-10 Musique Multihop QA 1K - 5K 5-20 KG RAG FinSec Doc Level QA 4K - 10K 20-40 QMSUM Summarization QA 4K - 12K 20-60 Table 1. Input and output length (# of tokens) distributions of the RAG datasets used in our evaluation. (7B parameters). Using this has minimal cost, as METIS only runs it on the query itself and in METIS as the query is at least 100× shorter than the context. Using this approach, METIS still saves cost as opposed to using a large LLM for inference (as shown in Section 7). We also show that METIS can use different closed and open-source LLMs as the profiler LLM for pruning and can still provide impressive delay reduction without hurting the accuracy in Section 7. 6 Implementation We implement METIS in about 2K lines of code in Python on top of the state-of-the-art popular LLM serving engine vLLM [41]. For the profiler used for configuration space pruning, we define a class LLMProfiler inheriting OpenAI’s Chat Completion API [61] interface (to invoke GPT-4o) and HuggingaceAPI [81] (to invoke LLama-3.1-70B) as models to profile the queries. We use Cohere-embed-v3.0 [4] as a state-of-the-art em- bedding method. We construct a FAISS [16] index using the IndexFlatL2 interface and perform L2-distance similarity search with index.search(query_embedding, top_k) on", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "54d68378c9c3a0744ece7a9ce8eefe998f03513d2303f3b8431c99e59ecc5090"}
{"doc_id": "arxiv:2412.10543#method:part-8", "url": "https://arxiv.org/abs/2412.10543", "anchor": "#method:part-8", "type": "paper", "title": "", "section": "Method", "text": "lines of code in Python on top of the state-of-the-art popular LLM serving engine vLLM [41]. For the profiler used for configuration space pruning, we define a class LLMProfiler inheriting OpenAI’s Chat Completion API [61] interface (to invoke GPT-4o) and HuggingaceAPI [81] (to invoke LLama-3.1-70B) as models to profile the queries. We use Cohere-embed-v3.0 [4] as a state-of-the-art em- bedding method. We construct a FAISS [16] index using the IndexFlatL2 interface and perform L2-distance similarity search with index.search(query_embedding, top_k) on the chunk embeddings to retrieve for RAG inference. We use the LLMChain interface from Langchain [8] in order to build efficient implementations of multiple synthesis methods. Finally, we use PyTorch’s [5] library modules support to perform query-level memory profiling and measurement to implement the best-fit scheduling logic and request batching. Particularly, we use pynvml to construct get_free_memory() with its interfaces of nvmlDeviceGetHandleByIndex and nvmlDeviceGetMemoryInfo to measure the amount of GPU memory available. We measure the current num-seqs and num-batched-tokens within vLLM to calculate which con- figuration can be fit into the current batch, based on the GPU availability and the request’s memory requirement. 7", "source": "arxiv_pdf", "published": "", "tokens": 184, "sha256": "4e33ce4c2d31ea7919623137436843b03789184715abb20c523c90f2af1be49b"}
{"doc_id": "arxiv:2412.10543#evaluation:part-1", "url": "https://arxiv.org/abs/2412.10543", "anchor": "#evaluation:part-1", "type": "paper", "title": "", "section": "Evaluation", "text": "The key takeaways from the evaluation are • Lower delay : Across 4 task representative datasets for RAG QA, METIS achieves 1.64 −2.54× lower response delay compared to fixed configurations of comparable quality. • Higher throughput : METIS achieves 1.8 −4.5× higher throughput than RAG serving systems which use fixed configurations reaching similar quality. 9 • Negligible overhead : METIS’ profiler’s delay is negligible compared to the overall delay of the LLM’s RAG inference. 7.1 Setup Models and hardware: : We evaluate METIS on a popular model for LLM inference, specifically the fine-tuned version of Mistral-7B-v3. We also use Llama3.1-70B for additional experiments. All models are fine-tuned such that they can take long contexts (up to 32K and 128K respectively). We apply AWQ-model quantization both models. We use an NVIDIA A40 GPU server with 2 GPUs to benchmark our results. The server is equipped with 384GB of memory and two Intel(R) Xeon(R) Gold 6130 CPUs with Hyper-threading and Turbo Boost enabled by default. We use 1 GPU to serve Mistral-7B-v3 and 2 GPUs to serve Llama3.1-70B. Datasets: We use multiple RAG QA datasets with various query profiles, in order to have task-representative work- loads. Table 1 summarizes their input-output statistics. • Squad [66]: Squad is a single-hop reading comprehension dataset, consisting of questions on articles, where the an- swer to every question is a segment from the correspond- ing reading passage. • Musique [78]: Musique is a multihop QA dataset with reasoning-based questions. It is designated to test LLM’s reasoning ability where one reasoning step critically relies on information from another. • KG RAG FinSec [50]: KG RAG Finsec is part of a Knowledge Graph family of RAG datasets and focuses on financial do- main questions from Fortune 500 companies. This dataset contains quarterly financial reports and queries need to read information for multiple chunks for answering. • QMSUM [93]: QMSUM is a human-annotated query-based multi-domain meeting summarization benchmark designed to test LLM’s reasoning-based summarization capabilities. This dataset contains multiple meeting transcripts and queries to summarize relevant spans of meetings. We build a retrieval database database by splitting the queries’ contexts into fixed-sized chunks using Langchain [8] for the database, with Cohere embed-v3.0 [4] embeddings and FAISS [16] L2-distance similarity search in order to re- trieve relevant chunks for RAG inference. To simulate a real RAG workload, we create a mix of queries from each dataset, and send them to METIS using arrival rates that follow a Poisson distribution. We report the results per dataset. Quality Metric: We adopt the following standard metric to measure the generation quality. • F1-score is used to evaluate the METIS’s serving model’s generated response (defined in §2) It is the most widely adopted metric for evaluating RAG QA tasks [10, 69, 72] System Metrics: We adopt the following system metrics: • Delay is used to measure the generation response delay of the model for every RAG query. We choose this system metric similar to other RAG serving papers [44, 70, 76] • Dollar Cost is used to measure the lower cost of using METIS’s profiler as", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "13da1df748fe2324a50c0df6fca43f3bcb54efc5f5b589682246a768cacdf843"}
{"doc_id": "arxiv:2412.10543#evaluation:part-2", "url": "https://arxiv.org/abs/2412.10543", "anchor": "#evaluation:part-2", "type": "paper", "title": "", "section": "Evaluation", "text": "to evaluate the METIS’s serving model’s generated response (defined in §2) It is the most widely adopted metric for evaluating RAG QA tasks [10, 69, 72] System Metrics: We adopt the following system metrics: • Delay is used to measure the generation response delay of the model for every RAG query. We choose this system metric similar to other RAG serving papers [44, 70, 76] • Dollar Cost is used to measure the lower cost of using METIS’s profiler as compared to using larger serving mod- els with fixed configurations having the closest accuracy. Baselines: We compare METIS with the following baselines. • vLLM: We serve RAG with vLLM with multiple static con- figurations across different queries. • Parrot*: We implement Parrot’s [44] configuration-based batching. Parrot* does not adapt the configuration per query. We compare with Parrot* using fixed RAG configu- rations which achieve the closest quality to us. • AdaptiveRAG*: We implement AdaptiveRAG’s [32], query complexity-based RAG-configuration selection and choose the configuration which maximizes the F1-score, without considering the system resource cost. 7.2 Overall improvement Lower delay without sacrificing generation quality: Fig- ure 10 shows METIS achieves delay reduction 1.64 −2.54× over AdaptiveRAG* with no reduction in F1-score. Over us- ing fixed configurations of similar delay, served with both Parrot* and vLLM, METIS achieves 12 −18% higher F1-score. Higher throughput at lower delay: Figure 11 shows METIS achieves higher throughput compared to fixed config- uration baselines when they choose the fixed-config which achieves the closest quality. Compared to Parrot* and vLLM, METIS achieves 1.8 −4.5× times higher throughput. Understanding METIS’ improvement: METIS’s gains come from jointly selecting the configuration based on the available resource, along with performing scheduling. METIS achieves higher quality than the fixed-config baselines as it is adapts the RAG-configuration per query. It reduces delay by resource-aware scheduling, making it better than fixed configurations which achieve closest quality. METIS achieves higher throughput due to being able to adapt configurations based on resource availability as com- pared to the baselines. Both Parrot* and vLLM schedule fixed RAG-configurations and cannot benefit from delay achieved by adapting the configuration like METIS. Parrot* can im- prove the delay over using fixed configurations with vLLM by 1.4 −1.8× but cannot improve the quality. 7.3 Analyzing the gains from METIS Delay saving: Figure 12 shows the contribution of every component of METIS. We compare with vLLM’s fixed config- uration, which achieves the highest quality (blue bar). Using the profiler’s outputs and choosing the median value every time (orange bar), we achieve 1.4 −1.68× reduction in delay. Next, we see the effect of batching (like Parrot*), by choosing the median value configuration and batching, we achieve 1.1 −1.2× reduction in delay. Finally, METIS achieves even greater delay reduction by 1.45 −1.75× by adapting the con- figuration based on available GPU memory with batching. 10 2.41X faster 16% higher 2.24X faster 12% higher 1.64X faster 15% higher 2.54X faster 18% higher Figure 10. METIS achieves 1.64−2.54× lower delay compared to both best fixed configuration baselines and quality-optimized RAG configuration without sacrificing generation quality. 0 2 4 6", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "a3341a95dece57c34b5e9fe8212acbbcbaab9c8292d87c88aad6389428db4b44"}
{"doc_id": "arxiv:2412.10543#evaluation:part-3", "url": "https://arxiv.org/abs/2412.10543", "anchor": "#evaluation:part-3", "type": "paper", "title": "", "section": "Evaluation", "text": "by choosing the median value configuration and batching, we achieve 1.1 −1.2× reduction in delay. Finally, METIS achieves even greater delay reduction by 1.45 −1.75× by adapting the con- figuration based on available GPU memory with batching. 10 2.41X faster 16% higher 2.24X faster 12% higher 1.64X faster 15% higher 2.54X faster 18% higher Figure 10. METIS achieves 1.64−2.54× lower delay compared to both best fixed configuration baselines and quality-optimized RAG configuration without sacrificing generation quality. 0 2 4 6 8 2 4 6 Average Delay (s) Dataset : KG RAG FinSec 0 2 4 6 8 2 4 6 Dataset: Musique 0 2 4 6 8 1 2 3 Dataset: Squad 0 2 4 6 8 5 10 Datset: QMSUM Average Queries per Second METIS (w/ adapted RAG config and batching) Parrot * (w/ fixed RAG config) vLLM (w/ fixed RAG config) Figure 11. METIS achieves 1.8 −4.5× higher throughput (at 1.8 seconds) than baselines which use fixed configurations of closest (not higher) quality. 1.68x 1.2x 1.75x 1.4x 1.1x 1.45x Figure 12. Understanding the delay improvement in METIS Better Better Figure 13. Even with increasing the inference model size, fixed configurations have 2.38 −6.8× higher cost and lower quality compared to METIS. Cost saving: Figure 13 shows METIS (including its pro- filer) has significant lower dollar cost and higher F1-score, compared to choosing the best fixed configuration, with increasing model complexity. The cost of using a (LLama3- 70B) inference model with vLLM and a fixed configuration 6% increase 4% increase Figure 14. Improvement for METIS using feedback from the output helps improve the F1-score by 4 −6%. 0 2 4 6 0.3 0.4 0.5 F1 Score Dataset: Musique 0 3 6 9 0.3 0.4 0.5 Dataset: QMSUM Average Delay (s) METIS Parrot* AdaptiveRAG* vLLM Figure 15. METIS achieves lower delay by 2.1 −2.4× at the same quality even with a larger inference LLM. is higher by 2.38× times while also having a lower F1-score of 6.5% times across datasets. Even more powerful inference models like GPT-4o fail to achieve the same F1-score with fixed configurations but have a much higher cost of 6.8×. Profiler feedback-based improvement: In Figure 14 we show the effect of the golden-configuration-based feedback to the profiler in order to improve its output. We use a 350 11 vLLM (fixed config) vLLM (change num_chunks) vLLM (change num_chunks + synthesis_method) vLLM (change num_chunks + synthesis_method + intermediate_length) METIS (change num_chunks + synthesis_method + intermediate_length + scheduling) Figure 16. Breakdown analysis: By tuning more knobs in METIS, we can see better quality-delay tradeoffs. 0 1 2 3 0.4 0.6 F1 Score Dataset: KG RAG FinSec 0 0.3 0.6 0.9 0.42 0.50 0.58 Dataset: Squad Average Delay (s) METIS Parrot* AdaptiveRAG* vLLM Figure 17. METIS’ performance gains remain substantial even with a smaller, open-source LLM profiler. Dataset: QMSUM Dataset: KG RAG FinSec Dataset: Musique Dataset: Squad Figure 18. METIS’ profiler delay is at most 1/10th of end-to- end response delay across queries from all datasets. query sample for the QMSUM and KG RAG FinSec dataset as the workload. We see that", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "ca33962a19de3919fb9d9cffa1f61eafc15cfa62ce7dd0cd7b083a1226578b80"}
{"doc_id": "arxiv:2412.10543#evaluation:part-4", "url": "https://arxiv.org/abs/2412.10543", "anchor": "#evaluation:part-4", "type": "paper", "title": "", "section": "Evaluation", "text": "Score Dataset: KG RAG FinSec 0 0.3 0.6 0.9 0.42 0.50 0.58 Dataset: Squad Average Delay (s) METIS Parrot* AdaptiveRAG* vLLM Figure 17. METIS’ performance gains remain substantial even with a smaller, open-source LLM profiler. Dataset: QMSUM Dataset: KG RAG FinSec Dataset: Musique Dataset: Squad Figure 18. METIS’ profiler delay is at most 1/10th of end-to- end response delay across queries from all datasets. query sample for the QMSUM and KG RAG FinSec dataset as the workload. We see that with the feedback mechanism (blue line), the F1-score improves by 4 −6% as compared to not having feedback (red line) from the outputs of the golden configuration. We ensure that the feedback mechanism can- not result in the output of very expensive configurations, as METIS’ joint scheduler will not pick increasingly expensive configurations based on the GPU resource constraint. 7.4 Sensitivity analysis Changing the inference LLM: Figure 15 shows the out- come of changing the inference LLM to a larger LLM (Llama3.1- 70B) on the Musique and QMSUM datasets. Even with a more powerful LLM, METIS achieves 2.1 −2.4× lower delay than AdaptiveRAG* at a similar F1-score. The best fixed- configuration baselines such as Parrot* and vLLM have a lower F1-score of 7 −10%. In RAG, models mainly rely on the external context to answer the question instead of the model weights and we only get a 2% improvement in F1-score compared to the smaller inference models. Incrementally tuning knobs in METIS: In Figure 16, we show the benefit we the improvement we get by incremen- tally adding more knobs to METIS. We measure this for the QMSUM dataset with the original Mistral-7B-v3 model. We first only tune the num_chunks (red point). Progressively we tune the RAG-configuration knobs of synthesis_method and intermediate_length and scheduling. We achieve 5, 4, 3% higher F1-Score compared to vLLM. Finally, by adding the scheduling, 2.8× lower delay reduction in delay. Changing the profiler LLM: Figure 17 shows the effect of changing the LLM profiler from GPT-4o to a smaller Llama3.1-70B model. METIS with the new profiler, still achieves 1.4 −2.1× over AdaptiveRAG* with a similar F1-score. Static configurations of Parrot* and vLLM which achieve similar delay, METIS achieves 10 −14% higher F1-score. Changing the embedding algorithm: METIS picks a state- of-art retrieval algorithm Cohere-embed-v3.0 [4]. Using two other popular retrieval algorithms All-mpnet-base-v2 [67] and text-embedding-3-large-256 [18], the F1-score change is within 1%. The delay has no measurable difference as the retrieval is > 100× faster than LLM synthesis [6]. Delay overhead of METIS’s per-query profiling: We show the negligible delay overhead of using an LLM profiler within METIS. Figure 18 shows the fraction of METIS’ pro- filer of the total end-to-end delay. Using the profiler at most adds 0.1 fraction and in the average case only adds 0.03−0.06 fraction to the total delay across queries from all datasets. 8", "source": "arxiv_pdf", "published": "", "tokens": 474, "sha256": "a939e2012d34d87ac309466557f752ee1be1e304b90b7147d78fb64cc0f6addc"}
{"doc_id": "arxiv:2412.10543#related-work", "url": "https://arxiv.org/abs/2412.10543", "anchor": "#related-work", "type": "paper", "title": "", "section": "Related work", "text": "Systems for serving RAG: Several systems have been proposed for RAG [2, 17, 32, 34, 37, 40, 44, 54, 76, 87, 90] which focus on improving retrieval using complex, iterative retrieval algorithms or on serving model selection. METIS can work in conjunction with such systems as METIS focuses on optimizing quality and serving latency, independent of how the retrieval algorithm identifies chunks for retrieval. KV cache storage and retrieval: Storing and reusing KV cache across different requests have been commonly studied in recent work [2, 14, 22, 29, 33, 41, 46, 48, 49, 63, 75, 86, 92]. METIS can work alongside these systems, where instead of retrieving chunks, it can retrieve the KV Caches for generat- ing the output. In RAG, some additional optimizations are needed to combine KV Caches of different chunks that don’t share a common prefix. This is important as the trivial con- catenation of KV Caches loses important cross-attention and reasoning between chunks. These optimizations are enabled by KV Cache blending-based approaches [9, 26, 30, 38, 80, 85]. However RAG workloads have a large number of related contexts across queries and storing all the KV Cache is ex- tremely expensive. We do not measure the KV Cache reuse ratio across queries and leave it for future work. 12 Prefill-Decode Optimizations: Several systems have pro- posed optimizations to speed-up prefill and decode for LLMs by leveraging unique properties of each phase [3, 11, 35, 65, 74, 82, 94, 95]. Notable techniques include chunked-prefill which allows interleaving prefill and decode requests and dis- aggregated prefill which separates compute nodes for prefill and decode. All of these optimizations enable faster genera- tion speed but don’t focus on generation quality. METIS can be applied with such LLM serving systems optimizations. 9 Limitations METIS is currently designed to work with commonly de- ployed RAG pipelines. New research directions in RAG [17, 89] have developed further complex pipelines with more agents and stages for deep chain-of-thought RAG workloads. These pipelines improve on complex workloads but achieve similar performance on all the commonly used RAG QA workloads we consider [1]. We leave METIS’ design exten- sion to chain-of-thought pipelines to future work. 10", "source": "arxiv_pdf", "published": "", "tokens": 360, "sha256": "c380f8272437143b26e7b663f2125f95b79c14c412b1af0073a83a58bddab78a"}
{"doc_id": "arxiv:2412.10543#conclusion", "url": "https://arxiv.org/abs/2412.10543", "anchor": "#conclusion", "type": "paper", "title": "", "section": "Conclusion", "text": "This paper introduces METIS, the first system that focuses on optimizing the tradeoffs between response delay and generation quality in RAG, by by jointly scheduling RAG queries and adapting key configurations on a per-query basis. Evaluation on four datasets shows that METIS outperforms the state-of-the-art, reducing generation latency by 1.64 − 2.54× without compromising response quality.", "source": "arxiv_pdf", "published": "", "tokens": 56, "sha256": "bdc2d24ec1b0123b44e3245a244cf152936dee29fdb2ccddfe2687d6adb2b70f"}
{"doc_id": "blog:news.microsoft.com#body:part-1", "url": "https://news.microsoft.com/source/features/ai/ai-agents-what-they-are-and-how-theyll-change-the-way-we-work/", "anchor": "#body:part-1", "type": "blog", "title": "", "section": "Body", "text": "--- title: AI agents — what they are, and how they'll change the way we work - Source author: Wp-Block-Co-Authors-Plus-Coauthors Is-Layout-Flow url: https://news.microsoft.com/source/features/ai/ai-agents-what-they-are-and-how-theyll-change-the-way-we-work/ hostname: microsoft.com description: AI agents take the power of generative AI a step further by working alongside you or even on your behalf, and they can be built and used by anyone. sitename: Source date: 2024-11-21 --- AI agents — what they are, and how they’ll change the way we work It’s Monday morning, the caffeine hasn’t kicked in yet, and you have a busy day ahead: Maybe you have piles of returns or new shipping invoices to review, or you need to get the latest updates out to your field technicians or help employees get more efficient IT support. Now you can get help with all of this and more by simply asking an AI agent to take care of it — while you drink a second cup of coffee and focus on your team’s long-term strategy. An agent can tackle certain tasks with you or for you, from acting as a virtual project manager to handling more complex assignments like reconciling financial statements to close the books. Microsoft 365 Copilot is already a personal assistant that helps with everything from tedious daily duties to jumpstarting creative projects. Using it to interact with various agents brings a new world of possibilities for organizations to empower their employees, drive business and accomplish even more. Agents can operate around the clock to review and approve customer returns or go over shipping invoices to help businesses avoid costly supply-chain errors. They can reason over reams of product information to give field technicians step-by-step instructions or use context and memory to open and close tickets for an IT help desk. “Think of agents as the new apps for an AI-powered world,” says Jared Spataro, Microsoft’s chief marketing officer for AI at Work. “We’re rapidly adding new capabilities to tackle individuals’ biggest pain points at work and drive real business results.” What are agents, anyway? An agent takes the power of generative AI a step further, because instead of just assisting you, agents can work alongside you or even on your behalf. Agents can do a range of things, from responding to questions to more complicated or multistep assignments. What sets them apart from a personal assistant is that they can be tailored to have a particular expertise. For example, you could create an agent to know everything about your company’s product catalog so it can draft detailed responses to customer questions or automatically compile product details for an upcoming presentation. Other agents can do even more, acting on your behalf, like one that helps fulfill sales orders — freeing you up to focus on building new customer relationships. Having agents handle some of these routine needs can boost productivity across industries, from manufacturing and research to finance and retail, helping businesses save time and money. You can use ready-made agents in Microsoft 365 and Dynamics 365, or build custom agents to help with more specific needs in Copilot Studio. Imagine you’re a", "source": "blog:news.microsoft.com", "published": "", "authors": "", "tokens": 512, "sha256": "30cab09af14e484587feb10a255be4320cd8752e60dcfc4816dc64ecdbcae086"}
{"doc_id": "blog:news.microsoft.com#body:part-2", "url": "https://news.microsoft.com/source/features/ai/ai-agents-what-they-are-and-how-theyll-change-the-way-we-work/", "anchor": "#body:part-2", "type": "blog", "title": "", "section": "Body", "text": "can do even more, acting on your behalf, like one that helps fulfill sales orders — freeing you up to focus on building new customer relationships. Having agents handle some of these routine needs can boost productivity across industries, from manufacturing and research to finance and retail, helping businesses save time and money. You can use ready-made agents in Microsoft 365 and Dynamics 365, or build custom agents to help with more specific needs in Copilot Studio. Imagine you’re a salesperson with big quarterly goals to meet. Copilot acts as your personal assistant, drafting emails, recapping a meeting you missed and helping you design a polished sales presentation. Meanwhile, an agent specialized in sales lead generation works autonomously in the background to find new prospects you can follow up with later in the week. Copilot partners on daily tasks, and your purpose-built agent uses its customized skills to help you meet your end-of-quarter goals. Agents are not new. Microsoft has done extensive research in the area and even created a multi-agent library last year for developers around the world, work that helped shape what agents can do today. They’re getting more attention now because recent advances in large language models (LLMs) help anyone — even outside the developer community — communicate with AI. That agent-LLM duo makes AI tools more tangibly useful. “People expect AI to do things for them,” not to just generate language, says Ece Kamar, the managing director of Microsoft’s AI Frontiers Lab. “If you want to have a system that can really solve real world problems and help people, that system has to have a good understanding of the world we live in, and when something happens, that system has to perceive that change and take action accordingly.” Agents are like layers on top of the language models that observe and collect information, provide input to the model and together generate an action plan and communicate that to the user — or even act on their own, if permitted. So both agents and models are equally important pieces of the puzzle, as far as generative AI tools go. Agents will become more useful and able to have more autonomy with innovations in their three necessary elements: memory, entitlements and tools. Memory helps provide continuity so that each time you ask for something, it isn’t like starting from scratch. “To be autonomous you have to carry context through a bunch of actions, but the models are very disconnected and don’t have continuity the way we do, so every prompt is in a vacuum and it might pull the wrong memory out,” says Sam Schillace, Microsoft’s deputy chief technology officer. “It’s like you’re watching a stop-motion animation, one isolated frame after another, and your mind puts it into motion. The clay model doesn’t move on its own.” To build up the memory infrastructure to address this, Schillace and his team are working on a process of chunking and chaining. That’s essentially what it sounds like: They’re experimenting with dividing up interactions in bits that can be stored and linked together", "source": "blog:news.microsoft.com", "published": "", "authors": "", "tokens": 512, "sha256": "55af94d66f2d25283176fd0d8107ba56bb6bba4854c50349f48393badd196e78"}
{"doc_id": "blog:news.microsoft.com#body:part-3", "url": "https://news.microsoft.com/source/features/ai/ai-agents-what-they-are-and-how-theyll-change-the-way-we-work/", "anchor": "#body:part-3", "type": "blog", "title": "", "section": "Body", "text": "memory out,” says Sam Schillace, Microsoft’s deputy chief technology officer. “It’s like you’re watching a stop-motion animation, one isolated frame after another, and your mind puts it into motion. The clay model doesn’t move on its own.” To build up the memory infrastructure to address this, Schillace and his team are working on a process of chunking and chaining. That’s essentially what it sounds like: They’re experimenting with dividing up interactions in bits that can be stored and linked together by relevance for faster access, akin to a memory — like grouping conversations about a certain project so an agent can recall those details when you ask for a status update and not have to search through its entire database. The work with entitlements and tools is making sure agents have secure access to, or are entitled to, information they need in order to accomplish things for you, with your permission — like who your boss is, for example — and to the computer programs they need to take action on your behalf, like Teams and PowerPoint. How to use and build agents for work You can already create and publish agents in Microsoft 365 Copilot that can help you in your daily work as easily as you’d create a spreadsheet or presentation — no coding skills required. You don’t need to be a developer to build agents using Copilot Studio, either. Anyone can connect them to relevant business data such as emails, reports and customer management systems so they can perform tasks and provide insights. And you’ll soon be able to enlist new agents in Microsoft 365 to help with common workflows and tasks. Interpreter in Teams will provide real-time speech-to-speech translation during meetings, for example, and you can opt to have it simulate your voice. The Employee Self-Service Agent will simplify human resource and IT help desk-related tasks like helping workers resolve a laptop issue or find out if they’ve maxed out certain benefits, and it can connect to company systems for further customization in Copilot Studio. Microsoft Dynamics 365 will have agents as well for a range of common business workflows across sales, supply chain, finance and customer service functions. And every SharePoint site will soon come equipped with an agent tailored to your organization’s content that allows employees to quickly tap into these vast knowledge bases and find exactly what they need in seconds, whether it’s project details buried in a workback schedule or a summary of a recent product memo. Developers have even more options. With the new Azure AI Agent Service, you’ll be able to choose from small or large language models to orchestrate, develop and scale agent-powered apps to streamline and automate complex workflows like order processing and customer data synchronization. It provides a software development kit with tools for developing agents, allowing you to efficiently integrate agent capabilities using Visual Studio Code and GitHub. One type of model, OpenAI’s recently announced o1 series, can bring more advanced reasoning capabilities to agents, allowing them to take on more complicated tasks by breaking them down into", "source": "blog:news.microsoft.com", "published": "", "authors": "", "tokens": 512, "sha256": "47cf1e24f3279396150288276637e6812a40f10d772da2d02ae0470bc033254f"}
{"doc_id": "blog:news.microsoft.com#body:part-4", "url": "https://news.microsoft.com/source/features/ai/ai-agents-what-they-are-and-how-theyll-change-the-way-we-work/", "anchor": "#body:part-4", "type": "blog", "title": "", "section": "Body", "text": "from small or large language models to orchestrate, develop and scale agent-powered apps to streamline and automate complex workflows like order processing and customer data synchronization. It provides a software development kit with tools for developing agents, allowing you to efficiently integrate agent capabilities using Visual Studio Code and GitHub. One type of model, OpenAI’s recently announced o1 series, can bring more advanced reasoning capabilities to agents, allowing them to take on more complicated tasks by breaking them down into steps — like getting the information someone on an IT help desk would need to solve a problem, factoring in solutions they’ve tried and coming up with a plan. You can also use the power of agents in LinkedIn; the platform’s first agent can help recruiters with hiring. Assessing risk for autonomous action There are extra safety considerations with agents that can act autonomously, and Microsoft is focused on making sure agents only access what you want them to, says Sarah Bird, the company’s chief product officer of Responsible AI. “Agents certainly up the stakes from a responsible AI point of view,” Bird says. “So we have to have much, much lower error rates. And there’s many more nuanced ways in which something could be an error. This is the big challenge with agents.” But the same responsible AI foundational playbook for other AI applications can be used to assess and mitigate risk with agents, she says. The new Copilot Control System helps IT departments manage Copilot and agents with data access and governance, management and security controls, as well as measurement reports and tools to track adoption and business value. Many agents, like those created for Microsoft 365 and Dynamics 365, include “human in the loop” approvals, where people are required to take the final step of reviewing and sending an email the Sales Order Agent wrote, for example. And for agents developed in Copilot Studio, authors can review the records to see which actions the agent took and why. The key is to focus on testing and moderating to ensure accuracy, Bird says, and for organizations to choose the right starting point for their needs. “We will of course make progress by building on the foundation we already have, so we’re starting the journey from a strong place,” Bird says. Looking back — and into the future Technologists have long been excited by the idea of autonomous systems working side-by-side with people to help them, says Kamar, who has been working on AI agents since 2005 and even wrote her Ph.D. thesis on the topic in 2010. The hurdle was that “we lacked that general problem-solving power” on the back end, she says. With LLMs, “we finally have this missing component,” she says. “Now we can bring back a lot of the ideas from our decades of research.” Going forward, Kamar envisions a new ecosystem or marketplace of agents, sort of like how apps empower people to do more with their smartphones. Agents already have “the basic building blocks of what it takes to complete a task,” she says. “Like", "source": "blog:news.microsoft.com", "published": "", "authors": "", "tokens": 512, "sha256": "edd8c4a310eb39ea330003ad833ea2cdb9b4e5ba1fd775a6c0a4c3d1d751e61f"}
{"doc_id": "blog:news.microsoft.com#body:part-5", "url": "https://news.microsoft.com/source/features/ai/ai-agents-what-they-are-and-how-theyll-change-the-way-we-work/", "anchor": "#body:part-5", "type": "blog", "title": "", "section": "Body", "text": "that “we lacked that general problem-solving power” on the back end, she says. With LLMs, “we finally have this missing component,” she says. “Now we can bring back a lot of the ideas from our decades of research.” Going forward, Kamar envisions a new ecosystem or marketplace of agents, sort of like how apps empower people to do more with their smartphones. Agents already have “the basic building blocks of what it takes to complete a task,” she says. “Like observing, ‘I can see your meeting is taking longer; I should delay the next meeting.’” They’re getting more helpful as they gain autonomy through the innovations in memory and entitlements. They’re relieving pain points for employees by helping with things like expense reporting, project management and meeting facilitation. And they’re driving exponential impact for businesses by taking on duties like alerting supply chain managers to low inventory and then automatically reordering to help drive sales and keep customers satisfied. Agents matter because they “open up a whole set of opportunities for working with people for getting tasks done, and that’s what we expect from AI systems,” Kamar says. “AI agents are not only a way to get more value for people but are going to be a paradigm shift in terms of how work gets done.” And this is just the beginning. Copilot is set to evolve with new capabilities like Copilot Actions, designed to handle routine tasks that can bog down employees like summarizing emails missed during time off, compiling agenda items and generating monthly reports. More capabilities like these are coming over the next year to lift the weight of work for employees and teams. “Copilot will empower every employee to do their best work in less time, and focus on more meaningful tasks,” Spataro says. “And agents created in Copilot Studio will transform every business process, helping companies streamline operations, enhance collaboration and drive innovation at scale.” Illustrations by Michał Bednarski / Makeshift Studios Story published on November 19, 2024", "source": "blog:news.microsoft.com", "published": "", "authors": "", "tokens": 333, "sha256": "2007988b18e359b4183d1751542115ddfba19712caae0a2355729eea906ad7ec"}
{"doc_id": "blog:www.databricks.com#body:part-1", "url": "https://www.databricks.com/blog/llm-inference-performance-engineering-best-practices", "anchor": "#body:part-1", "type": "blog", "title": "", "section": "Body", "text": "--- title: LLM Inference Performance Engineering: Best Practices url: https://www.databricks.com/blog/llm-inference-performance-engineering-best-practices hostname: databricks.com description: Learn best practices for optimizing LLM inference performance on Databricks, enhancing the efficiency of your machine learning models. sitename: Databricks date: 2023-12-10 --- In this blog post, the MosaicML engineering team shares best practices for how to capitalize on popular open source large language models (LLMs) for production usage. We also provide guidelines for deploying inference services built around these models to help users in their selection of models and deployment hardware. We have worked with multiple PyTorch-based backends in production; these guidelines are drawn from our experience with FasterTransformers, vLLM, NVIDIA's soon-to-be-released TensorRT-LLM, and others. Understanding LLM Text Generation Large Language Models (LLMs) generate text in a two-step process: \"prefill\", where the tokens in the input prompt are processed in parallel, and \"decoding\", where text is generated one 'token' at a time in an autoregressive manner. Each generated token is appended to the input and fed back into the model to generate the next token. Generation stops when the LLM outputs a special stop token or when a user-defined condition is met (e.g., some maximum number of tokens has been generated). If you'd like more background on how LLMs use decoder blocks, check out this blog post. Tokens can be words or sub-words; the exact rules for splitting text into tokens vary from model to model. For instance, you can compare how Llama models tokenize text to how OpenAI models tokenize text. Although LLM inference providers often talk about performance in token-based metrics (e.g., tokens/second), these numbers are not always comparable across model types given these variations. For a concrete example, the team at Anyscale found that Llama 2 tokenization is 19% longer than ChatGPT tokenization (but still has a much lower overall cost). And researchers at HuggingFace also found that Llama 2 required ~20% more tokens to train over the same amount of text as GPT-4. Important Metrics for LLM Serving So, how exactly should we think about inference speed? Our team uses four key metrics for LLM serving: - Time To First Token (TTFT): How quickly users start seeing the model's output after entering their query. Low waiting times for a response are essential in real-time interactions, but less important in offline workloads. This metric is driven by the time required to process the prompt and then generate the first output token. - Time Per Output Token (TPOT): Time to generate an output token for each user that is querying our system. This metric corresponds with how each user will perceive the \"speed\" of the model. For example, a TPOT of 100 milliseconds/tok would be 10 tokens per second per user, or ~450 words per minute, which is faster than a typical person can read. - Latency: The overall time it takes for the model to generate the full response for a user. Overall response latency can be calculated using the previous two metrics: latency = (TTFT) + (TPOT) * (the number of tokens to be generated). - Throughput: The number of output tokens per second", "source": "blog:www.databricks.com", "published": "", "authors": "", "tokens": 512, "sha256": "4d36c51acb2f7b3440e4c10381c4004c8d72f9c91f21400d6ada1f6f409d44c2"}
{"doc_id": "blog:www.databricks.com#body:part-2", "url": "https://www.databricks.com/blog/llm-inference-performance-engineering-best-practices", "anchor": "#body:part-2", "type": "blog", "title": "", "section": "Body", "text": "For example, a TPOT of 100 milliseconds/tok would be 10 tokens per second per user, or ~450 words per minute, which is faster than a typical person can read. - Latency: The overall time it takes for the model to generate the full response for a user. Overall response latency can be calculated using the previous two metrics: latency = (TTFT) + (TPOT) * (the number of tokens to be generated). - Throughput: The number of output tokens per second an inference server can generate across all users and requests. Our goal? The fastest time to first token, the highest throughput, and the quickest time per output token. In other words, we want our models to generate text as fast as possible for as many users as we can support. Notably, there is a tradeoff between throughput and time per output token: if we process 16 user queries concurrently, we'll have higher throughput compared to running the queries sequentially, but we'll take longer to generate output tokens for each user. If you have overall inference latency targets, here are some useful heuristics for evaluating models: - Output length dominates overall response latency: For average latency, you can usually just take your expected/max output token length and multiply it by an overall average time per output token for the model. - Input length is not significant for performance but important for hardware requirements: The addition of 512 input tokens increases latency less than the production of 8 additional output tokens in the MPT models. However, the need to support long inputs can make models harder to serve. For example, we recommend using the A100-80GB (or newer) to serve MPT-7B with its maximum context length of 2048 tokens. - Overall latency scales sub-linearly with model size: On the same hardware, larger models are slower, but the speed ratio won't necessarily match the parameter count ratio. MPT-30B latency is ~2.5x that of MPT-7B latency. Llama2-70B latency is ~2x that of Llama2-13B latency. We are often asked by prospective customers to provide an average inference latency. We recommend that before you anchor yourself to specific latency targets (\"we need less than 20 ms per token\"), you should spend some time characterizing your expected input and desired output lengths. Challenges in LLM Inference Optimizing LLM inference benefits from general techniques such as: - Operator Fusion: Combining different adjacent operators together often results in better latency. - Quantization: Activations and weights are compressed to use a smaller number of bits. - Compression: Sparsity or Distillation. - Parallelization: Tensor parallelism across multiple devices or pipeline parallelism for larger models. Beyond these methods, there are many important Transformer-specific optimizations. A prime example of this is KV (key-value) caching. The Attention mechanism in decoder-only Transformer-based models is computationally inefficient. Each token attends to all previously seen tokens, and thus recomputes many of the same values as each new token is generated. For example, while generating the Nth token, the (N-1)th token attends to (N-2)th, (N-3)th … 1st tokens. Similarly, while generating (N+1)th token, attention for the Nth token again needs to", "source": "blog:www.databricks.com", "published": "", "authors": "", "tokens": 512, "sha256": "29629928a931ffb3e82519c274466351c37cfe4aeaf0e0ace8b65a978657b0d3"}
{"doc_id": "blog:www.databricks.com#body:part-3", "url": "https://www.databricks.com/blog/llm-inference-performance-engineering-best-practices", "anchor": "#body:part-3", "type": "blog", "title": "", "section": "Body", "text": "Beyond these methods, there are many important Transformer-specific optimizations. A prime example of this is KV (key-value) caching. The Attention mechanism in decoder-only Transformer-based models is computationally inefficient. Each token attends to all previously seen tokens, and thus recomputes many of the same values as each new token is generated. For example, while generating the Nth token, the (N-1)th token attends to (N-2)th, (N-3)th … 1st tokens. Similarly, while generating (N+1)th token, attention for the Nth token again needs to look at the (N-1)th, (N-2)th, (N-3)th, … 1st tokens. KV caching, i.e., saving of intermediate keys/values for the attention layers, is used to preserve those results for later reuse, avoiding repeated computation. Memory Bandwidth is Key Computations in LLMs are mainly dominated by matrix-matrix multiplication operations; these operations with small dimensions are typically memory-bandwidth-bound on most hardware. When generating tokens in an autoregressive manner, one of the activation matrix dimensions (defined by batch size and number of tokens in the sequence) is small at small batch sizes. Therefore, the speed is dependent on how quickly we can load model parameters from GPU memory to local caches/registers, rather than how quickly we can compute on loaded data. Available and achieved memory bandwidth in inference hardware is a better predictor of speed of token generation than their peak compute performance. Inference hardware utilization is very important in terms of serving costs. GPUs are expensive and we need them to do as much work as possible. Shared inference services promise to keep costs low by combining workloads from many users, filling in individual gaps and batching together overlapping requests. For large models like Llama2-70B, we only achieve good cost/performance at large batch sizes. Having an inference serving system that can operate at large batch sizes is critical for cost efficiency. However, a large batch means larger KV cache size, and that in turn increases the number of GPUs required to serve the model. There's a tug-of-war here and shared service operators need to make some cost trade-offs and implement systems optimizations. Model Bandwidth Utilization (MBU) How optimized is an LLM inference server? As briefly explained earlier, inference for LLMs at smaller batch sizes—especially at decode time—is bottlenecked on how quickly we can load model parameters from the device memory to the compute units. Memory bandwidth dictates how quickly the data movement happens. To measure the underlying hardware's utilization, we introduce a new metric called Model Bandwidth Utilization (MBU). MBU is defined as (achieved memory bandwidth) / (peak memory bandwidth) where achieved memory bandwidth is ((total model parameter size + KV cache size) / TPOT). For example, if a 7B parameter running with 16-bit precision has TPOT equal to 14ms, then it's moving 14GB of parameters in 14ms translating to 1TB/sec bandwidth usage. If the peak bandwidth of the machine is 2TB/sec, we are running at an MBU of 50%. For simplicity, this example ignores KV cache size, which is small for smaller batch sizes and shorter sequence lengths. MBU values close to 100% imply that the inference system is effectively utilizing the available memory bandwidth.", "source": "blog:www.databricks.com", "published": "", "authors": "", "tokens": 512, "sha256": "3fb17a287f431181d3ab682cf1bf162131fc7c5f81f1d3f159c9582e6bdbfcd4"}
{"doc_id": "blog:www.databricks.com#body:part-4", "url": "https://www.databricks.com/blog/llm-inference-performance-engineering-best-practices", "anchor": "#body:part-4", "type": "blog", "title": "", "section": "Body", "text": "example, if a 7B parameter running with 16-bit precision has TPOT equal to 14ms, then it's moving 14GB of parameters in 14ms translating to 1TB/sec bandwidth usage. If the peak bandwidth of the machine is 2TB/sec, we are running at an MBU of 50%. For simplicity, this example ignores KV cache size, which is small for smaller batch sizes and shorter sequence lengths. MBU values close to 100% imply that the inference system is effectively utilizing the available memory bandwidth. MBU is also useful to compare different inference systems (hardware + software) in a normalized manner. MBU is complementary to the Model Flops Utilization (MFU; introduced in the PaLM paper) metric which is important in compute-bound settings. Figure 1 shows a pictorial representation of MBU in a plot similar to a roofline plot. The solid sloped line of the orange-shaded region shows the maximum possible throughput if memory bandwidth is fully saturated at 100%. However, in reality for low batch sizes (white dot), the observed performance is lower than maximum – how much lower is a measure of the MBU. For large batch sizes (yellow region), the system is compute bound, and the achieved throughput as a fraction of the peak possible throughput is measured as the Model Flops Utilization (MFU). MBU and MFU determine how much more room is available to push the inference speed further on a given hardware setup. Figure 2 shows measured MBU for different degrees of tensor parallelism with our TensorRT-LLM-based inference server. Peak memory bandwidth utilization is attained when transferring large contiguous memory chunks. When smaller models like MPT-7B are distributed across multiple GPUs, we observe lower MBU as we are moving smaller memory chunks in each GPU. Figure 3 shows empirically observed MBU for different degrees of tensor parallelism and batch sizes on the NVIDIA H100 GPUs. MBU decreases as batch size increases. However, as we scale GPUs, the relative decrease in MBU is less significant. It is also worthy to note that picking hardware with greater memory bandwidth can boost performance with fewer GPUs. At batch size 1, we can achieve a higher MBU of 60% on 2xH100-80GBs as compared to 55% on 4xA100-40GB GPUs (Figure 2). Benchmarking Results Latency We have measured time to first token (TTFT) and time per output token (TPOT) across different degrees of tensor parallelism for MPT-7B and Llama2-70B models. As input prompts lengthen, time to generate the first token starts to consume a substantial portion of total latency. Tensor parallelizing across multiple GPUs helps reduce this latency. Unlike model training, scaling to more GPUs offers significant diminishing returns for inference latency. Eg. for Llama2-70B going from 4x to 8x GPUs only decreases latency by 0.7x at small batch sizes. One reason for this is that higher parallelism has lower MBU (as discussed earlier). Another reason is that tensor parallelism introduces communication overhead across a GPU node. | Time to first token (ms) | |||| |---|---|---|---|---| | Model | 1xA100-40GB | 2xA100-40GB | 4xA100-40GB | 8xA100-40GB | | MPT-7B | 46 (1x) | 34 (0.73x) | 26 (0.56x) |", "source": "blog:www.databricks.com", "published": "", "authors": "", "tokens": 512, "sha256": "594db46a3ae310e38c22fb057df6dfc94c9902b20d6164522e94e88b148c413f"}
{"doc_id": "blog:www.databricks.com#body:part-5", "url": "https://www.databricks.com/blog/llm-inference-performance-engineering-best-practices", "anchor": "#body:part-5", "type": "blog", "title": "", "section": "Body", "text": "for inference latency. Eg. for Llama2-70B going from 4x to 8x GPUs only decreases latency by 0.7x at small batch sizes. One reason for this is that higher parallelism has lower MBU (as discussed earlier). Another reason is that tensor parallelism introduces communication overhead across a GPU node. | Time to first token (ms) | |||| |---|---|---|---|---| | Model | 1xA100-40GB | 2xA100-40GB | 4xA100-40GB | 8xA100-40GB | | MPT-7B | 46 (1x) | 34 (0.73x) | 26 (0.56x) | - | | Llama2-70B | Doesn't fit | 154 (1x) | 114 (0.74x) | Table 1: Time to first token given input requests are 512 tokens length with batch size of 1. Larger models like Llama2 70B needs at least 4xA100-40B GPUs to fit in memory At larger batch sizes, higher tensor parallelism leads to a more significant relative decrease in token latency. Figure 4 shows how time per output token varies for MPT-7B. At batch size 1, going from 2x to 4x only reduces token latency by ~12%. At batch size 16, latency with 4x is 33% lower than with 2x. This goes in line with our earlier observation that the relative decrease in MBU is smaller at higher degrees of tensor parallelism for batch size 16 as compared to batch size 1. Figure 5 shows similar results for Llama2-70B, except the relative improvement between 4x and 8x is less pronounced. We also compare GPU scaling across two different hardware. Because H100-80GB has 2.15x GPU memory bandwidth as compared to A100-40GB, we can see that latency is 36% lower at batch size 1 and 52% lower at batch size 16 for 4x systems. Throughput We can trade off throughput and time per token by batching requests together. Grouping queries during GPU evaluation increases throughput compared to processing queries sequentially, but each query will take longer to complete (ignoring queueing effects). There are a few common techniques for batching inference requests: - Static batching: Client packs multiple prompts into requests and a response is returned after all sequences in the batch have been completed. Our inference servers support this but do not require it. - Dynamic batching: Prompts are batched together on the fly inside the server. Typically, this method performs worse than static batching but can get close to optimal if responses are short or of uniform length. Does not work well when requests have different parameters. - Continuous batching: The idea of batching requests together as they arrive was introduced in this excellent paper and is currently the SOTA method. Instead of waiting for all sequences in a batch to finish, it groups sequences together at the iteration level. It can achieve 10x-20x better throughput than dynamic batching. Continuous batching is usually the best approach for shared services, but there are situations where the other two might be better. In low-QPS environments, dynamic batching can outperform continuous batching. It is sometimes easier to implement low-level GPU optimizations in a simpler batching framework. For offline batch inference workloads, static batching can avoid significant overhead and achieve better throughput. Batch Size How", "source": "blog:www.databricks.com", "published": "", "authors": "", "tokens": 512, "sha256": "e16c710e4342f47d4d10657bfbe59717d69eb20e8eed87cd39beca06abe6ab6e"}
{"doc_id": "blog:www.databricks.com#body:part-6", "url": "https://www.databricks.com/blog/llm-inference-performance-engineering-best-practices", "anchor": "#body:part-6", "type": "blog", "title": "", "section": "Body", "text": "finish, it groups sequences together at the iteration level. It can achieve 10x-20x better throughput than dynamic batching. Continuous batching is usually the best approach for shared services, but there are situations where the other two might be better. In low-QPS environments, dynamic batching can outperform continuous batching. It is sometimes easier to implement low-level GPU optimizations in a simpler batching framework. For offline batch inference workloads, static batching can avoid significant overhead and achieve better throughput. Batch Size How well batching works is highly dependent on the request stream. But we can get an upper bound on its performance by benchmarking static batching with uniform requests. | Batch size | ||||||| |---|---|---|---|---|---|---|---| | Hardware | 1 | 4 | 8 | 16 | 32 | 64 | 128 | | 1 x A10 | 0.4 (1x) | 1.4 (3.5x) | 2.3 (6x) | 3.5 (9x) | OOM (Out of Memory) error | || | 2 x A10 | 0.8 | 2.5 | 4.0 | 7.0 | 8.0 | || | 1 x A100 | 0.9 (1x) | 3.2 (3.5x) | 5.3 (6x) | 8.0 (9x) | 10.5 (12x) | 12.5 (14x) | | | 2 x A100 | 1.3 | 3.0 | 5.5 | 9.5 | 14.5 | 17.0 | 22.0 | | 4 x A100 | 1.7 | 6.2 | 11.5 | 18.0 | 25.0 | 33.0 | 36.5 | Table 2: Peak MPT-7B throughput (req/sec) with static batching and a FasterTransformers-based backend. Requests: 512 input and 64 output tokens. For larger inputs, the OOM boundary will be at smaller batch sizes. Latency Trade-Off Request latency increases with batch size. With one NVIDIA A100 GPU, for example, if we maximize throughput with a batch size of 64, latency increases by 4x while throughput increases by 14x. Shared inference services typically pick a balanced batch size. Users hosting their own models should decide the appropriate latency/throughput trade-off for their applications. In some applications, like chatbots, low latency for fast responses is the top priority. In other applications, like batched processing of unstructured PDFs, we might want to sacrifice the latency to process an individual document to process all of them fast in parallel. Figure 7 shows the throughput vs latency curve for the 7B model. Each line on this curve is obtained by increasing the batch size from 1 to 256. This is useful in determining how large we can make the batch size, subject to different latency constraints. Recalling our roofline plot above, we find that these measurements are consistent with what we would expect. After a certain batch size, i.e., when we cross to the compute bound regime, every doubling of batch size just increases the latency without increasing throughput. When using parallelism, it's important to understand low-level hardware details. For instance, not all 8xA100 instances are the same across different clouds. Some servers have high bandwidth connections between all GPUs, others pair GPUs and have lower bandwidth connections between pairs. This could introduce bottlenecks, causing real-world performance to deviate significantly from the curves above. Optimization Case Study: Quantization", "source": "blog:www.databricks.com", "published": "", "authors": "", "tokens": 512, "sha256": "dee0375f7b7419532d3a283e07390da86e87f854d6d832b6f264697768521c56"}
{"doc_id": "blog:www.databricks.com#body:part-7", "url": "https://www.databricks.com/blog/llm-inference-performance-engineering-best-practices", "anchor": "#body:part-7", "type": "blog", "title": "", "section": "Body", "text": "i.e., when we cross to the compute bound regime, every doubling of batch size just increases the latency without increasing throughput. When using parallelism, it's important to understand low-level hardware details. For instance, not all 8xA100 instances are the same across different clouds. Some servers have high bandwidth connections between all GPUs, others pair GPUs and have lower bandwidth connections between pairs. This could introduce bottlenecks, causing real-world performance to deviate significantly from the curves above. Optimization Case Study: Quantization Quantization is a common technique used to reduce the hardware requirements for LLM inference. Reducing the precision of model weights and activations during inference can dramatically reduce hardware requirements. For instance, switching from 16-bit weights to 8-bit weights can halve the number of required GPUs in memory constrained environments (eg. Llama2-70B on A100s). Dropping down to 4-bit weights makes it possible to run inference on consumer hardware (eg. Llama2-70B on Macbooks). In our experience, quantization should be implemented with caution. Naive quantization techniques can lead to a substantial degradation in model quality. The impact of quantization also varies across model architectures (eg. MPT vs Llama) and sizes. We will explore this in more detail in a future blog post. When experimenting with techniques like quantization, we recommend using an LLM quality benchmark like the Mosaic Eval Gauntlet to evaluate the quality of the inference system, not just the quality of the model in isolation. Additionally, it's important to explore deeper systems optimizations. In particular, quantization can make KV caches much more efficient. As mentioned previously, in autoregressive token generation, past Key/Values (KV) from the attention layers are cached instead of recomputing them at every step. The size of the KV cache varies based on the number of sequences processed at a time and the length of these sequences. Moreover, during each iteration of the next token generation, new KV items are added to the existing cache making it bigger as new tokens are generated. Therefore, effective KV cache memory management when adding these new values is critical for good inference performance. Llama2 models use a variant of attention called Grouped Query Attention (GQA). Please note that when the number of KV heads is 1, GQA is the same as Multi-Query-Attention (MQA). GQA helps with keeping the KV cache size down by sharing Keys/Values. The formula to calculate KV cache size is batch_size * seqlen * (d_model/n_heads) * n_layers * 2 (K and V) * 2 (bytes per Float16) * n_kv_heads Table 3 shows GQA KV cache size calculated at different batch sizes at a sequence length of 1024 tokens. The parameter size for Llama2 models, in comparison, is 140 GB (Float16) for the 70B model. Quantization of KV cache is another technique (in addition to GQA/MQA) to reduce the size of KV cache, and we are actively evaluating its impact on the generation quality. | Batch Size | GQA KV cache memory (FP16) | GQA KV cache memory (Int8) | |---|---|---| | 1 | .312 GiB | .156 GiB | | 16 | 5 GiB | 2.5 GiB | | 32 |", "source": "blog:www.databricks.com", "published": "", "authors": "", "tokens": 512, "sha256": "0e423ac1739c45f2c4799aec6224fa0e086cd28a66e9816b0b06582df22137cd"}
{"doc_id": "blog:www.databricks.com#body:part-8", "url": "https://www.databricks.com/blog/llm-inference-performance-engineering-best-practices", "anchor": "#body:part-8", "type": "blog", "title": "", "section": "Body", "text": "for Llama2 models, in comparison, is 140 GB (Float16) for the 70B model. Quantization of KV cache is another technique (in addition to GQA/MQA) to reduce the size of KV cache, and we are actively evaluating its impact on the generation quality. | Batch Size | GQA KV cache memory (FP16) | GQA KV cache memory (Int8) | |---|---|---| | 1 | .312 GiB | .156 GiB | | 16 | 5 GiB | 2.5 GiB | | 32 | 10 GiB | 5 GiB | | 64 | 20 GiB | 10 GiB | Table 3: KV cache size for Llama-2-70B at a sequence length of 1024 As mentioned previously, token generation with LLMs at low batch sizes is a GPU memory bandwidth-bound problem, i.e. the speed of generation depends on how quickly model parameters can be moved from the GPU memory to on-chip caches. Converting model weights from FP16 (2 bytes) to INT8 (1 byte) or INT4 (0.5 byte) requires moving less data and thus speeds up token generation. However, quantization may negatively impact the model generation quality. We are currently evaluating the impact on model quality using Model Gauntlet and plan to publish a followup blog post on it soon. Conclusions and Key Results Each of the factors we've outlined above influences the way we build and deploy models. We use these results to make data-driven decisions that take into consideration the hardware type, the software stack, the model architecture, and typical usage patterns. Here are some recommendations drawn from our experience. Identify your optimization target: Do you care about interactive performance? Maximizing throughput? Minimizing cost? There are predictable trade-offs here. Pay attention to the components of latency: For interactive applications time-to-first-token drives how responsive your service will feel and time-per-output-token determines how fast it will feel. Memory bandwidth is key: Generating the first token is typically compute-bound, while subsequent decoding is memory-bound operation. Because LLM inference often operates in memory-bound settings, MBU is a useful metric to optimize for and can be used to compare the efficiency of inference systems. Batching is critical: Processing multiple requests concurrently is critical for achieving high throughput and for effectively utilizing expensive GPUs. For shared online services continuous batching is indispensable, whereas offline batch inference workloads can achieve high throughput with simpler batching techniques. In depth optimizations: Standard inference optimization techniques are important (eg. operator fusion, weight quantization) for LLMs but it's important to explore deeper systems optimizations, especially those which improve memory utilization. One example is KV cache quantization. Hardware configurations: The model type and expected workload should be used to decide deployment hardware. For instance, when scaling to multiple GPUs MBU falls much more rapidly for smaller models, such as MPT-7B, than it does for larger models, such as Llama2-70B. Performance also tends to scale sub-linearly with higher degrees of tensor parallelism. That said, a high degree of tensor parallelism might still make sense for smaller models if traffic is high or if users are willing to pay a premium for extra low latency. Data Driven Decisions: Understanding the theory", "source": "blog:www.databricks.com", "published": "", "authors": "", "tokens": 512, "sha256": "980eda31f106d849fd8da6eb60da2e2bfe69456a55882de1440e1024f1e1a017"}
{"doc_id": "blog:www.databricks.com#body:part-9", "url": "https://www.databricks.com/blog/llm-inference-performance-engineering-best-practices", "anchor": "#body:part-9", "type": "blog", "title": "", "section": "Body", "text": "decide deployment hardware. For instance, when scaling to multiple GPUs MBU falls much more rapidly for smaller models, such as MPT-7B, than it does for larger models, such as Llama2-70B. Performance also tends to scale sub-linearly with higher degrees of tensor parallelism. That said, a high degree of tensor parallelism might still make sense for smaller models if traffic is high or if users are willing to pay a premium for extra low latency. Data Driven Decisions: Understanding the theory is important, but we recommend always measuring end-to-end server performance. There are many reasons an inference deployment can perform worse than expected. MBU could be unexpectedly low because of software inefficiencies. Or differences in hardware between cloud providers could lead to surprises (we have observed a 2x latency difference between 8xA100 servers from two cloud providers). To get started with LLM inference, try out Databricks Model Serving. Check out the documentation to learn more.", "source": "blog:www.databricks.com", "published": "", "authors": "", "tokens": 154, "sha256": "a458875070ded0e014d53d10a324231a08db255b72ef16428d010c6c10426683"}
{"doc_id": "blog:medium.com#body:part-1", "url": "https://medium.com/@williamwarley/understanding-ai-agents-how-they-work-types-and-practical-applications-bd261845f7c3", "anchor": "#body:part-1", "type": "blog", "title": "", "section": "Body", "text": "--- title: Understanding AI Agents: How They Work, Types, and Practical Applications author: Warley's CatOps url: https://medium.com/@williamwarley/understanding-ai-agents-how-they-work-types-and-practical-applications-bd261845f7c3 hostname: medium.com description: Introduction to AI Agents sitename: Medium date: 2024-06-11 --- Understanding AI Agents: How They Work, Types, and Practical Applications Introduction to AI Agents Definition and Importance AI Agents are autonomous entities that use artificial intelligence (AI) to perceive their environment, make decisions, and perform actions to achieve specific goals. These agents can operate independently or interact with other agents and systems to accomplish tasks. AI agents are designed to simulate human-like intelligence, enabling them to solve complex problems, adapt to changing conditions, and learn from experiences. Key Characteristics of AI Agents: - Autonomy: Operate without human intervention, making decisions and taking actions based on their programming and learned experiences. - Perception: Use sensors or input mechanisms to perceive their environment, gather data, and understand the context in which they operate. - Decision-Making: Apply reasoning and decision-making processes to choose the best course of action based on their goals and current state. - Learning: Improve their performance over time by learning from past experiences, adapting to new situations, and optimizing their strategies. Historical Background and Evolution The concept of AI agents has evolved significantly since its inception, influenced by advancements in computer science, robotics, and cognitive science. Here’s a brief overview of the historical development: 1950s-1960s: The early days of AI research focused on creating machines that could mimic human thought processes. Pioneering work by researchers like Alan Turing and John McCarthy laid the foundation for AI, introducing concepts such as the Turing Test and symbolic AI. 1970s-1980s: The development of expert systems marked a significant milestone in AI. These systems used rule-based logic to emulate the decision-making abilities of human experts in specific domains. However, their lack of learning capabilities and rigidity limited their adaptability. 1990s: The emergence of machine learning (ML) and neural networks revolutionized AI. Agents could now learn from data and experiences, improving their performance over time. Reinforcement learning (RL) also gained prominence, enabling agents to learn optimal strategies through trial and error. 2000s: The advent of big data and increased computational power further accelerated AI development. AI agents became more sophisticated, capable of handling complex tasks such as natural language processing (NLP), computer vision, and autonomous navigation. 2010s-Present: Deep learning, a subset of ML, has driven significant advancements in AI agents. Techniques such as convolutional neural networks (CNNs) and recurrent neural networks (RNNs) have enabled agents to achieve state-of-the-art performance in various domains. Additionally, the integration of AI agents with the Internet of Things (IoT) and cloud computing has expanded their capabilities and applications even further. Importance of AI Agents AI agents are crucial in the modern technological landscape due to their ability to automate complex tasks, make informed decisions, and continuously improve through learning. Their importance can be seen in several key areas: 1. Automation: AI agents can automate repetitive and time-consuming tasks, freeing up human resources for more strategic and creative work. This increases efficiency and productivity across various industries. 2. Scalability: AI agents can handle", "source": "blog:medium.com", "published": "", "authors": "", "tokens": 512, "sha256": "fc39282c59b0e70684197dc155633a75b8c5ee2084f795f95fb31a4a96d9befa"}
{"doc_id": "blog:medium.com#body:part-2", "url": "https://medium.com/@williamwarley/understanding-ai-agents-how-they-work-types-and-practical-applications-bd261845f7c3", "anchor": "#body:part-2", "type": "blog", "title": "", "section": "Body", "text": "has expanded their capabilities and applications even further. Importance of AI Agents AI agents are crucial in the modern technological landscape due to their ability to automate complex tasks, make informed decisions, and continuously improve through learning. Their importance can be seen in several key areas: 1. Automation: AI agents can automate repetitive and time-consuming tasks, freeing up human resources for more strategic and creative work. This increases efficiency and productivity across various industries. 2. Scalability: AI agents can handle large-scale operations and processes, making them ideal for applications that require processing vast amounts of data and performing tasks simultaneously. 3. Real-Time Decision Making: AI agents can process information and make decisions in real-time, which is crucial for applications like autonomous driving, financial trading, and real-time customer support. 4. Adaptability: Through machine learning and reinforcement learning, AI agents can adapt to new environments and situations, improving their performance and decision-making capabilities over time. 5. Personalization: AI agents can analyze individual user behavior and preferences to provide personalized experiences in applications such as recommendation systems, personal assistants, and targeted marketing. Evolution of AI Agents The evolution of AI agents can be traced through several key developments and milestones: 1. Early AI and Expert Systems: Initial AI research focused on rule-based systems and symbolic reasoning. Expert systems, which were designed to mimic the decision-making abilities of human experts, were among the first AI agents. However, their lack of learning capabilities and flexibility limited their effectiveness. 2. Machine Learning and Neural Networks: The introduction of machine learning algorithms allowed AI agents to learn from data rather than relying solely on predefined rules. Neural networks, inspired by the human brain, enabled agents to recognize patterns and make predictions, leading to significant improvements in tasks such as image and speech recognition. 3. Reinforcement Learning: Reinforcement learning (RL) provided a framework for AI agents to learn optimal behaviors through trial and error. Agents receive feedback in the form of rewards or penalties, allowing them to refine their strategies and actions. This approach has been particularly successful in applications like game playing and robotics. 4. Deep Learning: Deep learning, a subset of machine learning, involves training large neural networks with many layers. This has led to breakthroughs in natural language processing, computer vision, and other complex tasks. AI agents powered by deep learning can achieve state-of-the-art performance in various domains. 5. Integration with IoT and Cloud Computing: The integration of AI agents with the Internet of Things (IoT) and cloud computing has expanded their capabilities. AI agents can now leverage vast amounts of data collected from IoT devices and process it in real-time using cloud-based infrastructure. This integration has enabled applications such as smart homes, industrial automation, and predictive maintenance. Conclusion AI agents represent a significant advancement in artificial intelligence, offering the ability to automate complex tasks, make real-time decisions, and continuously improve through learning. Their evolution from early rule-based systems to sophisticated deep learning models has expanded their capabilities and applications across various industries. As we continue to innovate and integrate AI agents with emerging technologies, their impact", "source": "blog:medium.com", "published": "", "authors": "", "tokens": 512, "sha256": "18fc581f7317d47f7beaa2d7f5e99bfdf7c5019a38c2d4f9c4c38315d7ec8585"}
{"doc_id": "blog:medium.com#body:part-3", "url": "https://medium.com/@williamwarley/understanding-ai-agents-how-they-work-types-and-practical-applications-bd261845f7c3", "anchor": "#body:part-3", "type": "blog", "title": "", "section": "Body", "text": "process it in real-time using cloud-based infrastructure. This integration has enabled applications such as smart homes, industrial automation, and predictive maintenance. Conclusion AI agents represent a significant advancement in artificial intelligence, offering the ability to automate complex tasks, make real-time decisions, and continuously improve through learning. Their evolution from early rule-based systems to sophisticated deep learning models has expanded their capabilities and applications across various industries. As we continue to innovate and integrate AI agents with emerging technologies, their impact on our daily lives and the broader technological landscape will only grow. This introduction provides a comprehensive overview of AI agents, highlighting their definition, importance, historical evolution, and key advancements. How AI Agents Work To understand how AI agents work, it is essential to delve into their core concepts, components, and learning mechanisms. This chapter provides a detailed explanation of these elements to illustrate the functioning of AI agents. Core Concepts and Components 1. Perception: — AI agents use sensors or input mechanisms to perceive their environment. This can involve collecting data from various sources such as cameras, microphones, or other sensors. — Example: In autonomous vehicles, sensors like LIDAR, cameras, and radar gather information about the vehicle’s surroundings. 2. Reasoning: — After perceiving the environment, the agent processes the information to make informed decisions. This involves reasoning and applying logical rules or learned knowledge to interpret the data. — Example: A recommendation system analyzes user preferences and behaviors to suggest relevant products. 3. Action: — Based on its reasoning, the AI agent takes appropriate actions to achieve its goals. This can involve physical actions (e.g., a robot moving objects) or digital actions (e.g., sending an email). — Example: A robotic vacuum cleaner navigates a room to clean it efficiently. 4. Learning: — AI agents improve their performance over time by learning from experiences. This can involve supervised learning, unsupervised learning, or reinforcement learning, depending on the task and data available. — Example: A chatbot learns to provide better responses by analyzing previous interactions with users. Types of AI Agents 1. Simple Reflex Agents: — Operate based on a set of predefined rules and respond directly to specific stimuli from the environment. — Example: A thermostat that adjusts the temperature based on the current room temperature. 2. Model-Based Reflex Agents: — Maintain an internal model of the world to keep track of unobservable aspects of the environment, allowing for more informed decision-making. — Example: A navigation system that uses a map to plan routes and update the user’s location. 3. Goal-Based Agents: — Use goals to guide their actions, making decisions based on the desirability of outcomes and the likelihood of achieving their objectives. — Example: An AI planning system that schedules tasks to maximize efficiency and meet deadlines. 4. Utility-Based Agents: — Evaluate the utility of different actions to choose the one that maximizes overall satisfaction or performance, considering both goals and preferences. — Example: An autonomous trading system that selects trades to maximize profit while minimizing risk. 5. Learning Agents: — Continuously learn from their environment and experiences to improve", "source": "blog:medium.com", "published": "", "authors": "", "tokens": 512, "sha256": "f16b20c950fcacafa5c48a7ea7d75b2babc3cdf87709c4e4eefd2384f0e16e24"}
{"doc_id": "blog:medium.com#body:part-4", "url": "https://medium.com/@williamwarley/understanding-ai-agents-how-they-work-types-and-practical-applications-bd261845f7c3", "anchor": "#body:part-4", "type": "blog", "title": "", "section": "Body", "text": "on the desirability of outcomes and the likelihood of achieving their objectives. — Example: An AI planning system that schedules tasks to maximize efficiency and meet deadlines. 4. Utility-Based Agents: — Evaluate the utility of different actions to choose the one that maximizes overall satisfaction or performance, considering both goals and preferences. — Example: An autonomous trading system that selects trades to maximize profit while minimizing risk. 5. Learning Agents: — Continuously learn from their environment and experiences to improve their performance over time, adapting to new situations and optimizing their behavior. — Example: A recommendation engine that refines its suggestions based on user feedback and interactions. Learning Mechanisms 1. Supervised Learning: — Involves training an agent using labeled data, where the correct output is provided for each input example. The agent learns to map inputs to outputs by minimizing prediction errors. — Example: Training an image recognition model to classify images of cats and dogs using labeled datasets. 2. Unsupervised Learning: — Involves training an agent using unlabeled data, where the agent identifies patterns and structures in the data without explicit instructions. Techniques like clustering and dimensionality reduction are common. — Example: Grouping similar customer profiles for targeted marketing campaigns. 3. Reinforcement Learning (RL): — Involves training an agent to make sequences of decisions by rewarding desirable behaviors and penalizing undesirable ones. The agent learns to maximize cumulative rewards over time. — Example: Training a game-playing AI to learn optimal strategies by receiving points for winning and penalties for losing. Practical Implementation Implementing AI agents involves several practical steps, including data collection, model training, and deployment. Here’s a high-level overview: 1. Data Collection and Preprocessing: — Gather relevant data from sensors or databases, preprocess it to remove noise, and structure it for analysis. — Example: Collecting and cleaning data from sensors for an autonomous robot. 2. Model Training: — Train the agent using appropriate learning algorithms and techniques, such as neural networks, decision trees, or RL algorithms. — Example: Training a neural network to recognize objects in images. 3. Deployment: — Deploy the trained agent in the target environment, ensuring it can interact with other systems and perform its tasks effectively. — Example: Deploying a chatbot on a company’s customer service platform. 4. Monitoring and Maintenance: — Continuously monitor the agent’s performance, update it with new data, and retrain as necessary to maintain its effectiveness. — Example: Regularly updating a recommendation engine with new user data to improve suggestions. This chapter provides a detailed explanation of how AI agents work, covering their core concepts, types, learning mechanisms, and practical implementation. Types of AI Agents AI agents come in various types, each designed to handle specific tasks and environments. Understanding these types helps in selecting the right kind of agent for a particular application. This chapter explores the different types of AI agents, their characteristics, and use cases. 1. Simple Reflex Agents Overview: - Simple reflex agents act solely based on the current percept and predefined rules, without considering the history of past perceptions. - They are the simplest form of agents", "source": "blog:medium.com", "published": "", "authors": "", "tokens": 512, "sha256": "1291049a131f623687635c519aabfe7de0974ee1fc340caee5f8cfd08a6d0715"}
{"doc_id": "blog:medium.com#body:part-5", "url": "https://medium.com/@williamwarley/understanding-ai-agents-how-they-work-types-and-practical-applications-bd261845f7c3", "anchor": "#body:part-5", "type": "blog", "title": "", "section": "Body", "text": "of AI Agents AI agents come in various types, each designed to handle specific tasks and environments. Understanding these types helps in selecting the right kind of agent for a particular application. This chapter explores the different types of AI agents, their characteristics, and use cases. 1. Simple Reflex Agents Overview: - Simple reflex agents act solely based on the current percept and predefined rules, without considering the history of past perceptions. - They are the simplest form of agents and work well in environments that are fully observable and deterministic. How They Work: - These agents use condition-action rules (if-then statements) to decide on actions. - They do not maintain any internal state or model of the environment. Example: - A thermostat that turns the heating on or off based on the current temperature reading. - Use Case: Simple household appliances and basic automated systems. Advantages: - Easy to design and implement. - Efficient in predictable environments. Disadvantages: - Limited functionality in complex or partially observable environments. - Cannot learn or adapt to changes in the environment. 2. Model-Based Reflex Agents Overview: - Model-based reflex agents maintain an internal model of the environment, allowing them to handle partially observable environments better than simple reflex agents. - They can consider the history of past perceptions to make more informed decisions. How They Work: - These agents update their internal model based on incoming percepts and use this model to infer unseen aspects of the environment. - They use condition-action rules, but these rules can reference the internal model. Example: - A navigation system that uses a map to plan routes and update the user’s location. - Use Case: GPS navigation, industrial automation systems. Advantages: - Can handle partially observable environments. - More flexible and capable than simple reflex agents. Disadvantages: - More complex to design and implement. - Requires more computational resources to maintain and update the internal model. 3. Goal-Based Agents Overview: - Goal-based agents operate based on predefined goals. They make decisions by evaluating how well different actions achieve these goals. - They can plan sequences of actions to achieve their objectives. How They Work: - These agents use search and planning algorithms to determine the best course of action to reach a goal. - They consider both the current state and the desired goal state. Example: - An AI planning system that schedules tasks to maximize efficiency and meet deadlines. - Use Case: Automated scheduling, robotic path planning. Advantages: - Capable of complex decision-making and planning. - Can adapt to changes in goals and environment. Disadvantages: - Requires complex algorithms for search and planning. - May be computationally intensive, especially in dynamic environments. 4. Utility-Based Agents Overview: - Utility-based agents aim to maximize their overall utility or satisfaction by evaluating the desirability of different outcomes. - They make decisions based on a utility function that assigns a value to each possible state. How They Work: - These agents use optimization techniques to select actions that maximize their expected utility. - They consider multiple factors and trade-offs to achieve the", "source": "blog:medium.com", "published": "", "authors": "", "tokens": 512, "sha256": "c2761f0ddb77fd6880e59aae306372d001ad87e661ad327d48ac9780dfcdd821"}
{"doc_id": "blog:medium.com#body:part-6", "url": "https://medium.com/@williamwarley/understanding-ai-agents-how-they-work-types-and-practical-applications-bd261845f7c3", "anchor": "#body:part-6", "type": "blog", "title": "", "section": "Body", "text": "algorithms for search and planning. - May be computationally intensive, especially in dynamic environments. 4. Utility-Based Agents Overview: - Utility-based agents aim to maximize their overall utility or satisfaction by evaluating the desirability of different outcomes. - They make decisions based on a utility function that assigns a value to each possible state. How They Work: - These agents use optimization techniques to select actions that maximize their expected utility. - They consider multiple factors and trade-offs to achieve the best overall outcome. Example: - An autonomous trading system that selects trades to maximize profit while minimizing risk. - Use Case: Financial trading, resource management. Advantages: - Can handle complex decision-making scenarios involving trade-offs. - Capable of balancing multiple objectives and preferences. Disadvantages: - Designing an appropriate utility function can be challenging. - May require significant computational resources for optimization. 5. Learning Agents Overview: - Learning agents improve their performance over time by learning from experiences and adapting to new situations. - They can operate in dynamic and uncertain environments by continuously updating their knowledge and strategies. How They Work: - These agents use various learning algorithms, such as supervised learning, unsupervised learning, and reinforcement learning, to acquire new knowledge and skills. - They have four main components: a learning element, a performance element, a critic, and a problem generator. Example: - A recommendation engine that refines its suggestions based on user feedback and interactions. - Use Case: Personalized recommendations, autonomous systems, adaptive control. Advantages: - Capable of continuous improvement and adaptation. - Can handle complex and changing environments. Disadvantages: - Requires significant amounts of data for effective learning. - The learning process can be computationally intensive and time-consuming. This chapter explores the various types of AI agents, highlighting their characteristics, how they work, and their respective advantages and disadvantages. Applications of AI Agents AI agents are deployed across various industries to automate tasks, enhance decision-making, and improve overall efficiency. This chapter explores several practical applications of AI agents, highlighting their impact and benefits in different domains. 1. Autonomous Vehicles Overview: - AI agents play a crucial role in the development of autonomous vehicles, enabling them to perceive their environment, make driving decisions, and navigate safely. How They Work: - Autonomous vehicles use sensors like cameras, LIDAR, and radar to gather data about the surroundings. - AI agents process this data to identify objects, predict their movements, and make real-time driving decisions. - The agents use machine learning algorithms to improve their performance over time, adapting to different driving conditions. Example: - Waymo’s self-driving cars use AI agents to navigate complex urban environments and handle various driving scenarios. - Benefits: Enhanced safety, reduced traffic congestion, and increased mobility for individuals who cannot drive. 2. Robotics Overview: - AI agents are integral to robotics, enabling robots to perform tasks autonomously in various settings, from manufacturing to healthcare. How They Work: - Robots equipped with AI agents use sensors to perceive their environment and execute tasks with precision. - AI agents in robots can learn from interactions and adapt to new tasks, improving efficiency", "source": "blog:medium.com", "published": "", "authors": "", "tokens": 512, "sha256": "0edafbfdfeee96e12231dd9f7c2d4476f5eb9cb7af71cbf55d475997e7ce5136"}
{"doc_id": "blog:medium.com#body:part-7", "url": "https://medium.com/@williamwarley/understanding-ai-agents-how-they-work-types-and-practical-applications-bd261845f7c3", "anchor": "#body:part-7", "type": "blog", "title": "", "section": "Body", "text": "environments and handle various driving scenarios. - Benefits: Enhanced safety, reduced traffic congestion, and increased mobility for individuals who cannot drive. 2. Robotics Overview: - AI agents are integral to robotics, enabling robots to perform tasks autonomously in various settings, from manufacturing to healthcare. How They Work: - Robots equipped with AI agents use sensors to perceive their environment and execute tasks with precision. - AI agents in robots can learn from interactions and adapt to new tasks, improving efficiency and accuracy. - Robots use goal-based and utility-based agents to optimize their actions and achieve specific objectives. Example: - Collaborative robots (cobots) in manufacturing work alongside human workers, performing repetitive and precise tasks. - Benefits: Increased productivity, enhanced precision, and improved workplace safety. 3. Personal Assistants Overview: - AI agents power personal assistants like Siri, Alexa, and Google Assistant, enabling them to understand and respond to user queries. How They Work: - Personal assistants use natural language processing (NLP) to understand spoken or written commands. - AI agents process the input, retrieve relevant information, and generate appropriate responses. - These agents continuously learn from interactions to improve their understanding and accuracy. Example: - Amazon Alexa uses AI agents to control smart home devices, provide weather updates, and play music based on user preferences. - Benefits: Convenience, hands-free control, and personalized user experiences. 4. Game AI Overview: - AI agents are widely used in video games to create intelligent and adaptive non-player characters (NPCs) that enhance gameplay. How They Work: - Game AI agents use rule-based and learning algorithms to control NPC behavior, making them respond dynamically to player actions. - Agents can adapt their strategies based on player performance, providing a challenging and engaging experience. - Reinforcement learning is often used to train game AI agents, allowing them to optimize their behavior through trial and error. Example: - In games like “The Sims,” AI agents control the behavior of virtual characters, making decisions based on their needs and environment. - Benefits: Improved player engagement, realistic NPC behavior, and dynamic gameplay experiences. 5. Financial Trading Overview: - AI agents are employed in financial trading to analyze market data, make trading decisions, and execute trades autonomously. How They Work: - AI agents use machine learning algorithms to analyze historical and real-time market data, identifying patterns and trends. - These agents make trading decisions based on predefined strategies and continuously learn to improve their performance. - Utility-based agents optimize trading strategies to maximize profits while minimizing risks. Example: - AI-powered trading platforms like QuantConnect use AI agents to develop and execute automated trading strategies. - Benefits: Enhanced trading efficiency, reduced human error, and the ability to operate 24/7. 6. Healthcare Overview: - AI agents are transforming healthcare by assisting in diagnostics, treatment planning, and patient management. How They Work: - AI agents analyze medical data, such as patient records and imaging, to assist in diagnosing diseases and recommending treatments. - They use supervised learning to learn from labeled medical data and improve their diagnostic accuracy. - Agents can also monitor patient health in real-time, providing", "source": "blog:medium.com", "published": "", "authors": "", "tokens": 512, "sha256": "17f0967d569f50feed0d1b75fff4db4d59fe8ee3388027b2ec37e414f50229a6"}
{"doc_id": "blog:medium.com#body:part-8", "url": "https://medium.com/@williamwarley/understanding-ai-agents-how-they-work-types-and-practical-applications-bd261845f7c3", "anchor": "#body:part-8", "type": "blog", "title": "", "section": "Body", "text": "Benefits: Enhanced trading efficiency, reduced human error, and the ability to operate 24/7. 6. Healthcare Overview: - AI agents are transforming healthcare by assisting in diagnostics, treatment planning, and patient management. How They Work: - AI agents analyze medical data, such as patient records and imaging, to assist in diagnosing diseases and recommending treatments. - They use supervised learning to learn from labeled medical data and improve their diagnostic accuracy. - Agents can also monitor patient health in real-time, providing alerts and recommendations based on the collected data. Example: - IBM Watson for Oncology uses AI agents to analyze medical literature and patient data, helping oncologists develop personalized cancer treatment plans. - Benefits: Improved diagnostic accuracy, personalized treatment, and enhanced patient care. This chapter explores the diverse applications of AI agents across various industries, demonstrating their impact and benefits. Advantages of AI Agents AI agents offer numerous advantages that make them indispensable in various applications. This chapter discusses the key benefits of AI agents, highlighting how they contribute to efficiency, scalability, real-time decision-making, and adaptability. 1. Efficiency and Automation Task Automation: - AI agents excel at automating repetitive and time-consuming tasks, freeing up human resources for more complex and creative work. - Example: In customer service, AI agents can handle common inquiries, process transactions, and provide instant support, allowing human agents to focus on more complex issues. Increased Productivity: - By performing tasks continuously without fatigue, AI agents significantly increase productivity and operational efficiency. - Example: In manufacturing, robotic AI agents can work 24/7, assembling products with precision and speed. Error Reduction: - AI agents reduce the likelihood of human error by performing tasks consistently and accurately. - Example: In data entry and processing, AI agents ensure accuracy and consistency, reducing errors that can occur with manual handling. 2. Scalability Handling Large Volumes: - AI agents can process vast amounts of data and manage large-scale operations, making them ideal for applications that require scalability. - Example: In financial trading, AI agents can analyze and act on market data from multiple sources in real-time, scaling to handle increased trading volumes. Flexible Resource Allocation: - AI agents can dynamically allocate resources based on demand, ensuring optimal performance and cost-efficiency. - Example: Cloud-based AI agents can scale computing resources up or down based on application needs, optimizing performance and costs. Global Reach: - AI agents can operate across different time zones and geographies, providing services and support around the clock. - Example: AI-driven customer support agents can assist customers worldwide, ensuring continuous service availability. 3. Real-Time Decision Making Immediate Responses: - AI agents can process information and make decisions in real-time, which is critical for applications requiring instant responses. - Example: In autonomous driving, AI agents process sensor data and make split-second decisions to navigate safely. Adaptive Strategies: - AI agents can adapt their strategies based on real-time data and changing conditions, optimizing outcomes dynamically. - Example: In online advertising, AI agents adjust bidding strategies in real-time to maximize ad performance and budget efficiency. Proactive Monitoring: - AI agents continuously monitor systems and environments, detecting", "source": "blog:medium.com", "published": "", "authors": "", "tokens": 512, "sha256": "7637014467b449533fabba6363ef8c3a856ee1b2acaa88cfba0cb07b074d16fd"}
{"doc_id": "blog:medium.com#body:part-9", "url": "https://medium.com/@williamwarley/understanding-ai-agents-how-they-work-types-and-practical-applications-bd261845f7c3", "anchor": "#body:part-9", "type": "blog", "title": "", "section": "Body", "text": "information and make decisions in real-time, which is critical for applications requiring instant responses. - Example: In autonomous driving, AI agents process sensor data and make split-second decisions to navigate safely. Adaptive Strategies: - AI agents can adapt their strategies based on real-time data and changing conditions, optimizing outcomes dynamically. - Example: In online advertising, AI agents adjust bidding strategies in real-time to maximize ad performance and budget efficiency. Proactive Monitoring: - AI agents continuously monitor systems and environments, detecting anomalies and triggering appropriate actions immediately. - Example: In cybersecurity, AI agents detect and respond to threats in real-time, protecting systems from potential breaches. 4. Adaptability and Learning Continuous Improvement: - AI agents improve their performance over time by learning from experiences and feedback, adapting to new situations and tasks. - Example: Personalized recommendation systems learn from user interactions to provide increasingly relevant suggestions. Handling Uncertainty: - AI agents can operate effectively in uncertain and dynamic environments by adapting their behavior based on learned patterns and real-time data. - Example: In robotics, AI agents adapt to changes in their environment, such as obstacles or varying conditions, to complete tasks efficiently. Customization and Personalization: - AI agents can tailor their actions and responses to individual user preferences and needs, providing personalized experiences. - Example: Virtual personal assistants learn user preferences over time, offering personalized recommendations and assistance. 5. Cost Efficiency Reduced Operational Costs: - Automating tasks with AI agents reduces the need for manual labor, lowering operational costs and increasing profitability. - Example: Automated warehouses use AI agents to manage inventory and logistics, reducing labor costs and increasing efficiency. Optimized Resource Utilization: - AI agents optimize the use of resources, such as energy and materials, leading to cost savings and sustainability. - Example: Smart energy management systems use AI agents to optimize energy usage in buildings, reducing costs and environmental impact. Investment in Innovation: - The efficiency gains and cost savings from AI agents allow organizations to invest more in innovation and strategic initiatives. - Example: Companies can allocate resources saved from automation to research and development, driving future growth and competitiveness. This chapter highlights the numerous advantages of AI agents, emphasizing their role in enhancing efficiency, scalability, real-time decision-making, adaptability, and cost-efficiency. Implementing AI Agents Implementing AI agents involves a series of practical steps and considerations, from selecting the right tools and frameworks to addressing common challenges. This chapter provides a comprehensive guide to implementing AI agents effectively. Steps for Implementing AI Agents 1. Define Objectives and Requirements: — Clearly outline the goals you aim to achieve with the AI agent, including specific tasks and performance metrics. — Example: An e-commerce platform might aim to implement an AI agent for personalized product recommendations, with the objective of increasing sales and customer satisfaction. 2. Select Appropriate Tools and Frameworks: — Choose the tools and frameworks that best suit your needs based on the complexity of the task, available resources, and technical expertise. — Example: For natural language processing tasks, frameworks like Hugging Face Transformers or spaCy might be appropriate. 3. Data Collection and", "source": "blog:medium.com", "published": "", "authors": "", "tokens": 512, "sha256": "7da69a6cc7838be688bb29867a8476b46bbbffdcf007c4b8d7a8696d63c19ffb"}
{"doc_id": "blog:medium.com#body:part-10", "url": "https://medium.com/@williamwarley/understanding-ai-agents-how-they-work-types-and-practical-applications-bd261845f7c3", "anchor": "#body:part-10", "type": "blog", "title": "", "section": "Body", "text": "specific tasks and performance metrics. — Example: An e-commerce platform might aim to implement an AI agent for personalized product recommendations, with the objective of increasing sales and customer satisfaction. 2. Select Appropriate Tools and Frameworks: — Choose the tools and frameworks that best suit your needs based on the complexity of the task, available resources, and technical expertise. — Example: For natural language processing tasks, frameworks like Hugging Face Transformers or spaCy might be appropriate. 3. Data Collection and Preprocessing: — Gather and preprocess the data required for training the AI agent. Ensure that the data is clean, labeled (if necessary), and representative of the problem domain. — Example: Collect customer interaction logs and preprocess them to remove noise and irrelevant information for training a customer service chatbot. 4. Model Selection and Training: — Select the appropriate machine learning or deep learning model for your AI agent. Train the model using the preprocessed data, and fine-tune it to achieve optimal performance. — Example: Use a pre-trained transformer model like BERT for fine-tuning on a specific NLP task. Example Code for Training a Model with Hugging Face Transformers: from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments from datasets import load_dataset # Load dataset dataset = load_dataset('csv', data_files={'train': 'train.csv', 'test': 'test.csv'}) # Load tokenizer and model tokenizer = BertTokenizer.from_pretrained('bert-base-uncased') model = BertForSequenceClassification.from_pretrained('bert-base-uncased') # Tokenize data def tokenize_function(examples): return tokenizer(examples['text'], padding='max_length', truncation=True) tokenized_datasets = dataset.map(tokenize_function, batched=True) # Define training arguments training_args = TrainingArguments( output_dir='./results', evaluation_strategy='epoch', learning_rate=2e-5, per_device_train_batch_size=8, per_device_eval_batch_size=8, num_train_epochs=3, weight_decay=0.01, ) # Initialize Trainer trainer = Trainer( model=model, args=training_args, train_dataset=tokenized_datasets['train'], eval_dataset=tokenized_datasets['test'], ) # Train model trainer.train() 5. Evaluation and Testing: — Evaluate the AI agent’s performance using appropriate metrics and test it in various scenarios to ensure robustness and reliability. — Example: Evaluate a recommendation system using metrics like precision, recall, and F1-score on a validation dataset. 6. Deployment: — Deploy the AI agent in the target environment, ensuring it integrates smoothly with existing systems and can operate at scale. — Example: Deploy a trained chatbot on a cloud platform like AWS Lambda for scalable, serverless execution. Example Code for Deploying a Model on AWS Lambda: import json import boto3 from transformers import BertTokenizer, BertForSequenceClassification # Initialize AWS Lambda client client = boto3.client('lambda') # Define the Lambda function def lambda_handler(event, context): # Load tokenizer and model tokenizer = BertTokenizer.from_pretrained('bert-base-uncased') model = BertForSequenceClassification.from_pretrained('bert-base-uncased') # Parse input input_text = event['text'] inputs = tokenizer(input_text, return_tensors='pt') # Perform inference outputs = model(**inputs) predictions = outputs.logits.argmax(dim=-1).item() # Return response return { 'statusCode': 200, 'body': json.dumps({'prediction': predictions}) } # Deploy the Lambda function response = client.create_function( FunctionName='AIChatbot', Runtime='python3.8', Role='your-aws-role', Handler='lambda_function.lambda_handler', Code={'ZipFile': open('function.zip', 'rb').read()}, Timeout=15, MemorySize=128, ) 7. Monitoring and Maintenance: — Continuously monitor the AI agent’s performance and make necessary updates or retrain the model to maintain its effectiveness. — Example: Regularly update the training data and retrain a sentiment analysis model to adapt to changing trends and language usage. Tools and Frameworks 1. Machine Learning Frameworks: — TensorFlow: Widely used for building and deploying machine learning models, with extensive support for neural networks and deep learning. — PyTorch: Popular for", "source": "blog:medium.com", "published": "", "authors": "", "tokens": 512, "sha256": "80bee5fb3d1b45677f42423e475133e3e4191c2bb25f85154395fc97edea9f27"}
{"doc_id": "blog:medium.com#body:part-11", "url": "https://medium.com/@williamwarley/understanding-ai-agents-how-they-work-types-and-practical-applications-bd261845f7c3", "anchor": "#body:part-11", "type": "blog", "title": "", "section": "Body", "text": "open('function.zip', 'rb').read()}, Timeout=15, MemorySize=128, ) 7. Monitoring and Maintenance: — Continuously monitor the AI agent’s performance and make necessary updates or retrain the model to maintain its effectiveness. — Example: Regularly update the training data and retrain a sentiment analysis model to adapt to changing trends and language usage. Tools and Frameworks 1. Machine Learning Frameworks: — TensorFlow: Widely used for building and deploying machine learning models, with extensive support for neural networks and deep learning. — PyTorch: Popular for its dynamic computation graph and ease of use in research and development of deep learning models. — Scikit-Learn: Ideal for implementing traditional machine learning algorithms and preprocessing data. 2. Natural Language Processing (NLP) Frameworks: — Hugging Face Transformers: Provides pre-trained models and tools for NLP tasks such as text classification, question answering, and language translation. — spaCy: Efficient and scalable library for NLP tasks, including tokenization, named entity recognition, and dependency parsing. 3. Deployment Platforms: — AWS SageMaker: Comprehensive platform for building, training, and deploying machine learning models at scale. — Google Cloud AI Platform: Managed services for training and deploying machine learning models on Google Cloud. — Azure Machine Learning: End-to-end platform for training, deploying, and managing machine learning models on Azure. Best Practices 1. Data Quality: — Ensure high-quality data by cleaning, preprocessing, and labeling it accurately. Good data is crucial for training effective AI agents. — Example: Remove duplicates and outliers from your dataset to improve model accuracy. 2. Model Evaluation: — Use appropriate metrics to evaluate model performance and ensure it meets the desired objectives. — Example: Evaluate a classification model using metrics like accuracy, precision, recall, and F1-score. 3. Scalability and Efficiency: — Design AI agents to scale efficiently, ensuring they can handle increasing workloads and data volumes. — Example: Use distributed training and inference techniques to scale your AI agent across multiple machines. 4. Security and Privacy: — Implement security measures to protect data and ensure privacy, especially when dealing with sensitive information. — Example: Encrypt data at rest and in transit, and implement access controls to protect user data. Common Challenges and Solutions 1. Data Availability: — Challenge: Lack of sufficient labeled data for training. — Solution: Use data augmentation techniques, transfer learning, or synthetic data generation to augment your dataset. 2. Model Overfitting: — Challenge: The model performs well on training data but poorly on unseen data. — Solution: Implement regularization techniques, such as dropout and L2 regularization, and use cross-validation to assess model performance. 3. Integration Complexity: — Challenge: Integrating the AI agent with existing systems and workflows. — Solution: Use APIs and modular design principles to ensure smooth integration and compatibility with other systems. 4. Computational Resources: — Challenge: High computational requirements for training and inference. — Solution: Use cloud-based platforms and distributed computing to leverage scalable resources and reduce costs. This chapter provides a comprehensive guide to implementing AI agents, covering practical steps, tools and frameworks, best practices, and common challenges. Future Trends and Developments in AI Agents The field of AI agents is rapidly evolving, with continuous advancements that promise", "source": "blog:medium.com", "published": "", "authors": "", "tokens": 512, "sha256": "82c268770d0f29bd77ef0ae2cb6477b22177d3a8fdda5826231a3de61df5f370"}
{"doc_id": "blog:medium.com#body:part-12", "url": "https://medium.com/@williamwarley/understanding-ai-agents-how-they-work-types-and-practical-applications-bd261845f7c3", "anchor": "#body:part-12", "type": "blog", "title": "", "section": "Body", "text": "modular design principles to ensure smooth integration and compatibility with other systems. 4. Computational Resources: — Challenge: High computational requirements for training and inference. — Solution: Use cloud-based platforms and distributed computing to leverage scalable resources and reduce costs. This chapter provides a comprehensive guide to implementing AI agents, covering practical steps, tools and frameworks, best practices, and common challenges. Future Trends and Developments in AI Agents The field of AI agents is rapidly evolving, with continuous advancements that promise to enhance their capabilities and applications. This chapter explores some of the key future trends and developments in AI agents, including advances in reinforcement learning, integration with the Internet of Things (IoT), ethical considerations, and human-agent collaboration. Advances in Reinforcement Learning 1. Deep Reinforcement Learning (DRL): — Combining deep learning with reinforcement learning has led to significant breakthroughs in creating more capable and sophisticated AI agents. DRL algorithms enable agents to learn complex behaviors in high-dimensional environments. — Future Trend: Development of more efficient DRL algorithms that can learn faster and require less computational power, making them accessible for a broader range of applications. 2. Meta-Learning: — Meta-learning, or “learning to learn,” involves training AI agents to adapt quickly to new tasks with minimal data. This approach enhances the flexibility and generalization of AI agents. — Future Trend: Increased focus on meta-learning techniques to create AI agents that can efficiently transfer knowledge across different tasks and domains. 3. Multi-Agent Systems: — Multi-agent reinforcement learning (MARL) involves multiple AI agents interacting and learning within the same environment. This approach is useful for tasks requiring coordination and collaboration. — Future Trend: Advancements in MARL will enable more complex and realistic simulations, such as autonomous traffic management and collaborative robotics. Integration with IoT 1. Edge AI: — Edge AI involves deploying AI agents on edge devices, allowing for real-time data processing and decision-making closer to the source. This reduces latency and bandwidth usage. — Future Trend: Greater integration of AI agents with IoT devices to enable intelligent and autonomous operations in smart homes, industrial automation, and healthcare. 2. Distributed AI Systems: — Distributed AI systems leverage multiple connected devices to share computational loads and improve overall system performance and reliability. — Future Trend: Development of robust distributed AI frameworks that facilitate seamless collaboration between AI agents and IoT devices. 3. Predictive Maintenance: — AI agents can analyze data from IoT sensors to predict equipment failures and schedule maintenance proactively, reducing downtime and costs. — Future Trend: Enhanced predictive maintenance solutions using AI agents to improve efficiency and reliability in various industries, including manufacturing and energy. Ethical and Responsible AI 1. Bias Mitigation: — Addressing biases in AI agents is crucial to ensure fairness and equity. Researchers are developing techniques to detect and mitigate biases in training data and algorithms. — Future Trend: Implementation of more advanced bias detection and mitigation methods to promote ethical AI practices. 2. Transparency and Explainability: — As AI agents become more complex, understanding and explaining their decision-making processes is essential for building trust and accountability. — Future Trend: Development of", "source": "blog:medium.com", "published": "", "authors": "", "tokens": 512, "sha256": "14a559968231d021077028b43db24b5569a3e4fca439208f43ab4cf754cb8087"}
{"doc_id": "blog:medium.com#body:part-13", "url": "https://medium.com/@williamwarley/understanding-ai-agents-how-they-work-types-and-practical-applications-bd261845f7c3", "anchor": "#body:part-13", "type": "blog", "title": "", "section": "Body", "text": "Ethical and Responsible AI 1. Bias Mitigation: — Addressing biases in AI agents is crucial to ensure fairness and equity. Researchers are developing techniques to detect and mitigate biases in training data and algorithms. — Future Trend: Implementation of more advanced bias detection and mitigation methods to promote ethical AI practices. 2. Transparency and Explainability: — As AI agents become more complex, understanding and explaining their decision-making processes is essential for building trust and accountability. — Future Trend: Development of explainable AI (XAI) techniques that provide insights into how AI agents make decisions, enhancing transparency and user confidence. 3. Regulatory Compliance: — Ensuring that AI agents comply with regulatory standards and guidelines is vital for their safe and ethical deployment. — Future Trend: Establishment of comprehensive AI regulations and standards that guide the development and deployment of responsible AI agents. Human-Agent Collaboration 1. Human-in-the-Loop Systems: — Human-in-the-loop (HITL) systems involve human oversight and interaction with AI agents, combining human expertise with AI efficiency. — Future Trend: Increased adoption of HITL systems in critical applications such as healthcare, finance, and autonomous systems to ensure safe and effective operation. 2. Augmented Intelligence: — Augmented intelligence focuses on enhancing human capabilities with AI agents, rather than replacing humans. This approach leverages the strengths of both humans and AI. — Future Trend: Development of collaborative tools and platforms that empower humans to work alongside AI agents, improving productivity and decision-making. 3. Interactive Learning: — Interactive learning involves AI agents learning from direct interactions with humans, receiving feedback, and improving their performance. — Future Trend: Enhanced interactive learning frameworks that facilitate seamless and intuitive human-agent interactions, leading to more personalized and adaptive AI systems. Conclusion The future of AI agents is filled with exciting possibilities and challenges. Advances in reinforcement learning, integration with IoT, ethical considerations, and human-agent collaboration will drive the next wave of innovations in AI. By staying informed about these trends and developments, organizations and developers can harness the full potential of AI agents to create smarter, more efficient, and ethical solutions. This chapter explores the future trends and developments in AI agents, highlighting key advancements and their potential impact. Case Studies and Real-World Examples AI agents have made significant strides in various industries, solving complex problems and enhancing operational efficiency. This chapter presents several case studies and real-world examples to illustrate the successful application of AI agents in different domains. Case Study 1: Autonomous Vehicles Company: Waymo Challenge: Developing self-driving cars that can safely navigate complex urban environments and interact with other road users. Solution: Waymo uses AI agents to process data from sensors (LIDAR, cameras, radar) and make real-time driving decisions. Implementation: - Perception: AI agents use sensor data to detect and classify objects such as pedestrians, vehicles, and traffic signals. - Decision-Making: Agents apply machine learning algorithms to predict the behavior of other road users and plan safe driving maneuvers. - Action: The AI system controls the vehicle’s acceleration, braking, and steering to navigate through traffic. Outcome: Waymo’s self-driving cars have successfully logged millions of miles on public roads, demonstrating the", "source": "blog:medium.com", "published": "", "authors": "", "tokens": 512, "sha256": "710e7f0c045800c3fd24fbdc5e64995fad77bfcefa411fe865b25e936cc7400b"}
{"doc_id": "blog:medium.com#body:part-14", "url": "https://medium.com/@williamwarley/understanding-ai-agents-how-they-work-types-and-practical-applications-bd261845f7c3", "anchor": "#body:part-14", "type": "blog", "title": "", "section": "Body", "text": "sensors (LIDAR, cameras, radar) and make real-time driving decisions. Implementation: - Perception: AI agents use sensor data to detect and classify objects such as pedestrians, vehicles, and traffic signals. - Decision-Making: Agents apply machine learning algorithms to predict the behavior of other road users and plan safe driving maneuvers. - Action: The AI system controls the vehicle’s acceleration, braking, and steering to navigate through traffic. Outcome: Waymo’s self-driving cars have successfully logged millions of miles on public roads, demonstrating the potential of AI agents to enhance transportation safety and efficiency. Case Study 2: Healthcare Diagnostics Company: IBM Watson Health Challenge: Assisting doctors in diagnosing diseases and recommending personalized treatment plans based on vast amounts of medical data. Solution: IBM Watson for Oncology uses AI agents to analyze medical records, research papers, and clinical guidelines to provide evidence-based recommendations. Implementation: - Data Integration: AI agents aggregate and analyze data from electronic health records (EHRs), medical literature, and clinical trial results. - Natural Language Processing (NLP): Agents use NLP to interpret unstructured medical texts and extract relevant information. - Decision Support: The AI system suggests potential diagnoses and treatment options based on the latest medical evidence and patient-specific factors. Outcome: Watson for Oncology has been deployed in several hospitals worldwide, aiding oncologists in developing effective and personalized treatment plans, thus improving patient outcomes. Case Study 3: Financial Trading Company: BlackRock Challenge: Optimizing investment strategies and managing large portfolios with real-time market analysis and trading decisions. Solution: BlackRock’s Aladdin platform employs AI agents to analyze market data, assess risks, and execute trades autonomously. Implementation: - Market Analysis: AI agents continuously monitor and analyze financial news, market trends, and economic indicators. - Risk Management: Agents assess portfolio risks and suggest adjustments to optimize performance. - Automated Trading: The AI system executes trades based on predefined strategies and real-time market conditions. Outcome: Aladdin has enhanced BlackRock’s ability to manage assets efficiently, providing clients with optimized investment strategies and improved financial returns. Case Study 4: E-commerce Personalization Company: Amazon Challenge: Providing personalized shopping experiences to millions of customers by recommending relevant products. Solution: Amazon uses AI agents in its recommendation engine to analyze customer behavior and suggest products tailored to individual preferences. Implementation: - Data Collection: AI agents gather data on customer browsing history, purchase behavior, and product interactions. - Machine Learning: Agents use collaborative filtering and deep learning algorithms to identify patterns and preferences. - Personalized Recommendations: The AI system generates real-time product recommendations for each customer based on their unique profile. Outcome: Amazon’s recommendation engine significantly boosts customer engagement and sales, contributing to its status as a leading e-commerce platform. Case Study 5: Customer Service Automation Company: Bank of America Challenge: Enhancing customer service by providing instant, accurate, and personalized responses to customer inquiries. Solution: Bank of America implemented Erica, an AI-powered virtual assistant that helps customers with banking tasks and queries. Implementation: - Natural Language Understanding (NLU): Erica uses NLU to understand customer queries expressed in natural language. - Machine Learning: The virtual assistant learns from interactions to improve its responses and provide personalized assistance.", "source": "blog:medium.com", "published": "", "authors": "", "tokens": 512, "sha256": "87dfd521726c29e844071fef5a75c297aaf935faef6fdf236856f173130490ca"}
{"doc_id": "blog:medium.com#body:part-15", "url": "https://medium.com/@williamwarley/understanding-ai-agents-how-they-work-types-and-practical-applications-bd261845f7c3", "anchor": "#body:part-15", "type": "blog", "title": "", "section": "Body", "text": "a leading e-commerce platform. Case Study 5: Customer Service Automation Company: Bank of America Challenge: Enhancing customer service by providing instant, accurate, and personalized responses to customer inquiries. Solution: Bank of America implemented Erica, an AI-powered virtual assistant that helps customers with banking tasks and queries. Implementation: - Natural Language Understanding (NLU): Erica uses NLU to understand customer queries expressed in natural language. - Machine Learning: The virtual assistant learns from interactions to improve its responses and provide personalized assistance. - Integration: Erica integrates with the bank’s systems to access account information, perform transactions, and provide financial advice. Outcome: Erica handles millions of customer interactions, improving response times, reducing workload for human agents, and enhancing customer satisfaction. Case Study 6: Smart Home Management Company: Google Challenge: Creating a smart home ecosystem that automates household tasks and enhances convenience for users. Solution: Google Assistant uses AI agents to control smart home devices, manage schedules, and provide information. Implementation: - Voice Recognition: AI agents use speech recognition to understand voice commands from users. - Device Control: The assistant interacts with smart home devices (e.g., lights, thermostats, security systems) to execute commands. - Personalization: The AI system learns user preferences and routines to automate tasks and provide relevant information. Outcome: Google Assistant enhances the smart home experience, making it easier for users to manage their homes efficiently and conveniently. This chapter showcases successful applications of AI agents across various industries, demonstrating their versatility and impact. Key Insights and Final Recommendations As we conclude this comprehensive guide on AI agents, it is important to summarize the key insights and provide final recommendations for effectively leveraging AI agents in various applications and industries. Summary of Key Insights 1. Definition and Importance: — AI agents are autonomous entities that use AI to perceive their environment, make decisions, and perform actions to achieve specific goals. — They play a crucial role in automating tasks, enhancing decision-making, and improving operational efficiency across various domains. 2. How AI Agents Work: — AI agents operate based on core concepts such as perception, reasoning, action, and learning. — They can be categorized into different types, including simple reflex agents, model-based reflex agents, goal-based agents, utility-based agents, and learning agents. 3. Types of AI Agents: — Simple Reflex Agents: Operate based on predefined rules and immediate perception. — Model-Based Reflex Agents: Maintain an internal model of the environment to make informed decisions. — Goal-Based Agents: Make decisions based on predefined goals and desired outcomes. — Utility-Based Agents: Evaluate the utility of different actions to maximize overall satisfaction or performance. — Learning Agents: Continuously learn from experiences and adapt to new situations. 4. Applications of AI Agents: — AI agents are used in various industries, including autonomous vehicles, robotics, personal assistants, game AI, financial trading, and healthcare. — Their ability to automate tasks, make real-time decisions, and adapt to changing conditions makes them valuable assets in these fields. 5. Advantages of AI Agents: — Efficiency and Automation: Automate repetitive tasks, increase productivity, and reduce errors. — Scalability: Handle large-scale operations and adapt to varying workloads.", "source": "blog:medium.com", "published": "", "authors": "", "tokens": 512, "sha256": "1a01135065758864d0b7b4093740e0b41a2801a4de4dfc6955715649040054ec"}
{"doc_id": "blog:medium.com#body:part-16", "url": "https://medium.com/@williamwarley/understanding-ai-agents-how-they-work-types-and-practical-applications-bd261845f7c3", "anchor": "#body:part-16", "type": "blog", "title": "", "section": "Body", "text": "from experiences and adapt to new situations. 4. Applications of AI Agents: — AI agents are used in various industries, including autonomous vehicles, robotics, personal assistants, game AI, financial trading, and healthcare. — Their ability to automate tasks, make real-time decisions, and adapt to changing conditions makes them valuable assets in these fields. 5. Advantages of AI Agents: — Efficiency and Automation: Automate repetitive tasks, increase productivity, and reduce errors. — Scalability: Handle large-scale operations and adapt to varying workloads. — Real-Time Decision Making: Provide immediate responses and adapt strategies based on real-time data. — Adaptability and Learning: Improve performance over time and handle complex, dynamic environments. — Cost Efficiency: Reduce operational costs and optimize resource utilization. 6. Implementing AI Agents: — The implementation process involves defining objectives, selecting tools and frameworks, collecting and preprocessing data, training models, evaluating and testing, deploying, and maintaining AI agents. — Best practices include ensuring data quality, using appropriate evaluation metrics, designing for scalability and efficiency, and addressing security and privacy concerns. 7. Future Trends and Developments: — Advances in reinforcement learning, integration with IoT, ethical considerations, and human-agent collaboration are shaping the future of AI agents. — These trends promise to enhance the capabilities, applications, and ethical deployment of AI agents. 8. Case Studies and Real-World Examples: — Successful applications of AI agents in various industries highlight their practical benefits and impact. — Case studies demonstrate the versatility of AI agents in solving complex problems and improving operational efficiency. Final Recommendations 1. Stay Informed and Adaptable: — The field of AI agents is rapidly evolving. Stay informed about the latest developments, research, and best practices to leverage new opportunities and advancements. 2. Invest in Data Quality: — High-quality data is crucial for training effective AI agents. Ensure that your data is clean, representative, and accurately labeled. 3. Select the Right Tools and Frameworks: — Choose tools and frameworks that align with your specific requirements and technical expertise. Consider factors such as scalability, ease of use, and community support. 4. Focus on Ethical and Responsible AI: — Address ethical considerations, including bias mitigation, transparency, and regulatory compliance. Implement robust measures to ensure the responsible deployment of AI agents. 5. Optimize for Scalability and Efficiency: — Design AI agents to scale efficiently and handle varying workloads. Use cloud-based platforms and distributed computing to optimize performance and costs. 6. Continuous Monitoring and Improvement: — Continuously monitor the performance of AI agents and make necessary updates or retrain models to maintain their effectiveness. Stay proactive in addressing any issues that arise. 7. Leverage Human-Agent Collaboration: — Implement human-in-the-loop systems and augmented intelligence approaches to combine human expertise with AI efficiency. Facilitate seamless collaboration between humans and AI agents. 8. Explore Diverse Applications: — Explore various applications of AI agents across different industries. Identify opportunities where AI agents can provide significant value and drive innovation. By following these recommendations and leveraging the insights provided in this guide, you can effectively implement and benefit from AI agents in your projects and applications. Feel free to reach out if you have any", "source": "blog:medium.com", "published": "", "authors": "", "tokens": 512, "sha256": "2a72235b86b580bb9e7d773e070d2eb8eec239b6d684cb5f07294436c935ed52"}
{"doc_id": "blog:medium.com#body:part-17", "url": "https://medium.com/@williamwarley/understanding-ai-agents-how-they-work-types-and-practical-applications-bd261845f7c3", "anchor": "#body:part-17", "type": "blog", "title": "", "section": "Body", "text": "and augmented intelligence approaches to combine human expertise with AI efficiency. Facilitate seamless collaboration between humans and AI agents. 8. Explore Diverse Applications: — Explore various applications of AI agents across different industries. Identify opportunities where AI agents can provide significant value and drive innovation. By following these recommendations and leveraging the insights provided in this guide, you can effectively implement and benefit from AI agents in your projects and applications. Feel free to reach out if you have any questions or need further assistance with specific aspects of AI agents.", "source": "blog:medium.com", "published": "", "authors": "", "tokens": 91, "sha256": "adead498c82e56d7bf29fe8393130979553492f46dd1095812179a0bb1a85bd0"}
