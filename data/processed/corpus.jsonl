{"doc_id": "arxiv:2404.10630#model", "url": "https://arxiv.org/abs/2404.10630", "anchor": "#model", "type": "paper", "title": "", "section": "Model", "text": "Sizes Sequence length OpenLLaMA1 7B, 13B 2048 OpenLLaMA2 7B 2048 LLaMA1 7B, 13B, 33B, 65B 2048 LLaMA2 7B, 13B, 70B 4096 HLAT 7B, 70B 4096 Evaluation Tasks: We evaluate HLAT against baselines on 7 groups of tasks including both zero-shot and few-shot tasks [36]. We use HumanEval [37] for coding tasks, and Language Model Evaluation Harness [38] for others. Massive Multitask Language Understanding (MMLU) [15], [39] contains 57 tasks, spanning STEM, social sciences, humanities, and other subjects. The difficulty ranges from elementary to professional levels. The breadth of the dataset tests model’s overall problem solving and knowledge ability. Commonsense Reasoning (CR) consists of 6 datasets: PIQA [40], HellaSwag [41], WinoGrande [42], ARC easy and challenge [43], and OpenBookQA [29]. These multi- choice tasks include carefully crafted riddles, puzzles, and scenarios designed to probe a model’s ability to leverage implicit knowledge, make logical inferences, and navigate the rules of physical and social worlds. World Knowledge (WK) includes NaturalQuestions [44] and TriviaQA [45]. Both tasks are designed to test model’s question-answering ability in closed book setting. The models are not provided documents that may contain information about the question, and it has to rely on information learnt or memorized in pre-training data. Reading Comprehension (RC) uses BoolQ [46] to test model’s open book comprehension ability. BoolQ is a question answering dataset for yes/no questions. Each example is a triplet of (question, passage, answer), with the title of the page as optional additional context. The model is required to answer the question based on the given context in passage. Math ability is evaluated with GSM8K (Grade School Math 8K) [47]. GSM8K contains 8,500 grade school math problems. Both problems and answers are provided in natural language. These problems take between 2 and 8 steps to solve, which is ideal for testing basic multi-step reasoning ability. Code evaluation uses HumanEval [37] dataset including 164 programming problems with a function signature, docstring, body, and several unit tests. They were handwritten to ensure not present in the training set of the models. A. Performance against open-source Models We compare the performance of HLAT with other open- source benchmarks in Table II. The numbers are reported in percentage and for HLAT results, we include both mean and TABLE II: Evaluation of HLAT against 4 open-source models on 6 groups of tasks described in Section V. Numbers in the parentheses represent standard deviation, if available.", "source": "arxiv_pdf", "published": "", "tokens": 397, "sha256": "1f7a04ae12cdbefc36365643cd475c081a310e246a966cd9d7d68b198939e1e1"}
{"doc_id": "arxiv:2404.10630#model:part-1", "url": "https://arxiv.org/abs/2404.10630", "anchor": "#model:part-1", "type": "paper", "title": "", "section": "Model", "text": "Size MMLU CR WK RC Math Code Average - - accuracy accuracy exact match accuracy accuracy pass@1 pass@10 - OpenLLaMA-1 7B 30.5 58.4 40.6 70.5 5.2 4.5 13.4 41.2 OpenLLaMA-2 7B 41.1 61.3 37.9 72.4 6.8 9.7 25 44.9 LLaMA-1 7B 35.1 63.5 43.6 76.5 11 10.5 21.3 47.4 LLaMA-2 7B 45.3 64 45.2 77.4 14.6 12.2 25.2 49.2 HLAT-7B 7B 41.3 (3.6) 59.5 (1.2) 38.8 (0.5) 72.5 (0.8) 9.4 (0.8) 7.6 19.8 44.6 OpenLLaMA-1 13B 43.5 62 45.9 72.3 8.3 7 17 47.1 LLaMA-1 13B 46.9 65.3 49.7 78.1 17.8 15.8 22.6 53.1 LLaMA-2 13B 45.3 66.3 50.5 81.7 28.7 18.3 30.5 54.6 LLaMA-1 33B 57.8 68.9 54.6 83.1 35.6 21.7 38.4 59.2 LLaMA-1 65B 63.4 69.8 57 85.3 50.9 23.7 - 62.1 LLaMA-2 70B 68.9 70.7 59 85 56.8 30.5 59.4 64.7 HLAT-70B 70B 65.1 (3.4) 67.3 (1.2) 54.5 (0.6) 82.6 (0.7) 48.5 (1.4) 21.4 57.9 60.8 standard deviation (in the parentheses, if available). We also report an average score over all tasks in the last column. HLAT-7B performs better than OpenLLaMA-1 and is on- par with OpenLLaMA-2. Both HLAT-7B and OpenLLaMA models have some gap with LLaMA-1 and LLaMA-2, which is likely due to the training data quality. Even though the data composition of RedPajama-1T is similar as those used in LLaMA-1, the data cleaning pipeline and final data quality are different, which therefore affects the model performance [48]. For HLAT-70B, we use the same training dataset as the 7B model for consistency. Although there is no OpenLLaMA baseline for a fair comparison, HLAT-70B performs better than LLaMA-1 and LLaMA-2 models of smaller sizes. The model performance gap with LLaMA-1 (65B) and LLaMA-2 (70B) is also smaller than those on 7B models. We acknowl- edge the lack of effort on data quality improvement, but our main goal is to showcase the effectiveness and efficiency of AWS TRAINIUM. On MMLU (5-shot), both HLAT models perform better than OpenLLaMA-1 and LLaMA-1 models of similar size. The performance is slightly worse than LLaMA-2 family of models, likely due to the difference in training dataset size and composition [8]. On Commonsense Reasoning (0-shot) and World Knowl- edge (5-shot), HLAT-7B performs similar to OpenLLaMA-1 and OpenLLaMA-2 models. By diving deep into performance on each individual task, HLAT-7B excels in 19/29 tasks as compared with OpenLLaMA-1, and 15/29 tasks compared with OpenLLaMA-2. Both HLAT and OpenLLaMA models have some gaps with LLaMA-1 and LLaMA-2 models, which may be due to the training set quality. Nevertheless, the gap (∼3%) is consistent on 7B and 70B models. On Math problems (GSM8K, 8-shot), HLAT-7B performs significantly better than OpenLLaMA-1 and OpenLLaMA-2. As will be discussed in the next section, HLAT-70B has a big improvement of Math ability in later training stage. HLAT- 70B performs similar as LLaMA1-65B, and we observed significant improvement in upsampling training stage. On Coding problems, both HLAT-7B and HLAT-70B perform comparable with LLaMA-1. HLAT-7B performs better than OpenLLaMA-1 and worse than OpenLLaMA- 2 and LLaMA-2. First, for OpenLLaMA-1, the tokenizer merges consecutive spaces which negatively affects the coding performance, as it eliminates important information", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "a7552a7c57daa5102b6e9cb04cf7a777ce27d4c763396756430de988a1e565d1"}
{"doc_id": "arxiv:2404.10630#model:part-2", "url": "https://arxiv.org/abs/2404.10630", "anchor": "#model:part-2", "type": "paper", "title": "", "section": "Model", "text": "OpenLLaMA-1 and OpenLLaMA-2. As will be discussed in the next section, HLAT-70B has a big improvement of Math ability in later training stage. HLAT- 70B performs similar as LLaMA1-65B, and we observed significant improvement in upsampling training stage. On Coding problems, both HLAT-7B and HLAT-70B perform comparable with LLaMA-1. HLAT-7B performs better than OpenLLaMA-1 and worse than OpenLLaMA- 2 and LLaMA-2. First, for OpenLLaMA-1, the tokenizer merges consecutive spaces which negatively affects the coding performance, as it eliminates important information such as indentation and line breaks. This issue is subsequently fixed in OpenLLaMA-2, which explains its better performance. Besides, OpenLLaMA-2 is trained with additional code data from StarCoder which also contributes to performance im- provement. B. Intermediate Model Performance During the model training, we also evaluate the intermediate checkpoints about every 200 billion tokens. Figure 3 and Figure 4 show the model performance of HLAT-7B and HLAT-70B with respect to number of seen training tokens (in billions), respectively. On most benchmarks, the performance improves steadily, and correlates with the training loss. We found that for different tasks, the model converges at different rates. For Commonsense Reasoning, the model accuracy improves quickly at beginning of training, and starts to saturate at later training stages. This is similar as the trends observed in other LLM model trainings [8], [49]. However, for Math task (GSM8K) shown in Figure 3e, the learning curve shows an exponentially increasing trend. It increase very gradually for the initial ∼1 trillion tokens and begins to improve significantly during the later stages of training. Intuitively, this seems to indicate that the model is able to grasp more logical abilities after entering a relatively stable training plateau. We defer further research into this behavior as a future work. For World Knowledge task shown in Figure 3c, the per- formance increases almost linearly with number of training tokens. Since this is a closed book test and mainly evaluates the model’s ability of memorizing facts in pre-training data, the model seems to consistently improve its ability on this domain with more training steps and epochs. In addition, we tested if the trending is related to number of shots used in evaluation. It turns out that the trends are very similar for zero-shot, 3-shot, and 5-shot tests. 250 500 750 1000 1250 1500 1750 Tokens (in Billions) 25 30 35 40 45 Accuracy (norm) MMLU (a) 250 500 750 1000 1250 1500 1750 Tokens (in Billions) 50 52 54 56 58 60 Accuracy (norm) Commonsense Reasoning (b) 250 500 750 1000 1250 1500 1750 Tokens (in Billions) 5 10 15 20 25 30 35 40 Exact match World Knowledge 0 shots 3 shots 5 shots (c) 250 500 750 1000 1250 1500 1750 Tokens (in Billions) 55.0 57.5 60.0 62.5 65.0 67.5 70.0 72.5 Accuracy BoolQ (d) 250 500 750 1000 1250 1500 1750 Tokens (in Billions) 2 4 6 8 10 Accuracy GSM8K (e) 250 500 750 1000 1250 1500 1750 Tokens (in Billions) 5 10 15 20 25 30 35 Percentage Code pass@1 pass@10 pass@100 (f) Fig. 3: Intermediate model performance with number", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "e73cb464adeaf1fc0a25a88e8ec0a68dbd5f59bd15a217af33fa8f0378a3727b"}
{"doc_id": "arxiv:2404.10630#model:part-3", "url": "https://arxiv.org/abs/2404.10630", "anchor": "#model:part-3", "type": "paper", "title": "", "section": "Model", "text": "Exact match World Knowledge 0 shots 3 shots 5 shots (c) 250 500 750 1000 1250 1500 1750 Tokens (in Billions) 55.0 57.5 60.0 62.5 65.0 67.5 70.0 72.5 Accuracy BoolQ (d) 250 500 750 1000 1250 1500 1750 Tokens (in Billions) 2 4 6 8 10 Accuracy GSM8K (e) 250 500 750 1000 1250 1500 1750 Tokens (in Billions) 5 10 15 20 25 30 35 Percentage Code pass@1 pass@10 pass@100 (f) Fig. 3: Intermediate model performance with number of seen tokens for HLAT-7B. 250 500 750 1000 1250 1500 1750 Tokens (in Billions) 30 40 50 60 70 Accuracy MMLU (a) 250 500 750 1000 1250 1500 1750 Tokens (in Billions) 54 56 58 60 62 64 66 68 Accuracy (norm) Commonsense Reasoning (b) 250 500 750 1000 1250 1500 1750 Tokens (in Billions) 35 40 45 50 55 Exact match World Knowledge (c) 250 500 750 1000 1250 1500 1750 Tokens (in Billions) 65 70 75 80 85 Accuracy BoolQ (d) 250 500 750 1000 1250 1500 1750 Tokens (in Billions) 0 10 20 30 40 50 Exact match GSM8K (e) 200 400 600 800 1000 1200 1400 1600 1800 Tokens (in Billions) 10 20 30 40 50 Percentage Code pass@1 pass@10 (f) Fig. 4: Intermediate model performance with number of seen tokens for HLAT-70B. Those observations indicate the necessity of a set of eval- uation tasks covering a wide range of domains for LLM pre-training. A single validation set or evaluation tasks from narrow domains may not fully reflect the actual over- or under- fitting of the model for general downstream performance. C. Upsampling During HLAT-70B training, we upsampled the training dataset in last 400B tokens. Specifically, we use 35.47% web data, 41.27% math data, and 23.26% coding data with more details listed in Table III. In Figure 4, we plot the evalua- tion performance of HLAT-70B with seen training tokens. In upsampling training stage, that is, after 1400B tokens, TABLE III: Upsampling dataset composition for HLAT-70B. Datasets Size Percentage (billions of tokens) Web Data Wikipedia [21] 90 35.47% C4 [21] Domain Specific StackExchange 104.7 41.27% Arxiv [21] Open-Web-Math [23] PeS2o [22] Code Github [21] 59 23.26% Total - 253.7 15.16% we observe significant model performance improvement over math, coding, and MMLU performance. It improved math by 10% and coding by 5%. This is consistent with the findings in LLaMA-3 [10], where the researchers found significant improvement of LLaMA-3 8B model on math problems. However, they mentioned such method did not help much for 405B models. Our experiment fills the model size gap, and shows that upsampling still helps for a 70B model. D. Checkpoint Averaging For HLAT-70B, we average the last two checkpoints used in pre-training to generate a checkpoint for final evaluation [10], [33]. Table IV compares the individual checkpoints with 1740B training tokens and 1800B training tokens, as well as the averaged checkpoints. The averaged checkpoints outperforms individual checkpoints on average performance, as well as some individual tasks. TABLE IV: Evaluation of HLAT-70B with individual and averaged checkpoints. Checkpoint MMLU RC WK CR Math Code", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "a8bb281e58c90887129de05b676493b851f3bec063df775da898963a0c5d5a78"}
{"doc_id": "arxiv:2404.10630#model:part-4", "url": "https://arxiv.org/abs/2404.10630", "anchor": "#model:part-4", "type": "paper", "title": "", "section": "Model", "text": "helps for a 70B model. D. Checkpoint Averaging For HLAT-70B, we average the last two checkpoints used in pre-training to generate a checkpoint for final evaluation [10], [33]. Table IV compares the individual checkpoints with 1740B training tokens and 1800B training tokens, as well as the averaged checkpoints. The averaged checkpoints outperforms individual checkpoints on average performance, as well as some individual tasks. TABLE IV: Evaluation of HLAT-70B with individual and averaged checkpoints. Checkpoint MMLU RC WK CR Math Code Avg. 1740B 64.5 67.5 54 83.1 47.3 18.3 60.3 1800B 64.2 66.9 54 82.1 47.2 21.8 60.2 Average 65.1 67.3 54.5 82.6 48.5 21.4 60.8 E. Truthfulness and Bias We report the model’s truthfulness and bias using Truth- fulQA [50] and CrowS-pairs [51]. TruthfulQA presents a collection of meticulously crafted questions spanning diverse domains such as health, law, finance, and even politics. These queries deliberately target areas where human intuition and personal biases can lead to incorrect responses, and measure an LLM’s resistance to misinformed or erroneous knowledge. CrowS-Pairs is a benchmark designed to probe LLMs for social biases across nine categories, including gender, reli- gion, race/color, sexual orientation, age, nationality, disability, physical appearance and socioeconomic status. Each example is composed of a stereotype and an anti-stereotype. TABLE V: Model Truthfulness and Bias evaluation. CrowS- pairs (CSP) uses percentage of stereotypes as metric and TruthfulQA (TQA) uses multiple choice accuracy as metric. Dataset Size CSP (↓) CSP TQA (↑) TQA Tasks - english french mc1 mc2 OpenLLaMA-1 7B 64.6 50.1 23.1 35.1 OpenLLaMA-2 7B 65.6 51.7 22.6 34.6 LLaMA-1 7B 53.7 47.5 22.0 34.1 LLaMA-2 7B 66.9 54.9 25.2 39.0 HLAT-7B 7B 65.2 54.5 23.6 37.2 LLaMA-1 65B 69.3 58.3 27.9 42.6 LLaMA-2 70B 69.8 63.5 30.6 44.8 HLAT-70B 70B 68.1 59.1 32.3 45.9 We present the results in Table V with 0 shot inference. For TruthfulQA, we measure the multiple-choice score, and higher score shows better truthfulness. For CrowS-Pairs, it measures the percentage of models choosing answers of stereotypes, so lower scores indicates smaller bias. Overall, HLAT performs similar to other open-source models. F. Efficiency and Scalability We describe the training efficiency in terms of Cost per 4-million tokens (CPT) and scalability reported in [52]. The CPT is defined as CPT = C T ×3600 × 4e6, where C is instance cost per hour ($21.50 for Trainium, and $32.77 for GPU), T is training throughput (tokens per second). CPT quantifies both the training speed and also hardware cost. We use this metric to compare training efficiency of Trainium and GPU. 4 8 16 32 64 Number of nodes 0 2 4 6 8 10 12 Normalized CPT trn1, 7B p4d, 7B trn1, 70B p4d, 70B Fig. 5: Normalized cost per 4 million tokens (CPT) for 7B and 70B models on AWS TRAINIUM with various number of nodes. CPT of GPU baseline (p4d, 7B) with 4 nodes is normalized to 100%. 70B models ran into out-of-memory on 4 nodes. For comparison, the GPU baseline is established using p4d.24xlarge instances and NeMo 23.08 [25] (available inside NeMo docker container with tag 23.08) software", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "16290b00b94467bce4ef9dc5eb01a8d0c4f47501d979f21cfe631896e70c702d"}
{"doc_id": "arxiv:2404.10630#model:part-5", "url": "https://arxiv.org/abs/2404.10630", "anchor": "#model:part-5", "type": "paper", "title": "", "section": "Model", "text": "6 8 10 12 Normalized CPT trn1, 7B p4d, 7B trn1, 70B p4d, 70B Fig. 5: Normalized cost per 4 million tokens (CPT) for 7B and 70B models on AWS TRAINIUM with various number of nodes. CPT of GPU baseline (p4d, 7B) with 4 nodes is normalized to 100%. 70B models ran into out-of-memory on 4 nodes. For comparison, the GPU baseline is established using p4d.24xlarge instances and NeMo 23.08 [25] (available inside NeMo docker container with tag 23.08) software stack. Fig- ure 5 plots the normalized CPT of training on TRAINIUM and scaling. The TRAINIUM CPT is normalized, such that the CPT of the GPU baseline (p4d, 7B) on 4 nodes is 100%. Overall, the training cost on trn1 is approximately 60% of GPU, and is consistent with the number of nodes. In addition, the CPTs on 70B models are roughly 10 times of those on 7B models. G. Model Limitation We note some limitations of HLAT in this section. Similar as other LLMs, HLAT suffers a set of limitations such as hallucinations, potential non-factual generations, biases, and toxicity [53]. For example, although comparable with other open-source pre-trained models, the bias of HLAT is still relative high on some subjects such as sexual orientation, physical appearance, religion, and socioeconomic (see Table V). This is partially due to the usage of publicly available datasets. More importantly, as a pre-trained model, HLAT has not gone through a supervised finetuning and human prefer- ence alignment. Those fine-tuning methods have been shown to be able to alleviate some limitations of pre-trained LLMs [9]. Another limitation is that our training is stopped after 1.8 trillion tokens. As is suggested by LLaMA-3 [10], HLAT may be able to further improve on certain tasks, such as math, world knowledge, MMLU, and coding, with more training tokens. VI. BEST PRACTICES & FUTURE DIRECTIONS In this section, we share some best practices we observed for training on AWS TRAINIUM, and raise open questions for future research. Parallelism: NxDT supports TP up to 32 degrees and pipeline parallelism. For a 7B model, we found that the combination of TP=8 and PP=1 provides the highest training throughput, but not for HLAT-70B. So the optimal parallelism configuration varies with model sizes and architectures. To achieve the highest training throughput, parallelism configu- ration needs to be jointly optimized with choice of activation checkpointing method, gradient accumulation steps, and train- ing precision, to balance memory and communication costs. Training Precision: NxDT supports various training pre- cision configurations, including full precision (FP32), BF16 with and without SR, standard mixed precision training, etc. Full precision training is often memory-wise infeasible for multi-billion LLMs. We compared multiple training strategies for HLAT: pure BF16, BF16 with SR and standard mixed precision training. Empirically, we found that training loss of pure BF16 diverges. BF16 with SR shows similar training loss as mixed precision on HLAT-7B model, but converges slower on HLAT-70B. We finally chose BF16 with SR for higher throughput on HLAT-7B, but standard mixed precision on HLAT-70B. For models of other sizes and architecture, warm-up study may be needed", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "2f5e409cfd33be307785bce29ac7af9f1d41871bbc242b54daeb78df32791f87"}
{"doc_id": "arxiv:2404.10630#model:part-6", "url": "https://arxiv.org/abs/2404.10630", "anchor": "#model:part-6", "type": "paper", "title": "", "section": "Model", "text": "often memory-wise infeasible for multi-billion LLMs. We compared multiple training strategies for HLAT: pure BF16, BF16 with SR and standard mixed precision training. Empirically, we found that training loss of pure BF16 diverges. BF16 with SR shows similar training loss as mixed precision on HLAT-7B model, but converges slower on HLAT-70B. We finally chose BF16 with SR for higher throughput on HLAT-7B, but standard mixed precision on HLAT-70B. For models of other sizes and architecture, warm-up study may be needed to decide the optimal training precision. Usually, the divergence can be observed in first few thousands of steps. Choice of β2: We observed that using β2 = 0.99 causes training instability and slower convergence. This is related to the choice of BF16 with SR training precision. A large β2 fails to capture the gradient explosion at current and recent steps, and hence does not effectively reduce the gradients in occurrence of gradient explosion. Switching to β2 = 0.95 addresses the above-mentioned problem. Weight decay: We applied weight decay to all layers. Empirically, weight decay is not applied to normalization and bias layers [54]. In our experiment, we did not found much performance-wise difference of those two methods. Pre-compilation: TRAINIUM requires pre-compiling the scripts to graphs. The compilation takes some time, especially for large models. Debugging on training scripts (e.g., printing out intermediate tensors) may require re-compilation. Instead of directly developing on a large model, we found it more efficient to develop and test on a smaller model and scale up afterwards. VII. RELATED WORK LLM pre-training: After the Transformer architecture [1] was introduced, BERT [54] was proposed to pre-train a language model on a large corpus of unlabeled data. Fol- lowing the success of BERT model on various NLP tasks, many pre-trained language models are later introduced with different architectures and training methods, such as GPT-2 [55], RoBERTa [56], BART [57], and so on [6]. Studies later observed significant performance improvement of language models by increasing model size and training data [58]. Such abilities are further demonstrated in LLMs such as GPT-3 [7], PaLM [59], LLaMA [8]–[10], Falcon [60], Gemini [61], Phi [48], etc. Pre-trained on trillions of tokens, LLMs with tens or hundreds of billions parameters show remarkable ability in generating creative text contents, as well as a variety of downstream tasks, such as question answering, summarization, machine translation, programming, etc. [6]. AI accelerators: Most models are trained on NVIDIA GPU accelerators, such as GPT [7], [62] and LLaMA [8], [9]. Falcon-180B [60] was trained on AWS SageMaker, with up to 4,096 A100 40GB GPUs using p4d instances. However, the landscape of hardware accelerators for deep learning train- ing has blossomed in recent years, with established players like NVIDIA GPUs facing fierce competition from custom offerings like Google’s TPU and AWS TRAINIUM. PaLM- 2 [59] and OpenLLaMA [29] have demonstreated successful LLM pre-training on Google TPU. Recently, OLMo [49] is an open-source model developed by AI2. It has two models trained on AMD and Nvidia GPUs, separately. The two models have nearly identical performance on their evaluation suite by 2T tokens.", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "c96b0fd22b26e5e58be5abed3349e5bb069d60164005e5b9aac25e1774330083"}
{"doc_id": "arxiv:2404.10630#model:part-7", "url": "https://arxiv.org/abs/2404.10630", "anchor": "#model:part-7", "type": "paper", "title": "", "section": "Model", "text": "of hardware accelerators for deep learning train- ing has blossomed in recent years, with established players like NVIDIA GPUs facing fierce competition from custom offerings like Google’s TPU and AWS TRAINIUM. PaLM- 2 [59] and OpenLLaMA [29] have demonstreated successful LLM pre-training on Google TPU. Recently, OLMo [49] is an open-source model developed by AI2. It has two models trained on AMD and Nvidia GPUs, separately. The two models have nearly identical performance on their evaluation suite by 2T tokens. AWS TRAINIUM is a machine learning accelerator developed for deep learning training with high performance and cost-competitiveness. Our work is the first demonstration of end-to-end multi-billion LLM pre-trained on AWS TRAINIUM. Ultimately, the optimal choice depends on the specific needs of the training task, with further research required to fully explore the potential of each accelerator and their possible convergence in future architectures. VIII. CONCLUSION In this paper, we pre-train HLAT, a family of 7 bil- lion and 70 billion parameter large language models, using AWS TRAINIUM over ∼1.8 trillion tokens. HLAT follows the decoder-only architecture and is trained with up to 256 Amazon EC2 trn1.32xlarge instances. We evaluate the per- formance of HLAT against popular open-source baseline models including LLaMA and OpenLLaMA on a variety of popular benchmarking tasks. We find that HLAT achieves model quality on par with these baseline models of similar sizes. This work demonstrates, for the first time, that AWS TRAINIUM with NxDT is able to successfully pre-train high- quality LLMs with high efficiency and low cost.", "source": "arxiv_pdf", "published": "", "tokens": 252, "sha256": "16d93448a27f17f55f281fffad1dc07f8c560e5ec9eb6a999f9690fc6ff65bab"}
{"doc_id": "arxiv:2412.10543#abstract", "url": "https://arxiv.org/abs/2412.10543", "anchor": "#abstract", "type": "paper", "title": "", "section": "Abstract", "text": "RAG (Retrieval Augmented Generation) allows LLMs (large language models) to generate better responses with exter- nal knowledge, but using more external knowledge causes higher response delay. Prior work focuses either on reducing the response delay (e.g., better scheduling of RAG queries) or on maximizing quality (e.g., tuning the RAG workflow), but they fall short in systematically balancing the tradeoff between the delay and quality of RAG responses. To bal- ance both quality and response delay, this paper presents METIS, the first RAG system that jointly schedules queries and adapts the key RAG configurations of each query, such as the number of retrieved text chunks and synthesis meth- ods. Using four popular RAG-QA datasets, we show that compared to the state-of-the-art RAG optimization schemes, METIS reduces the generation latency by 1.64 −2.54× with- out sacrificing generation quality. 1", "source": "arxiv_pdf", "published": "", "tokens": 137, "sha256": "b406a2a0b9db36394819f2e170b6319f504c0bec0ef9ea59bec676bf50e21d4f"}
{"doc_id": "arxiv:2412.10543#introduction:part-1", "url": "https://arxiv.org/abs/2412.10543", "anchor": "#introduction:part-1", "type": "paper", "title": "", "section": "Introduction", "text": "Retrieval-augmented generation (RAG) is a popular LLM inference technique that augments an LLM inference query with relevant text chunks, or “context”, retrieved from a large corpus.1 RAG systems, which include retrieval and LLM inference2, have found many use cases in QA tasks, personal assistants, chatbots, and LLM-powered search [10, 62]. While RAG can enhance the quality (accuracy and relevance) of LLM-generated responses [7, 53, 58, 91, 96], RAG queries are inherently slow as they need more compute and mem- ory resources to process the long input context to answer a query [6, 15, 42]. Thus, it is essential to balance high response quality and low response delays in RAG inference systems. 1RAG vs. long-context models is an active field of research, with the industry widely deploying RAG for its task-focused model inference quality and better resource-sharing capabilities [68]. 2Though RAG sometimes refers to the retrieval step, in this work, a RAG system includes both retrieval and LLM inference based on the retrieved texts, and we aim to optimize the whole pipeline. Past research efforts have optimized RAG, regarding ei- ther response quality or response delay, but they fall short in optimizing the quality-delay tradeoffs of RAG. RAG queries have an associated RAG configuration which de- scribes how and how much data to input for the query (more in §2) [72, 79, 83]. One line of prior work focuses on reducing response delay through better query schedul- ing (e.g., GPU allocation and inference batching) for RAG queries [2, 44, 45, 70, 76], without adapting the RAG con- figuration themselves. An alternate line of work focuses on maximizing generation quality by tuning the configurations of RAG queries [32, 77, 83], but this is often done at the cost of longer response delay. The RAG configuration simultaneously affects generation quality and response delay (e.g., retrieving too many chunks for a simple RAG query may unnecessarily inflate delay with- out increasing quality). Unlike traditional data queries (e.g., SQL) which specify the inputs and operators, RAG queries are inherently under-specified as they consist of a text query written in natural language [27, 32, 57, 64] and do not directly specify the exact RAG configuration of its execution. Moreover, multiple configuration knobs can influence the delay-quality tradeoffs. For instance, besides how many chunks to retrieve, how to use them in the LLM’s input involves two design choices—should the chunks be processed by the LLM jointly, or should the chunks be summarized first before being fed into the LLM together (and how long should a summary be). Recent works also attempt to tune RAG con- figuration [32, 77], but they focus on either tuning individual knobs or maximizing quality at the cost of higher delay. How- ever, tuning configurations across multiple knobs quickly hits a prohibitive combinatorial space (more in §3) and re- quires optimizations to reduce the search cost. What’s more, the RAG configuration should be tuned jointly with scheduling. Consider two configurations: 𝐴feeds all retrieved text chunks in one LLM input, and 𝐵summarizes first each chunk with an LLM and then feeds the summaries 1 arXiv:2412.10543v2 [cs.LG]", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "d9142e9627112b69af8cb5f22978e474158cd1ffcbf75d2f7635604ecdc58a0b"}
{"doc_id": "arxiv:2412.10543#introduction:part-2", "url": "https://arxiv.org/abs/2412.10543", "anchor": "#introduction:part-2", "type": "paper", "title": "", "section": "Introduction", "text": "they focus on either tuning individual knobs or maximizing quality at the cost of higher delay. How- ever, tuning configurations across multiple knobs quickly hits a prohibitive combinatorial space (more in §3) and re- quires optimizations to reduce the search cost. What’s more, the RAG configuration should be tuned jointly with scheduling. Consider two configurations: 𝐴feeds all retrieved text chunks in one LLM input, and 𝐵summarizes first each chunk with an LLM and then feeds the summaries 1 arXiv:2412.10543v2 [cs.LG] 16 Jul 2025 to an LLM input for a final generation. While 𝐴(which calls the LLM once) is seemingly faster than 𝐵(which calls the LLM multiple times), 𝐴could be slower as it requires more GPU memory than 𝐵and thus could be delayed in the sched- uler queue. Without making batching and configuration se- lection jointly, it would be difficult to avoid such pitfalls. Finally, the impact of RAG configurations on quality-delay tradeoffs also varies significantly with queries. For example, to answer “In which country is the Kimbrough Memorial Sta- dium located?”, the RAG may retrieve and analyze one text chunk about the stadium. In contrast, to answer “Compare NVIDIA’s operating cost over the first three quarters of 2024 and identify the highest one”, the RAG may need multiple chunks, each containing the quarter’s operating cost, and process these chunks jointly, instead of reading them separately. The above examples illustrate queries differ in complexity (more in §4), leading to needing different configurations per-query for optimal quality-delay tradeoffs. Empirically, we show that picking RAG configuration per-query achieves 12 −15% higher quality and 2.5 −3× lower delay than using any fixed configuration across all queries in a dataset (§5). Thus, RAG configurations should be adapted on a per-query basis. Yet, existing RAG systems, which hand-pick a static config- uration offline based on a few example queries [1, 21, 39, 85], lose out on quality or response time. This paper presents METIS, the first RAG system that adapts multiple configuration knobs on a per-query basis and jointly makes configuration selections and scheduling decisions (i.e., which LLM inference in a batch) to optimize the delay-quality tradeoffs for RAG. As this would require solving a joint combinatorial prob- lem for every query, which can be prohibitively expensive (§3), METIS tackles the challenge with a two-step approach. First, METIS prunes the massive configuration space for each received query to a smaller yet promising one that con- tains configurations that likely yield high-quality output for the given query. Specifically, METIS uses a separate LLM to estimate the query’s profile, including how many pieces of information are required to answer the query and whether joint reasoning is likely required across these pieces of in- formation (more in §4.1). The intuition of the query profiles is that they can effectively filter out undesirable RAG config- urations. For the earlier query example “Compare NVIDIA’s operating cost over the first three quarters of 2024 and identify the highest one,” the estimated profile would suggest that it involves at least three separate pieces of information, so the number of chunks (one of the configuration knobs)", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "0a67a5bbdd0544d7e6604402f5d0f8df2c7cfcf1ca854e48afbfcffc7ad3a80c"}
{"doc_id": "arxiv:2412.10543#introduction:part-3", "url": "https://arxiv.org/abs/2412.10543", "anchor": "#introduction:part-3", "type": "paper", "title": "", "section": "Introduction", "text": "query and whether joint reasoning is likely required across these pieces of in- formation (more in §4.1). The intuition of the query profiles is that they can effectively filter out undesirable RAG config- urations. For the earlier query example “Compare NVIDIA’s operating cost over the first three quarters of 2024 and identify the highest one,” the estimated profile would suggest that it involves at least three separate pieces of information, so the number of chunks (one of the configuration knobs) should be at least three. It should be noted that the LLM-based profiler is an extra overhead in METIS, but fortunately, its input only contains the RAG query itself and the metadata of the RAG database, which are orders of magnitude shorter than the long contexts in RAG, so the estimation can be relatively fast, about 1/10 of the delay of the execution of the RAG query. METIS (Ours) AdaptiveRAG (ACL 2024) Parrot (OSDI 2024) vLLM : SOTA LLM Engine Better METIS (Ours) AdaptiveRAG (ACL 2024) Parrot (OSDI 2024) vLLM : SOTA LLM Engine Better Figure 1. Performance of METIS on the KG RAG FinSec [50] dataset compared to the baselines. Full results shown in §7. Using the narrowed configuration space, METIS reduces the RAG response delays by jointly deciding the per-query configuration and query scheduling based on available re- sources (§4.3). The insight is that within the pruned configu- ration space, the scheduler can make optimal configuration decisions without exploring the original, large configuration space and the implications on quality. In short, METIS’s two-level design loosely decouples the problem into (1) pruning configuration space to a smaller yet promising range of configurations, which focuses solely on keeping the accuracy high, and (2) jointly optimizing configuration (within the narrowed range) and scheduling to optimize response delay by choosing configurations which best-fit into the GPU memory. We evaluate METIS across four RAG datasets with diverse query profiles (e.g., reasoning vs. domain-specific QA). Fig- ure 1 shows a preview of our results. Our key takeaways are as follows. When achieving the same or higher quality than the baselines, METIS reduces the response delay by 1.6−2.8× compared to the latest vLLM (a state-of-the-art serving en- gine), Parrot (the latest LLM query-scheduling method), as well as AdaptiveRAG (the latest RAG configuration-tuning method). METIS also achieves 1.8 −4.5× higher through- put compared to these baselines when achieving the same response delay and same/higher quality. The general concept of using LLMs to guide system tuning is not exactly new [60, 88], but our key contribution lies in applying the concept to RAG systems, through joint sched- uling with resource-aware configuration selection, leading to significantly better resource sharing (§4.2, §4.3). 2 RAG systems and configurations As an LLM often does not have domain-specific or up-to-date knowledge, LLM applications commonly employ RAG to sup- plement LLM inference with external knowledge to generate high-quality responses. Despite the growth of model context length, using RAG to pinpoint the relevant context is still significantly cheaper in terms of resource cost (GPU require- ment), latency, and memory consumption (KV Cache size). For 2 general-purpose", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "47a76b7d8d9debb103d1a73314c681eb320a0d3d23b207a942306207ed5d05f3"}
{"doc_id": "arxiv:2412.10543#introduction:part-4", "url": "https://arxiv.org/abs/2412.10543", "anchor": "#introduction:part-4", "type": "paper", "title": "", "section": "Introduction", "text": "resource-aware configuration selection, leading to significantly better resource sharing (§4.2, §4.3). 2 RAG systems and configurations As an LLM often does not have domain-specific or up-to-date knowledge, LLM applications commonly employ RAG to sup- plement LLM inference with external knowledge to generate high-quality responses. Despite the growth of model context length, using RAG to pinpoint the relevant context is still significantly cheaper in terms of resource cost (GPU require- ment), latency, and memory consumption (KV Cache size). For 2 general-purpose QA pipelines, RAG is cost-efficient with re- trieving targeted chunks based on semantic similarity to the query. Using LLMs with long-context documents in contrast has much higher GPU memory usage and delay [43, 45, 71]. Before processing queries, a RAG system organizes back- ground documents by splitting them into chunks (each with a fixed number of tokens), embedding each chunk using models like Bert [12, 19], and storing the embeddings with the chunks in a vector database. Processing a RAG query involves two main steps: • Retrieval: The RAG system retrieves one or more rele- vant context chunks from the database by comparing the query’s embedding, (using the same embedding model as for database indexing), with the stored embeddings. • Synthesis: After retrieving the relevant chunks, the RAG system combines these chunks and the RAG query to form a single/multiple LLM call(s) to generate the response. Retrieval is computationally lightweight and much faster than synthesis (> 100×), so the response delay is typically dominated by the synthesis step [90]. RAG configuration: This work focuses on optimizing three configuration knobs, illustrated in Figure 2, which are de- rived from key design questions that affect RAG performance in terms of response delay and quality: • How many chunks to retrieve (num_chunks): The number of context chunks directly affects the delay of the synthesis step, with more computation needed to process the longer sequences with more chunks. In the meantime, retrieving too few chunks risks low response quality if the retrieved chunks do not contain enough useful information. • How to synthesize (synthesis_method): If the LLM should read the chunks separately, RAG uses the LLM to generate one answer for the query using each chunk separately and picks the output with the highest confidence, which is called map_rerank. This often incurs the least compu- tation but can cause low quality if the useful information is scattered in different chunks, in which case the LLM should read the chunks jointly. The RAG system can feed these chunks in the LLM input directly by concatenating them within a single prompt (called stuff) or to create a shorter summary for each chunk first before feeding the summaries and the query into the LLM to generate the final response (called map_reduce). stuff needs less computa- tion than map_reduce, but risks degraded output quality for long inputs due to the lost-in-the-middle problem [47]. • How long is each summary (intermediate_length): Fi- nally, if the LLM produces the summary for each chunk based on the user query, the length of each summary greatly affects the quality and response of map_reduce— shorter", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "3b568ece8ef08daa463a1d90b44e98742e87a8d6b61869bc8b9e9d5ef7a30a9d"}
{"doc_id": "arxiv:2412.10543#introduction:part-5", "url": "https://arxiv.org/abs/2412.10543", "anchor": "#introduction:part-5", "type": "paper", "title": "", "section": "Introduction", "text": "summary for each chunk first before feeding the summaries and the query into the LLM to generate the final response (called map_reduce). stuff needs less computa- tion than map_reduce, but risks degraded output quality for long inputs due to the lost-in-the-middle problem [47]. • How long is each summary (intermediate_length): Fi- nally, if the LLM produces the summary for each chunk based on the user query, the length of each summary greatly affects the quality and response of map_reduce— shorter summaries yield lower delay but also risk not feed- ing enough information to the final LLM inference. How many chunks to retrieve? If jointly, should the LLM summarize each chunk first? If so, how long should each summary be? If multiple chunks, should the LLM read them jointly? Knob 1: num_chunks Knob 2: synthesis_method Knob 3: intermediate_length Key design choices of RAG Figure 2. The configuration knobs adapted by METIS are derived from key design choices of RAG systems. Chunk 1 Chunk 2 Chunk 3 LLM Final Answer Chunk 1 Chunk 2 Chunk 3 Final Answer 1 Confidence : 80% Final Answer 2 Confidence : 99% Final Answer 3 Confidence : 90% Chunk 1 Chunk 2 Chunk 3 S1 S2 S3 Final Answer (a) Stuff (b) Map Rerank (c) Map Reduce LLM LLM LLM Figure 3. Illustration of different RAG synthesis methods, which have various LLM reasoning capabilities. In this work, while we focus on universal RAG knobs which affect quality and delay common to all RAG systems, METIS can be extended to other tunable knobs (e.g., some RAG system may dynamically choose the embedding model, retrieval index or serving LLM). METIS’ design is extensible to any RAG configuration knob based on the query profile. Performance metrics: We evaluate the performance of a RAG system using two metrics: • Response quality calculates the F1 score of the generated response against the ground truth. The F1 score is the harmonic mean of precision (# correctly generated words) and recall (# of correct words successfully generated). This metric is widely used in prior works [10, 69, 72]. • Response delay measures the time elapsed from when the RAG system receives a RAG request to when it completes generating the response. Next, we show that these knobs need to be properly tuned on a per-query basis to achieve optimal tradeoff between quality and delay in §3. 3 Towards better quality-delay tradeoffs Prior work on RAG either optimizes for lower delay or higher quality, i.e., the first picks static configurations and focuses on reducing the delay by smart scheduling and resource allo- cation [44, 70, 76] and the second picks RAG configurations to maximize quality without regard to resource usage or delay [32, 77, 83]. For the first time, we explore the potential of optimizing the quality-delay tradeoffs for RAG. 3 (a) Change: synthesis method from map_rerank (circle) , stuff (plus) and map_reduce (square) (b) Change: number of chunks from 1 to 35 with stuff (c) Change: intermediate length from 1 to 100 with map_reduce Q1 Q1 Q1 Q2 Q2 Q2 Q3 Q3 Q3 Figure 4.", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "aa2a62e6a2676c1bc20827d57ee1fe6cbb65e434a71ff2e9c1182f9a553a14f2"}
{"doc_id": "arxiv:2412.10543#introduction:part-6", "url": "https://arxiv.org/abs/2412.10543", "anchor": "#introduction:part-6", "type": "paper", "title": "", "section": "Introduction", "text": "and the second picks RAG configurations to maximize quality without regard to resource usage or delay [32, 77, 83]. For the first time, we explore the potential of optimizing the quality-delay tradeoffs for RAG. 3 (a) Change: synthesis method from map_rerank (circle) , stuff (plus) and map_reduce (square) (b) Change: number of chunks from 1 to 35 with stuff (c) Change: intermediate length from 1 to 100 with map_reduce Q1 Q1 Q1 Q2 Q2 Q2 Q3 Q3 Q3 Figure 4. Varying each RAG configuration knob leads to different quality-latency tradeoffs, and these tradeoffs differ across queries (Q1 in green, Q2 in blue, and Q3 in red). To improve the delay-quality tradeoff, our insight is that quality and delay should jointly be optimized in this large tradeoff space created by the choice of RAG configuration knobs. Importantly, the configurations with better quality- delay tradeoffs vary significantly across queries. To showcase this observation, we use three queries from Musique [78], a popular reasoning QA dataset (§7.1). • Q1: “In what county was William W. Blair’s born?” • Q2: “Are Alison Skipper, Diane Gilliam Fisher, and Rachel McAdams from the same country?” • Q3: “When and why did the Voyager 1, the spacecraft that detected storms on Neptune, leave our solar system?” We chose queries with different natural language complexity and reasoning, Q1 being relatively less complex than Q2 and Q3. Then, we adjust the value of each configuration knob in order to quantify each knob’s impact on the quality- delay tradeoffs in each of the queries. Impact of synthesis method: Figure 4 (a) changes the syn- thesis method and shows its effect on the quality-delay trade- off, while keeping the other RAG configuration knobs con- stant. We vary the synthesis method as map_rerank, stuff, and map_reduce from left to right. The insight is that the optimal synthesis method that strikes the best quality-delay tradeoff (closest to the top left corner) differs significantly across the different queries. For simple queries like Q1 (green), quality plateaus for more complex synthesis methods (stuff and map_reduce). Because it only needs a single piece of context, map_rerank which processes chunks in isolation suffices, whereas cross- chunk reasoning (stuff and map_reduce) adds undue delay (2×) without improving quality. For queries such as Q2 (blue) that require cross-chunk rea- soning, stuff and map_reduce provide significant quality improvements (35% increase) by processing chunks jointly. For more complex queries, such as Q3 (red), which require even more reasoning and information (why Voyager 1 left has multiple reasons), methods like map_reduce improve quality (30% increase) by removing unnecessary text in the mapper phase, to help the LLM focus on the relevant content. Impact of the number of retrieved chunks: Figure 4 (b) fixes the synthesis method (stuff) and shows the impact of the number of retrieved chunks (1-35) on quality and delay. Simple queries, like Q1 (green), can often be answered using just one or two chunks (needs only birth county). For more complex queries, Q2 (blue) and Q3 (red), increasing the number of chunks (1-15) improves the likelihood of retriev- ing all relevant", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "60112ce6bcfaf4e7f034686d17df1802d9f1667ef6a3a09f8d321c375c38fbed"}
{"doc_id": "arxiv:2412.10543#introduction:part-7", "url": "https://arxiv.org/abs/2412.10543", "anchor": "#introduction:part-7", "type": "paper", "title": "", "section": "Introduction", "text": "help the LLM focus on the relevant content. Impact of the number of retrieved chunks: Figure 4 (b) fixes the synthesis method (stuff) and shows the impact of the number of retrieved chunks (1-35) on quality and delay. Simple queries, like Q1 (green), can often be answered using just one or two chunks (needs only birth county). For more complex queries, Q2 (blue) and Q3 (red), increasing the number of chunks (1-15) improves the likelihood of retriev- ing all relevant context and improves quality. Blindly retrieving more chunks than necessary risks di- luting the relevance of actual important information, due to commonly known problems such as “lost-in-the-middle” [28, 47]. In all three queries, retrieving more chunks beyond a point harms the quality (up to 20% drop) and unnecessar- ily inflates delay (up to 3×). Hence we have a quality-delay tradeoff where increasing chunks up to a point helps quality but beyond that it increases delay while degrading quality. Impact of the intermediate output length: Figure 4 (c) shows the impact of our third configuration knob, vary- ing the intermediate output length (1-100) for map_reduce synthesis methods on the quality-delay tradeoff. For simple queries like Q1 (green), short amounts of intermediate length are enough to answer the query (10-20 words). For more com- plex queries Q2 (blue) and Q3 (red), increasing the amount of intermediate length (70-100 words) provided helps the model with enough information to answer the query. Overall, we see that RAG queries naturally vary in com- plexity, requiring differing levels of inter-chunk reasoning and varying numbers of context chunks. More complex queries, which require more reasoning and context, ben- efit from increased LLM computation, which can come at the cost of increased delay. Adding more context chunks helps to a point beyond which it harms the output quality and delay. Thus, adapting RAG configuration on a per-query basis is crucial. Figures 2, 3, 4 illustrate tuning most popular RAG configuration knobs, however the tuning extends to more RAG configurations with richer tradeoff spaces (§4.2). 4 Pareto Boundary of fixed configuration with vLLM Pareto Boundary of fixed configuration with vLLM Per-Query Configuration Per-Query Configuration Figure 5. Per-query configuration can achieve significantly better quality-delay tradeoffs across queries compared to every fixed configuration choice. Figure 5 uses queries from two datasets (Musique and QM- SUM, see §7.1) and shows that picking the best configuration for each query (the best configuration is the one with the lowest delay that achieves less than 2% drop than the highest achievable quality) achieves superior quality-delay tradeoff than picking any static configuration for all queries. Choos- ing the configuration per-query allows up to 3× delay saving compared to static configurations which are the closest in quality. Every single static configuration choice that achieves comparable delay has at least a 10% quality drop. In spite of the potential benefits, per-query configuration adaptation faces challenges that hinder their real-world adop- tion. Each RAG query comes in plain text with practically no associated RAG configurations. Moreover, the space of configurations grows exponentially with multiple knobs. For example, for a map_reduce configuration,", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "943a60526132d3179fa53f81ee80181bbc3d2be0426a4ab4f1177a0a25cb94b3"}
{"doc_id": "arxiv:2412.10543#introduction:part-8", "url": "https://arxiv.org/abs/2412.10543", "anchor": "#introduction:part-8", "type": "paper", "title": "", "section": "Introduction", "text": "configuration per-query allows up to 3× delay saving compared to static configurations which are the closest in quality. Every single static configuration choice that achieves comparable delay has at least a 10% quality drop. In spite of the potential benefits, per-query configuration adaptation faces challenges that hinder their real-world adop- tion. Each RAG query comes in plain text with practically no associated RAG configurations. Moreover, the space of configurations grows exponentially with multiple knobs. For example, for a map_reduce configuration, with 30 values for num_chunks and 50 values for intermediate_length leads to 1500 configurations for a query. Exhaustively profiling all configurations per-query and choosing the best is infeasible. Alternatively, if we profile periodically, we lose out on the potential configuration selection for each query, as variance in query profile leads to different quality-delay tradeoffs. Pro- filing cost is also prohibitively expensive as the LLM needs to be run with many synthesis methods, number of chunks etc., which require high GPU usage. Additionally, the delay of profiling can be ∼100× the inference delay due to multiple LLM calls during profiling. Online RAG queries have strin- gent requirements for GPU resource usage and end-to-end delay [70, 76]. This makes it hard to systematically decide what an optimal per-input configuration should be. To truly achieve the benefit of per-query configuration adaptation, we need a smart system to drastically reduce to a useful configuration space, in a fast and cheap manner. 4 METIS: Enabling per-query configuration adaptation for RAG We present METIS, a novel system for serving RAG queries focusing on high generation quality and minimal delay. METIS is a RAG controller (Figure 6) with two main components: Configuration Space Pruning (§ 4.1, 4.2 ) Joint scheduler (§ 4.3) RAG Queries Vector Database GPU Memory Serving LLM RAG Configs Text Chunks Check Resource Status Generated Output Retriever RAG Synthesis Chosen Config Figure 6. METIS consists of a RAG controller which per- forms configuration space pruning and joint scheduling. • Pruning configuration space: We estimate each query’s pro- file (§4.1) and reduce the RAG configuration space to a smaller yet promising one that still yields high generation quality (§4.2) (leading to a 50-100× reduction). • RAG scheduler: Within the pruned configuration space for the query, METIS’ scheduler chooses the best config- uration for the query to achieve the best quality-latency trade-off based on the available system resources (§4.3). Once the configuration is chosen, the METIS’ executes the query using the chosen configuration—retrieving the selected number of chunks and uses the selected synthesis method to feed into the LLM’s input. 4.1 Estimating a query’s profile Query profile: To choose the correct RAG configurations, the first step of METIS is to create the profile of the query (as we see in Figure 7) by querying an LLM (we call this LLM query profiler). We ask the query profiler to estimate four high-level dimensions for each query. • Query complexity refers to the intricacy of the query itself. Queries with less complexity are more like simple yes/no questions, while queries with high complexity are more like why questions, which require", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "04052c26574ccf1c6894d4cd522e43fbaa41c43502576fb7ac3a3e6cdeb53d0c"}
{"doc_id": "arxiv:2412.10543#introduction:part-9", "url": "https://arxiv.org/abs/2412.10543", "anchor": "#introduction:part-9", "type": "paper", "title": "", "section": "Introduction", "text": "choose the correct RAG configurations, the first step of METIS is to create the profile of the query (as we see in Figure 7) by querying an LLM (we call this LLM query profiler). We ask the query profiler to estimate four high-level dimensions for each query. • Query complexity refers to the intricacy of the query itself. Queries with less complexity are more like simple yes/no questions, while queries with high complexity are more like why questions, which require deeper reasoning than yes/no questions. As a result, it requires more LLM com- putation to correctly answer complex queries. The output for this dimension is binary “High/Low” • Joint reasoning requirement describes whether multiple pieces of information are needed to answer the query. Even relatively simple queries may require joint reasoning (e.g., checking whether the annual income from two years is the same). The output for this dimension is binary “Yes/No” • Pieces of information required refers to the distinct, stan- dalone pieces of information required to fully answer the query (e.g., the annual income from how many years is required to draw the trend of annual income). The output for this dimension is a number from 1-10. 5 Query Profiler ( LLM ) § 4.1 Estimate the query complexity How many pieces of information? How much can we summarize? Input Prompt Content Query complexity: High/ Low Needs X pieces of information Summary length: X to Y Rule-based Mapping § 4.2 Query Synthesis", "source": "arxiv_pdf", "published": "", "tokens": 243, "sha256": "ced0960f22e0f95a13cb8c636578d5a4be831abe66773826495af351833b97cf"}
{"doc_id": "arxiv:2412.10543#method:part-1", "url": "https://arxiv.org/abs/2412.10543", "anchor": "#method:part-1", "type": "paper", "title": "", "section": "Method", "text": "Number of chunks (value) Intermediat e Length (value) Do we need joint reasoning? Joint reasoning: Yes/No Figure 7. METIS RAG configuration selection workflow. • The length of the summarization: If the query is complex and needs a lot of different information, it is often neces- sary to first summarize the relevant information chunks first (to reduce the noise inside these chunks) and then gen- erate the final answer from these summaries. The output for this dimension is a number from 30-200. METIS is not the first to use query profile as a metric for deciding RAG configurations, it extends upon methods like AdaptiveRAG [32] which have used LLM’s to estimate query profile but they only focus on one dimension (the number of chunks to retrieve). In Section 7, we show the impact of each dimension on the overall improvement. Why the query profile could be estimated: Estimating the aforementioned query profile is feasible, not only be- cause of the reasoning power of LLMs3 in analyzing natural language queries, but also because we provide sufficient in- formation to the LLM-based profiler. METIS feeds the profile estimator with not only the query, but also a metadata of the database that contains the background document. The metadata is a short description about the type of con- tent in the database and its data size (chunk_size). Specif- ically, we use a single-line summaries already attached to the original source datasets as the metadata of the dataset. For example, the metadata for the KG RAG Finsec’s database [50] contains quarterly financial reports and questions of Fortune 500 companies with a chunk_size of 1000. It de- scribes the content topics of the chunks with information such as revenue growth indicators, product release informa- tion, sales etc.,. When presented with a query on financials of such a company, the LLM can use the metadata to decide questions like how much to summarize/how much reasoning is required. We give details on the prompt and the intuition to generate metadata for new datasets in Appendix §A. It is important to acknowledge that for highly under- specified queries, it is hard for any model (even human) to reasonably estimate the query’s profile. For an example 3We have tested both GPT and Llama models as the profile query-profiler, and they yield similarly impressive results (§7). query “Compare current US Stock Market trends,” the query profile here does not provide enough information (e.g., how many years should the trend be derived from). To answer such highly under-specified queries, more information about the dataset will unlikely help.4 Moreover, we observed that extra information does not significantly improve the profiler’s estimates. For instance, in theory, it helps to know the embedding algorithm used by RAG. Yet, the embedding models perform similarly overall across queries and datasets under our consideration. This ex- plains their limited contribution to the profiler, though more future work is needed to understand the wider implications. 4.2 Mapping query profiles to RAG configurations After METIS obtains the query profile using the LLM, it per- forms rule-based mapping to generate values for RAG con-", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "43cbd0071ab281e42dc44526049de45b52c2f8e97c3b2d15842d061cdddffb4d"}
{"doc_id": "arxiv:2412.10543#method:part-2", "url": "https://arxiv.org/abs/2412.10543", "anchor": "#method:part-2", "type": "paper", "title": "", "section": "Method", "text": "significantly improve the profiler’s estimates. For instance, in theory, it helps to know the embedding algorithm used by RAG. Yet, the embedding models perform similarly overall across queries and datasets under our consideration. This ex- plains their limited contribution to the profiler, though more future work is needed to understand the wider implications. 4.2 Mapping query profiles to RAG configurations After METIS obtains the query profile using the LLM, it per- forms rule-based mapping to generate values for RAG con- figuration knobs (e.g., synthesis_method etc. introduced in §2). based on the query profiler’s outputs. How we map and why the profile helps: To understand the role of query profiles, consider the following examples: • “Who is the current CEO of NVIDIA?” This query is not complex and does not require joint reasoning. Due to the query being simple with no reasoning required and one piece of information (name of CEO). • “Which month had the highest NVIDIA’s stock price the six months from January to June 2024?” This query is simple but still needs to read information jointly, specifically six pieces of information (stock price for every month) • “What are the reasons for NVIDIA’s month-on-month stock price change from January to June 2024” This query is complex and needs to read multiple pieces of information jointly (stock prices, reasons for change etc.) As multiple reasons need to be analyzed here, summarizing all of the in- formation first helps narrow down to relevant information and perform clearer reasoning (why the prices changed). 4Maybe some chat history from the same user will help, but that is beyond the scope of this work. 6 Algorithm 1: Rule based mapping algorithm Input: Query complexity, Joint reasoning required Input: Pieces of information , Summarization length range Result: synthesis_method, num_chunks, intermediate_length 1 if Joint reasoning required == “no” then 2 synthesis_method = map_rerank 3 else 4 if Query complexity == “low” then 5 synthesis_method = stuff 6 else 7 synthesis_method = stuff, map_reduce 8 num_chunks = [Pieces of information , 3× Pieces of information] 9 intermediate_length_range = Summarization length range Algorithm 1 outlines the rule-based mapping process. This mapping is significantly helpful, it improves upon raw pro- filer outputs and converts them to usable RAG configurations. It reduces the cost of the profiler LLM by restricting it to provide short binary decisions only. We decide the range of synthesis_method selections based on two of the profile dimensions estimated in §4.1, i.e., the “Query complexity” and the “Joint reasoning require- ment”. Simple queries that don’t need any reasoning can an- swered with map_rerank while queries that require joint rea- soning need stuff or map_reduce. We then decide the range of values for num_chunks based on the profile dimension of the “Pieces of information required”, i.e., 𝑛—specifically, we set the range of num_chunks to be 1−3 times of 𝑛. We do not directly set num_chunks at𝑛, because it (1) gives some leeway for the retrieval logic (e.g., typically Bert-embedding-based)5 to find necessary information, and (2) provides the room for the scheduler to select the configuration that fits in available", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "f265b10e1abcb4c9a7843379ed75bc814f64bf8bb91b83c17d0688ba17448d1d"}
{"doc_id": "arxiv:2412.10543#method:part-3", "url": "https://arxiv.org/abs/2412.10543", "anchor": "#method:part-3", "type": "paper", "title": "", "section": "Method", "text": "rea- soning need stuff or map_reduce. We then decide the range of values for num_chunks based on the profile dimension of the “Pieces of information required”, i.e., 𝑛—specifically, we set the range of num_chunks to be 1−3 times of 𝑛. We do not directly set num_chunks at𝑛, because it (1) gives some leeway for the retrieval logic (e.g., typically Bert-embedding-based)5 to find necessary information, and (2) provides the room for the scheduler to select the configuration that fits in available memory. Finally, we get the intermediate_length range from the “summary length” estimate, which is already a value range (derived from the query, metadata and chunk size). Algorithm 1 is central to METIS’ design to reduce to the space to our useful RAG configurations and this is extendable to other RAG configurations. For instance, a particular RAG pipeline might use an external re-ranker [23, 52], query re- writer [36, 51] or perform an external web-search [73] along with database retrieval. The mapping algorithm can map the profiling LLM’s output (e.g., of Query complexity) and be used to guide such decisions for these newer RAG configurations. Additionally, such mapping algorithms greatly reduce the overall inference cost of RAG inference. Attempting to use 5A typical RAG retriever these days will have to retrieve 2-3× more chunks than minimally required to provide sufficient information for the LLM inference [24, 55]. Used GPU Mem (6GB) Used GPU mem (6GB) time time Map 1 (6GB) Map 2 (6GB) Reduce (6GB) Chunk 1, Query Chunk 2, Query Chunk 1, Chunk 2, Query Stuff (12GB) (a) Baseline Separates configuration selection and scheduling In general, \"Stuff\" is faster than \"MapReduce\" as a RAG config Yet, \"Stuff\" is memory-intensive and thus is slower when available GPU RAM is limited Free mem (6GB) (b) Ours performs configuration selection and scheduling jointly Delay saved We select MapReduce as it can readily fits in the current batch Figure 8. METIS joint schedules RAG configurations with available GPU memory (chosen example - map_reduce) the LLM profiler to directly provide the exact RAG configu- ration values does not work. For this, the LLM needs to be regularly retrained for this task to adapt to new configura- tions and will require significantly greater system resources (e.g., GPUs blocked for this). In contrast, METIS uses the LLM to only analyze natural language properties and provide bi- nary decisions, which the mapping algorithm translates to useful configurations with a significantly lower cost. It is important to note that the concept of METIS belongs to an active research trend in the ML and systems community that leverages LLM outputs and mapping functions to guide real system decisions and optimizations, an example of which is LLM routing [13, 31, 56, 59]. While current LLM routers use trained LLMs to map decisions from query complexity to only choose from families of inference models (outside the realm of RAG), we differ by mapping the output to the configuration knob we run for the RAG queries. Like these prior efforts, METIS is a heuristic to best utilize the LLM-generated information to guide system optimiza- tions.", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "c9ffdbe285143ef221b3746bf25e04ff45fb406a69526c91cfc8cdcedcec9d9d"}
{"doc_id": "arxiv:2412.10543#method:part-4", "url": "https://arxiv.org/abs/2412.10543", "anchor": "#method:part-4", "type": "paper", "title": "", "section": "Method", "text": "functions to guide real system decisions and optimizations, an example of which is LLM routing [13, 31, 56, 59]. While current LLM routers use trained LLMs to map decisions from query complexity to only choose from families of inference models (outside the realm of RAG), we differ by mapping the output to the configuration knob we run for the RAG queries. Like these prior efforts, METIS is a heuristic to best utilize the LLM-generated information to guide system optimiza- tions. While it demonstrates remarkable improvement in practice, more work will be needed to complement it for better interpretability and robustness. 4.3 Joint configuration-scheduling adaptation Once provided with the narrowed range of each RAG con- figuration knob (synthesis_method, num_chunks and intermediate_length), we need to choose a RAG configu- ration, which is aware of the current system resource (GPU memory). If we pick configurations which do not fit in cur- rent memory, it will lead to additional queuing delay waiting for the GPU memory to free up. We have METIS’s pruned configuration space where the quality is high, we now focus on choosing the best configu- ration which fits in memory, without focusing on quality. 7 Why we need to choose the scheduling jointly: We motivate the need for joint scheduling along with the RAG configuration choice in Figure 8. Consider a setup where we tune only one RAG configura- tion knob of synthesis_method. Other knobs num_chunks and intermediate_length are fixed at 20 and 100 respec- tively. Let’s assume both stuff and map_reduce are present in the pruned space. For the scheduling knob, we consider the amount of GPU memory available for the current batch. Consider a baseline system which separates the joint de- cision from the scheduling and picks only the RAG con- figuration knob (synthesis_method). It chooses the stuff configuration knob as it has lower compute requirement, so given enough memory it should be fast. The baseline system in Figure 8 (a) does not consider other jobs in the system and does not evaluate the amount of available resource to make its scheduling decision. Due to its long input length with 20 chunks, stuff turns out to be memory-intensive. If the available GPU memory is low, stuff doesn’t fit in memory and needs to be queued. This ends up with stuff being slow. Jointly considering the available GPU memory with choos- ing the RAG configuration knob avoids this pitfall. For exam- ple, in Figure 8 (b), if the original configuration was stuff, METIS can choose to use map_reduce (based on the current GPU memory available). By doing so, METIS can start putting the mappers which fit in memory, into the current running_batch of requests which fits in the GPU. While map_reduce requires more compute, in this case, it benefits from being able to start execution much faster, as some of the mappers fit in memory. METIS does not need to wait for the GPU memory to free up and changes the configuration aware of system resource, to save delay and achieve a better quality-delay tradeoff. Jointly choosing the configuration knobs: METIS first", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "b418ac55abf8a09f57ef90b9486cc62ef341adac4f029e987777f934d8ea9148"}
{"doc_id": "arxiv:2412.10543#method:part-5", "url": "https://arxiv.org/abs/2412.10543", "anchor": "#method:part-5", "type": "paper", "title": "", "section": "Method", "text": "the mappers which fit in memory, into the current running_batch of requests which fits in the GPU. While map_reduce requires more compute, in this case, it benefits from being able to start execution much faster, as some of the mappers fit in memory. METIS does not need to wait for the GPU memory to free up and changes the configuration aware of system resource, to save delay and achieve a better quality-delay tradeoff. Jointly choosing the configuration knobs: METIS first provides us with a pruned range of configurations. A straw- man solution is to pick a constant value from the across queries. (e.g., the median value of the num_chunks). While this is better than using one static configuration for all queries, it is still sub-optimal as it does not look at the current system resource availability. This prevents us from exploiting the best quality-delay tradeoff across RAG queries. We use a best-fit algorithm to allow for variation in config- urations across queries. We first compute the GPU memory requirement for the RAG query from the RAG configura- tion knobs (e.g., num_chunks) for every configuration in the pruned space. Then, we measure the current available mem- ory on the GPU to see what can fit into the current batch. We then pick the best configuration from the pruned space that fits into the GPU. METIS defines the best configuration as the one with overall highest memory requirement, from all which fit in memory. The insight here is that within the reduced range of good quality configurations, higher mem- ory configurations correspond to expensive configurations (e.g. more number of chunks, higher intermediate length). In general, these configurations should lead to slightly higher quality in the reduced space. For example, if the pruned space says num_chunks is 5-10 and the synthesis_method is stuff and both 5 or 6 chunks can fit in memory, we choose 6 chunks. We don’t pick a configuration that doesn’t fit in GPU, so we would never choose more than 6 chunks. If we do that, the system will queue the request inflating the delay. After choosing the configuration that fits into the current running_batch, the vLLM engine is optimized to perform chunked_prefill. However, even with chunked_prefill, it can only offload parts of long prefill of stuff requests which do not fit in the current batch and still inflates the queuing de- lay. Jointly scheduling RAG configurations enables efficient resource usage, which cannot be obtained by only relying on the output of the LLM profiler. What if none of the configurations fit in the GPU? A main insight for METIS’s design comes from the observation that in general, the RAG-specific focused configurations can be loosely-decoupled from the scheduling-specific configura- tions. METIS tries to fit the best possible configurations into GPU memory after it gets the profiler’s reduced configura- tion space. It can sometimes happen that the current GPU memory availability is too low and none of the profiler’s configurations fit in the currently available GPU. One way we handle this is by falling back to a cheaper fixed configuration and choosing", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "0467499c95c579e49bf99a02f274eeb8ef1eae536adf30d35ef5ee4cdab06f26"}
{"doc_id": "arxiv:2412.10543#method:part-6", "url": "https://arxiv.org/abs/2412.10543", "anchor": "#method:part-6", "type": "paper", "title": "", "section": "Method", "text": "comes from the observation that in general, the RAG-specific focused configurations can be loosely-decoupled from the scheduling-specific configura- tions. METIS tries to fit the best possible configurations into GPU memory after it gets the profiler’s reduced configura- tion space. It can sometimes happen that the current GPU memory availability is too low and none of the profiler’s configurations fit in the currently available GPU. One way we handle this is by falling back to a cheaper fixed configuration and choosing to ignore the output of the pruning. As we already have access to the query complexity profile and we can pick cheaper configurations, which would meet the requirement for the current query. For instance, if the query doesn’t require joint reason- ing, we can pick a map_rerank configuration with as many chunks that fit into the current GPU memory, ignoring the remaining pruned spaces. If joint reasoning is required, we pick a stuff or map_reduce configurations with the few chunks that fit into memory. We can choose which synthesis method to use once based on the exact memory availability. This allows loose-decoupling of the RAG configurations into a smaller space and then choosing configurations based on system resource availability. This also allows SLO-based constraints on RAG queries if certain queries have strict budgets on their generation latency. 5 Refinements to METIS In spite of it all, it is possible for the profiler to (sometimes) fail and in such cases, it is important to detect if METIS’s profiler fails on a query in a fast manner to prevent it from leading to bad RAG configurations. Also it is useful to decide how to provide feedback to METIS to improve. When is the quality profile reliable? METIS uses LLM to generate the quality profile. Inspired by recent work in use 8 Above threshold - 98% good profiles Above threshold - 96% good profiles 7% below threshold - 90% bad profiles 7% below threshold - 85% bad profiles 90% Threshold Figure 9. Confidence score threshold for different profiler outputs is used to decide when not to use the profiler output. of model confidence [20, 25, 84] as a quality metric, we use confidence scores for METIS’s LLM profiler as to measure the reliability of the profile provided. We obtain the confidence scores from the LLM’s log-probs values on the output (the logarithm of the confidence score, which is directly provided with the output with no extra overhead). We then threshold the confidence score using a confidence score threshold (90% across different datasets) to predict whether the quality profile derived from the quality profiler LLM is actually good (defined as whether the profile can lead to 10% increase in F1-score or 1.5 −2× reduction in delay or both) or not. Such 90% threshold can be tuned for better performance, and we leave it to future work. From Figure 9, we draw two conclusions. First, over 93% of the quality profiles derived from LLM are of high confidence (i.e., over 90%). Further, for those high-confidence profile, over 96% of them are good profiles, meaning that they can", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "a4c4c75ae3c41f03a22c0d711c7eae462cd3dc58039ef3019f9c174db36140e9"}
{"doc_id": "arxiv:2412.10543#method:part-7", "url": "https://arxiv.org/abs/2412.10543", "anchor": "#method:part-7", "type": "paper", "title": "", "section": "Method", "text": "actually good (defined as whether the profile can lead to 10% increase in F1-score or 1.5 −2× reduction in delay or both) or not. Such 90% threshold can be tuned for better performance, and we leave it to future work. From Figure 9, we draw two conclusions. First, over 93% of the quality profiles derived from LLM are of high confidence (i.e., over 90%). Further, for those high-confidence profile, over 96% of them are good profiles, meaning that they can be used to improve quality, or reduce latency, or both. To handle those cases where the quality profile is of con- fidence score lower than 90% , METIS will fall back to the pruned configuration space of recent 10 queries. How to improve the profiler over time? METIS improves the query profiler LLM by profiling extra feedback prompt to this LLM. We generate this feedback prompt by generating the most accurate output, which is obtained by performing inference on the most resource-demanding configuration (the map_reduce configuration with a large number of input chunks (30) and a high value of intermediate length (300 tokens)) and then ask the quality profiler LLM what config- uration it should choose based on the query and the most accurate answer to that query. The key insight is that, the most accurate answer to the query provides the quality profiler LLM extra knowledge and thus can be used to further improve its decision. To control the cost of generating feedback prompts, METIS only generates the feedback prompt once every 30 queries and we only keep the last four feedback prompts. The cost of METIS’ LLM quality profiler: For the profiler LLM, we use a larger LLM as compared to the serving LLM Dataset Task Type Input Output Squad Single hop QA 0.4K - 2K 5-10 Musique Multihop QA 1K - 5K 5-20 KG RAG FinSec Doc Level QA 4K - 10K 20-40 QMSUM Summarization QA 4K - 12K 20-60 Table 1. Input and output length (# of tokens) distributions of the RAG datasets used in our evaluation. (7B parameters). Using this has minimal cost, as METIS only runs it on the query itself and in METIS as the query is at least 100× shorter than the context. Using this approach, METIS still saves cost as opposed to using a large LLM for inference (as shown in Section 7). We also show that METIS can use different closed and open-source LLMs as the profiler LLM for pruning and can still provide impressive delay reduction without hurting the accuracy in Section 7. 6 Implementation We implement METIS in about 2K lines of code in Python on top of the state-of-the-art popular LLM serving engine vLLM [41]. For the profiler used for configuration space pruning, we define a class LLMProfiler inheriting OpenAI’s Chat Completion API [61] interface (to invoke GPT-4o) and HuggingaceAPI [81] (to invoke LLama-3.1-70B) as models to profile the queries. We use Cohere-embed-v3.0 [4] as a state-of-the-art em- bedding method. We construct a FAISS [16] index using the IndexFlatL2 interface and perform L2-distance similarity search with index.search(query_embedding, top_k) on", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "54d68378c9c3a0744ece7a9ce8eefe998f03513d2303f3b8431c99e59ecc5090"}
{"doc_id": "arxiv:2412.10543#method:part-8", "url": "https://arxiv.org/abs/2412.10543", "anchor": "#method:part-8", "type": "paper", "title": "", "section": "Method", "text": "lines of code in Python on top of the state-of-the-art popular LLM serving engine vLLM [41]. For the profiler used for configuration space pruning, we define a class LLMProfiler inheriting OpenAI’s Chat Completion API [61] interface (to invoke GPT-4o) and HuggingaceAPI [81] (to invoke LLama-3.1-70B) as models to profile the queries. We use Cohere-embed-v3.0 [4] as a state-of-the-art em- bedding method. We construct a FAISS [16] index using the IndexFlatL2 interface and perform L2-distance similarity search with index.search(query_embedding, top_k) on the chunk embeddings to retrieve for RAG inference. We use the LLMChain interface from Langchain [8] in order to build efficient implementations of multiple synthesis methods. Finally, we use PyTorch’s [5] library modules support to perform query-level memory profiling and measurement to implement the best-fit scheduling logic and request batching. Particularly, we use pynvml to construct get_free_memory() with its interfaces of nvmlDeviceGetHandleByIndex and nvmlDeviceGetMemoryInfo to measure the amount of GPU memory available. We measure the current num-seqs and num-batched-tokens within vLLM to calculate which con- figuration can be fit into the current batch, based on the GPU availability and the request’s memory requirement. 7", "source": "arxiv_pdf", "published": "", "tokens": 184, "sha256": "4e33ce4c2d31ea7919623137436843b03789184715abb20c523c90f2af1be49b"}
{"doc_id": "arxiv:2412.10543#evaluation:part-1", "url": "https://arxiv.org/abs/2412.10543", "anchor": "#evaluation:part-1", "type": "paper", "title": "", "section": "Evaluation", "text": "The key takeaways from the evaluation are • Lower delay : Across 4 task representative datasets for RAG QA, METIS achieves 1.64 −2.54× lower response delay compared to fixed configurations of comparable quality. • Higher throughput : METIS achieves 1.8 −4.5× higher throughput than RAG serving systems which use fixed configurations reaching similar quality. 9 • Negligible overhead : METIS’ profiler’s delay is negligible compared to the overall delay of the LLM’s RAG inference. 7.1 Setup Models and hardware: : We evaluate METIS on a popular model for LLM inference, specifically the fine-tuned version of Mistral-7B-v3. We also use Llama3.1-70B for additional experiments. All models are fine-tuned such that they can take long contexts (up to 32K and 128K respectively). We apply AWQ-model quantization both models. We use an NVIDIA A40 GPU server with 2 GPUs to benchmark our results. The server is equipped with 384GB of memory and two Intel(R) Xeon(R) Gold 6130 CPUs with Hyper-threading and Turbo Boost enabled by default. We use 1 GPU to serve Mistral-7B-v3 and 2 GPUs to serve Llama3.1-70B. Datasets: We use multiple RAG QA datasets with various query profiles, in order to have task-representative work- loads. Table 1 summarizes their input-output statistics. • Squad [66]: Squad is a single-hop reading comprehension dataset, consisting of questions on articles, where the an- swer to every question is a segment from the correspond- ing reading passage. • Musique [78]: Musique is a multihop QA dataset with reasoning-based questions. It is designated to test LLM’s reasoning ability where one reasoning step critically relies on information from another. • KG RAG FinSec [50]: KG RAG Finsec is part of a Knowledge Graph family of RAG datasets and focuses on financial do- main questions from Fortune 500 companies. This dataset contains quarterly financial reports and queries need to read information for multiple chunks for answering. • QMSUM [93]: QMSUM is a human-annotated query-based multi-domain meeting summarization benchmark designed to test LLM’s reasoning-based summarization capabilities. This dataset contains multiple meeting transcripts and queries to summarize relevant spans of meetings. We build a retrieval database database by splitting the queries’ contexts into fixed-sized chunks using Langchain [8] for the database, with Cohere embed-v3.0 [4] embeddings and FAISS [16] L2-distance similarity search in order to re- trieve relevant chunks for RAG inference. To simulate a real RAG workload, we create a mix of queries from each dataset, and send them to METIS using arrival rates that follow a Poisson distribution. We report the results per dataset. Quality Metric: We adopt the following standard metric to measure the generation quality. • F1-score is used to evaluate the METIS’s serving model’s generated response (defined in §2) It is the most widely adopted metric for evaluating RAG QA tasks [10, 69, 72] System Metrics: We adopt the following system metrics: • Delay is used to measure the generation response delay of the model for every RAG query. We choose this system metric similar to other RAG serving papers [44, 70, 76] • Dollar Cost is used to measure the lower cost of using METIS’s profiler as", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "13da1df748fe2324a50c0df6fca43f3bcb54efc5f5b589682246a768cacdf843"}
{"doc_id": "arxiv:2412.10543#evaluation:part-2", "url": "https://arxiv.org/abs/2412.10543", "anchor": "#evaluation:part-2", "type": "paper", "title": "", "section": "Evaluation", "text": "to evaluate the METIS’s serving model’s generated response (defined in §2) It is the most widely adopted metric for evaluating RAG QA tasks [10, 69, 72] System Metrics: We adopt the following system metrics: • Delay is used to measure the generation response delay of the model for every RAG query. We choose this system metric similar to other RAG serving papers [44, 70, 76] • Dollar Cost is used to measure the lower cost of using METIS’s profiler as compared to using larger serving mod- els with fixed configurations having the closest accuracy. Baselines: We compare METIS with the following baselines. • vLLM: We serve RAG with vLLM with multiple static con- figurations across different queries. • Parrot*: We implement Parrot’s [44] configuration-based batching. Parrot* does not adapt the configuration per query. We compare with Parrot* using fixed RAG configu- rations which achieve the closest quality to us. • AdaptiveRAG*: We implement AdaptiveRAG’s [32], query complexity-based RAG-configuration selection and choose the configuration which maximizes the F1-score, without considering the system resource cost. 7.2 Overall improvement Lower delay without sacrificing generation quality: Fig- ure 10 shows METIS achieves delay reduction 1.64 −2.54× over AdaptiveRAG* with no reduction in F1-score. Over us- ing fixed configurations of similar delay, served with both Parrot* and vLLM, METIS achieves 12 −18% higher F1-score. Higher throughput at lower delay: Figure 11 shows METIS achieves higher throughput compared to fixed config- uration baselines when they choose the fixed-config which achieves the closest quality. Compared to Parrot* and vLLM, METIS achieves 1.8 −4.5× times higher throughput. Understanding METIS’ improvement: METIS’s gains come from jointly selecting the configuration based on the available resource, along with performing scheduling. METIS achieves higher quality than the fixed-config baselines as it is adapts the RAG-configuration per query. It reduces delay by resource-aware scheduling, making it better than fixed configurations which achieve closest quality. METIS achieves higher throughput due to being able to adapt configurations based on resource availability as com- pared to the baselines. Both Parrot* and vLLM schedule fixed RAG-configurations and cannot benefit from delay achieved by adapting the configuration like METIS. Parrot* can im- prove the delay over using fixed configurations with vLLM by 1.4 −1.8× but cannot improve the quality. 7.3 Analyzing the gains from METIS Delay saving: Figure 12 shows the contribution of every component of METIS. We compare with vLLM’s fixed config- uration, which achieves the highest quality (blue bar). Using the profiler’s outputs and choosing the median value every time (orange bar), we achieve 1.4 −1.68× reduction in delay. Next, we see the effect of batching (like Parrot*), by choosing the median value configuration and batching, we achieve 1.1 −1.2× reduction in delay. Finally, METIS achieves even greater delay reduction by 1.45 −1.75× by adapting the con- figuration based on available GPU memory with batching. 10 2.41X faster 16% higher 2.24X faster 12% higher 1.64X faster 15% higher 2.54X faster 18% higher Figure 10. METIS achieves 1.64−2.54× lower delay compared to both best fixed configuration baselines and quality-optimized RAG configuration without sacrificing generation quality. 0 2 4 6", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "a3341a95dece57c34b5e9fe8212acbbcbaab9c8292d87c88aad6389428db4b44"}
{"doc_id": "arxiv:2412.10543#evaluation:part-3", "url": "https://arxiv.org/abs/2412.10543", "anchor": "#evaluation:part-3", "type": "paper", "title": "", "section": "Evaluation", "text": "by choosing the median value configuration and batching, we achieve 1.1 −1.2× reduction in delay. Finally, METIS achieves even greater delay reduction by 1.45 −1.75× by adapting the con- figuration based on available GPU memory with batching. 10 2.41X faster 16% higher 2.24X faster 12% higher 1.64X faster 15% higher 2.54X faster 18% higher Figure 10. METIS achieves 1.64−2.54× lower delay compared to both best fixed configuration baselines and quality-optimized RAG configuration without sacrificing generation quality. 0 2 4 6 8 2 4 6 Average Delay (s) Dataset : KG RAG FinSec 0 2 4 6 8 2 4 6 Dataset: Musique 0 2 4 6 8 1 2 3 Dataset: Squad 0 2 4 6 8 5 10 Datset: QMSUM Average Queries per Second METIS (w/ adapted RAG config and batching) Parrot * (w/ fixed RAG config) vLLM (w/ fixed RAG config) Figure 11. METIS achieves 1.8 −4.5× higher throughput (at 1.8 seconds) than baselines which use fixed configurations of closest (not higher) quality. 1.68x 1.2x 1.75x 1.4x 1.1x 1.45x Figure 12. Understanding the delay improvement in METIS Better Better Figure 13. Even with increasing the inference model size, fixed configurations have 2.38 −6.8× higher cost and lower quality compared to METIS. Cost saving: Figure 13 shows METIS (including its pro- filer) has significant lower dollar cost and higher F1-score, compared to choosing the best fixed configuration, with increasing model complexity. The cost of using a (LLama3- 70B) inference model with vLLM and a fixed configuration 6% increase 4% increase Figure 14. Improvement for METIS using feedback from the output helps improve the F1-score by 4 −6%. 0 2 4 6 0.3 0.4 0.5 F1 Score Dataset: Musique 0 3 6 9 0.3 0.4 0.5 Dataset: QMSUM Average Delay (s) METIS Parrot* AdaptiveRAG* vLLM Figure 15. METIS achieves lower delay by 2.1 −2.4× at the same quality even with a larger inference LLM. is higher by 2.38× times while also having a lower F1-score of 6.5% times across datasets. Even more powerful inference models like GPT-4o fail to achieve the same F1-score with fixed configurations but have a much higher cost of 6.8×. Profiler feedback-based improvement: In Figure 14 we show the effect of the golden-configuration-based feedback to the profiler in order to improve its output. We use a 350 11 vLLM (fixed config) vLLM (change num_chunks) vLLM (change num_chunks + synthesis_method) vLLM (change num_chunks + synthesis_method + intermediate_length) METIS (change num_chunks + synthesis_method + intermediate_length + scheduling) Figure 16. Breakdown analysis: By tuning more knobs in METIS, we can see better quality-delay tradeoffs. 0 1 2 3 0.4 0.6 F1 Score Dataset: KG RAG FinSec 0 0.3 0.6 0.9 0.42 0.50 0.58 Dataset: Squad Average Delay (s) METIS Parrot* AdaptiveRAG* vLLM Figure 17. METIS’ performance gains remain substantial even with a smaller, open-source LLM profiler. Dataset: QMSUM Dataset: KG RAG FinSec Dataset: Musique Dataset: Squad Figure 18. METIS’ profiler delay is at most 1/10th of end-to- end response delay across queries from all datasets. query sample for the QMSUM and KG RAG FinSec dataset as the workload. We see that", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "ca33962a19de3919fb9d9cffa1f61eafc15cfa62ce7dd0cd7b083a1226578b80"}
{"doc_id": "arxiv:2412.10543#evaluation:part-4", "url": "https://arxiv.org/abs/2412.10543", "anchor": "#evaluation:part-4", "type": "paper", "title": "", "section": "Evaluation", "text": "Score Dataset: KG RAG FinSec 0 0.3 0.6 0.9 0.42 0.50 0.58 Dataset: Squad Average Delay (s) METIS Parrot* AdaptiveRAG* vLLM Figure 17. METIS’ performance gains remain substantial even with a smaller, open-source LLM profiler. Dataset: QMSUM Dataset: KG RAG FinSec Dataset: Musique Dataset: Squad Figure 18. METIS’ profiler delay is at most 1/10th of end-to- end response delay across queries from all datasets. query sample for the QMSUM and KG RAG FinSec dataset as the workload. We see that with the feedback mechanism (blue line), the F1-score improves by 4 −6% as compared to not having feedback (red line) from the outputs of the golden configuration. We ensure that the feedback mechanism can- not result in the output of very expensive configurations, as METIS’ joint scheduler will not pick increasingly expensive configurations based on the GPU resource constraint. 7.4 Sensitivity analysis Changing the inference LLM: Figure 15 shows the out- come of changing the inference LLM to a larger LLM (Llama3.1- 70B) on the Musique and QMSUM datasets. Even with a more powerful LLM, METIS achieves 2.1 −2.4× lower delay than AdaptiveRAG* at a similar F1-score. The best fixed- configuration baselines such as Parrot* and vLLM have a lower F1-score of 7 −10%. In RAG, models mainly rely on the external context to answer the question instead of the model weights and we only get a 2% improvement in F1-score compared to the smaller inference models. Incrementally tuning knobs in METIS: In Figure 16, we show the benefit we the improvement we get by incremen- tally adding more knobs to METIS. We measure this for the QMSUM dataset with the original Mistral-7B-v3 model. We first only tune the num_chunks (red point). Progressively we tune the RAG-configuration knobs of synthesis_method and intermediate_length and scheduling. We achieve 5, 4, 3% higher F1-Score compared to vLLM. Finally, by adding the scheduling, 2.8× lower delay reduction in delay. Changing the profiler LLM: Figure 17 shows the effect of changing the LLM profiler from GPT-4o to a smaller Llama3.1-70B model. METIS with the new profiler, still achieves 1.4 −2.1× over AdaptiveRAG* with a similar F1-score. Static configurations of Parrot* and vLLM which achieve similar delay, METIS achieves 10 −14% higher F1-score. Changing the embedding algorithm: METIS picks a state- of-art retrieval algorithm Cohere-embed-v3.0 [4]. Using two other popular retrieval algorithms All-mpnet-base-v2 [67] and text-embedding-3-large-256 [18], the F1-score change is within 1%. The delay has no measurable difference as the retrieval is > 100× faster than LLM synthesis [6]. Delay overhead of METIS’s per-query profiling: We show the negligible delay overhead of using an LLM profiler within METIS. Figure 18 shows the fraction of METIS’ pro- filer of the total end-to-end delay. Using the profiler at most adds 0.1 fraction and in the average case only adds 0.03−0.06 fraction to the total delay across queries from all datasets. 8", "source": "arxiv_pdf", "published": "", "tokens": 474, "sha256": "a939e2012d34d87ac309466557f752ee1be1e304b90b7147d78fb64cc0f6addc"}
{"doc_id": "arxiv:2412.10543#related-work", "url": "https://arxiv.org/abs/2412.10543", "anchor": "#related-work", "type": "paper", "title": "", "section": "Related work", "text": "Systems for serving RAG: Several systems have been proposed for RAG [2, 17, 32, 34, 37, 40, 44, 54, 76, 87, 90] which focus on improving retrieval using complex, iterative retrieval algorithms or on serving model selection. METIS can work in conjunction with such systems as METIS focuses on optimizing quality and serving latency, independent of how the retrieval algorithm identifies chunks for retrieval. KV cache storage and retrieval: Storing and reusing KV cache across different requests have been commonly studied in recent work [2, 14, 22, 29, 33, 41, 46, 48, 49, 63, 75, 86, 92]. METIS can work alongside these systems, where instead of retrieving chunks, it can retrieve the KV Caches for generat- ing the output. In RAG, some additional optimizations are needed to combine KV Caches of different chunks that don’t share a common prefix. This is important as the trivial con- catenation of KV Caches loses important cross-attention and reasoning between chunks. These optimizations are enabled by KV Cache blending-based approaches [9, 26, 30, 38, 80, 85]. However RAG workloads have a large number of related contexts across queries and storing all the KV Cache is ex- tremely expensive. We do not measure the KV Cache reuse ratio across queries and leave it for future work. 12 Prefill-Decode Optimizations: Several systems have pro- posed optimizations to speed-up prefill and decode for LLMs by leveraging unique properties of each phase [3, 11, 35, 65, 74, 82, 94, 95]. Notable techniques include chunked-prefill which allows interleaving prefill and decode requests and dis- aggregated prefill which separates compute nodes for prefill and decode. All of these optimizations enable faster genera- tion speed but don’t focus on generation quality. METIS can be applied with such LLM serving systems optimizations. 9 Limitations METIS is currently designed to work with commonly de- ployed RAG pipelines. New research directions in RAG [17, 89] have developed further complex pipelines with more agents and stages for deep chain-of-thought RAG workloads. These pipelines improve on complex workloads but achieve similar performance on all the commonly used RAG QA workloads we consider [1]. We leave METIS’ design exten- sion to chain-of-thought pipelines to future work. 10", "source": "arxiv_pdf", "published": "", "tokens": 360, "sha256": "c380f8272437143b26e7b663f2125f95b79c14c412b1af0073a83a58bddab78a"}
{"doc_id": "arxiv:2412.10543#conclusion", "url": "https://arxiv.org/abs/2412.10543", "anchor": "#conclusion", "type": "paper", "title": "", "section": "Conclusion", "text": "This paper introduces METIS, the first system that focuses on optimizing the tradeoffs between response delay and generation quality in RAG, by by jointly scheduling RAG queries and adapting key configurations on a per-query basis. Evaluation on four datasets shows that METIS outperforms the state-of-the-art, reducing generation latency by 1.64 − 2.54× without compromising response quality.", "source": "arxiv_pdf", "published": "", "tokens": 56, "sha256": "bdc2d24ec1b0123b44e3245a244cf152936dee29fdb2ccddfe2687d6adb2b70f"}
{"doc_id": "blog:news.microsoft.com#body:part-1", "url": "https://news.microsoft.com/source/features/ai/ai-agents-what-they-are-and-how-theyll-change-the-way-we-work/", "anchor": "#body:part-1", "type": "blog", "title": "", "section": "Body", "text": "--- title: AI agents — what they are, and how they'll change the way we work - Source author: Wp-Block-Co-Authors-Plus-Coauthors Is-Layout-Flow url: https://news.microsoft.com/source/features/ai/ai-agents-what-they-are-and-how-theyll-change-the-way-we-work/ hostname: microsoft.com description: AI agents take the power of generative AI a step further by working alongside you or even on your behalf, and they can be built and used by anyone. sitename: Source date: 2024-11-21 --- AI agents — what they are, and how they’ll change the way we work It’s Monday morning, the caffeine hasn’t kicked in yet, and you have a busy day ahead: Maybe you have piles of returns or new shipping invoices to review, or you need to get the latest updates out to your field technicians or help employees get more efficient IT support. Now you can get help with all of this and more by simply asking an AI agent to take care of it — while you drink a second cup of coffee and focus on your team’s long-term strategy. An agent can tackle certain tasks with you or for you, from acting as a virtual project manager to handling more complex assignments like reconciling financial statements to close the books. Microsoft 365 Copilot is already a personal assistant that helps with everything from tedious daily duties to jumpstarting creative projects. Using it to interact with various agents brings a new world of possibilities for organizations to empower their employees, drive business and accomplish even more. Agents can operate around the clock to review and approve customer returns or go over shipping invoices to help businesses avoid costly supply-chain errors. They can reason over reams of product information to give field technicians step-by-step instructions or use context and memory to open and close tickets for an IT help desk. “Think of agents as the new apps for an AI-powered world,” says Jared Spataro, Microsoft’s chief marketing officer for AI at Work. “We’re rapidly adding new capabilities to tackle individuals’ biggest pain points at work and drive real business results.” What are agents, anyway? An agent takes the power of generative AI a step further, because instead of just assisting you, agents can work alongside you or even on your behalf. Agents can do a range of things, from responding to questions to more complicated or multistep assignments. What sets them apart from a personal assistant is that they can be tailored to have a particular expertise. For example, you could create an agent to know everything about your company’s product catalog so it can draft detailed responses to customer questions or automatically compile product details for an upcoming presentation. Other agents can do even more, acting on your behalf, like one that helps fulfill sales orders — freeing you up to focus on building new customer relationships. Having agents handle some of these routine needs can boost productivity across industries, from manufacturing and research to finance and retail, helping businesses save time and money. You can use ready-made agents in Microsoft 365 and Dynamics 365, or build custom agents to help with more specific needs in Copilot Studio. Imagine you’re a", "source": "blog:news.microsoft.com", "published": "", "authors": "", "tokens": 512, "sha256": "30cab09af14e484587feb10a255be4320cd8752e60dcfc4816dc64ecdbcae086"}
{"doc_id": "blog:news.microsoft.com#body:part-2", "url": "https://news.microsoft.com/source/features/ai/ai-agents-what-they-are-and-how-theyll-change-the-way-we-work/", "anchor": "#body:part-2", "type": "blog", "title": "", "section": "Body", "text": "can do even more, acting on your behalf, like one that helps fulfill sales orders — freeing you up to focus on building new customer relationships. Having agents handle some of these routine needs can boost productivity across industries, from manufacturing and research to finance and retail, helping businesses save time and money. You can use ready-made agents in Microsoft 365 and Dynamics 365, or build custom agents to help with more specific needs in Copilot Studio. Imagine you’re a salesperson with big quarterly goals to meet. Copilot acts as your personal assistant, drafting emails, recapping a meeting you missed and helping you design a polished sales presentation. Meanwhile, an agent specialized in sales lead generation works autonomously in the background to find new prospects you can follow up with later in the week. Copilot partners on daily tasks, and your purpose-built agent uses its customized skills to help you meet your end-of-quarter goals. Agents are not new. Microsoft has done extensive research in the area and even created a multi-agent library last year for developers around the world, work that helped shape what agents can do today. They’re getting more attention now because recent advances in large language models (LLMs) help anyone — even outside the developer community — communicate with AI. That agent-LLM duo makes AI tools more tangibly useful. “People expect AI to do things for them,” not to just generate language, says Ece Kamar, the managing director of Microsoft’s AI Frontiers Lab. “If you want to have a system that can really solve real world problems and help people, that system has to have a good understanding of the world we live in, and when something happens, that system has to perceive that change and take action accordingly.” Agents are like layers on top of the language models that observe and collect information, provide input to the model and together generate an action plan and communicate that to the user — or even act on their own, if permitted. So both agents and models are equally important pieces of the puzzle, as far as generative AI tools go. Agents will become more useful and able to have more autonomy with innovations in their three necessary elements: memory, entitlements and tools. Memory helps provide continuity so that each time you ask for something, it isn’t like starting from scratch. “To be autonomous you have to carry context through a bunch of actions, but the models are very disconnected and don’t have continuity the way we do, so every prompt is in a vacuum and it might pull the wrong memory out,” says Sam Schillace, Microsoft’s deputy chief technology officer. “It’s like you’re watching a stop-motion animation, one isolated frame after another, and your mind puts it into motion. The clay model doesn’t move on its own.” To build up the memory infrastructure to address this, Schillace and his team are working on a process of chunking and chaining. That’s essentially what it sounds like: They’re experimenting with dividing up interactions in bits that can be stored and linked together", "source": "blog:news.microsoft.com", "published": "", "authors": "", "tokens": 512, "sha256": "55af94d66f2d25283176fd0d8107ba56bb6bba4854c50349f48393badd196e78"}
{"doc_id": "blog:news.microsoft.com#body:part-3", "url": "https://news.microsoft.com/source/features/ai/ai-agents-what-they-are-and-how-theyll-change-the-way-we-work/", "anchor": "#body:part-3", "type": "blog", "title": "", "section": "Body", "text": "memory out,” says Sam Schillace, Microsoft’s deputy chief technology officer. “It’s like you’re watching a stop-motion animation, one isolated frame after another, and your mind puts it into motion. The clay model doesn’t move on its own.” To build up the memory infrastructure to address this, Schillace and his team are working on a process of chunking and chaining. That’s essentially what it sounds like: They’re experimenting with dividing up interactions in bits that can be stored and linked together by relevance for faster access, akin to a memory — like grouping conversations about a certain project so an agent can recall those details when you ask for a status update and not have to search through its entire database. The work with entitlements and tools is making sure agents have secure access to, or are entitled to, information they need in order to accomplish things for you, with your permission — like who your boss is, for example — and to the computer programs they need to take action on your behalf, like Teams and PowerPoint. How to use and build agents for work You can already create and publish agents in Microsoft 365 Copilot that can help you in your daily work as easily as you’d create a spreadsheet or presentation — no coding skills required. You don’t need to be a developer to build agents using Copilot Studio, either. Anyone can connect them to relevant business data such as emails, reports and customer management systems so they can perform tasks and provide insights. And you’ll soon be able to enlist new agents in Microsoft 365 to help with common workflows and tasks. Interpreter in Teams will provide real-time speech-to-speech translation during meetings, for example, and you can opt to have it simulate your voice. The Employee Self-Service Agent will simplify human resource and IT help desk-related tasks like helping workers resolve a laptop issue or find out if they’ve maxed out certain benefits, and it can connect to company systems for further customization in Copilot Studio. Microsoft Dynamics 365 will have agents as well for a range of common business workflows across sales, supply chain, finance and customer service functions. And every SharePoint site will soon come equipped with an agent tailored to your organization’s content that allows employees to quickly tap into these vast knowledge bases and find exactly what they need in seconds, whether it’s project details buried in a workback schedule or a summary of a recent product memo. Developers have even more options. With the new Azure AI Agent Service, you’ll be able to choose from small or large language models to orchestrate, develop and scale agent-powered apps to streamline and automate complex workflows like order processing and customer data synchronization. It provides a software development kit with tools for developing agents, allowing you to efficiently integrate agent capabilities using Visual Studio Code and GitHub. One type of model, OpenAI’s recently announced o1 series, can bring more advanced reasoning capabilities to agents, allowing them to take on more complicated tasks by breaking them down into", "source": "blog:news.microsoft.com", "published": "", "authors": "", "tokens": 512, "sha256": "47cf1e24f3279396150288276637e6812a40f10d772da2d02ae0470bc033254f"}
{"doc_id": "blog:news.microsoft.com#body:part-4", "url": "https://news.microsoft.com/source/features/ai/ai-agents-what-they-are-and-how-theyll-change-the-way-we-work/", "anchor": "#body:part-4", "type": "blog", "title": "", "section": "Body", "text": "from small or large language models to orchestrate, develop and scale agent-powered apps to streamline and automate complex workflows like order processing and customer data synchronization. It provides a software development kit with tools for developing agents, allowing you to efficiently integrate agent capabilities using Visual Studio Code and GitHub. One type of model, OpenAI’s recently announced o1 series, can bring more advanced reasoning capabilities to agents, allowing them to take on more complicated tasks by breaking them down into steps — like getting the information someone on an IT help desk would need to solve a problem, factoring in solutions they’ve tried and coming up with a plan. You can also use the power of agents in LinkedIn; the platform’s first agent can help recruiters with hiring. Assessing risk for autonomous action There are extra safety considerations with agents that can act autonomously, and Microsoft is focused on making sure agents only access what you want them to, says Sarah Bird, the company’s chief product officer of Responsible AI. “Agents certainly up the stakes from a responsible AI point of view,” Bird says. “So we have to have much, much lower error rates. And there’s many more nuanced ways in which something could be an error. This is the big challenge with agents.” But the same responsible AI foundational playbook for other AI applications can be used to assess and mitigate risk with agents, she says. The new Copilot Control System helps IT departments manage Copilot and agents with data access and governance, management and security controls, as well as measurement reports and tools to track adoption and business value. Many agents, like those created for Microsoft 365 and Dynamics 365, include “human in the loop” approvals, where people are required to take the final step of reviewing and sending an email the Sales Order Agent wrote, for example. And for agents developed in Copilot Studio, authors can review the records to see which actions the agent took and why. The key is to focus on testing and moderating to ensure accuracy, Bird says, and for organizations to choose the right starting point for their needs. “We will of course make progress by building on the foundation we already have, so we’re starting the journey from a strong place,” Bird says. Looking back — and into the future Technologists have long been excited by the idea of autonomous systems working side-by-side with people to help them, says Kamar, who has been working on AI agents since 2005 and even wrote her Ph.D. thesis on the topic in 2010. The hurdle was that “we lacked that general problem-solving power” on the back end, she says. With LLMs, “we finally have this missing component,” she says. “Now we can bring back a lot of the ideas from our decades of research.” Going forward, Kamar envisions a new ecosystem or marketplace of agents, sort of like how apps empower people to do more with their smartphones. Agents already have “the basic building blocks of what it takes to complete a task,” she says. “Like", "source": "blog:news.microsoft.com", "published": "", "authors": "", "tokens": 512, "sha256": "edd8c4a310eb39ea330003ad833ea2cdb9b4e5ba1fd775a6c0a4c3d1d751e61f"}
{"doc_id": "blog:news.microsoft.com#body:part-5", "url": "https://news.microsoft.com/source/features/ai/ai-agents-what-they-are-and-how-theyll-change-the-way-we-work/", "anchor": "#body:part-5", "type": "blog", "title": "", "section": "Body", "text": "that “we lacked that general problem-solving power” on the back end, she says. With LLMs, “we finally have this missing component,” she says. “Now we can bring back a lot of the ideas from our decades of research.” Going forward, Kamar envisions a new ecosystem or marketplace of agents, sort of like how apps empower people to do more with their smartphones. Agents already have “the basic building blocks of what it takes to complete a task,” she says. “Like observing, ‘I can see your meeting is taking longer; I should delay the next meeting.’” They’re getting more helpful as they gain autonomy through the innovations in memory and entitlements. They’re relieving pain points for employees by helping with things like expense reporting, project management and meeting facilitation. And they’re driving exponential impact for businesses by taking on duties like alerting supply chain managers to low inventory and then automatically reordering to help drive sales and keep customers satisfied. Agents matter because they “open up a whole set of opportunities for working with people for getting tasks done, and that’s what we expect from AI systems,” Kamar says. “AI agents are not only a way to get more value for people but are going to be a paradigm shift in terms of how work gets done.” And this is just the beginning. Copilot is set to evolve with new capabilities like Copilot Actions, designed to handle routine tasks that can bog down employees like summarizing emails missed during time off, compiling agenda items and generating monthly reports. More capabilities like these are coming over the next year to lift the weight of work for employees and teams. “Copilot will empower every employee to do their best work in less time, and focus on more meaningful tasks,” Spataro says. “And agents created in Copilot Studio will transform every business process, helping companies streamline operations, enhance collaboration and drive innovation at scale.” Illustrations by Michał Bednarski / Makeshift Studios Story published on November 19, 2024", "source": "blog:news.microsoft.com", "published": "", "authors": "", "tokens": 333, "sha256": "2007988b18e359b4183d1751542115ddfba19712caae0a2355729eea906ad7ec"}
{"doc_id": "blog:www.databricks.com#body:part-1", "url": "https://www.databricks.com/blog/llm-inference-performance-engineering-best-practices", "anchor": "#body:part-1", "type": "blog", "title": "", "section": "Body", "text": "--- title: LLM Inference Performance Engineering: Best Practices url: https://www.databricks.com/blog/llm-inference-performance-engineering-best-practices hostname: databricks.com description: Learn best practices for optimizing LLM inference performance on Databricks, enhancing the efficiency of your machine learning models. sitename: Databricks date: 2023-12-10 --- In this blog post, the MosaicML engineering team shares best practices for how to capitalize on popular open source large language models (LLMs) for production usage. We also provide guidelines for deploying inference services built around these models to help users in their selection of models and deployment hardware. We have worked with multiple PyTorch-based backends in production; these guidelines are drawn from our experience with FasterTransformers, vLLM, NVIDIA's soon-to-be-released TensorRT-LLM, and others. Understanding LLM Text Generation Large Language Models (LLMs) generate text in a two-step process: \"prefill\", where the tokens in the input prompt are processed in parallel, and \"decoding\", where text is generated one 'token' at a time in an autoregressive manner. Each generated token is appended to the input and fed back into the model to generate the next token. Generation stops when the LLM outputs a special stop token or when a user-defined condition is met (e.g., some maximum number of tokens has been generated). If you'd like more background on how LLMs use decoder blocks, check out this blog post. Tokens can be words or sub-words; the exact rules for splitting text into tokens vary from model to model. For instance, you can compare how Llama models tokenize text to how OpenAI models tokenize text. Although LLM inference providers often talk about performance in token-based metrics (e.g., tokens/second), these numbers are not always comparable across model types given these variations. For a concrete example, the team at Anyscale found that Llama 2 tokenization is 19% longer than ChatGPT tokenization (but still has a much lower overall cost). And researchers at HuggingFace also found that Llama 2 required ~20% more tokens to train over the same amount of text as GPT-4. Important Metrics for LLM Serving So, how exactly should we think about inference speed? Our team uses four key metrics for LLM serving: - Time To First Token (TTFT): How quickly users start seeing the model's output after entering their query. Low waiting times for a response are essential in real-time interactions, but less important in offline workloads. This metric is driven by the time required to process the prompt and then generate the first output token. - Time Per Output Token (TPOT): Time to generate an output token for each user that is querying our system. This metric corresponds with how each user will perceive the \"speed\" of the model. For example, a TPOT of 100 milliseconds/tok would be 10 tokens per second per user, or ~450 words per minute, which is faster than a typical person can read. - Latency: The overall time it takes for the model to generate the full response for a user. Overall response latency can be calculated using the previous two metrics: latency = (TTFT) + (TPOT) * (the number of tokens to be generated). - Throughput: The number of output tokens per second", "source": "blog:www.databricks.com", "published": "", "authors": "", "tokens": 512, "sha256": "4d36c51acb2f7b3440e4c10381c4004c8d72f9c91f21400d6ada1f6f409d44c2"}
{"doc_id": "blog:www.databricks.com#body:part-2", "url": "https://www.databricks.com/blog/llm-inference-performance-engineering-best-practices", "anchor": "#body:part-2", "type": "blog", "title": "", "section": "Body", "text": "For example, a TPOT of 100 milliseconds/tok would be 10 tokens per second per user, or ~450 words per minute, which is faster than a typical person can read. - Latency: The overall time it takes for the model to generate the full response for a user. Overall response latency can be calculated using the previous two metrics: latency = (TTFT) + (TPOT) * (the number of tokens to be generated). - Throughput: The number of output tokens per second an inference server can generate across all users and requests. Our goal? The fastest time to first token, the highest throughput, and the quickest time per output token. In other words, we want our models to generate text as fast as possible for as many users as we can support. Notably, there is a tradeoff between throughput and time per output token: if we process 16 user queries concurrently, we'll have higher throughput compared to running the queries sequentially, but we'll take longer to generate output tokens for each user. If you have overall inference latency targets, here are some useful heuristics for evaluating models: - Output length dominates overall response latency: For average latency, you can usually just take your expected/max output token length and multiply it by an overall average time per output token for the model. - Input length is not significant for performance but important for hardware requirements: The addition of 512 input tokens increases latency less than the production of 8 additional output tokens in the MPT models. However, the need to support long inputs can make models harder to serve. For example, we recommend using the A100-80GB (or newer) to serve MPT-7B with its maximum context length of 2048 tokens. - Overall latency scales sub-linearly with model size: On the same hardware, larger models are slower, but the speed ratio won't necessarily match the parameter count ratio. MPT-30B latency is ~2.5x that of MPT-7B latency. Llama2-70B latency is ~2x that of Llama2-13B latency. We are often asked by prospective customers to provide an average inference latency. We recommend that before you anchor yourself to specific latency targets (\"we need less than 20 ms per token\"), you should spend some time characterizing your expected input and desired output lengths. Challenges in LLM Inference Optimizing LLM inference benefits from general techniques such as: - Operator Fusion: Combining different adjacent operators together often results in better latency. - Quantization: Activations and weights are compressed to use a smaller number of bits. - Compression: Sparsity or Distillation. - Parallelization: Tensor parallelism across multiple devices or pipeline parallelism for larger models. Beyond these methods, there are many important Transformer-specific optimizations. A prime example of this is KV (key-value) caching. The Attention mechanism in decoder-only Transformer-based models is computationally inefficient. Each token attends to all previously seen tokens, and thus recomputes many of the same values as each new token is generated. For example, while generating the Nth token, the (N-1)th token attends to (N-2)th, (N-3)th … 1st tokens. Similarly, while generating (N+1)th token, attention for the Nth token again needs to", "source": "blog:www.databricks.com", "published": "", "authors": "", "tokens": 512, "sha256": "29629928a931ffb3e82519c274466351c37cfe4aeaf0e0ace8b65a978657b0d3"}
{"doc_id": "blog:www.databricks.com#body:part-3", "url": "https://www.databricks.com/blog/llm-inference-performance-engineering-best-practices", "anchor": "#body:part-3", "type": "blog", "title": "", "section": "Body", "text": "Beyond these methods, there are many important Transformer-specific optimizations. A prime example of this is KV (key-value) caching. The Attention mechanism in decoder-only Transformer-based models is computationally inefficient. Each token attends to all previously seen tokens, and thus recomputes many of the same values as each new token is generated. For example, while generating the Nth token, the (N-1)th token attends to (N-2)th, (N-3)th … 1st tokens. Similarly, while generating (N+1)th token, attention for the Nth token again needs to look at the (N-1)th, (N-2)th, (N-3)th, … 1st tokens. KV caching, i.e., saving of intermediate keys/values for the attention layers, is used to preserve those results for later reuse, avoiding repeated computation. Memory Bandwidth is Key Computations in LLMs are mainly dominated by matrix-matrix multiplication operations; these operations with small dimensions are typically memory-bandwidth-bound on most hardware. When generating tokens in an autoregressive manner, one of the activation matrix dimensions (defined by batch size and number of tokens in the sequence) is small at small batch sizes. Therefore, the speed is dependent on how quickly we can load model parameters from GPU memory to local caches/registers, rather than how quickly we can compute on loaded data. Available and achieved memory bandwidth in inference hardware is a better predictor of speed of token generation than their peak compute performance. Inference hardware utilization is very important in terms of serving costs. GPUs are expensive and we need them to do as much work as possible. Shared inference services promise to keep costs low by combining workloads from many users, filling in individual gaps and batching together overlapping requests. For large models like Llama2-70B, we only achieve good cost/performance at large batch sizes. Having an inference serving system that can operate at large batch sizes is critical for cost efficiency. However, a large batch means larger KV cache size, and that in turn increases the number of GPUs required to serve the model. There's a tug-of-war here and shared service operators need to make some cost trade-offs and implement systems optimizations. Model Bandwidth Utilization (MBU) How optimized is an LLM inference server? As briefly explained earlier, inference for LLMs at smaller batch sizes—especially at decode time—is bottlenecked on how quickly we can load model parameters from the device memory to the compute units. Memory bandwidth dictates how quickly the data movement happens. To measure the underlying hardware's utilization, we introduce a new metric called Model Bandwidth Utilization (MBU). MBU is defined as (achieved memory bandwidth) / (peak memory bandwidth) where achieved memory bandwidth is ((total model parameter size + KV cache size) / TPOT). For example, if a 7B parameter running with 16-bit precision has TPOT equal to 14ms, then it's moving 14GB of parameters in 14ms translating to 1TB/sec bandwidth usage. If the peak bandwidth of the machine is 2TB/sec, we are running at an MBU of 50%. For simplicity, this example ignores KV cache size, which is small for smaller batch sizes and shorter sequence lengths. MBU values close to 100% imply that the inference system is effectively utilizing the available memory bandwidth.", "source": "blog:www.databricks.com", "published": "", "authors": "", "tokens": 512, "sha256": "3fb17a287f431181d3ab682cf1bf162131fc7c5f81f1d3f159c9582e6bdbfcd4"}
{"doc_id": "blog:www.databricks.com#body:part-4", "url": "https://www.databricks.com/blog/llm-inference-performance-engineering-best-practices", "anchor": "#body:part-4", "type": "blog", "title": "", "section": "Body", "text": "example, if a 7B parameter running with 16-bit precision has TPOT equal to 14ms, then it's moving 14GB of parameters in 14ms translating to 1TB/sec bandwidth usage. If the peak bandwidth of the machine is 2TB/sec, we are running at an MBU of 50%. For simplicity, this example ignores KV cache size, which is small for smaller batch sizes and shorter sequence lengths. MBU values close to 100% imply that the inference system is effectively utilizing the available memory bandwidth. MBU is also useful to compare different inference systems (hardware + software) in a normalized manner. MBU is complementary to the Model Flops Utilization (MFU; introduced in the PaLM paper) metric which is important in compute-bound settings. Figure 1 shows a pictorial representation of MBU in a plot similar to a roofline plot. The solid sloped line of the orange-shaded region shows the maximum possible throughput if memory bandwidth is fully saturated at 100%. However, in reality for low batch sizes (white dot), the observed performance is lower than maximum – how much lower is a measure of the MBU. For large batch sizes (yellow region), the system is compute bound, and the achieved throughput as a fraction of the peak possible throughput is measured as the Model Flops Utilization (MFU). MBU and MFU determine how much more room is available to push the inference speed further on a given hardware setup. Figure 2 shows measured MBU for different degrees of tensor parallelism with our TensorRT-LLM-based inference server. Peak memory bandwidth utilization is attained when transferring large contiguous memory chunks. When smaller models like MPT-7B are distributed across multiple GPUs, we observe lower MBU as we are moving smaller memory chunks in each GPU. Figure 3 shows empirically observed MBU for different degrees of tensor parallelism and batch sizes on the NVIDIA H100 GPUs. MBU decreases as batch size increases. However, as we scale GPUs, the relative decrease in MBU is less significant. It is also worthy to note that picking hardware with greater memory bandwidth can boost performance with fewer GPUs. At batch size 1, we can achieve a higher MBU of 60% on 2xH100-80GBs as compared to 55% on 4xA100-40GB GPUs (Figure 2). Benchmarking Results Latency We have measured time to first token (TTFT) and time per output token (TPOT) across different degrees of tensor parallelism for MPT-7B and Llama2-70B models. As input prompts lengthen, time to generate the first token starts to consume a substantial portion of total latency. Tensor parallelizing across multiple GPUs helps reduce this latency. Unlike model training, scaling to more GPUs offers significant diminishing returns for inference latency. Eg. for Llama2-70B going from 4x to 8x GPUs only decreases latency by 0.7x at small batch sizes. One reason for this is that higher parallelism has lower MBU (as discussed earlier). Another reason is that tensor parallelism introduces communication overhead across a GPU node. | Time to first token (ms) | |||| |---|---|---|---|---| | Model | 1xA100-40GB | 2xA100-40GB | 4xA100-40GB | 8xA100-40GB | | MPT-7B | 46 (1x) | 34 (0.73x) | 26 (0.56x) |", "source": "blog:www.databricks.com", "published": "", "authors": "", "tokens": 512, "sha256": "594db46a3ae310e38c22fb057df6dfc94c9902b20d6164522e94e88b148c413f"}
{"doc_id": "blog:www.databricks.com#body:part-5", "url": "https://www.databricks.com/blog/llm-inference-performance-engineering-best-practices", "anchor": "#body:part-5", "type": "blog", "title": "", "section": "Body", "text": "for inference latency. Eg. for Llama2-70B going from 4x to 8x GPUs only decreases latency by 0.7x at small batch sizes. One reason for this is that higher parallelism has lower MBU (as discussed earlier). Another reason is that tensor parallelism introduces communication overhead across a GPU node. | Time to first token (ms) | |||| |---|---|---|---|---| | Model | 1xA100-40GB | 2xA100-40GB | 4xA100-40GB | 8xA100-40GB | | MPT-7B | 46 (1x) | 34 (0.73x) | 26 (0.56x) | - | | Llama2-70B | Doesn't fit | 154 (1x) | 114 (0.74x) | Table 1: Time to first token given input requests are 512 tokens length with batch size of 1. Larger models like Llama2 70B needs at least 4xA100-40B GPUs to fit in memory At larger batch sizes, higher tensor parallelism leads to a more significant relative decrease in token latency. Figure 4 shows how time per output token varies for MPT-7B. At batch size 1, going from 2x to 4x only reduces token latency by ~12%. At batch size 16, latency with 4x is 33% lower than with 2x. This goes in line with our earlier observation that the relative decrease in MBU is smaller at higher degrees of tensor parallelism for batch size 16 as compared to batch size 1. Figure 5 shows similar results for Llama2-70B, except the relative improvement between 4x and 8x is less pronounced. We also compare GPU scaling across two different hardware. Because H100-80GB has 2.15x GPU memory bandwidth as compared to A100-40GB, we can see that latency is 36% lower at batch size 1 and 52% lower at batch size 16 for 4x systems. Throughput We can trade off throughput and time per token by batching requests together. Grouping queries during GPU evaluation increases throughput compared to processing queries sequentially, but each query will take longer to complete (ignoring queueing effects). There are a few common techniques for batching inference requests: - Static batching: Client packs multiple prompts into requests and a response is returned after all sequences in the batch have been completed. Our inference servers support this but do not require it. - Dynamic batching: Prompts are batched together on the fly inside the server. Typically, this method performs worse than static batching but can get close to optimal if responses are short or of uniform length. Does not work well when requests have different parameters. - Continuous batching: The idea of batching requests together as they arrive was introduced in this excellent paper and is currently the SOTA method. Instead of waiting for all sequences in a batch to finish, it groups sequences together at the iteration level. It can achieve 10x-20x better throughput than dynamic batching. Continuous batching is usually the best approach for shared services, but there are situations where the other two might be better. In low-QPS environments, dynamic batching can outperform continuous batching. It is sometimes easier to implement low-level GPU optimizations in a simpler batching framework. For offline batch inference workloads, static batching can avoid significant overhead and achieve better throughput. Batch Size How", "source": "blog:www.databricks.com", "published": "", "authors": "", "tokens": 512, "sha256": "e16c710e4342f47d4d10657bfbe59717d69eb20e8eed87cd39beca06abe6ab6e"}
{"doc_id": "blog:www.databricks.com#body:part-6", "url": "https://www.databricks.com/blog/llm-inference-performance-engineering-best-practices", "anchor": "#body:part-6", "type": "blog", "title": "", "section": "Body", "text": "finish, it groups sequences together at the iteration level. It can achieve 10x-20x better throughput than dynamic batching. Continuous batching is usually the best approach for shared services, but there are situations where the other two might be better. In low-QPS environments, dynamic batching can outperform continuous batching. It is sometimes easier to implement low-level GPU optimizations in a simpler batching framework. For offline batch inference workloads, static batching can avoid significant overhead and achieve better throughput. Batch Size How well batching works is highly dependent on the request stream. But we can get an upper bound on its performance by benchmarking static batching with uniform requests. | Batch size | ||||||| |---|---|---|---|---|---|---|---| | Hardware | 1 | 4 | 8 | 16 | 32 | 64 | 128 | | 1 x A10 | 0.4 (1x) | 1.4 (3.5x) | 2.3 (6x) | 3.5 (9x) | OOM (Out of Memory) error | || | 2 x A10 | 0.8 | 2.5 | 4.0 | 7.0 | 8.0 | || | 1 x A100 | 0.9 (1x) | 3.2 (3.5x) | 5.3 (6x) | 8.0 (9x) | 10.5 (12x) | 12.5 (14x) | | | 2 x A100 | 1.3 | 3.0 | 5.5 | 9.5 | 14.5 | 17.0 | 22.0 | | 4 x A100 | 1.7 | 6.2 | 11.5 | 18.0 | 25.0 | 33.0 | 36.5 | Table 2: Peak MPT-7B throughput (req/sec) with static batching and a FasterTransformers-based backend. Requests: 512 input and 64 output tokens. For larger inputs, the OOM boundary will be at smaller batch sizes. Latency Trade-Off Request latency increases with batch size. With one NVIDIA A100 GPU, for example, if we maximize throughput with a batch size of 64, latency increases by 4x while throughput increases by 14x. Shared inference services typically pick a balanced batch size. Users hosting their own models should decide the appropriate latency/throughput trade-off for their applications. In some applications, like chatbots, low latency for fast responses is the top priority. In other applications, like batched processing of unstructured PDFs, we might want to sacrifice the latency to process an individual document to process all of them fast in parallel. Figure 7 shows the throughput vs latency curve for the 7B model. Each line on this curve is obtained by increasing the batch size from 1 to 256. This is useful in determining how large we can make the batch size, subject to different latency constraints. Recalling our roofline plot above, we find that these measurements are consistent with what we would expect. After a certain batch size, i.e., when we cross to the compute bound regime, every doubling of batch size just increases the latency without increasing throughput. When using parallelism, it's important to understand low-level hardware details. For instance, not all 8xA100 instances are the same across different clouds. Some servers have high bandwidth connections between all GPUs, others pair GPUs and have lower bandwidth connections between pairs. This could introduce bottlenecks, causing real-world performance to deviate significantly from the curves above. Optimization Case Study: Quantization", "source": "blog:www.databricks.com", "published": "", "authors": "", "tokens": 512, "sha256": "dee0375f7b7419532d3a283e07390da86e87f854d6d832b6f264697768521c56"}
{"doc_id": "blog:www.databricks.com#body:part-7", "url": "https://www.databricks.com/blog/llm-inference-performance-engineering-best-practices", "anchor": "#body:part-7", "type": "blog", "title": "", "section": "Body", "text": "i.e., when we cross to the compute bound regime, every doubling of batch size just increases the latency without increasing throughput. When using parallelism, it's important to understand low-level hardware details. For instance, not all 8xA100 instances are the same across different clouds. Some servers have high bandwidth connections between all GPUs, others pair GPUs and have lower bandwidth connections between pairs. This could introduce bottlenecks, causing real-world performance to deviate significantly from the curves above. Optimization Case Study: Quantization Quantization is a common technique used to reduce the hardware requirements for LLM inference. Reducing the precision of model weights and activations during inference can dramatically reduce hardware requirements. For instance, switching from 16-bit weights to 8-bit weights can halve the number of required GPUs in memory constrained environments (eg. Llama2-70B on A100s). Dropping down to 4-bit weights makes it possible to run inference on consumer hardware (eg. Llama2-70B on Macbooks). In our experience, quantization should be implemented with caution. Naive quantization techniques can lead to a substantial degradation in model quality. The impact of quantization also varies across model architectures (eg. MPT vs Llama) and sizes. We will explore this in more detail in a future blog post. When experimenting with techniques like quantization, we recommend using an LLM quality benchmark like the Mosaic Eval Gauntlet to evaluate the quality of the inference system, not just the quality of the model in isolation. Additionally, it's important to explore deeper systems optimizations. In particular, quantization can make KV caches much more efficient. As mentioned previously, in autoregressive token generation, past Key/Values (KV) from the attention layers are cached instead of recomputing them at every step. The size of the KV cache varies based on the number of sequences processed at a time and the length of these sequences. Moreover, during each iteration of the next token generation, new KV items are added to the existing cache making it bigger as new tokens are generated. Therefore, effective KV cache memory management when adding these new values is critical for good inference performance. Llama2 models use a variant of attention called Grouped Query Attention (GQA). Please note that when the number of KV heads is 1, GQA is the same as Multi-Query-Attention (MQA). GQA helps with keeping the KV cache size down by sharing Keys/Values. The formula to calculate KV cache size is batch_size * seqlen * (d_model/n_heads) * n_layers * 2 (K and V) * 2 (bytes per Float16) * n_kv_heads Table 3 shows GQA KV cache size calculated at different batch sizes at a sequence length of 1024 tokens. The parameter size for Llama2 models, in comparison, is 140 GB (Float16) for the 70B model. Quantization of KV cache is another technique (in addition to GQA/MQA) to reduce the size of KV cache, and we are actively evaluating its impact on the generation quality. | Batch Size | GQA KV cache memory (FP16) | GQA KV cache memory (Int8) | |---|---|---| | 1 | .312 GiB | .156 GiB | | 16 | 5 GiB | 2.5 GiB | | 32 |", "source": "blog:www.databricks.com", "published": "", "authors": "", "tokens": 512, "sha256": "0e423ac1739c45f2c4799aec6224fa0e086cd28a66e9816b0b06582df22137cd"}
{"doc_id": "blog:www.databricks.com#body:part-8", "url": "https://www.databricks.com/blog/llm-inference-performance-engineering-best-practices", "anchor": "#body:part-8", "type": "blog", "title": "", "section": "Body", "text": "for Llama2 models, in comparison, is 140 GB (Float16) for the 70B model. Quantization of KV cache is another technique (in addition to GQA/MQA) to reduce the size of KV cache, and we are actively evaluating its impact on the generation quality. | Batch Size | GQA KV cache memory (FP16) | GQA KV cache memory (Int8) | |---|---|---| | 1 | .312 GiB | .156 GiB | | 16 | 5 GiB | 2.5 GiB | | 32 | 10 GiB | 5 GiB | | 64 | 20 GiB | 10 GiB | Table 3: KV cache size for Llama-2-70B at a sequence length of 1024 As mentioned previously, token generation with LLMs at low batch sizes is a GPU memory bandwidth-bound problem, i.e. the speed of generation depends on how quickly model parameters can be moved from the GPU memory to on-chip caches. Converting model weights from FP16 (2 bytes) to INT8 (1 byte) or INT4 (0.5 byte) requires moving less data and thus speeds up token generation. However, quantization may negatively impact the model generation quality. We are currently evaluating the impact on model quality using Model Gauntlet and plan to publish a followup blog post on it soon. Conclusions and Key Results Each of the factors we've outlined above influences the way we build and deploy models. We use these results to make data-driven decisions that take into consideration the hardware type, the software stack, the model architecture, and typical usage patterns. Here are some recommendations drawn from our experience. Identify your optimization target: Do you care about interactive performance? Maximizing throughput? Minimizing cost? There are predictable trade-offs here. Pay attention to the components of latency: For interactive applications time-to-first-token drives how responsive your service will feel and time-per-output-token determines how fast it will feel. Memory bandwidth is key: Generating the first token is typically compute-bound, while subsequent decoding is memory-bound operation. Because LLM inference often operates in memory-bound settings, MBU is a useful metric to optimize for and can be used to compare the efficiency of inference systems. Batching is critical: Processing multiple requests concurrently is critical for achieving high throughput and for effectively utilizing expensive GPUs. For shared online services continuous batching is indispensable, whereas offline batch inference workloads can achieve high throughput with simpler batching techniques. In depth optimizations: Standard inference optimization techniques are important (eg. operator fusion, weight quantization) for LLMs but it's important to explore deeper systems optimizations, especially those which improve memory utilization. One example is KV cache quantization. Hardware configurations: The model type and expected workload should be used to decide deployment hardware. For instance, when scaling to multiple GPUs MBU falls much more rapidly for smaller models, such as MPT-7B, than it does for larger models, such as Llama2-70B. Performance also tends to scale sub-linearly with higher degrees of tensor parallelism. That said, a high degree of tensor parallelism might still make sense for smaller models if traffic is high or if users are willing to pay a premium for extra low latency. Data Driven Decisions: Understanding the theory", "source": "blog:www.databricks.com", "published": "", "authors": "", "tokens": 512, "sha256": "980eda31f106d849fd8da6eb60da2e2bfe69456a55882de1440e1024f1e1a017"}
{"doc_id": "blog:www.databricks.com#body:part-9", "url": "https://www.databricks.com/blog/llm-inference-performance-engineering-best-practices", "anchor": "#body:part-9", "type": "blog", "title": "", "section": "Body", "text": "decide deployment hardware. For instance, when scaling to multiple GPUs MBU falls much more rapidly for smaller models, such as MPT-7B, than it does for larger models, such as Llama2-70B. Performance also tends to scale sub-linearly with higher degrees of tensor parallelism. That said, a high degree of tensor parallelism might still make sense for smaller models if traffic is high or if users are willing to pay a premium for extra low latency. Data Driven Decisions: Understanding the theory is important, but we recommend always measuring end-to-end server performance. There are many reasons an inference deployment can perform worse than expected. MBU could be unexpectedly low because of software inefficiencies. Or differences in hardware between cloud providers could lead to surprises (we have observed a 2x latency difference between 8xA100 servers from two cloud providers). To get started with LLM inference, try out Databricks Model Serving. Check out the documentation to learn more.", "source": "blog:www.databricks.com", "published": "", "authors": "", "tokens": 154, "sha256": "a458875070ded0e014d53d10a324231a08db255b72ef16428d010c6c10426683"}
{"doc_id": "blog:medium.com#body:part-1", "url": "https://medium.com/@williamwarley/understanding-ai-agents-how-they-work-types-and-practical-applications-bd261845f7c3", "anchor": "#body:part-1", "type": "blog", "title": "", "section": "Body", "text": "--- title: Understanding AI Agents: How They Work, Types, and Practical Applications author: Warley's CatOps url: https://medium.com/@williamwarley/understanding-ai-agents-how-they-work-types-and-practical-applications-bd261845f7c3 hostname: medium.com description: Introduction to AI Agents sitename: Medium date: 2024-06-11 --- Understanding AI Agents: How They Work, Types, and Practical Applications Introduction to AI Agents Definition and Importance AI Agents are autonomous entities that use artificial intelligence (AI) to perceive their environment, make decisions, and perform actions to achieve specific goals. These agents can operate independently or interact with other agents and systems to accomplish tasks. AI agents are designed to simulate human-like intelligence, enabling them to solve complex problems, adapt to changing conditions, and learn from experiences. Key Characteristics of AI Agents: - Autonomy: Operate without human intervention, making decisions and taking actions based on their programming and learned experiences. - Perception: Use sensors or input mechanisms to perceive their environment, gather data, and understand the context in which they operate. - Decision-Making: Apply reasoning and decision-making processes to choose the best course of action based on their goals and current state. - Learning: Improve their performance over time by learning from past experiences, adapting to new situations, and optimizing their strategies. Historical Background and Evolution The concept of AI agents has evolved significantly since its inception, influenced by advancements in computer science, robotics, and cognitive science. Here’s a brief overview of the historical development: 1950s-1960s: The early days of AI research focused on creating machines that could mimic human thought processes. Pioneering work by researchers like Alan Turing and John McCarthy laid the foundation for AI, introducing concepts such as the Turing Test and symbolic AI. 1970s-1980s: The development of expert systems marked a significant milestone in AI. These systems used rule-based logic to emulate the decision-making abilities of human experts in specific domains. However, their lack of learning capabilities and rigidity limited their adaptability. 1990s: The emergence of machine learning (ML) and neural networks revolutionized AI. Agents could now learn from data and experiences, improving their performance over time. Reinforcement learning (RL) also gained prominence, enabling agents to learn optimal strategies through trial and error. 2000s: The advent of big data and increased computational power further accelerated AI development. AI agents became more sophisticated, capable of handling complex tasks such as natural language processing (NLP), computer vision, and autonomous navigation. 2010s-Present: Deep learning, a subset of ML, has driven significant advancements in AI agents. Techniques such as convolutional neural networks (CNNs) and recurrent neural networks (RNNs) have enabled agents to achieve state-of-the-art performance in various domains. Additionally, the integration of AI agents with the Internet of Things (IoT) and cloud computing has expanded their capabilities and applications even further. Importance of AI Agents AI agents are crucial in the modern technological landscape due to their ability to automate complex tasks, make informed decisions, and continuously improve through learning. Their importance can be seen in several key areas: 1. Automation: AI agents can automate repetitive and time-consuming tasks, freeing up human resources for more strategic and creative work. This increases efficiency and productivity across various industries. 2. Scalability: AI agents can handle", "source": "blog:medium.com", "published": "", "authors": "", "tokens": 512, "sha256": "fc39282c59b0e70684197dc155633a75b8c5ee2084f795f95fb31a4a96d9befa"}
{"doc_id": "blog:medium.com#body:part-2", "url": "https://medium.com/@williamwarley/understanding-ai-agents-how-they-work-types-and-practical-applications-bd261845f7c3", "anchor": "#body:part-2", "type": "blog", "title": "", "section": "Body", "text": "has expanded their capabilities and applications even further. Importance of AI Agents AI agents are crucial in the modern technological landscape due to their ability to automate complex tasks, make informed decisions, and continuously improve through learning. Their importance can be seen in several key areas: 1. Automation: AI agents can automate repetitive and time-consuming tasks, freeing up human resources for more strategic and creative work. This increases efficiency and productivity across various industries. 2. Scalability: AI agents can handle large-scale operations and processes, making them ideal for applications that require processing vast amounts of data and performing tasks simultaneously. 3. Real-Time Decision Making: AI agents can process information and make decisions in real-time, which is crucial for applications like autonomous driving, financial trading, and real-time customer support. 4. Adaptability: Through machine learning and reinforcement learning, AI agents can adapt to new environments and situations, improving their performance and decision-making capabilities over time. 5. Personalization: AI agents can analyze individual user behavior and preferences to provide personalized experiences in applications such as recommendation systems, personal assistants, and targeted marketing. Evolution of AI Agents The evolution of AI agents can be traced through several key developments and milestones: 1. Early AI and Expert Systems: Initial AI research focused on rule-based systems and symbolic reasoning. Expert systems, which were designed to mimic the decision-making abilities of human experts, were among the first AI agents. However, their lack of learning capabilities and flexibility limited their effectiveness. 2. Machine Learning and Neural Networks: The introduction of machine learning algorithms allowed AI agents to learn from data rather than relying solely on predefined rules. Neural networks, inspired by the human brain, enabled agents to recognize patterns and make predictions, leading to significant improvements in tasks such as image and speech recognition. 3. Reinforcement Learning: Reinforcement learning (RL) provided a framework for AI agents to learn optimal behaviors through trial and error. Agents receive feedback in the form of rewards or penalties, allowing them to refine their strategies and actions. This approach has been particularly successful in applications like game playing and robotics. 4. Deep Learning: Deep learning, a subset of machine learning, involves training large neural networks with many layers. This has led to breakthroughs in natural language processing, computer vision, and other complex tasks. AI agents powered by deep learning can achieve state-of-the-art performance in various domains. 5. Integration with IoT and Cloud Computing: The integration of AI agents with the Internet of Things (IoT) and cloud computing has expanded their capabilities. AI agents can now leverage vast amounts of data collected from IoT devices and process it in real-time using cloud-based infrastructure. This integration has enabled applications such as smart homes, industrial automation, and predictive maintenance. Conclusion AI agents represent a significant advancement in artificial intelligence, offering the ability to automate complex tasks, make real-time decisions, and continuously improve through learning. Their evolution from early rule-based systems to sophisticated deep learning models has expanded their capabilities and applications across various industries. As we continue to innovate and integrate AI agents with emerging technologies, their impact", "source": "blog:medium.com", "published": "", "authors": "", "tokens": 512, "sha256": "18fc581f7317d47f7beaa2d7f5e99bfdf7c5019a38c2d4f9c4c38315d7ec8585"}
{"doc_id": "blog:medium.com#body:part-3", "url": "https://medium.com/@williamwarley/understanding-ai-agents-how-they-work-types-and-practical-applications-bd261845f7c3", "anchor": "#body:part-3", "type": "blog", "title": "", "section": "Body", "text": "process it in real-time using cloud-based infrastructure. This integration has enabled applications such as smart homes, industrial automation, and predictive maintenance. Conclusion AI agents represent a significant advancement in artificial intelligence, offering the ability to automate complex tasks, make real-time decisions, and continuously improve through learning. Their evolution from early rule-based systems to sophisticated deep learning models has expanded their capabilities and applications across various industries. As we continue to innovate and integrate AI agents with emerging technologies, their impact on our daily lives and the broader technological landscape will only grow. This introduction provides a comprehensive overview of AI agents, highlighting their definition, importance, historical evolution, and key advancements. How AI Agents Work To understand how AI agents work, it is essential to delve into their core concepts, components, and learning mechanisms. This chapter provides a detailed explanation of these elements to illustrate the functioning of AI agents. Core Concepts and Components 1. Perception: — AI agents use sensors or input mechanisms to perceive their environment. This can involve collecting data from various sources such as cameras, microphones, or other sensors. — Example: In autonomous vehicles, sensors like LIDAR, cameras, and radar gather information about the vehicle’s surroundings. 2. Reasoning: — After perceiving the environment, the agent processes the information to make informed decisions. This involves reasoning and applying logical rules or learned knowledge to interpret the data. — Example: A recommendation system analyzes user preferences and behaviors to suggest relevant products. 3. Action: — Based on its reasoning, the AI agent takes appropriate actions to achieve its goals. This can involve physical actions (e.g., a robot moving objects) or digital actions (e.g., sending an email). — Example: A robotic vacuum cleaner navigates a room to clean it efficiently. 4. Learning: — AI agents improve their performance over time by learning from experiences. This can involve supervised learning, unsupervised learning, or reinforcement learning, depending on the task and data available. — Example: A chatbot learns to provide better responses by analyzing previous interactions with users. Types of AI Agents 1. Simple Reflex Agents: — Operate based on a set of predefined rules and respond directly to specific stimuli from the environment. — Example: A thermostat that adjusts the temperature based on the current room temperature. 2. Model-Based Reflex Agents: — Maintain an internal model of the world to keep track of unobservable aspects of the environment, allowing for more informed decision-making. — Example: A navigation system that uses a map to plan routes and update the user’s location. 3. Goal-Based Agents: — Use goals to guide their actions, making decisions based on the desirability of outcomes and the likelihood of achieving their objectives. — Example: An AI planning system that schedules tasks to maximize efficiency and meet deadlines. 4. Utility-Based Agents: — Evaluate the utility of different actions to choose the one that maximizes overall satisfaction or performance, considering both goals and preferences. — Example: An autonomous trading system that selects trades to maximize profit while minimizing risk. 5. Learning Agents: — Continuously learn from their environment and experiences to improve", "source": "blog:medium.com", "published": "", "authors": "", "tokens": 512, "sha256": "f16b20c950fcacafa5c48a7ea7d75b2babc3cdf87709c4e4eefd2384f0e16e24"}
{"doc_id": "blog:medium.com#body:part-4", "url": "https://medium.com/@williamwarley/understanding-ai-agents-how-they-work-types-and-practical-applications-bd261845f7c3", "anchor": "#body:part-4", "type": "blog", "title": "", "section": "Body", "text": "on the desirability of outcomes and the likelihood of achieving their objectives. — Example: An AI planning system that schedules tasks to maximize efficiency and meet deadlines. 4. Utility-Based Agents: — Evaluate the utility of different actions to choose the one that maximizes overall satisfaction or performance, considering both goals and preferences. — Example: An autonomous trading system that selects trades to maximize profit while minimizing risk. 5. Learning Agents: — Continuously learn from their environment and experiences to improve their performance over time, adapting to new situations and optimizing their behavior. — Example: A recommendation engine that refines its suggestions based on user feedback and interactions. Learning Mechanisms 1. Supervised Learning: — Involves training an agent using labeled data, where the correct output is provided for each input example. The agent learns to map inputs to outputs by minimizing prediction errors. — Example: Training an image recognition model to classify images of cats and dogs using labeled datasets. 2. Unsupervised Learning: — Involves training an agent using unlabeled data, where the agent identifies patterns and structures in the data without explicit instructions. Techniques like clustering and dimensionality reduction are common. — Example: Grouping similar customer profiles for targeted marketing campaigns. 3. Reinforcement Learning (RL): — Involves training an agent to make sequences of decisions by rewarding desirable behaviors and penalizing undesirable ones. The agent learns to maximize cumulative rewards over time. — Example: Training a game-playing AI to learn optimal strategies by receiving points for winning and penalties for losing. Practical Implementation Implementing AI agents involves several practical steps, including data collection, model training, and deployment. Here’s a high-level overview: 1. Data Collection and Preprocessing: — Gather relevant data from sensors or databases, preprocess it to remove noise, and structure it for analysis. — Example: Collecting and cleaning data from sensors for an autonomous robot. 2. Model Training: — Train the agent using appropriate learning algorithms and techniques, such as neural networks, decision trees, or RL algorithms. — Example: Training a neural network to recognize objects in images. 3. Deployment: — Deploy the trained agent in the target environment, ensuring it can interact with other systems and perform its tasks effectively. — Example: Deploying a chatbot on a company’s customer service platform. 4. Monitoring and Maintenance: — Continuously monitor the agent’s performance, update it with new data, and retrain as necessary to maintain its effectiveness. — Example: Regularly updating a recommendation engine with new user data to improve suggestions. This chapter provides a detailed explanation of how AI agents work, covering their core concepts, types, learning mechanisms, and practical implementation. Types of AI Agents AI agents come in various types, each designed to handle specific tasks and environments. Understanding these types helps in selecting the right kind of agent for a particular application. This chapter explores the different types of AI agents, their characteristics, and use cases. 1. Simple Reflex Agents Overview: - Simple reflex agents act solely based on the current percept and predefined rules, without considering the history of past perceptions. - They are the simplest form of agents", "source": "blog:medium.com", "published": "", "authors": "", "tokens": 512, "sha256": "1291049a131f623687635c519aabfe7de0974ee1fc340caee5f8cfd08a6d0715"}
{"doc_id": "blog:medium.com#body:part-5", "url": "https://medium.com/@williamwarley/understanding-ai-agents-how-they-work-types-and-practical-applications-bd261845f7c3", "anchor": "#body:part-5", "type": "blog", "title": "", "section": "Body", "text": "of AI Agents AI agents come in various types, each designed to handle specific tasks and environments. Understanding these types helps in selecting the right kind of agent for a particular application. This chapter explores the different types of AI agents, their characteristics, and use cases. 1. Simple Reflex Agents Overview: - Simple reflex agents act solely based on the current percept and predefined rules, without considering the history of past perceptions. - They are the simplest form of agents and work well in environments that are fully observable and deterministic. How They Work: - These agents use condition-action rules (if-then statements) to decide on actions. - They do not maintain any internal state or model of the environment. Example: - A thermostat that turns the heating on or off based on the current temperature reading. - Use Case: Simple household appliances and basic automated systems. Advantages: - Easy to design and implement. - Efficient in predictable environments. Disadvantages: - Limited functionality in complex or partially observable environments. - Cannot learn or adapt to changes in the environment. 2. Model-Based Reflex Agents Overview: - Model-based reflex agents maintain an internal model of the environment, allowing them to handle partially observable environments better than simple reflex agents. - They can consider the history of past perceptions to make more informed decisions. How They Work: - These agents update their internal model based on incoming percepts and use this model to infer unseen aspects of the environment. - They use condition-action rules, but these rules can reference the internal model. Example: - A navigation system that uses a map to plan routes and update the user’s location. - Use Case: GPS navigation, industrial automation systems. Advantages: - Can handle partially observable environments. - More flexible and capable than simple reflex agents. Disadvantages: - More complex to design and implement. - Requires more computational resources to maintain and update the internal model. 3. Goal-Based Agents Overview: - Goal-based agents operate based on predefined goals. They make decisions by evaluating how well different actions achieve these goals. - They can plan sequences of actions to achieve their objectives. How They Work: - These agents use search and planning algorithms to determine the best course of action to reach a goal. - They consider both the current state and the desired goal state. Example: - An AI planning system that schedules tasks to maximize efficiency and meet deadlines. - Use Case: Automated scheduling, robotic path planning. Advantages: - Capable of complex decision-making and planning. - Can adapt to changes in goals and environment. Disadvantages: - Requires complex algorithms for search and planning. - May be computationally intensive, especially in dynamic environments. 4. Utility-Based Agents Overview: - Utility-based agents aim to maximize their overall utility or satisfaction by evaluating the desirability of different outcomes. - They make decisions based on a utility function that assigns a value to each possible state. How They Work: - These agents use optimization techniques to select actions that maximize their expected utility. - They consider multiple factors and trade-offs to achieve the", "source": "blog:medium.com", "published": "", "authors": "", "tokens": 512, "sha256": "c2761f0ddb77fd6880e59aae306372d001ad87e661ad327d48ac9780dfcdd821"}
{"doc_id": "blog:medium.com#body:part-6", "url": "https://medium.com/@williamwarley/understanding-ai-agents-how-they-work-types-and-practical-applications-bd261845f7c3", "anchor": "#body:part-6", "type": "blog", "title": "", "section": "Body", "text": "algorithms for search and planning. - May be computationally intensive, especially in dynamic environments. 4. Utility-Based Agents Overview: - Utility-based agents aim to maximize their overall utility or satisfaction by evaluating the desirability of different outcomes. - They make decisions based on a utility function that assigns a value to each possible state. How They Work: - These agents use optimization techniques to select actions that maximize their expected utility. - They consider multiple factors and trade-offs to achieve the best overall outcome. Example: - An autonomous trading system that selects trades to maximize profit while minimizing risk. - Use Case: Financial trading, resource management. Advantages: - Can handle complex decision-making scenarios involving trade-offs. - Capable of balancing multiple objectives and preferences. Disadvantages: - Designing an appropriate utility function can be challenging. - May require significant computational resources for optimization. 5. Learning Agents Overview: - Learning agents improve their performance over time by learning from experiences and adapting to new situations. - They can operate in dynamic and uncertain environments by continuously updating their knowledge and strategies. How They Work: - These agents use various learning algorithms, such as supervised learning, unsupervised learning, and reinforcement learning, to acquire new knowledge and skills. - They have four main components: a learning element, a performance element, a critic, and a problem generator. Example: - A recommendation engine that refines its suggestions based on user feedback and interactions. - Use Case: Personalized recommendations, autonomous systems, adaptive control. Advantages: - Capable of continuous improvement and adaptation. - Can handle complex and changing environments. Disadvantages: - Requires significant amounts of data for effective learning. - The learning process can be computationally intensive and time-consuming. This chapter explores the various types of AI agents, highlighting their characteristics, how they work, and their respective advantages and disadvantages. Applications of AI Agents AI agents are deployed across various industries to automate tasks, enhance decision-making, and improve overall efficiency. This chapter explores several practical applications of AI agents, highlighting their impact and benefits in different domains. 1. Autonomous Vehicles Overview: - AI agents play a crucial role in the development of autonomous vehicles, enabling them to perceive their environment, make driving decisions, and navigate safely. How They Work: - Autonomous vehicles use sensors like cameras, LIDAR, and radar to gather data about the surroundings. - AI agents process this data to identify objects, predict their movements, and make real-time driving decisions. - The agents use machine learning algorithms to improve their performance over time, adapting to different driving conditions. Example: - Waymo’s self-driving cars use AI agents to navigate complex urban environments and handle various driving scenarios. - Benefits: Enhanced safety, reduced traffic congestion, and increased mobility for individuals who cannot drive. 2. Robotics Overview: - AI agents are integral to robotics, enabling robots to perform tasks autonomously in various settings, from manufacturing to healthcare. How They Work: - Robots equipped with AI agents use sensors to perceive their environment and execute tasks with precision. - AI agents in robots can learn from interactions and adapt to new tasks, improving efficiency", "source": "blog:medium.com", "published": "", "authors": "", "tokens": 512, "sha256": "0edafbfdfeee96e12231dd9f7c2d4476f5eb9cb7af71cbf55d475997e7ce5136"}
{"doc_id": "blog:medium.com#body:part-7", "url": "https://medium.com/@williamwarley/understanding-ai-agents-how-they-work-types-and-practical-applications-bd261845f7c3", "anchor": "#body:part-7", "type": "blog", "title": "", "section": "Body", "text": "environments and handle various driving scenarios. - Benefits: Enhanced safety, reduced traffic congestion, and increased mobility for individuals who cannot drive. 2. Robotics Overview: - AI agents are integral to robotics, enabling robots to perform tasks autonomously in various settings, from manufacturing to healthcare. How They Work: - Robots equipped with AI agents use sensors to perceive their environment and execute tasks with precision. - AI agents in robots can learn from interactions and adapt to new tasks, improving efficiency and accuracy. - Robots use goal-based and utility-based agents to optimize their actions and achieve specific objectives. Example: - Collaborative robots (cobots) in manufacturing work alongside human workers, performing repetitive and precise tasks. - Benefits: Increased productivity, enhanced precision, and improved workplace safety. 3. Personal Assistants Overview: - AI agents power personal assistants like Siri, Alexa, and Google Assistant, enabling them to understand and respond to user queries. How They Work: - Personal assistants use natural language processing (NLP) to understand spoken or written commands. - AI agents process the input, retrieve relevant information, and generate appropriate responses. - These agents continuously learn from interactions to improve their understanding and accuracy. Example: - Amazon Alexa uses AI agents to control smart home devices, provide weather updates, and play music based on user preferences. - Benefits: Convenience, hands-free control, and personalized user experiences. 4. Game AI Overview: - AI agents are widely used in video games to create intelligent and adaptive non-player characters (NPCs) that enhance gameplay. How They Work: - Game AI agents use rule-based and learning algorithms to control NPC behavior, making them respond dynamically to player actions. - Agents can adapt their strategies based on player performance, providing a challenging and engaging experience. - Reinforcement learning is often used to train game AI agents, allowing them to optimize their behavior through trial and error. Example: - In games like “The Sims,” AI agents control the behavior of virtual characters, making decisions based on their needs and environment. - Benefits: Improved player engagement, realistic NPC behavior, and dynamic gameplay experiences. 5. Financial Trading Overview: - AI agents are employed in financial trading to analyze market data, make trading decisions, and execute trades autonomously. How They Work: - AI agents use machine learning algorithms to analyze historical and real-time market data, identifying patterns and trends. - These agents make trading decisions based on predefined strategies and continuously learn to improve their performance. - Utility-based agents optimize trading strategies to maximize profits while minimizing risks. Example: - AI-powered trading platforms like QuantConnect use AI agents to develop and execute automated trading strategies. - Benefits: Enhanced trading efficiency, reduced human error, and the ability to operate 24/7. 6. Healthcare Overview: - AI agents are transforming healthcare by assisting in diagnostics, treatment planning, and patient management. How They Work: - AI agents analyze medical data, such as patient records and imaging, to assist in diagnosing diseases and recommending treatments. - They use supervised learning to learn from labeled medical data and improve their diagnostic accuracy. - Agents can also monitor patient health in real-time, providing", "source": "blog:medium.com", "published": "", "authors": "", "tokens": 512, "sha256": "17f0967d569f50feed0d1b75fff4db4d59fe8ee3388027b2ec37e414f50229a6"}
{"doc_id": "blog:medium.com#body:part-8", "url": "https://medium.com/@williamwarley/understanding-ai-agents-how-they-work-types-and-practical-applications-bd261845f7c3", "anchor": "#body:part-8", "type": "blog", "title": "", "section": "Body", "text": "Benefits: Enhanced trading efficiency, reduced human error, and the ability to operate 24/7. 6. Healthcare Overview: - AI agents are transforming healthcare by assisting in diagnostics, treatment planning, and patient management. How They Work: - AI agents analyze medical data, such as patient records and imaging, to assist in diagnosing diseases and recommending treatments. - They use supervised learning to learn from labeled medical data and improve their diagnostic accuracy. - Agents can also monitor patient health in real-time, providing alerts and recommendations based on the collected data. Example: - IBM Watson for Oncology uses AI agents to analyze medical literature and patient data, helping oncologists develop personalized cancer treatment plans. - Benefits: Improved diagnostic accuracy, personalized treatment, and enhanced patient care. This chapter explores the diverse applications of AI agents across various industries, demonstrating their impact and benefits. Advantages of AI Agents AI agents offer numerous advantages that make them indispensable in various applications. This chapter discusses the key benefits of AI agents, highlighting how they contribute to efficiency, scalability, real-time decision-making, and adaptability. 1. Efficiency and Automation Task Automation: - AI agents excel at automating repetitive and time-consuming tasks, freeing up human resources for more complex and creative work. - Example: In customer service, AI agents can handle common inquiries, process transactions, and provide instant support, allowing human agents to focus on more complex issues. Increased Productivity: - By performing tasks continuously without fatigue, AI agents significantly increase productivity and operational efficiency. - Example: In manufacturing, robotic AI agents can work 24/7, assembling products with precision and speed. Error Reduction: - AI agents reduce the likelihood of human error by performing tasks consistently and accurately. - Example: In data entry and processing, AI agents ensure accuracy and consistency, reducing errors that can occur with manual handling. 2. Scalability Handling Large Volumes: - AI agents can process vast amounts of data and manage large-scale operations, making them ideal for applications that require scalability. - Example: In financial trading, AI agents can analyze and act on market data from multiple sources in real-time, scaling to handle increased trading volumes. Flexible Resource Allocation: - AI agents can dynamically allocate resources based on demand, ensuring optimal performance and cost-efficiency. - Example: Cloud-based AI agents can scale computing resources up or down based on application needs, optimizing performance and costs. Global Reach: - AI agents can operate across different time zones and geographies, providing services and support around the clock. - Example: AI-driven customer support agents can assist customers worldwide, ensuring continuous service availability. 3. Real-Time Decision Making Immediate Responses: - AI agents can process information and make decisions in real-time, which is critical for applications requiring instant responses. - Example: In autonomous driving, AI agents process sensor data and make split-second decisions to navigate safely. Adaptive Strategies: - AI agents can adapt their strategies based on real-time data and changing conditions, optimizing outcomes dynamically. - Example: In online advertising, AI agents adjust bidding strategies in real-time to maximize ad performance and budget efficiency. Proactive Monitoring: - AI agents continuously monitor systems and environments, detecting", "source": "blog:medium.com", "published": "", "authors": "", "tokens": 512, "sha256": "7637014467b449533fabba6363ef8c3a856ee1b2acaa88cfba0cb07b074d16fd"}
{"doc_id": "blog:medium.com#body:part-9", "url": "https://medium.com/@williamwarley/understanding-ai-agents-how-they-work-types-and-practical-applications-bd261845f7c3", "anchor": "#body:part-9", "type": "blog", "title": "", "section": "Body", "text": "information and make decisions in real-time, which is critical for applications requiring instant responses. - Example: In autonomous driving, AI agents process sensor data and make split-second decisions to navigate safely. Adaptive Strategies: - AI agents can adapt their strategies based on real-time data and changing conditions, optimizing outcomes dynamically. - Example: In online advertising, AI agents adjust bidding strategies in real-time to maximize ad performance and budget efficiency. Proactive Monitoring: - AI agents continuously monitor systems and environments, detecting anomalies and triggering appropriate actions immediately. - Example: In cybersecurity, AI agents detect and respond to threats in real-time, protecting systems from potential breaches. 4. Adaptability and Learning Continuous Improvement: - AI agents improve their performance over time by learning from experiences and feedback, adapting to new situations and tasks. - Example: Personalized recommendation systems learn from user interactions to provide increasingly relevant suggestions. Handling Uncertainty: - AI agents can operate effectively in uncertain and dynamic environments by adapting their behavior based on learned patterns and real-time data. - Example: In robotics, AI agents adapt to changes in their environment, such as obstacles or varying conditions, to complete tasks efficiently. Customization and Personalization: - AI agents can tailor their actions and responses to individual user preferences and needs, providing personalized experiences. - Example: Virtual personal assistants learn user preferences over time, offering personalized recommendations and assistance. 5. Cost Efficiency Reduced Operational Costs: - Automating tasks with AI agents reduces the need for manual labor, lowering operational costs and increasing profitability. - Example: Automated warehouses use AI agents to manage inventory and logistics, reducing labor costs and increasing efficiency. Optimized Resource Utilization: - AI agents optimize the use of resources, such as energy and materials, leading to cost savings and sustainability. - Example: Smart energy management systems use AI agents to optimize energy usage in buildings, reducing costs and environmental impact. Investment in Innovation: - The efficiency gains and cost savings from AI agents allow organizations to invest more in innovation and strategic initiatives. - Example: Companies can allocate resources saved from automation to research and development, driving future growth and competitiveness. This chapter highlights the numerous advantages of AI agents, emphasizing their role in enhancing efficiency, scalability, real-time decision-making, adaptability, and cost-efficiency. Implementing AI Agents Implementing AI agents involves a series of practical steps and considerations, from selecting the right tools and frameworks to addressing common challenges. This chapter provides a comprehensive guide to implementing AI agents effectively. Steps for Implementing AI Agents 1. Define Objectives and Requirements: — Clearly outline the goals you aim to achieve with the AI agent, including specific tasks and performance metrics. — Example: An e-commerce platform might aim to implement an AI agent for personalized product recommendations, with the objective of increasing sales and customer satisfaction. 2. Select Appropriate Tools and Frameworks: — Choose the tools and frameworks that best suit your needs based on the complexity of the task, available resources, and technical expertise. — Example: For natural language processing tasks, frameworks like Hugging Face Transformers or spaCy might be appropriate. 3. Data Collection and", "source": "blog:medium.com", "published": "", "authors": "", "tokens": 512, "sha256": "7da69a6cc7838be688bb29867a8476b46bbbffdcf007c4b8d7a8696d63c19ffb"}
{"doc_id": "blog:medium.com#body:part-10", "url": "https://medium.com/@williamwarley/understanding-ai-agents-how-they-work-types-and-practical-applications-bd261845f7c3", "anchor": "#body:part-10", "type": "blog", "title": "", "section": "Body", "text": "specific tasks and performance metrics. — Example: An e-commerce platform might aim to implement an AI agent for personalized product recommendations, with the objective of increasing sales and customer satisfaction. 2. Select Appropriate Tools and Frameworks: — Choose the tools and frameworks that best suit your needs based on the complexity of the task, available resources, and technical expertise. — Example: For natural language processing tasks, frameworks like Hugging Face Transformers or spaCy might be appropriate. 3. Data Collection and Preprocessing: — Gather and preprocess the data required for training the AI agent. Ensure that the data is clean, labeled (if necessary), and representative of the problem domain. — Example: Collect customer interaction logs and preprocess them to remove noise and irrelevant information for training a customer service chatbot. 4. Model Selection and Training: — Select the appropriate machine learning or deep learning model for your AI agent. Train the model using the preprocessed data, and fine-tune it to achieve optimal performance. — Example: Use a pre-trained transformer model like BERT for fine-tuning on a specific NLP task. Example Code for Training a Model with Hugging Face Transformers: from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments from datasets import load_dataset # Load dataset dataset = load_dataset('csv', data_files={'train': 'train.csv', 'test': 'test.csv'}) # Load tokenizer and model tokenizer = BertTokenizer.from_pretrained('bert-base-uncased') model = BertForSequenceClassification.from_pretrained('bert-base-uncased') # Tokenize data def tokenize_function(examples): return tokenizer(examples['text'], padding='max_length', truncation=True) tokenized_datasets = dataset.map(tokenize_function, batched=True) # Define training arguments training_args = TrainingArguments( output_dir='./results', evaluation_strategy='epoch', learning_rate=2e-5, per_device_train_batch_size=8, per_device_eval_batch_size=8, num_train_epochs=3, weight_decay=0.01, ) # Initialize Trainer trainer = Trainer( model=model, args=training_args, train_dataset=tokenized_datasets['train'], eval_dataset=tokenized_datasets['test'], ) # Train model trainer.train() 5. Evaluation and Testing: — Evaluate the AI agent’s performance using appropriate metrics and test it in various scenarios to ensure robustness and reliability. — Example: Evaluate a recommendation system using metrics like precision, recall, and F1-score on a validation dataset. 6. Deployment: — Deploy the AI agent in the target environment, ensuring it integrates smoothly with existing systems and can operate at scale. — Example: Deploy a trained chatbot on a cloud platform like AWS Lambda for scalable, serverless execution. Example Code for Deploying a Model on AWS Lambda: import json import boto3 from transformers import BertTokenizer, BertForSequenceClassification # Initialize AWS Lambda client client = boto3.client('lambda') # Define the Lambda function def lambda_handler(event, context): # Load tokenizer and model tokenizer = BertTokenizer.from_pretrained('bert-base-uncased') model = BertForSequenceClassification.from_pretrained('bert-base-uncased') # Parse input input_text = event['text'] inputs = tokenizer(input_text, return_tensors='pt') # Perform inference outputs = model(**inputs) predictions = outputs.logits.argmax(dim=-1).item() # Return response return { 'statusCode': 200, 'body': json.dumps({'prediction': predictions}) } # Deploy the Lambda function response = client.create_function( FunctionName='AIChatbot', Runtime='python3.8', Role='your-aws-role', Handler='lambda_function.lambda_handler', Code={'ZipFile': open('function.zip', 'rb').read()}, Timeout=15, MemorySize=128, ) 7. Monitoring and Maintenance: — Continuously monitor the AI agent’s performance and make necessary updates or retrain the model to maintain its effectiveness. — Example: Regularly update the training data and retrain a sentiment analysis model to adapt to changing trends and language usage. Tools and Frameworks 1. Machine Learning Frameworks: — TensorFlow: Widely used for building and deploying machine learning models, with extensive support for neural networks and deep learning. — PyTorch: Popular for", "source": "blog:medium.com", "published": "", "authors": "", "tokens": 512, "sha256": "80bee5fb3d1b45677f42423e475133e3e4191c2bb25f85154395fc97edea9f27"}
{"doc_id": "blog:medium.com#body:part-11", "url": "https://medium.com/@williamwarley/understanding-ai-agents-how-they-work-types-and-practical-applications-bd261845f7c3", "anchor": "#body:part-11", "type": "blog", "title": "", "section": "Body", "text": "open('function.zip', 'rb').read()}, Timeout=15, MemorySize=128, ) 7. Monitoring and Maintenance: — Continuously monitor the AI agent’s performance and make necessary updates or retrain the model to maintain its effectiveness. — Example: Regularly update the training data and retrain a sentiment analysis model to adapt to changing trends and language usage. Tools and Frameworks 1. Machine Learning Frameworks: — TensorFlow: Widely used for building and deploying machine learning models, with extensive support for neural networks and deep learning. — PyTorch: Popular for its dynamic computation graph and ease of use in research and development of deep learning models. — Scikit-Learn: Ideal for implementing traditional machine learning algorithms and preprocessing data. 2. Natural Language Processing (NLP) Frameworks: — Hugging Face Transformers: Provides pre-trained models and tools for NLP tasks such as text classification, question answering, and language translation. — spaCy: Efficient and scalable library for NLP tasks, including tokenization, named entity recognition, and dependency parsing. 3. Deployment Platforms: — AWS SageMaker: Comprehensive platform for building, training, and deploying machine learning models at scale. — Google Cloud AI Platform: Managed services for training and deploying machine learning models on Google Cloud. — Azure Machine Learning: End-to-end platform for training, deploying, and managing machine learning models on Azure. Best Practices 1. Data Quality: — Ensure high-quality data by cleaning, preprocessing, and labeling it accurately. Good data is crucial for training effective AI agents. — Example: Remove duplicates and outliers from your dataset to improve model accuracy. 2. Model Evaluation: — Use appropriate metrics to evaluate model performance and ensure it meets the desired objectives. — Example: Evaluate a classification model using metrics like accuracy, precision, recall, and F1-score. 3. Scalability and Efficiency: — Design AI agents to scale efficiently, ensuring they can handle increasing workloads and data volumes. — Example: Use distributed training and inference techniques to scale your AI agent across multiple machines. 4. Security and Privacy: — Implement security measures to protect data and ensure privacy, especially when dealing with sensitive information. — Example: Encrypt data at rest and in transit, and implement access controls to protect user data. Common Challenges and Solutions 1. Data Availability: — Challenge: Lack of sufficient labeled data for training. — Solution: Use data augmentation techniques, transfer learning, or synthetic data generation to augment your dataset. 2. Model Overfitting: — Challenge: The model performs well on training data but poorly on unseen data. — Solution: Implement regularization techniques, such as dropout and L2 regularization, and use cross-validation to assess model performance. 3. Integration Complexity: — Challenge: Integrating the AI agent with existing systems and workflows. — Solution: Use APIs and modular design principles to ensure smooth integration and compatibility with other systems. 4. Computational Resources: — Challenge: High computational requirements for training and inference. — Solution: Use cloud-based platforms and distributed computing to leverage scalable resources and reduce costs. This chapter provides a comprehensive guide to implementing AI agents, covering practical steps, tools and frameworks, best practices, and common challenges. Future Trends and Developments in AI Agents The field of AI agents is rapidly evolving, with continuous advancements that promise", "source": "blog:medium.com", "published": "", "authors": "", "tokens": 512, "sha256": "82c268770d0f29bd77ef0ae2cb6477b22177d3a8fdda5826231a3de61df5f370"}
{"doc_id": "blog:medium.com#body:part-12", "url": "https://medium.com/@williamwarley/understanding-ai-agents-how-they-work-types-and-practical-applications-bd261845f7c3", "anchor": "#body:part-12", "type": "blog", "title": "", "section": "Body", "text": "modular design principles to ensure smooth integration and compatibility with other systems. 4. Computational Resources: — Challenge: High computational requirements for training and inference. — Solution: Use cloud-based platforms and distributed computing to leverage scalable resources and reduce costs. This chapter provides a comprehensive guide to implementing AI agents, covering practical steps, tools and frameworks, best practices, and common challenges. Future Trends and Developments in AI Agents The field of AI agents is rapidly evolving, with continuous advancements that promise to enhance their capabilities and applications. This chapter explores some of the key future trends and developments in AI agents, including advances in reinforcement learning, integration with the Internet of Things (IoT), ethical considerations, and human-agent collaboration. Advances in Reinforcement Learning 1. Deep Reinforcement Learning (DRL): — Combining deep learning with reinforcement learning has led to significant breakthroughs in creating more capable and sophisticated AI agents. DRL algorithms enable agents to learn complex behaviors in high-dimensional environments. — Future Trend: Development of more efficient DRL algorithms that can learn faster and require less computational power, making them accessible for a broader range of applications. 2. Meta-Learning: — Meta-learning, or “learning to learn,” involves training AI agents to adapt quickly to new tasks with minimal data. This approach enhances the flexibility and generalization of AI agents. — Future Trend: Increased focus on meta-learning techniques to create AI agents that can efficiently transfer knowledge across different tasks and domains. 3. Multi-Agent Systems: — Multi-agent reinforcement learning (MARL) involves multiple AI agents interacting and learning within the same environment. This approach is useful for tasks requiring coordination and collaboration. — Future Trend: Advancements in MARL will enable more complex and realistic simulations, such as autonomous traffic management and collaborative robotics. Integration with IoT 1. Edge AI: — Edge AI involves deploying AI agents on edge devices, allowing for real-time data processing and decision-making closer to the source. This reduces latency and bandwidth usage. — Future Trend: Greater integration of AI agents with IoT devices to enable intelligent and autonomous operations in smart homes, industrial automation, and healthcare. 2. Distributed AI Systems: — Distributed AI systems leverage multiple connected devices to share computational loads and improve overall system performance and reliability. — Future Trend: Development of robust distributed AI frameworks that facilitate seamless collaboration between AI agents and IoT devices. 3. Predictive Maintenance: — AI agents can analyze data from IoT sensors to predict equipment failures and schedule maintenance proactively, reducing downtime and costs. — Future Trend: Enhanced predictive maintenance solutions using AI agents to improve efficiency and reliability in various industries, including manufacturing and energy. Ethical and Responsible AI 1. Bias Mitigation: — Addressing biases in AI agents is crucial to ensure fairness and equity. Researchers are developing techniques to detect and mitigate biases in training data and algorithms. — Future Trend: Implementation of more advanced bias detection and mitigation methods to promote ethical AI practices. 2. Transparency and Explainability: — As AI agents become more complex, understanding and explaining their decision-making processes is essential for building trust and accountability. — Future Trend: Development of", "source": "blog:medium.com", "published": "", "authors": "", "tokens": 512, "sha256": "14a559968231d021077028b43db24b5569a3e4fca439208f43ab4cf754cb8087"}
{"doc_id": "blog:medium.com#body:part-13", "url": "https://medium.com/@williamwarley/understanding-ai-agents-how-they-work-types-and-practical-applications-bd261845f7c3", "anchor": "#body:part-13", "type": "blog", "title": "", "section": "Body", "text": "Ethical and Responsible AI 1. Bias Mitigation: — Addressing biases in AI agents is crucial to ensure fairness and equity. Researchers are developing techniques to detect and mitigate biases in training data and algorithms. — Future Trend: Implementation of more advanced bias detection and mitigation methods to promote ethical AI practices. 2. Transparency and Explainability: — As AI agents become more complex, understanding and explaining their decision-making processes is essential for building trust and accountability. — Future Trend: Development of explainable AI (XAI) techniques that provide insights into how AI agents make decisions, enhancing transparency and user confidence. 3. Regulatory Compliance: — Ensuring that AI agents comply with regulatory standards and guidelines is vital for their safe and ethical deployment. — Future Trend: Establishment of comprehensive AI regulations and standards that guide the development and deployment of responsible AI agents. Human-Agent Collaboration 1. Human-in-the-Loop Systems: — Human-in-the-loop (HITL) systems involve human oversight and interaction with AI agents, combining human expertise with AI efficiency. — Future Trend: Increased adoption of HITL systems in critical applications such as healthcare, finance, and autonomous systems to ensure safe and effective operation. 2. Augmented Intelligence: — Augmented intelligence focuses on enhancing human capabilities with AI agents, rather than replacing humans. This approach leverages the strengths of both humans and AI. — Future Trend: Development of collaborative tools and platforms that empower humans to work alongside AI agents, improving productivity and decision-making. 3. Interactive Learning: — Interactive learning involves AI agents learning from direct interactions with humans, receiving feedback, and improving their performance. — Future Trend: Enhanced interactive learning frameworks that facilitate seamless and intuitive human-agent interactions, leading to more personalized and adaptive AI systems. Conclusion The future of AI agents is filled with exciting possibilities and challenges. Advances in reinforcement learning, integration with IoT, ethical considerations, and human-agent collaboration will drive the next wave of innovations in AI. By staying informed about these trends and developments, organizations and developers can harness the full potential of AI agents to create smarter, more efficient, and ethical solutions. This chapter explores the future trends and developments in AI agents, highlighting key advancements and their potential impact. Case Studies and Real-World Examples AI agents have made significant strides in various industries, solving complex problems and enhancing operational efficiency. This chapter presents several case studies and real-world examples to illustrate the successful application of AI agents in different domains. Case Study 1: Autonomous Vehicles Company: Waymo Challenge: Developing self-driving cars that can safely navigate complex urban environments and interact with other road users. Solution: Waymo uses AI agents to process data from sensors (LIDAR, cameras, radar) and make real-time driving decisions. Implementation: - Perception: AI agents use sensor data to detect and classify objects such as pedestrians, vehicles, and traffic signals. - Decision-Making: Agents apply machine learning algorithms to predict the behavior of other road users and plan safe driving maneuvers. - Action: The AI system controls the vehicle’s acceleration, braking, and steering to navigate through traffic. Outcome: Waymo’s self-driving cars have successfully logged millions of miles on public roads, demonstrating the", "source": "blog:medium.com", "published": "", "authors": "", "tokens": 512, "sha256": "710e7f0c045800c3fd24fbdc5e64995fad77bfcefa411fe865b25e936cc7400b"}
{"doc_id": "blog:medium.com#body:part-14", "url": "https://medium.com/@williamwarley/understanding-ai-agents-how-they-work-types-and-practical-applications-bd261845f7c3", "anchor": "#body:part-14", "type": "blog", "title": "", "section": "Body", "text": "sensors (LIDAR, cameras, radar) and make real-time driving decisions. Implementation: - Perception: AI agents use sensor data to detect and classify objects such as pedestrians, vehicles, and traffic signals. - Decision-Making: Agents apply machine learning algorithms to predict the behavior of other road users and plan safe driving maneuvers. - Action: The AI system controls the vehicle’s acceleration, braking, and steering to navigate through traffic. Outcome: Waymo’s self-driving cars have successfully logged millions of miles on public roads, demonstrating the potential of AI agents to enhance transportation safety and efficiency. Case Study 2: Healthcare Diagnostics Company: IBM Watson Health Challenge: Assisting doctors in diagnosing diseases and recommending personalized treatment plans based on vast amounts of medical data. Solution: IBM Watson for Oncology uses AI agents to analyze medical records, research papers, and clinical guidelines to provide evidence-based recommendations. Implementation: - Data Integration: AI agents aggregate and analyze data from electronic health records (EHRs), medical literature, and clinical trial results. - Natural Language Processing (NLP): Agents use NLP to interpret unstructured medical texts and extract relevant information. - Decision Support: The AI system suggests potential diagnoses and treatment options based on the latest medical evidence and patient-specific factors. Outcome: Watson for Oncology has been deployed in several hospitals worldwide, aiding oncologists in developing effective and personalized treatment plans, thus improving patient outcomes. Case Study 3: Financial Trading Company: BlackRock Challenge: Optimizing investment strategies and managing large portfolios with real-time market analysis and trading decisions. Solution: BlackRock’s Aladdin platform employs AI agents to analyze market data, assess risks, and execute trades autonomously. Implementation: - Market Analysis: AI agents continuously monitor and analyze financial news, market trends, and economic indicators. - Risk Management: Agents assess portfolio risks and suggest adjustments to optimize performance. - Automated Trading: The AI system executes trades based on predefined strategies and real-time market conditions. Outcome: Aladdin has enhanced BlackRock’s ability to manage assets efficiently, providing clients with optimized investment strategies and improved financial returns. Case Study 4: E-commerce Personalization Company: Amazon Challenge: Providing personalized shopping experiences to millions of customers by recommending relevant products. Solution: Amazon uses AI agents in its recommendation engine to analyze customer behavior and suggest products tailored to individual preferences. Implementation: - Data Collection: AI agents gather data on customer browsing history, purchase behavior, and product interactions. - Machine Learning: Agents use collaborative filtering and deep learning algorithms to identify patterns and preferences. - Personalized Recommendations: The AI system generates real-time product recommendations for each customer based on their unique profile. Outcome: Amazon’s recommendation engine significantly boosts customer engagement and sales, contributing to its status as a leading e-commerce platform. Case Study 5: Customer Service Automation Company: Bank of America Challenge: Enhancing customer service by providing instant, accurate, and personalized responses to customer inquiries. Solution: Bank of America implemented Erica, an AI-powered virtual assistant that helps customers with banking tasks and queries. Implementation: - Natural Language Understanding (NLU): Erica uses NLU to understand customer queries expressed in natural language. - Machine Learning: The virtual assistant learns from interactions to improve its responses and provide personalized assistance.", "source": "blog:medium.com", "published": "", "authors": "", "tokens": 512, "sha256": "87dfd521726c29e844071fef5a75c297aaf935faef6fdf236856f173130490ca"}
{"doc_id": "blog:medium.com#body:part-15", "url": "https://medium.com/@williamwarley/understanding-ai-agents-how-they-work-types-and-practical-applications-bd261845f7c3", "anchor": "#body:part-15", "type": "blog", "title": "", "section": "Body", "text": "a leading e-commerce platform. Case Study 5: Customer Service Automation Company: Bank of America Challenge: Enhancing customer service by providing instant, accurate, and personalized responses to customer inquiries. Solution: Bank of America implemented Erica, an AI-powered virtual assistant that helps customers with banking tasks and queries. Implementation: - Natural Language Understanding (NLU): Erica uses NLU to understand customer queries expressed in natural language. - Machine Learning: The virtual assistant learns from interactions to improve its responses and provide personalized assistance. - Integration: Erica integrates with the bank’s systems to access account information, perform transactions, and provide financial advice. Outcome: Erica handles millions of customer interactions, improving response times, reducing workload for human agents, and enhancing customer satisfaction. Case Study 6: Smart Home Management Company: Google Challenge: Creating a smart home ecosystem that automates household tasks and enhances convenience for users. Solution: Google Assistant uses AI agents to control smart home devices, manage schedules, and provide information. Implementation: - Voice Recognition: AI agents use speech recognition to understand voice commands from users. - Device Control: The assistant interacts with smart home devices (e.g., lights, thermostats, security systems) to execute commands. - Personalization: The AI system learns user preferences and routines to automate tasks and provide relevant information. Outcome: Google Assistant enhances the smart home experience, making it easier for users to manage their homes efficiently and conveniently. This chapter showcases successful applications of AI agents across various industries, demonstrating their versatility and impact. Key Insights and Final Recommendations As we conclude this comprehensive guide on AI agents, it is important to summarize the key insights and provide final recommendations for effectively leveraging AI agents in various applications and industries. Summary of Key Insights 1. Definition and Importance: — AI agents are autonomous entities that use AI to perceive their environment, make decisions, and perform actions to achieve specific goals. — They play a crucial role in automating tasks, enhancing decision-making, and improving operational efficiency across various domains. 2. How AI Agents Work: — AI agents operate based on core concepts such as perception, reasoning, action, and learning. — They can be categorized into different types, including simple reflex agents, model-based reflex agents, goal-based agents, utility-based agents, and learning agents. 3. Types of AI Agents: — Simple Reflex Agents: Operate based on predefined rules and immediate perception. — Model-Based Reflex Agents: Maintain an internal model of the environment to make informed decisions. — Goal-Based Agents: Make decisions based on predefined goals and desired outcomes. — Utility-Based Agents: Evaluate the utility of different actions to maximize overall satisfaction or performance. — Learning Agents: Continuously learn from experiences and adapt to new situations. 4. Applications of AI Agents: — AI agents are used in various industries, including autonomous vehicles, robotics, personal assistants, game AI, financial trading, and healthcare. — Their ability to automate tasks, make real-time decisions, and adapt to changing conditions makes them valuable assets in these fields. 5. Advantages of AI Agents: — Efficiency and Automation: Automate repetitive tasks, increase productivity, and reduce errors. — Scalability: Handle large-scale operations and adapt to varying workloads.", "source": "blog:medium.com", "published": "", "authors": "", "tokens": 512, "sha256": "1a01135065758864d0b7b4093740e0b41a2801a4de4dfc6955715649040054ec"}
{"doc_id": "blog:medium.com#body:part-16", "url": "https://medium.com/@williamwarley/understanding-ai-agents-how-they-work-types-and-practical-applications-bd261845f7c3", "anchor": "#body:part-16", "type": "blog", "title": "", "section": "Body", "text": "from experiences and adapt to new situations. 4. Applications of AI Agents: — AI agents are used in various industries, including autonomous vehicles, robotics, personal assistants, game AI, financial trading, and healthcare. — Their ability to automate tasks, make real-time decisions, and adapt to changing conditions makes them valuable assets in these fields. 5. Advantages of AI Agents: — Efficiency and Automation: Automate repetitive tasks, increase productivity, and reduce errors. — Scalability: Handle large-scale operations and adapt to varying workloads. — Real-Time Decision Making: Provide immediate responses and adapt strategies based on real-time data. — Adaptability and Learning: Improve performance over time and handle complex, dynamic environments. — Cost Efficiency: Reduce operational costs and optimize resource utilization. 6. Implementing AI Agents: — The implementation process involves defining objectives, selecting tools and frameworks, collecting and preprocessing data, training models, evaluating and testing, deploying, and maintaining AI agents. — Best practices include ensuring data quality, using appropriate evaluation metrics, designing for scalability and efficiency, and addressing security and privacy concerns. 7. Future Trends and Developments: — Advances in reinforcement learning, integration with IoT, ethical considerations, and human-agent collaboration are shaping the future of AI agents. — These trends promise to enhance the capabilities, applications, and ethical deployment of AI agents. 8. Case Studies and Real-World Examples: — Successful applications of AI agents in various industries highlight their practical benefits and impact. — Case studies demonstrate the versatility of AI agents in solving complex problems and improving operational efficiency. Final Recommendations 1. Stay Informed and Adaptable: — The field of AI agents is rapidly evolving. Stay informed about the latest developments, research, and best practices to leverage new opportunities and advancements. 2. Invest in Data Quality: — High-quality data is crucial for training effective AI agents. Ensure that your data is clean, representative, and accurately labeled. 3. Select the Right Tools and Frameworks: — Choose tools and frameworks that align with your specific requirements and technical expertise. Consider factors such as scalability, ease of use, and community support. 4. Focus on Ethical and Responsible AI: — Address ethical considerations, including bias mitigation, transparency, and regulatory compliance. Implement robust measures to ensure the responsible deployment of AI agents. 5. Optimize for Scalability and Efficiency: — Design AI agents to scale efficiently and handle varying workloads. Use cloud-based platforms and distributed computing to optimize performance and costs. 6. Continuous Monitoring and Improvement: — Continuously monitor the performance of AI agents and make necessary updates or retrain models to maintain their effectiveness. Stay proactive in addressing any issues that arise. 7. Leverage Human-Agent Collaboration: — Implement human-in-the-loop systems and augmented intelligence approaches to combine human expertise with AI efficiency. Facilitate seamless collaboration between humans and AI agents. 8. Explore Diverse Applications: — Explore various applications of AI agents across different industries. Identify opportunities where AI agents can provide significant value and drive innovation. By following these recommendations and leveraging the insights provided in this guide, you can effectively implement and benefit from AI agents in your projects and applications. Feel free to reach out if you have any", "source": "blog:medium.com", "published": "", "authors": "", "tokens": 512, "sha256": "2a72235b86b580bb9e7d773e070d2eb8eec239b6d684cb5f07294436c935ed52"}
{"doc_id": "blog:medium.com#body:part-17", "url": "https://medium.com/@williamwarley/understanding-ai-agents-how-they-work-types-and-practical-applications-bd261845f7c3", "anchor": "#body:part-17", "type": "blog", "title": "", "section": "Body", "text": "and augmented intelligence approaches to combine human expertise with AI efficiency. Facilitate seamless collaboration between humans and AI agents. 8. Explore Diverse Applications: — Explore various applications of AI agents across different industries. Identify opportunities where AI agents can provide significant value and drive innovation. By following these recommendations and leveraging the insights provided in this guide, you can effectively implement and benefit from AI agents in your projects and applications. Feel free to reach out if you have any questions or need further assistance with specific aspects of AI agents.", "source": "blog:medium.com", "published": "", "authors": "", "tokens": 91, "sha256": "adead498c82e56d7bf29fe8393130979553492f46dd1095812179a0bb1a85bd0"}
{"doc_id": "blog:www.amazon.science#body", "url": "https://www.amazon.science/publications/distributed-training-of-large-language-models-on-aws-trainium", "anchor": "#body", "type": "blog", "title": "", "section": "Body", "text": "--- title: Distributed training of large language models on AWS Trainium author: Xinwei Fu; Zhen Zhang; Haozheng Fan; Guangtai Huang; Randy Huang; Rahul Solanki; Fei Wu; Ron Diamant; Yida Wang url: https://www.amazon.science/publications/distributed-training-of-large-language-models-on-aws-trainium hostname: amazon.science description: Large language models (LLMs) are ubiquitously powerful but prohibitively expensive to train, often requiring thousands of compute devices, typically GPUs. To reduce the cost of training LLMs for customers, Amazon Web Services (AWS) launched the Amazon EC2 trn1 instances, powered by AWS Trainium,… sitename: Amazon Science date: 2025-10-16 --- Distributed training of large language models on AWS Trainium 2024 Large language models (LLMs) are ubiquitously powerful but prohibitively expensive to train, often requiring thousands of compute devices, typically GPUs. To reduce the cost of training LLMs for customers, Amazon Web Services (AWS) launched the Amazon EC2 trn1 instances, powered by AWS Trainium, Amazon’s homegrown deep-learning accelerator, as an alternative to distributed LLM training. The trn1 instances provide a high-performance LLM training solution at a lower cost compared to their GPU-based counterpart, the p4d instances, which are powered by Nvidia A100 GPUs. This paper describes the design and development of the Neuron Distributed Training Library, a component of the AWS Neuron SDK, which enables distributed training of large language models on AWS Trainium. Neuron Distributed Training Library supports a variety of existing distributed training techniques with unified interfaces, and provides features to address trn1-specific challenges as well. Our evaluation shows that trn1 instances, specifically the trn1.32xlarge, achieve better or comparable performance (up to 24.6% improvement) while offering significant lower costs (up to 46.3% cost saving) in selected workloads when compared to p4d.24xlarge instances. As a result, AWS Trainium has been adopted for training numerous external and internal models, showcasing its high-performance and cost effectiveness. Several supported open-source LLMs are accessible via HuggingFace Optimum Neuron. Research areas", "source": "blog:www.amazon.science", "published": "", "authors": "", "tokens": 300, "sha256": "a5ec3f94275fee5369b8b92bbd4659e5c2a1d4b9cd05784ed919eafc0ea59d14"}
{"doc_id": "blog:zhihaojia.medium.com#body:part-1", "url": "https://zhihaojia.medium.com/compiling-llms-into-a-megakernel-a-path-to-low-latency-inference-cf7840913c17", "anchor": "#body:part-1", "type": "blog", "title": "", "section": "Body", "text": "--- title: Compiling LLMs into a MegaKernel: A Path to Low-Latency Inference author: Zhihao Jia url: https://zhihaojia.medium.com/compiling-llms-into-a-megakernel-a-path-to-low-latency-inference-cf7840913c17 hostname: medium.com description: TL;DR: We developed a compiler that automatically transforms LLM inference into a single megakernel — a fused GPU kernel that performs… sitename: Medium date: 2025-06-19 --- Compiling LLMs into a MegaKernel: A Path to Low-Latency Inference TL;DR: We developed a compiler that automatically transforms LLM inference into a single megakernel — a fused GPU kernel that performs all necessary computation and communication in one launch. This end-to-end GPU fusion approach reduces LLM inference latency by 1.2-6.7x. Our compiler is easy to use — you can compile your LLM into a high-performance megakernel with just a few dozen lines of Python. What’s the key idea? Traditional LLM systems often rely on sequences of GPU kernel launches and external communication calls, resulting in underutilized hardware. Our compiler automatically fuses these operations — spanning multiple layers, iterations, and GPUs — into a megakernel. This design eliminates launch overhead, enables fine-grained software pipelining, and overlaps computation with communication across GPUs. Team members: Xinhao Cheng, Bohan Hou, Yingyi Huang, Jianan Ji, Jinchen Jiang, Hongyi Jin, Ruihang Lai, Shengjie Lin, Xupeng Miao, Gabriele Oliaro, Zihao Ye, Zhihao Zhang, Yilong Zhao, Tianqi Chen, Zhihao Jia One of the most effective ways to reduce latency in LLM inference is to fuse all computation and communication into a single megakernel — also known as a persistent kernel. In this design, the system launches just one GPU kernel to execute the entire model — from layer-by-layer computation to inter-GPU communication — without interruption. This approach offers several key performance advantages: - Eliminates kernel launch overhead, even in multi-GPU settings, by avoiding repeated kernel invocations; - Enables software pipelining across layers, allowing the kernel to begin loading data for the next layer while computing the current one; - Overlaps computation and communication, as a megakernel can simultaneously execute compute operations and inter-GPU communication to hide latency. Despite these advantages, compiling an LLM into a megakernel is highly challenging. Existing high-level ML frameworks — such as PyTorch, Triton, and TVM — do not natively support end-to-end megakernel generation. Additionally, modern LLM systems are built from a diverse collection of specialized kernel libraries: NCCL or NVSHMEM for communication, FlashInfer or FlashAttention for efficient attention, and CUDA or Triton for custom computation. This fragmentation makes it difficult to consolidate the entire inference pipeline into a single, unified kernel. Can we automate this process through compilation? Motivated by this question, our team from CMU, UW, Berkeley, NVIDIA, and Tsinghua developed Mirage Persistent Kernel (MPK) — a compiler and runtime system that automatically transforms multi-GPU LLM inference into a high-performance megakernel. MPK unlocks the benefits of end-to-end GPU fusion while requiring minimal manual effort from developers. Why MPK? A key advantage of MPK is extremely low latency for LLM inference by eliminating kernel launch overhead and maximally overlapping computation, data loading, and inter-GPU communication across layers. Figure 1 illustrates a performance comparison between MPK and existing LLM inference systems on both single- and multi-GPU configurations. On a single", "source": "blog:zhihaojia.medium.com", "published": "", "authors": "", "tokens": 512, "sha256": "aa38190950687c1e82b1cb7a1731f226dcf2489817e072148c639d22b0b0e2da"}
{"doc_id": "blog:zhihaojia.medium.com#body:part-2", "url": "https://zhihaojia.medium.com/compiling-llms-into-a-megakernel-a-path-to-low-latency-inference-cf7840913c17", "anchor": "#body:part-2", "type": "blog", "title": "", "section": "Body", "text": "and runtime system that automatically transforms multi-GPU LLM inference into a high-performance megakernel. MPK unlocks the benefits of end-to-end GPU fusion while requiring minimal manual effort from developers. Why MPK? A key advantage of MPK is extremely low latency for LLM inference by eliminating kernel launch overhead and maximally overlapping computation, data loading, and inter-GPU communication across layers. Figure 1 illustrates a performance comparison between MPK and existing LLM inference systems on both single- and multi-GPU configurations. On a single NVIDIA A100 40GB GPU, MPK reduces per-token decoding latency from 14.5 ms — as achieved by optimized systems like vLLM and SGLang — to 12.5 ms, approaching the theoretical lower bound of 10 ms (based on loading 16 GB of weights with 1.6 TB/s memory bandwidth). Beyond single-GPU optimization, MPK fuses computation and inter-GPU communication into a single megakernel. This design enables MPK to maximally overlap computation and communication. As a result, the performance improvements of MPK over current systems increase with the number of GPUs, making it particularly effective for multi-GPU deployments. What’s Next? The rest of this blog dives deeper into how MPK works: - Part 1 introduces the MPK compiler, which transforms an LLM’s computation graph into an optimized task graph; - Part 2 covers the MPK runtime, which executes this task graph within a megakernel to achieve high throughput and low latency. Part 1. The Compiler: Transforming an LLM into a Fine-Grained Task Graph The computation performed by a large language model (LLM) is typically represented as a computation graph, where each node corresponds to a compute operation (e.g., matrix multiplication, attention) or a collective communication primitive (e.g., all-reduce), and edges denote data dependencies between operations. In existing systems, each operator is generally executed via a dedicated GPU kernel. However, this kernel-per-operator execution model often fails to exploit pipelining opportunities, since dependencies are enforced at a coarse granularity — across entire kernels — rather than the actual data units. Consider a typical example: an allreduce operation following a matrix multiplication. In existing kernel-per-operator systems, the allreduce kernel must wait until the entire matmul kernel completes. In reality, though, each chunk of data for the allreduce only depends on a portion of the matmul output. This mismatch between logical and actual data dependencies limits the potential for overlapping computation and communication. To address this issue, MPK introduces a compiler that automatically transforms the LLM’s computation graph into a fine-grained task graph. This task graph explicitly captures dependencies at the sub-kernel level, enabling more aggressive pipelining across layers. In an MPK task graph: - Each task (shown as a rectangle in Figure 2) represents a unit of computation or communication assigned to a single GPU streaming multiprocessor (SM). - Each event (shown as a circle) represents a synchronization point between tasks. - Each task has an outgoing edge to a triggering event, which is activated once all associated tasks complete. - Each tasks also has an incoming edge from a dependent event, indicating the task can start execution as soon as the event is activated. Task graphs allow MPK to", "source": "blog:zhihaojia.medium.com", "published": "", "authors": "", "tokens": 512, "sha256": "a7d97b057695ff049a8cb657a271c6cd468ab1f4966fffc7efffe06f3bb20250"}
{"doc_id": "blog:zhihaojia.medium.com#body:part-3", "url": "https://zhihaojia.medium.com/compiling-llms-into-a-megakernel-a-path-to-low-latency-inference-cf7840913c17", "anchor": "#body:part-3", "type": "blog", "title": "", "section": "Body", "text": "in Figure 2) represents a unit of computation or communication assigned to a single GPU streaming multiprocessor (SM). - Each event (shown as a circle) represents a synchronization point between tasks. - Each task has an outgoing edge to a triggering event, which is activated once all associated tasks complete. - Each tasks also has an incoming edge from a dependent event, indicating the task can start execution as soon as the event is activated. Task graphs allow MPK to uncover pipelining opportunities that would be missed in computation graphs. For example, MPK can construct an optimized task graph where each allreduce task depends only on the corresponding matmul task that produces its input — enabling partial execution and overlap. In addition to generating an optimized task graph, MPK also automatically generates high-performance CUDA implementations for each task using the Mirage kernel superoptimizer. This ensures that each task runs efficiently on a GPU SM. (For more about the kernel superoptimizer, see this post.) Part 2. The Runtime: Executing a Task Graph in a MegaKernel MPK includes an on-GPU runtime system that executes the task graph entirely within a single GPU megakernel, allowing for fine-grained control over task execution and scheduling without any kernel launches during inference. To achieve this, MPK statically partitions all streaming multiprocessors (SMs) on a GPU into two roles: workers and schedulers. The number of worker and scheduler SMs is fixed at kernel launch time and matches the total number of physical SMs, avoiding any dynamic context switching overhead. Workers Each worker operates on an SM and maintains a dedicated task queue. It follows a simple but efficient execution loop: - Fetch the next task from its queue. - Execute the task (e.g., matrix multiplication, attention, or inter-GPU data transfers). - Notify the triggering event upon task completion. - Repeat. This design ensures that workers remain fully utilized while enabling task execution to proceed asynchronously across layers and operations. Schedulers Scheduling decisions are handled by MPK’s distributed schedulers, each of which runs on a single warp. Because each SM can accommodate multiple warps, up to four schedulers can run concurrently per SM. Each scheduler maintains a queue of activated events. It continuously: - Dequeues activated events whose dependencies are satisfied (i.e., all prerequisite tasks have completed). - Launches the set of tasks that depend on the activated event. This decentralized scheduling mechanism minimizes coordination overhead while enabling scalable execution across SMs. Event-Driven Execution Figure 3 illustrates MPK’s execution timeline. Each rectangle represents a task running on a worker; each circle represents an event. As a task completes, it increments the counter for its corresponding triggering event. When the event counter reaches a pre-defined threshold, the event is considered activated and is enqueued into a scheduler’s event queue. The scheduler then launches any downstream tasks that depend on this event. This design allows for fine-grained software pipelining and overlap between computation and communication. For example: - Matmul tasks can execute in parallel with attention tasks from different layers. - Allreduce communication can begin as soon as partial matmul results are", "source": "blog:zhihaojia.medium.com", "published": "", "authors": "", "tokens": 512, "sha256": "506b645a092d930507e1e4d2a5dc35423610c386b06ef44a8beb15a6ef4d1ed7"}
{"doc_id": "blog:zhihaojia.medium.com#body:part-4", "url": "https://zhihaojia.medium.com/compiling-llms-into-a-megakernel-a-path-to-low-latency-inference-cf7840913c17", "anchor": "#body:part-4", "type": "blog", "title": "", "section": "Body", "text": "the counter for its corresponding triggering event. When the event counter reaches a pre-defined threshold, the event is considered activated and is enqueued into a scheduler’s event queue. The scheduler then launches any downstream tasks that depend on this event. This design allows for fine-grained software pipelining and overlap between computation and communication. For example: - Matmul tasks can execute in parallel with attention tasks from different layers. - Allreduce communication can begin as soon as partial matmul results are available. Because all scheduling and task transitions occur within a single kernel context, the overhead between tasks is extremely low — typically just 1–2 microseconds — enabling efficient execution of multi-layer, multi-GPU LLM workloads. Looking Forward Our vision for MPK is to make megakernel compilation both easy to use and highly performant. Currently you can compile an LLM into a megakernel with just a few dozen lines of Python code — mainly to specify the megakernel’s inputs and outputs. We’re excited about this direction, and there’s still much more to explore. Some of the key areas we’re actively working on include: - Support for modern GPU architectures. One of our next milestones is extending MPK to support next-generation architectures such as NVIDIA Blackwell. A major challenge lies in integrating warp specialization — a key optimization for newer GPUs — with MPK’s megakernel execution model. - Handling workload dynamism. MPK currently builds a static task graph, which limits its ability to handle dynamic workloads such as mixture-of-experts (MoE) models. We’re developing new compilation strategies that allow MPK to support dynamic control flow and conditional execution inside megakernels. - Advanced scheduling and task assignment: MPK unlocks a new level of fine-grained scheduling at the task level. While our current implementation uses simple round-robin scheduling to distribute tasks across SMs, we see exciting opportunities in advanced scheduling policies — such as priority-aware or throughput-optimized strategies — for use cases like latency-SLO-driven serving or hybrid batching. We believe MPK represents a foundational shift in how LLM inference workloads are compiled and executed on GPUs, and we’re eager to collaborate with the community to push this vision forward. Want to Learn More? To learn more about MPK and explore our code and documentation, please visit our project website: https://github.com/mirage-project/mirage. We welcome feedback, contributions, and collaborations from the community!", "source": "blog:zhihaojia.medium.com", "published": "", "authors": "", "tokens": 383, "sha256": "46a386ef010ce6d11b54aac6ddcb9c3755d6855cc20c8ee3c34c304d34322605"}
{"doc_id": "blog:hazyresearch.stanford.edu#body:part-1", "url": "https://hazyresearch.stanford.edu/blog/2025-09-28-tp-llama-main", "anchor": "#body:part-1", "type": "blog", "title": "", "section": "Body", "text": "--- title: We Bought the Whole GPU, So We're Damn Well Going to Use the Whole GPU url: https://hazyresearch.stanford.edu/blog/2025-09-28-tp-llama-main hostname: stanford.edu sitename: hazyresearch.stanford.edu date: 2025-09-28 --- Sep 28, 2025 · 24 min read We Bought the Whole GPU, So We're Damn Well Going to Use the Whole GPU Benjamin Spector*, Jordan Juravsky*, Stuart Sul*, Dylan Lim, Owen Dugan, Simran Arora, Chris Ré Intro Post | Code | Low-Latency Megakernels | Brr TLDR: We're releasing a throughput-optimized megakernel for tensor-parallel inference with Llama-70B on H100s. Our kernel can aggressively overlap compute, memory, and communication ops in order to simultaneously use the different hardware resources available on a GPU. When integrated into the Tokasaurus inference engine, our megakernel can outperform SGLang by >22% on end-to-end throughput (measured as time to finish 65,536 prompts from the ShareGPT benchmark). We're releasing the code here; please be warned that this really is research code; it is sensitive to compiler versions, GPU setup, and sometimes even being looked at the wrong way, and we have no intention whatsoever of supporting it. We hope you'll find the ideas and results interesting nonetheless! Figure 1: Zoooommmm A few months ago, we showed how we could fuse an entire model forward pass into a single \"megakernel\" in order to deliver low-latency inference with Llama-1B. In that post, we teased that many of the same concepts we introduced would also be useful for optimizing for throughput. We're now excited to bring receipts and release a new megakernel optimized for high-throughput inference with Llama-70B. The inference workloads targeted by our low-latency and high-throughput megakernels are quite different and require distinct optimizations. Our low-latency megakernel targeted inference using Llama-1B when running on a single GPU with batch size one. This workload was entirely memory bound, and our focus was therefore on eliminating stalls that delayed loading model weights from global memory. With large-batch Llama-70B inference, our workload is much more heterogeneous. Large portions of it (e.g. matrix multiplies, attention prefill) are compute-bound. Other parts (e.g. attention decode, RMS norm) are still bottlenecked by global memory bandwidth. Additionally, by distributing our model across multiple GPUs, we now need to perform cross-GPU communication that throttles the NVLink connections between devices. By running these components sequentially, we've paid for the whole GPU, but are only using little bits and pieces of it at a time. :( Overall, these different operations in our model each make use of different resources available on the GPU (e.g. tensor cores, non-matmul compute units, HBM bandwidth, NVLink bandwidth) in unique ways. Therefore, a key area for optimizing this high-throughput workload is to overlap multiple kinds of work in order to simultaneously use more of the GPU's resources. We want to do this across many levels of the GPU -- within an individual SM, across multiple SMs, and even across GPUs. Existing approaches to overlapping include assigning different SMs to different ops, developing custom kernels to run prefill and decode simultaneously, and running kernels in parallel with cross-gpu memory copying operations. Here, we show that the same simple, interpreter-based megakernel patterns we previously", "source": "blog:hazyresearch.stanford.edu", "published": "", "authors": "", "tokens": 512, "sha256": "1dbe7437f7df8a8a62be2418f7044545666d592f5024b8e7de7b28fc03412043"}
{"doc_id": "blog:hazyresearch.stanford.edu#body:part-2", "url": "https://hazyresearch.stanford.edu/blog/2025-09-28-tp-llama-main", "anchor": "#body:part-2", "type": "blog", "title": "", "section": "Body", "text": "overlap multiple kinds of work in order to simultaneously use more of the GPU's resources. We want to do this across many levels of the GPU -- within an individual SM, across multiple SMs, and even across GPUs. Existing approaches to overlapping include assigning different SMs to different ops, developing custom kernels to run prefill and decode simultaneously, and running kernels in parallel with cross-gpu memory copying operations. Here, we show that the same simple, interpreter-based megakernel patterns we previously introduced can also achieve all of these fine-grained overlapping patterns -- and more! Most excitingly, despite the significant differences between our low-latency and high-throughput workloads, our core megakernel abstraction (a pipelined instruction interpreter that runs on each SM) is highly transferable across both domains. In the rest of this blog, we will: - Give a brief recap on the design of our megakernels from our last, low-latency post. - Walk through the details of the tensor-parallel Llama forward pass that we map into our megakernel, including a novel approach to communicating intermediate results across GPUs right after running attention. This new operation requires a complicated multi-GPU transpose not efficiently expressable with standard communication patterns, but is trivial to implement within the megakernel! - Show how megakernels can achieve fine-grained resource overlapping at multiple levels of the GPU hierarchy: within individual SMs, across multiple SMs, and across multiple GPUs! - Within individual SMs, the same inter-instruction pipelining we used in low-latency llama can also help keep overlap memory movement and compute across instructions, thereby keeping the tensor cores running. - Across multiple SMs, careful scheduling of instructions can overlap both compute-intensive (e.g. matrix multiply) and memory-intensive (e.g. RMS norm) kinds of work at once, on an individual GPU. - Across GPUs, we can hide communication costs within special \"storer\" threads, leaving other threads free to do work on the next instruction while communication happens in the background. - Finally, we put it all together by benchmarking our megakernel against vLLM and SGLang. Megakernels: A Brief Recap In our last post, we wrote our first full-model megakernel in order to optimize a low-latency scenario: running inference with Llama-3.2-1B and batch size one. We discovered that popular inference engines like vLLM and SGLang were only using about half of the available GPU bandwidth on an H100. The problem is that traditional systems break down model forward passes into dozens or hundreds of separate kernels, each with setup and teardown periods where no useful work gets done. These overhead periods create \"memory pipeline bubbles\" where an SM (i.e. a streaming multiprocessor, one of the compute subunits on a GPU) sits idle instead of loading model weights. Our solution to this was to merge the entire Llama-1B forward pass into a single fused \"megakernel\" that eliminates kernel boundaries altogether. We found that on small models, our megakernel could provide per-user throughput around 50% higher than inference frameworks like SGLang and vLLM. The core abstraction behind our megakernel lays in an instruction-and-interpreter model. - Instructions: Instead of decomposing a model forward pass into a series of coarse-grained kernels,", "source": "blog:hazyresearch.stanford.edu", "published": "", "authors": "", "tokens": 512, "sha256": "a45c8729d48f3fe1d877360134a66a2f1fd2069d3e12c3475bce13dcb32fcd68"}
{"doc_id": "blog:hazyresearch.stanford.edu#body:part-3", "url": "https://hazyresearch.stanford.edu/blog/2025-09-28-tp-llama-main", "anchor": "#body:part-3", "type": "blog", "title": "", "section": "Body", "text": "on a GPU) sits idle instead of loading model weights. Our solution to this was to merge the entire Llama-1B forward pass into a single fused \"megakernel\" that eliminates kernel boundaries altogether. We found that on small models, our megakernel could provide per-user throughput around 50% higher than inference frameworks like SGLang and vLLM. The core abstraction behind our megakernel lays in an instruction-and-interpreter model. - Instructions: Instead of decomposing a model forward pass into a series of coarse-grained kernels, we instead decomposed it into a sequence of fine-grained instructions. Instructions can have distinct types, loosely corresponding to the kinds of kernels one would use in conventional implementations (e.g. matrix multiply, attention prefill, RMS norm). Each instruction specifies a unit of work that would traditionally be performed by a thread block, e.g. compute an output tile for a matrix multiplication. Furthermore, each instruction is organized into dedicated sections, e.g. a load function that reads from global memory, a compute function, and a store function that writes out results. - Interpreter: We execute these instructions using an on-GPU interpreter. When the megakernel launches, each SM initializes an interpreter and starts executing a sequence of instructions (these sequences are scheduled into per-SM queues ahead of time). A key feature of these interpreters is that they can aggressively pipeline across instruction boundaries, starting tasks for the next instruction (e.g. loading model weights) while the current instruction finishes. For our low-latency megakernel, this let us eliminate most of the memory bubbles between operations. For more details on the interpreter design (e.g. how we manage shared memory across instructions, how we synchronize different SMs), see the original blog post and the codebase. In this blog, we'll focus on a high-throughput workload with very different performance considerations than our previous Llama-1B target. However, as we'll describe below, this same core instruction/interpreter abstraction will be extremely helpful for achieving high throughput. THE WORKLOAD Anatomy of a Llama First, we'll start with a brief walkthrough of the operations needed to perform a large-batch forwards pass using tensor-parallel Llama-70B. Specifically, we implement the \"sequence parallel\" variant of TP, where some operations are performed data-parallel (i.e. each GPU holds full activation vectors for a slice of the tokens in a batch) and some operations are performed tensor-parallel (i.e. each GPU holds a slice of the activation vectors for all tokens). Concretely, with sequence parallelism each transformer block receives a data-parallel chunk of the hidden states (i.e. the full hidden states for a subset of tokens) as input and performs the following operations: - Data-parallel pre-attention RMS norm. - All-gather (i.e. each GPU collects the activations from all other GPUs, so that each GPU now has the activations from all tokens). - Tensor-parallel QKV projections, attention, and O projection (each GPU is responsible for a subset of attention heads). - Reduce-scatter. - Data-parallel post-attention residual connection and pre-MLP RMS norm. - All-gather again. - Tensor-parallel MLP. - Reduce-scatter. - Post-MLP residual connection. However, we've made one important change to this formulation. One of our targets for operation overlapping is to overlap the O", "source": "blog:hazyresearch.stanford.edu", "published": "", "authors": "", "tokens": 512, "sha256": "c3610180da8142a86c6da8fa64158b3111dd241530c346247f43b476451127f8"}
{"doc_id": "blog:hazyresearch.stanford.edu#body:part-4", "url": "https://hazyresearch.stanford.edu/blog/2025-09-28-tp-llama-main", "anchor": "#body:part-4", "type": "blog", "title": "", "section": "Body", "text": "collects the activations from all other GPUs, so that each GPU now has the activations from all tokens). - Tensor-parallel QKV projections, attention, and O projection (each GPU is responsible for a subset of attention heads). - Reduce-scatter. - Data-parallel post-attention residual connection and pre-MLP RMS norm. - All-gather again. - Tensor-parallel MLP. - Reduce-scatter. - Post-MLP residual connection. However, we've made one important change to this formulation. One of our targets for operation overlapping is to overlap the O projection matrix multiplication with the subsequent reduce-scatter. However, the tensor-parallel sharded O matrix is too small for us to effectively hide the reduce-scatter communication cost. To solve this, we instead choose to replicate the O projection matrix across each GPU and run the O projection with data parallelism instead of with tensor parallelism. Alongside this change, we eliminate the post-attention reduce-scatter and replace it with a \"distributed transpose\" operation after attention that repartitions our data from a tensor-parallel configuration to a data-parallel configuration. When using 8 GPUs, this reduces the network traffic by a factor of 8, which makes it much easier to hide the cost of communication cost by overlapping it with matrix multiplications. Note that the downside of this approach is it reduces the maximum batch size by about 15%, because replicating the O projection weights consumes an additional 9 GB of memory per GPU. Defining our Megakernel Instruction Set With our parallelism scheme decided on, we are able to construct the instruction set for our high-throughput megakernel. We partition our workload into the following fused instructions: - RMS norm + all-gather. - A QKV matrix multiply + RoPE. - Attention + distributed transpose. - O-projection matrix multiply + residual. - Gate matrix multiply + SiLU. - Up-projection matrix multiply + elementwise multiplication with the output of the gate. - Down matrix multiply + reduce-scatter + residual. - RMS norm without the all-gather (for the LM head) - LM head matrix multiply. Relative to our latency-focused Llama-1B megakernel, this instruction set contains several high-level changes in our approach: - Most of our instructions for low-latency centered around matrix-vector multiplication, rather than the matrix-matrix multiplications we do here. The optimal work partitioning for these two operations is generally completely different. For matrix-vector products, each instruction computes several complete columns of the output vector. However, in matrix-matrix products, each instruction computes a tile of the output matrix instead. - To avoid extra trips to global memory, for our low-latency megakernel we frequently recomputed results across the GPU rather than communicating them through memory. This allowed us to fuse operations more aggressively than usual. For example, our QKV matrix-vector product fused the RMS norm into its beginning, saving an instruction boundary, but performing identical RMS computations over a hundred times. When focusing on throughput, this recomputation is not worth it. - Putting many tokens in our batch requires us to expand our inter-instruction signalling scheme to track data dependencies across tokens. This signalling can vary by instruction -- for some operations (e.g. attention, RMS norms), sometimes synchronization is at the granularity of 128", "source": "blog:hazyresearch.stanford.edu", "published": "", "authors": "", "tokens": 512, "sha256": "a10b1f9797f169a4f85905861936f137ca67c6976a707ef8888ceb4d2ce351e4"}
{"doc_id": "blog:hazyresearch.stanford.edu#body:part-5", "url": "https://hazyresearch.stanford.edu/blog/2025-09-28-tp-llama-main", "anchor": "#body:part-5", "type": "blog", "title": "", "section": "Body", "text": "usual. For example, our QKV matrix-vector product fused the RMS norm into its beginning, saving an instruction boundary, but performing identical RMS computations over a hundred times. When focusing on throughput, this recomputation is not worth it. - Putting many tokens in our batch requires us to expand our inter-instruction signalling scheme to track data dependencies across tokens. This signalling can vary by instruction -- for some operations (e.g. attention, RMS norms), sometimes synchronization is at the granularity of 128 output rows of a matrix; other times it's at the granularity of an individual attention head for an individual token. Overlapping Resources Within a Megakernel Our primary goal when optimizing forward pass for throughput is to overlap hardware resources (e.g. use as much GPU memory bandwidth, compute units, and interconnect bandwidth as possible). Below, we show that our megakernel allows us to do this at three levels of hierarchy: within an SM by overlapping the stages of different instructions, across SMs by running different instructions, and across GPUs by overlapping communication with other work. Overlapping within the SM Within individual SMs, we make use of our megakernel template's instruction pipeline (previously described here) to overlap loading weights and activations for the next instruction with performing compute for the previous instruction. Even though our objective is now throughput, it's still useful to be able to start loading the next data in advance -- to keep matrix multiplies running as quickly as possible. Within the SM, our interpreter specializes threads to different functions: each of the load, compute, and store functions within the instruction template is executed by its own, independent set of threads. This means that even while a compute or store is running, the loads for the next matrix multiplies can start as soon as possible. To help understand this, we've written profiling tools that make it easier to see what's going on here. Figure 2, below, shows the brief transition between two different kinds of instructions on a single SM -- the Gate SiLU instruction (brown) and the Up matmul instruction (pink). If you'd like to build a better intuition for the profiler, you can access it here, alongside an example profile to download and play with. Figure 2: A zoomed-in snapshot of a single SM across about 15 microseconds, as it transitions from a Gate SiLU instruction to an Up matmul instruction. First, a quick tutorial on how to read what's going on in this zoomed-in profile snapshot: - The three horizontal tracks represent different kinds of threads within the interpreter that runs instructions. - At the top are the loader threads, which pull data from global memory into shared memory. - The thick band in the middle represents consumer threads that perform the main work -- in this case, running the tensor cores. - The bottom row tracks storer threads, which store results from shared memory back up to global memory. - Finally, in the background are controller threads that help coordinate the interpreter, although they're not programmed by the user. - Different colored bars represent different kinds of", "source": "blog:hazyresearch.stanford.edu", "published": "", "authors": "", "tokens": 512, "sha256": "ebce6ed8d9840b353cb6122ee61e39db3bd086285e87adb650fe482513f5ae09"}
{"doc_id": "blog:hazyresearch.stanford.edu#body:part-6", "url": "https://hazyresearch.stanford.edu/blog/2025-09-28-tp-llama-main", "anchor": "#body:part-6", "type": "blog", "title": "", "section": "Body", "text": "loader threads, which pull data from global memory into shared memory. - The thick band in the middle represents consumer threads that perform the main work -- in this case, running the tensor cores. - The bottom row tracks storer threads, which store results from shared memory back up to global memory. - Finally, in the background are controller threads that help coordinate the interpreter, although they're not programmed by the user. - Different colored bars represent different kinds of instructions. In this case, the brown bars on the left correspond to the Gate SiLU instruction, and the pink bars on the right correspond to the Up matmul instruction. - Thin vertical lines represent different kinds of events. For example, blue and cyan lines correspond to different kinds of loads being issued. Purple lines represent the beginning of a compute phase, and yellow and orange lines correspond to different kinds of stores. Finally, red lines represent wait events, and green lines represent ready states. In general, we have instructions only report events from the first 8 and last 4 stages, since we have limited timing slots. - Tall vertical lines in the background represent events happening within the controller warp of the interpreter. The salmon line tells the SM to fetch the next instruction, the pale green line indicates that the next instruction is set up, and the white line indicates the last instruction has finished and can now be torn down. Here's a complete timeline of this little snapshot. | Time | Threads | Action | |---|---|---| | 4234.78 μs | Loader | Starts issuing loads for last four stages of matrix multiply pipeline (128 x 64 x 256). Dark blue lines = A matrix loads, cyan lines = B matrix loads (300ns later) | | + 0.51 μs | Loader | Signals controller to begin setting up next instruction (after fourth-to-last load) | | + 2.53 μs | Consumer | Begins running fourth-to-last matrix multiply (associated with first load) | | + 3.23 μs | Controller | Finishes setting up next Up matmul instruction, entailing fetching and decoding instruction, setting up semaphores, and remapping shared memory pages. | | + 3.68 μs | Loader | Begin running dependency check before loading inputs. | | + 5.25 μs | Consumer | Finish running matrix multiplies, begin storing results into two unreleased shared memory pages | | + 5.31 μs | Loader | Resolves dependency check, and issues loads for first 2.5 stages, before stalling due to lack of available shared memory pages. | | + 8.16 μs | Storer | Receives results, launches asynchronous store to global memory. Consumer threads start Up matmul matrix multiplies while shared memory still in use | | + 8.64 μs | Consumer | Begins running matrix multiplies on Up matmul instruction. | | + 9.09 μs | Storer | First store finishes reading from shared memory, releases one page. Loader threads restart load pipeline. | | + 10.18 μs | Storer | Final store finishes reading from shared memory, releases last page to Up matmul", "source": "blog:hazyresearch.stanford.edu", "published": "", "authors": "", "tokens": 512, "sha256": "705296739f5582d9fcc0be34bb6817d99fee77d7ce1285ae41b897a294df231c"}
{"doc_id": "blog:hazyresearch.stanford.edu#body:part-7", "url": "https://hazyresearch.stanford.edu/blog/2025-09-28-tp-llama-main", "anchor": "#body:part-7", "type": "blog", "title": "", "section": "Body", "text": "results, launches asynchronous store to global memory. Consumer threads start Up matmul matrix multiplies while shared memory still in use | | + 8.64 μs | Consumer | Begins running matrix multiplies on Up matmul instruction. | | + 9.09 μs | Storer | First store finishes reading from shared memory, releases one page. Loader threads restart load pipeline. | | + 10.18 μs | Storer | Final store finishes reading from shared memory, releases last page to Up matmul instruction. Up matrix multiply pipeline completely unblocked for the rest of the instruction. | | + 12.13 μs | Storer | Asynchronous stores to global memory complete. Atomically increments flag in global memory signifying instruction completion. | | + 12.38 μs | Controller | Notified all threads completed instruction work. Begins teardown, invalidating previous instruction semaphores, and writing timing data to global memory. | Now let's contrast this with a snapshot with the instruction pipeline disabled. Figure 3: The exact same profile, but with no inter-instruction pipelining. In this ablated profile, the store must finish, instruction be torn down, next instruction set up, and memory loaded, before matrix multiplies can begin again. Whereas with instruction pipelining enabled, the extra gap between consecutive matrix multiply stages is just 3.4 microseconds, without the pipeline, this gap jumps to 10.2 microseconds -- meaning this optimization alone reduces runtime by over 7% on these instructions. Nor is this effect isolated to the boundary of these two particular instructions; it shows up everywhere. In Figure 4, we take a look at some zoomed out profiles showing all 8 GPUs, and we'll use similar profiles for other ablations in the rest of this post, so it's worth understanding this profile well. Figure 4: Block profiles of pipelined versus serial instruction execution. Serial execution has a lot more gaps! What we're looking at here represents a little over two full transformer blocks of a Llama-70B forward pass with a batch size of 8,192, across all 8 GPUs. Unlike the zoomed-in view, where each SM separates into three separate bars, each horizontal bar here just represents the activity of the consumers for that SM. And, as before, each different color represents a different kind of instruction. From left to right: - Blue represents the Attention RMS norm and all-gather. - Orange represents the QKV matrix multiply. - Green represents the attention and inter-GPU transpose. - Red represents the O-projection matrix multiply. - Purple represents the MLP RMS norm and all-gather. - Brown represents the Gate SiLU. - Pink represents the Up matrix multiply. - Grey represents the Down projection matrix multiply. We also lightly shade the background to make it easier to distinguish SMs on different GPUs: if you look closely you should see light color bands to help make this visible. In the case of figure 4, the pipelined profile runs at 31,516 decoding TPS for this particular test workload (Prefill \"tell me a funny joke about cookies\", 30 decode tokens -- we have absolutely cornered the market on efficiently generating short cookie jokes), whereas the serial profile runs", "source": "blog:hazyresearch.stanford.edu", "published": "", "authors": "", "tokens": 512, "sha256": "ac397fd353dfbfe7774fa8c6ec86c27791d082973aec2416f557ef74f0a5d5d8"}
{"doc_id": "blog:hazyresearch.stanford.edu#body:part-8", "url": "https://hazyresearch.stanford.edu/blog/2025-09-28-tp-llama-main", "anchor": "#body:part-8", "type": "blog", "title": "", "section": "Body", "text": "multiply. We also lightly shade the background to make it easier to distinguish SMs on different GPUs: if you look closely you should see light color bands to help make this visible. In the case of figure 4, the pipelined profile runs at 31,516 decoding TPS for this particular test workload (Prefill \"tell me a funny joke about cookies\", 30 decode tokens -- we have absolutely cornered the market on efficiently generating short cookie jokes), whereas the serial profile runs at just 29,607 TPS, corresponding to a difference of over 6%. This difference turns out to persist across different batch sizes, and generally provides around 2-6% end to end MFU, as shown in the table below. | Batch size | Best config | Best config minus pipelining | % Difference | |---|---|---|---| | 1024 | 18,676 | 18,201 | 2.5% | | 2048 | 26,388 | 24,641 | 6.6% | | 4096 | 29,214 | 28,426 | 2.7% | | 8192 | 31,516 | 29,607 | 6.1% | Table 1: Decoding TPS Generating Short Cookie Jokes at Various Batch Sizes, with and without Pipelining As a final note of this section, relating to the profiling itself, one might reasonably ask what overhead generating these plots incurs. It turns out to be very little: across 32 separate experiments we ran in the course of writing this post, we measured each runtime with and without generating timing data, each of which is a separate compilation path. We found the average difference to be just 0.39%, with a maximum of 1.07%. So, although timing may introduce a small amount of distortion, we think that this data is overall quite reliable. All TPS numbers are reported without timing recording enabled. Overlapping across SMs With our low-latency megakernel, each SM was assigned its own queue of instructions that are scheduled in advance. Instead, for our high-throughput megakernel, we create a global work queue -- a single list of instructions that defines all the work that needs to run on the GPU. When an SM needs to fetch a new instruction to run, it atomically increments a global instruction counter that keeps track of the next instruction to be assigned. This approach is automatically robust to jitter in the execution across different SMs; if one SM is slow to finish its instruction relative to others, it will simply delay its request for new work, allowing other SMs to pick up the slack. This solution wasn't possible for our low-latency megakernel, because the runtime of each instruction was so fast that the latency of this atomic increment would be prohibitive. But with a throughput-oriented megakernel -- where individual instructions frequently take 100 microseconds or more -- this cost can be entirely hidden as part of our instruction pipeline. Figure 5: Ablating the global work queue. On top, we use the global work queue. On the bottom, we use a simple round robin scheduler to assign work. The global work queue effectively smooths out variances in runtime that are present in the round-robin scheduler. In figure 5, we ablate the", "source": "blog:hazyresearch.stanford.edu", "published": "", "authors": "", "tokens": 512, "sha256": "f630e4e15cd9ac52b223a16b75de6631b5d8229967ceeb4fda1e5bc8428c3300"}
{"doc_id": "blog:hazyresearch.stanford.edu#body:part-9", "url": "https://hazyresearch.stanford.edu/blog/2025-09-28-tp-llama-main", "anchor": "#body:part-9", "type": "blog", "title": "", "section": "Body", "text": "prohibitive. But with a throughput-oriented megakernel -- where individual instructions frequently take 100 microseconds or more -- this cost can be entirely hidden as part of our instruction pipeline. Figure 5: Ablating the global work queue. On top, we use the global work queue. On the bottom, we use a simple round robin scheduler to assign work. The global work queue effectively smooths out variances in runtime that are present in the round-robin scheduler. In figure 5, we ablate the global work queue by replacing it with a simple round-robin scheduler, and find a 14.2% end-to-end reduction in performance at a batch size of 8,192. A broader report is provided in table 2. | Batch size | Best config | Best config without GWQ | % Difference | |---|---|---|---| | 1024 | 18,320 | 18,676 | -1.9% | | 2048 | 26,388 | 25,518 | 3.3% | | 4096 | 29,214 | 27,372 | 6.3% | | 8192 | 31,516 | 27,033 | 14.2% | Table 2: Decoding TPS Generating Short Cookie Jokes at Various Batch Sizes, with and without the Global Work Queue As one can see from table 2, the global work queue becomes useful at large batch sizes, where there is enough that work that jitter across SM's becomes important, and eliminating that jitter with dynamic scheduling makes a big difference. At very small batch sizes, the overhead in the global work queue actually outweighs its benefit. Finally, it's worth noting that the global work queue is not the only way to improve scheduling over a naive round-robin scheduler; many other schedulers might work well. However, static schedulers cannot adapt to runtime jitter in the same way that the global work queue does; the variance in runtime across GPUs in figure 5 (despite them having nearly identical schedules) suggests that this jitter is a major factor. Overlapping across GPUs Networking Background In order to implement our tensor-parallel Llama, we need to be able to exchange data between GPUs. In general, NVIDIA gives us two ways to do this. One common approach is to use the GPU's copy engines -- dedicated hardware for copying big, contiguous chunks of memory within or across devices. One advantage of using the copy engines is that these copies don't need to run within a kernel, freeing up the GPU's SMs to do other useful work! By using multiple cuda streams, we can launch copy engine operations that overlap with kernel computations (e.g. as is done in PyTorch's AsyncTP). The other way to transfer data between GPUs is to do so within a kernel, using the GPU's SMs to write data into remote memory on other GPUs via CUDA's unified memory architecture (thanks Bill!) We've added a corresponding new abstraction to ThunderKittens called the Parallel Global Layout (PGL). With PGLs, we perform asynchronous loads and stores directly to global memory on other devices, overlapping them with compute and local memory operations to achieve near-zero cost. We also leverage NVSwitch's central accelerator to offload collective operations to hardware outside the GPU. Read more about PGLs and", "source": "blog:hazyresearch.stanford.edu", "published": "", "authors": "", "tokens": 512, "sha256": "fe4ce6c4036261b88a5875baacecd4e1efc77b0dff65110630f1a5c81e935015"}
{"doc_id": "blog:hazyresearch.stanford.edu#body:part-10", "url": "https://hazyresearch.stanford.edu/blog/2025-09-28-tp-llama-main", "anchor": "#body:part-10", "type": "blog", "title": "", "section": "Body", "text": "the GPU's SMs to write data into remote memory on other GPUs via CUDA's unified memory architecture (thanks Bill!) We've added a corresponding new abstraction to ThunderKittens called the Parallel Global Layout (PGL). With PGLs, we perform asynchronous loads and stores directly to global memory on other devices, overlapping them with compute and local memory operations to achieve near-zero cost. We also leverage NVSwitch's central accelerator to offload collective operations to hardware outside the GPU. Read more about PGLs and our multi-GPU approach in our earlier blog post. In our megakernel we use the second approach, because it gives us the control we need to perform all-gathers, reduce-scatters, and our post-attention distributed transpose (which allows us to do the O-projection in data-parallel form). We perform all communication from our dedicated storer threads, allowing loader and compute threads to move onto future work while inter-GPU communication is performed in the background on the same SM. Interleaving Warp specialization, instruction pipelining, and the global work queue now give us a way to overlap different hardware resources within an SM. However, we can further benefit from GPU resource overlapping if we can assign different types of instructions to different SMs. For example, with a large batch size, tokens at the beginning of the batch will have their compute-bound Down projections completed earlier than tokens at the end of the batch. This makes these early tokens ready to start the next instruction, which is the network-bound pre-attention RMS norm and all-gather. If we run that RMS norm and all-gather for these early tokens on some SMs, while computing the Down projection for the later tokens on different SMs, we can reduce peak network bandwidth and better exploit the hardware resources available on the device. Some prior work, like NanoFlow, implemented this technique on A100s by constructing a schedule ahead-of-time that assigns SMs to different groups, namely compute-focused SMs that compute matmul-heavy ops, memory-focused instructions that compute attention decoding, and comms-focused SMs that communicate results across devices. With our megakernel, we can perform this overlapping at a much finer granularity by interleaving instructions from different ops into our global work queue. Once we have scheduled enough Down projection instructions for some tokens to be ready for attention, we can start interleaving RMS norm instructions while we add the remaining Down projection instructions into our schedule. This interleaving lets different SMs run different kinds of work without needing to explicitly assign SMs to groups like in NanoFlow. Figure 6: Ablating interleaving. On top, we use our standard interleaved schedule. On the bottom, we disable interleaving. Notice how the two RMS norms (purple and blue) are no longer quilted the same way, but instead take up 200-300 microseconds each time they come up. (Note that although QKV and Gate SiLU instructions may begin early, they rely on many RMS norms to satisfy their dependencies, and usually start after all of the RMS norm instructions are finished.) The effect of the interleaving is easy to see in our profiles, since it creates \"quilts\" (i.e. dense multi-colored regions where different SMs", "source": "blog:hazyresearch.stanford.edu", "published": "", "authors": "", "tokens": 512, "sha256": "3252547c7a8cbe43a8dd436e2156bdf669ffc0bc152efc034f3648ce9c21f039"}
{"doc_id": "blog:hazyresearch.stanford.edu#body:part-11", "url": "https://hazyresearch.stanford.edu/blog/2025-09-28-tp-llama-main", "anchor": "#body:part-11", "type": "blog", "title": "", "section": "Body", "text": "two RMS norms (purple and blue) are no longer quilted the same way, but instead take up 200-300 microseconds each time they come up. (Note that although QKV and Gate SiLU instructions may begin early, they rely on many RMS norms to satisfy their dependencies, and usually start after all of the RMS norm instructions are finished.) The effect of the interleaving is easy to see in our profiles, since it creates \"quilts\" (i.e. dense multi-colored regions where different SMs are running different kinds of instructions at a given point in time). | Batch size | Best config | Best config minus interleaving | % Difference | |---|---|---|---| | 1024 | 18,676 | 18,663 | 0.1% | | 2048 | 26,388 | 26,388 | 0.0% | | 4096 | 29,214 | 28,520 | 2.4% | | 8192 | 31,516 | 29,492 | 6.4% | Table 3: Decoding TPS Generating Short Cookie Jokes at Various Batch Sizes, with and without Interleaving As one can see from table 3, interleaving also kicks in at large batch sizes (like the global work queue), where there is enough work on the GPU to allow several waves of instructions of each type, and therefore the effective interleaving of those waves. At these large batch sizes, it's just as important of an optimization as intra-SM pipelining. The key advantage of our approach is that it eliminates the overhead of additional kernel launches and cross-stream synchronization. We also note that we've removed our reliance on NCCL entirely, continuing in our march towards Obadiah Stane's paragon of minimal dependencies. Of course, there's been an astounding amount of work attempting to hide communication overhead alongside compute -- it was surprising to us how straightforward it was to overlap communication within the megakernel framework! REEEEE(sults) To evaluate our megakernel, we integrated it into Tokasaurus, which helps schedule batches of prefill and decode, alongside KV pages. This also allows us to schedule megakernel instructions on the CPU while the previous batch is running on the GPUs; with 64 threads generating instructions in C++, we generally find >90% CPU idle time. For our benchmark, we sampled a set of 65,536 prompts + completion lengths from the ShareGPT dataset, and ran them through both SGLang and our Megakernel. We reproduced SGLang using their recommended benchmarking settings; nonetheless, we recognize that expert tuning is sensitive and important. We report the input, output, and total throughputs in Table 4 (a more precise restatement of the results from Figure 1). | System | Input Throughput (Tokens/s) | Output Throughput (Tokens/s) | Total Throughput (Tokens/s) | |---|---|---|---| | SGLang | 11,783 | 7,387 | 19,170 | | Megakernel | 14,425 | 9,043 | 23,468 | Even despite these promising initial results, we suspect there's still considerable room for optimization within megakernels. Our scheduling heuristics are quite simple, our instructions frequently stall on synchronizations that could likely be hidden better, and our megakernel still has register spills and other low-level problems. All of these point towards this being an exciting direction for future work! Conclusion: Megakernels are Cool In this", "source": "blog:hazyresearch.stanford.edu", "published": "", "authors": "", "tokens": 512, "sha256": "c07569c90b2230192a1a02b9cd70eb08c0820b3377a3c96939f889002972e241"}
{"doc_id": "blog:hazyresearch.stanford.edu#body:part-12", "url": "https://hazyresearch.stanford.edu/blog/2025-09-28-tp-llama-main", "anchor": "#body:part-12", "type": "blog", "title": "", "section": "Body", "text": "| 11,783 | 7,387 | 19,170 | | Megakernel | 14,425 | 9,043 | 23,468 | Even despite these promising initial results, we suspect there's still considerable room for optimization within megakernels. Our scheduling heuristics are quite simple, our instructions frequently stall on synchronizations that could likely be hidden better, and our megakernel still has register spills and other low-level problems. All of these point towards this being an exciting direction for future work! Conclusion: Megakernels are Cool In this post, we introduced a tensor-parallel Llama-70B megakernel focused on maximizing decoding throughput. We designed a custom instruction set for this megakernel within our megakernel interpreter framework, and ablated several key scheduling decisions including pipelining across instruction boundaries, choosing processors for each instruction, and interleaving communication with compute. Finally, we integrated our megakernel into Tokasaurus, and found it outperformed SGLang by 22% on ShareGPT prompts. A direction for future work: a key challenge of writing megakernels is that there is tremendous complexity in both designing these custom instruction sets, and coordinating (and especially debugging) synchronization patterns across GPUs. A corresponding learning from this work is that, going forward, we'd like to design a more general megakernel instruction set and abstract these decisions into the host-side scheduler, in order to simplify the process of designing high-performance megakernels. We think this might make megakernels for training viable, alongside megakernels for inference. *Work by Jordan done while at Stanford.", "source": "blog:hazyresearch.stanford.edu", "published": "", "authors": "", "tokens": 235, "sha256": "504893c08c7c38a337733f291c45a376262dccdaf2183fd18db0278c826f75db"}
{"doc_id": "blog:www.usenix.org#body", "url": "https://www.usenix.org/conference/nsdi23/presentation/hwang", "anchor": "#body", "type": "blog", "title": "", "section": "Body", "text": "--- title: {ARK}: {GPU-driven} Code Execution for Distributed Deep Learning author: Changho Hwang; KyoungSoo Park; Ran Shu; Xinyuan Qu; Peng Cheng; Yongqiang Xiong url: https://www.usenix.org/conference/nsdi23/presentation/hwang hostname: usenix.org sitename: usenix.org date: 2023-01-01 --- Changho Hwang, KAIST, Microsoft Research; KyoungSoo Park, KAIST; Ran Shu, Xinyuan Qu, Peng Cheng, and Yongqiang Xiong, Microsoft Research Modern state-of-the-art deep learning (DL) applications tend to scale out to a large number of parallel GPUs. Unfortunately, we observe that the collective communication overhead across GPUs is often the key limiting factor of performance for distributed DL. It under-utilizes the networking bandwidth by frequent transfers of small data chunks, which also incurs a substantial I/O overhead on GPU that interferes with computation on GPU. The root cause lies in the inefficiency of CPU-based communication event handling as well as the inability to control the GPU's internal DMA engine with GPU threads. To address the problem, we propose a GPU-driven code execution system that leverages a GPU-controlled hardware DMA engine for I/O offloading. Our custom DMA engine pipelines multiple DMA requests to support efficient small data transfer while it eliminates the I/O overhead on GPU cores. Unlike existing GPU DMA engines initiated only by CPU, we let GPU threads to directly control DMA operations, which leads to a highly efficient system where GPUs drive their own execution flow and handle communication events autonomously without CPU intervention. Our prototype DMA engine achieves a line-rate from a message size as small as 8KB (3.9x better throughput) with only 4.3us of communication latency (9.1x faster) while it incurs little interference with computation on GPU, achieving 1.8x higher all-reduce throughput in a real training workload. NSDI '23 Open Access Sponsored by King Abdullah University of Science and Technology (KAUST) Open Access Media USENIX is committed to Open Access to the research presented at our events. Papers and proceedings are freely available to everyone once the event begins. Any video, audio, and/or slides that are posted after the event are also free and open to everyone. Support USENIX and our commitment to Open Access. This content is available to: author = {Changho Hwang and KyoungSoo Park and Ran Shu and Xinyuan Qu and Peng Cheng and Yongqiang Xiong}, title = {{ARK}: {GPU-driven} Code Execution for Distributed Deep Learning}, booktitle = {20th USENIX Symposium on Networked Systems Design and Implementation (NSDI 23)}, year = {2023}, isbn = {978-1-939133-33-5}, address = {Boston, MA}, pages = {87--101}, url = {https://www.usenix.org/conference/nsdi23/presentation/hwang}, publisher = {USENIX Association}, month = apr }", "source": "blog:www.usenix.org", "published": "", "authors": "", "tokens": 411, "sha256": "a3c760e06f1f56d284830c34b6e5a82c240ea18b78ab8b71f3bc21a5846a63de"}
{"doc_id": "arxiv:2503.01840#abstract", "url": "https://arxiv.org/abs/2503.01840", "anchor": "#abstract", "type": "paper", "title": "", "section": "Abstract", "text": "The sequential nature of modern LLMs makes them expensive and slow, and speculative sam- pling has proven to be an effective solution to this problem. Methods like EAGLE perform autoregression at the feature level, reusing top- layer features from the target model to achieve better results than vanilla speculative sampling. A growing trend in the LLM community is scaling up training data to improve model in- telligence without increasing inference costs. However, we observe that scaling up data pro- vides limited improvements for EAGLE. We identify that this limitation arises from EA- GLE’s feature prediction constraints. In this paper, we introduce EAGLE-3, which aban- dons feature prediction in favor of direct token prediction and replaces reliance on top-layer features with multi-layer feature fusion via a technique named training-time test. These im- provements significantly enhance performance and enable the draft model to fully benefit from scaling up training data. Our experiments in- clude both chat models and reasoning mod- els, evaluated on five tasks. The results show that EAGLE-3 achieves a speedup ratio up to 6.5x, with about 1.4x improvement over EAGLE-2. In the SGLang framework, EAGLE- 3 achieves a 1.38x throughput improvement at a batch size of 64. The code is available at https://github.com/SafeAILab/EAGLE. 1", "source": "arxiv_pdf", "published": "", "tokens": 205, "sha256": "5e36dcbba95d702d3487322b07dc27e96194651fd5c0bcdb87627bec857753f3"}
{"doc_id": "arxiv:2503.01840#introduction:part-1", "url": "https://arxiv.org/abs/2503.01840", "anchor": "#introduction:part-1", "type": "paper", "title": "", "section": "Introduction", "text": "Modern Large Language Models (LLMs) are being applied to more domains, with their improved capa- bilities driven by scaling model parameters—some LLMs now exceed hundreds of billions of param- eters. In autoregressive generation, each token requires accessing all model parameters, making LLM inference slow and costly. Recently, test-time scaling up has gained sig- nificant attention. Models like ChatGPT o1 and 1 2 4 8 3.2 3.4 3.6 3.8 4.0 4.2 4.4 Speedup EAGLE-2 EAGLE-3 1 2 4 8 4.0 4.5 5.0 5.5 6.0 Accept length EAGLE-2 EAGLE-3 Figure 1: Scaling law evaluated on the MT-bench using LLaMA-Instruct 3.1 8B as the target model, with the x-axis representing the data scale relative to ShareGPT. The new architectural designs in EAGLE-3 enable an increasing scaling curve, which was never observed in the previous works. DeepSeek-R1 (Guo et al., 2025) engage in delib- erate reasoning before responding, pushing the boundaries of LLM capabilities at the cost of longer inference time. However, these models often re- quire lengthy reasoning processes, making them extremely costly, while the increased response time severely impacts user satisfaction. These reasoning models significantly increase the proportion of in- ference costs in the overall LLM pipeline, driving researchers to explore cheaper and faster inference optimization methods. Speculative sampling methods can reduce LLM latency by partially parallelizing the generation pro- cess. These methods rapidly generate draft tokens and then verify them in parallel. This allows multi- ple tokens to be produced in a single forward pass, significantly reducing inference latency. In vanilla speculative sampling, the draft model is a separate, smaller LLM, typically a lower-parameter version from the same series as the target model. This arXiv:2503.01840v3 [cs.CL] 23 Apr 2025 Vicuna 13B LLaMA-Instruct 3.1 8B LLaMA-Instruct 3.3 70B DeepSeek R1 LLaMA 8B 0 1 2 3 4 5 Speedup 1.0x 1.9x 2.1x 3.1x 4.1x 5.6x 1.0x 3.6x 3.2x 4.4x 1.0x 2.8x 4.1x 1.0x 3.4x 5.0x Vanilla Speculative sampling Medusa HASS EAGLE EAGLE-2 EAGLE-3 Figure 2: Speedup ratios of different methods at temperature=0. For the standard speculative sampling, Vicuna-13B uses Vicuna-68M as the draft model. In Table 1, we present comparisons with additional methods, but this figure only showcases a subset. Chat model’s evaluation dataset is MT-bench, and the reasoning model’s evaluation dataset is GSM8K. DeepSeek R1 LLaMA 8B refers to DeepSeek-R1-Distill-LLaMA 8B. draft model operates independently of the target model. Unlike the vanilla speculative sampling, EAGLE (Li et al., 2024c) reuses the top-layer fea- tures of the target model (the features before the LM head). It trains the draft model to autoregres- sively predict the next feature and then uses the target model’s LM head to obtain the draft token. By leveraging the rich information from the target model, EAGLE achieves significantly better accel- eration compared to vanilla speculative sampling. Subsequent methods such as HASS (Zhang et al., 2024) and Falcon (Gao et al., 2024) also adopt the approach of predicting the next feature using the current feature sequence. Recent LLMs have increasingly relied on larger training datasets to achieve better performance. For example, LLaMA series models with sizes of 7B (8B) have used", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "e0a2ca243aabb579911a6320c407fd7f7df9fc3c5f2cc8e648a3e8b4e89ebc1d"}
{"doc_id": "arxiv:2503.01840#introduction:part-2", "url": "https://arxiv.org/abs/2503.01840", "anchor": "#introduction:part-2", "type": "paper", "title": "", "section": "Introduction", "text": "head to obtain the draft token. By leveraging the rich information from the target model, EAGLE achieves significantly better accel- eration compared to vanilla speculative sampling. Subsequent methods such as HASS (Zhang et al., 2024) and Falcon (Gao et al., 2024) also adopt the approach of predicting the next feature using the current feature sequence. Recent LLMs have increasingly relied on larger training datasets to achieve better performance. For example, LLaMA series models with sizes of 7B (8B) have used 1T, 2T, and 15T tokens of training data for LLaMA 1 (Touvron et al., 2023a), LLaMA 2 (Touvron et al., 2023b), and LLaMA 3 (Dubey et al., 2024), respectively, resulting in significant improvements across various metrics while keeping the model architecture and inference cost largely unchanged. Similarly, we aim to improve the ac- ceptance rate and acceleration ratio of EAGLE by increasing its training data. Unfortunately, we ob- serve that the gains from additional training data for EAGLE are limited. We analyze the reasons behind this phenomenon. As shown in the upper part of Figure 3, EAGLE performs autoregressive prediction at the feature level, predicting the next feature and then feeding the feature into the LM head of the target model to obtain the token dis- tribution. EAGLE’s loss function consists of two components: the feature prediction loss lfea and the token prediction loss ltoken. Thanks to the feature prediction loss, the draft model trained only at Step 1 can adapt to Step 2 and acquire multi-step predic- tion capabilities. However, with token prediction as the ultimate goal, feature prediction can be seen as an additional constraint, which limits the expres- siveness of the draft model and makes it difficult to benefit from increased data. After removing the feature constraint and expanding the training data (the middle part of Figure 3), as shown in Figure 4, the acceptance rate 0-α of the first draft token improves significantly. However, the output of the draft model in Step 1, denoted as ˆat+1, is far away from the ground-truth ft+1, causing the input sequence f1, f2, · · · , ft, ˆat+1 in Step 2 to de- viate significantly from the training distribution, resulting in a very low acceptance rate 1-α for the second draft token, as shown in Figure 4. We can address this issue by incorporating Step 1 into the training process (the bottom of Figure 3). Using this method, the benefits of increasing training data become more pronounced. We name this technique as training-time test. EAGLE and speculative sampling methods such as Medusa (Cai et al., 2024) reuse the top-layer fea- tures of the target model, specifically the features immediately before the LM head. For an LM head with a full-rank weight matrix, the top-layer fea- tures corresponding to the logits of the next token are unique, ensuring that the information contained in these features aligns directly with the logits of the next token. However, predicting the next-next token based solely on top-layer features—which are inherently limited to the next token—poses a significant challenge. Fortunately, the training-time test technique described above enables", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "75460e749b6f86d5369463ee6ce1a0ddfc7666e5d956f60659f203fa9589ff16"}
{"doc_id": "arxiv:2503.01840#introduction:part-3", "url": "https://arxiv.org/abs/2503.01840", "anchor": "#introduction:part-3", "type": "paper", "title": "", "section": "Introduction", "text": "of the target model, specifically the features immediately before the LM head. For an LM head with a full-rank weight matrix, the top-layer fea- tures corresponding to the logits of the next token are unique, ensuring that the information contained in these features aligns directly with the logits of the next token. However, predicting the next-next token based solely on top-layer features—which are inherently limited to the next token—poses a significant challenge. Fortunately, the training-time test technique described above enables the use of features from intermediate layers instead of relying solely on the top layer, as the feature prediction loss lfea has been removed during training. Draft model 𝑓\"#$% ≈𝑓#$% LM head 𝑡̂#$( 𝑡#$( 𝑙*+, 𝑙#-.+/ Draft model 𝑓\"#$( 𝑓#$( LM head 𝑡̂#$0 𝑡#$0 Step 1 Step 2 Draft model 𝑎2#$% LM head 𝑡̂#$( 𝑡#$( 𝑙#-.+/ Draft model 𝑎2#$( LM head 𝑡̂#$0 𝑡#$0 𝑙#-.+/ Step 1 Step 2 𝑓# … 𝑓% 𝑓#3% 𝑎2#$% … 𝑓% 𝑓# Training/Test ≈ ≈ ≈ ≈ ≈ Draft model 𝑎2#$% LM head 𝑡̂#$( 𝑡#$( 𝑙#-.+/ Draft model 𝑎2#$( LM head 𝑡̂#$0 𝑡#$0 𝑎2#$% … 𝑓% 𝑓# Step 1 Step 2 Training ≈ ≈ Test 𝑓# … 𝑓% 𝑓#3% 𝑓\"#$% … 𝑓% 𝑓# Training 𝑓# 𝑓% 𝑓#3% Test … EAGLE EAGLE-3 EAGLE + 𝑙*+, removal Training/Test Training-time test Figure 3: Illustration of training-time test (the bottom part) and its comparison with other draft methods (the upper and middle parts). f denotes the feature, t denotes the token, and a represents the unconstrained vectors. We use the hat to denote the predictions from models. All the methods shown in the figure use the token se- quence from the previous time step, but for simplicity, this is not depicted in the figure. The input to EAGLE-3 is not actually f, but it is not shown in this figure. We will provide a detailed explanation in the following sec- tion. To summarize, this paper introduces EAGLE- 3, an enhanced version of EAGLE that achieves a significant speedup. EAGLE-3 is parallelized and fully compatible with the drafting tree tech- nique from EAGLE-2 (Li et al., 2024b). Our key contributions include: • A novel training-time test architecture for the draft model: We remove the feature pre- diction constraint and directly predict tokens while simulating multi-step generation during training. This direct token prediction provides complete flexibility in the draft model’s input. Instead of reusing only the top-layer features, we integrate and leverage low-, mid-, and high- level features from the target model, capturing rich semantic information from different lay- 1 2 4 8 0.72 0.74 0.76 0.78 0.80 0- EAGLE EAGLE without fea pred EAGLE-3 1 2 4 8 0.2 0.3 0.4 0.5 0.6 0.7 0.8 1- EAGLE EAGLE without fea pred EAGLE-3 Figure 4: Comparison of acceptance rates across differ- ent methods, with the x-axis representing the data scale relative to ShareGPT. ers. • Discovery of a scaling law for inference ac- celeration in large language models: With the new architecture, we observe that increas- ing the amount of training data for the draft model leads to a proportional increase in the speedup", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "8f28a3e031200fc313cbe18579219d3cc7c4b7914256ccb3a9baf8f3acec0796"}
{"doc_id": "arxiv:2503.01840#introduction:part-4", "url": "https://arxiv.org/abs/2503.01840", "anchor": "#introduction:part-4", "type": "paper", "title": "", "section": "Introduction", "text": "EAGLE-3 1 2 4 8 0.2 0.3 0.4 0.5 0.6 0.7 0.8 1- EAGLE EAGLE without fea pred EAGLE-3 Figure 4: Comparison of acceptance rates across differ- ent methods, with the x-axis representing the data scale relative to ShareGPT. ers. • Discovery of a scaling law for inference ac- celeration in large language models: With the new architecture, we observe that increas- ing the amount of training data for the draft model leads to a proportional increase in the speedup ratio of EAGLE-3. This scaling be- havior was not observed in the original EA- GLE architecture, as shown in Figure 1 • Improved inference acceleration: EAGLE- 3, trained with approximately 8x more data than EAGLE, achieves a 1.4x latency speedup over EAGLE-2 at batch size 1. Specula- tive sampling is often thought to reduce throughput at large batch sizes. How- ever, in SGLang (Zheng et al., 2024), a production-grade framework, EAGLE-3 im- proves throughput by 40% at a batch size of 64. We expect larger data size would lead to further improved speedup ratio. 2 Preliminaries 2.1 Speculative Sampling Speculative sampling (Leviathan et al., 2023; Chen et al., 2023; Sun et al., 2024c,b) is a lossless LLM acceleration technique that alternates between draft- ing and verification, where drafting is performed at low cost and verification is parallelized, corre- sponding to the generation of drafts and the veri- fication process, respectively. We use ti to denote the i-th token and Ta:b to represent the token se- quence ta, ta+1, · · · , tb. When T1:j is used as the prefix, the two stages of speculative sampling are as follows. In the drafting stage, speculative sampling uti- lizes a draft model (a smaller version from the same series as the target model) to autoregressively gen- erate k tokens to form the draft. ˆTj+1:j+k, while also recording the probability ˆp for each token. In the verification stage, speculative sampling invokes the target model to evaluate the draft ˆTj+1:j+k and records its probability p. Specula- tive sampling then determines the acceptance of draft tokens sequentially, from front to back. For token ˆtj+i, the probability of acceptance is given by min(1, pj+i(ˆtj+i)/ˆpj+i(ˆtj+i)). If the token is accepted, the process moves to the next token. Oth- erwise, a token is sampled from the distribution norm(max(0, pj+i −ˆpj+i)) to replace ˆtj+i, and the remaining tokens in the draft are discarded. Ap- pendix A.1 of (Leviathan et al., 2023) proves that speculative sampling is consistent with the distri- bution of vanilla autoregressive decoding. 2.2 EAGLE and EAGLE-2 The draft model with limited capacity struggles to precisely approximate the large-scale target model. EAGLE leverages the top-layer features of the tar- get model as additional information and performs autoregression at the feature level, simplifying the drafting process. EAGLE performs autoregression at the feature level and then uses the LM head of the target model to obtain the draft token. Due to the sampling results at the token layer being hidden, feature-level autoregression introduces un- certainty. EAGLE addresses this issue by feeding the token sequence from the previous time step, i.e., the", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "13b9d0cdd8d52f9d1a3d2688a47d9f5cb6b7f2588e6b184d964294f411ae60b5"}
{"doc_id": "arxiv:2503.01840#introduction:part-5", "url": "https://arxiv.org/abs/2503.01840", "anchor": "#introduction:part-5", "type": "paper", "title": "", "section": "Introduction", "text": "target model. EAGLE leverages the top-layer features of the tar- get model as additional information and performs autoregression at the feature level, simplifying the drafting process. EAGLE performs autoregression at the feature level and then uses the LM head of the target model to obtain the draft token. Due to the sampling results at the token layer being hidden, feature-level autoregression introduces un- certainty. EAGLE addresses this issue by feeding the token sequence from the previous time step, i.e., the sampling results, into the draft model. Unlike the chain-like drafts of Vanilla speculative sam- pling, EAGLE generates multiple draft tokens at the same position, resulting in a tree-like draft. In the verification stage, EAGLE uses tree attention to parallelize the verification of the draft tree. Interest- ingly, EAGLE inspired the multi-token prediction technique used in the pre-training of DeepSeek- v3 (Liu et al., 2024a), which in turn inspired new architectural designs in EAGLE-3. EAGLE (Li et al., 2024c) and Medusa (Cai et al., 2024), among others, use tree-shaped drafts, where the structure of the draft tree is predefined, static, and context-independent. The difficulty of draft- ing is closely related to the context, and a static draft tree can lead to resource wastage. EAGLE- 2 (Li et al., 2024b) approximates the acceptance rate using the confidence of the draft model and dy- namically generates the draft tree based on this, per- forming pruning of the draft tree at the end of the drafting stage. EAGLE-3 also adopts the context- aware dynamic draft tree proposed in EAGLE-2. How can LM Head I Target Model 𝑙how 𝑙can Decoder Layers Embedding 𝑚how 𝑚can Decoder Layers ℎhow ℎcan Decoder Layers Decoder Layers ℎhow ℎcan Concat 𝑚how 𝑚can 𝑙how 𝑙can FC Layer 𝑔how 𝑔can I Decoder Layer FC Layer can 𝑔can 𝑔how 𝑒can 𝑒I 𝑎I LM Head I can do do 𝑔can 𝑔how 𝑒can 𝑒I 𝑎I 𝑒do Decoder Layer FC Layer 𝑎do LM Head it I do 𝑔can 𝑔how 𝑒can 𝑒I 𝑎I 𝑒do it 𝑎do 𝑒it ... ① ② ③ can 𝑎can 𝑎I 𝑎can Figure 5: Diagram of the EAGLE-3 inference pipeline, illustrating the three steps of the draft model. l, m, and h represent the low, middle, and high-level features of the target model, respectively. e denotes the embedding. 3 EAGLE-3 In this section, we provide a detailed description of the implementation of EAGLE-3. 3.1 Inference Pipeline Consistent with other speculative sampling meth- ods, EAGLE-3 alternates between the drafting and verification stages. The difference between EAGLE-3 and EAGLE lies in the drafting stage, which we introduce with an example, as shown in Figure 5. Consider the prefix “How can”. Dur- ing the prefill phase or the previous verification stage, the target model performs a forward pass to generate the next token, “I”. We record the low, middle, and high-level feature sequences from the target model’s forward pass, denoted as l, m, and h, respectively. We concatenate the k-dimensional vectors l, m, and h to form a 3k-dimensional vector, then pass it through a fully connected (FC) layer to reduce it to k-dimensions, obtaining a feature g that", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "576e5c91d93c7e7071d87654997e3a7d6b812864f9c790dbe4eaf889e9f9f3f5"}
{"doc_id": "arxiv:2503.01840#introduction:part-6", "url": "https://arxiv.org/abs/2503.01840", "anchor": "#introduction:part-6", "type": "paper", "title": "", "section": "Introduction", "text": "“How can”. Dur- ing the prefill phase or the previous verification stage, the target model performs a forward pass to generate the next token, “I”. We record the low, middle, and high-level feature sequences from the target model’s forward pass, denoted as l, m, and h, respectively. We concatenate the k-dimensional vectors l, m, and h to form a 3k-dimensional vector, then pass it through a fully connected (FC) layer to reduce it to k-dimensions, obtaining a feature g that integrates information from different layers. Here, k refers to the hidden size of the target model. Our goal is to generate a draft token sequence with the prefix “How can I”. By inputting only ghow and gcan, the draft model cannot access the random sampling process. Therefore, similar to EAGLE (Li et al., 2024c), we introduce the embed- ding eI of the sampled token “I”. The concatenated vector is then passed through an FC layer to reduce its dimensionality to k, and subsequently inputted into a single layer decoder, producing the output a. Finally, we input aI into the LM head and sample to obtain the draft token “do”. In Step 1, with the prefix “How can”, we reuse ghow and gcan from the target model. In Step 2, the prefix becomes “How can I”. Ideally, we would reuse ghow, gcan, and gI from the target model. How- ever, this is not possible because the token “I” has not yet been checked by the target model, and we cannot obtain gI. Instead, we use the output aI from the draft model in the previous step to replace gI, and concatenate aI with the embedding edo of the sampled result “do” as the input to the draft model in Step 1. In Step 3, we similarly cannot obtain gdo, so we use ado as a replacement, concatenating ado with eit as the input to the draft model. The same approach is followed for subsequent steps. 3.2 Draft Model Training The input to the draft model in EAGLE is either, or at least approximately, the top-layer features f1, f2, · · · , ft of the target model. In contrast, the input to the draft model in EAGLE-3 may include the features g1, g2, · · · , gt from the target model, or it may include the output at+1, at+2 · · · , at+j from the draft model. Therefore, we need to train the draft model to adapt to different inputs. During training, we perform test steps, where we generate a and feed it back into the draft model for further training. The core of the draft model in EAGLE-3 is a Transformer decoder layer. Aside from the self- attention operation, no other components interact with the context, so no further modifications are required during training or testing. The only com- ponent that requires slight modification is the self- attention, which we will describe in detail below. Although the actual input consists of features, for clarity, we describe the process using tokens as input. As shown in Figure 6, the original train-", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "8c675fa9fc0eab08d6d370bc346e1e6df3e14da2466f69013d8027e6515d7804"}
{"doc_id": "arxiv:2503.01840#introduction:part-7", "url": "https://arxiv.org/abs/2503.01840", "anchor": "#introduction:part-7", "type": "paper", "title": "", "section": "Introduction", "text": "The core of the draft model in EAGLE-3 is a Transformer decoder layer. Aside from the self- attention operation, no other components interact with the context, so no further modifications are required during training or testing. The only com- ponent that requires slight modification is the self- attention, which we will describe in detail below. Although the actual input consists of features, for clarity, we describe the process using tokens as input. As shown in Figure 6, the original train- ing data is a sequence of length 3, “How can I”, with a normal sequential dependency in the context. Therefore, the attention mask is a standard lower triangular matrix. The outputs at the three positions are “are”, “we”, and “do”, which have a tree-like contextual relationship with “how”, “can”, and “I”. As a result, when the input “are”, “we”, and “do” is fed into Step 2, the attention mask needs to be adjusted accordingly, as shown in the top-right cor- ner of Figure 6. All attention masks are diagonal, except when the original training data is used as the key. Using matrix multiplication in this case would result in significant computational waste, so we can use vector dot products to calculate the attention score only for the corresponding positions. HASS (Zhang et al., 2024) and EAGLE-3 both make similar modifications to the attention mecha- nism to simulate the testing process during training, but this is not the main focus of EAGLE-3. The motivations, methods, and outcomes of the two ap- proaches are distinctly different. The motivation behind HASS is to mitigate the error accumula- tion caused by inaccurate feature predictions in EAGLE. HASS still performs feature prediction, includes a feature prediction loss lfea, and the in- put to the draft model must be the top-layer fea- tures. In contrast, the motivation behind EAGLE-3 is to remove unnecessary constraints to enhance the model’s expressive power. EAGLE-3 no longer requires the draft model’s output to fit the top-layer features of the target model, thus avoiding error accumulation. After removing feature prediction, the input to EAGLE-3 is completely free, and it is replaced by a fusion of features from different layers of semantic information. The removal of the feature prediction loss also enables us to discover a new scaling law for inference acceleration which was never found before. Figure 2 also shows the speedup of EAGLE-3 and HASS, with EAGLE-3 demonstrating significantly better performance. 4", "source": "arxiv_pdf", "published": "", "tokens": 402, "sha256": "3a67daebea8414f6ac623db14d476791273e6f7703f8f424e6a88d3b39bc4fed"}
{"doc_id": "arxiv:2503.01840#experiments:part-1", "url": "https://arxiv.org/abs/2503.01840", "anchor": "#experiments:part-1", "type": "paper", "title": "", "section": "Experiments", "text": "Models. We conduct experiments with state-of- the-art open-source chat and reasoning models, in- cluding Vicuna 13B (Chiang et al., 2023), LLaMA- Instruct 3.1 8B, LLaMA-Instruct 3.3 70B (Dubey et al., 2024), and DeepSeek-R1-Distill-LLaMA 8B (DeepSeek-AI et al., 2025). Due to the GPU constraint, we are unable to test EAGLE-3 on the 405B and 671B models. Tasks. Following EAGLE (Li et al., 2024c) and Spec-Bench (Xia et al., 2024), we evaluate on five common tasks, using the same weights for all tasks without fine-tuning on the respective tasks. For multi-turn conversation, code genera- How can I How can I ✓ ✓✓ ✓✓✓ How can I How can I are we do How can I ✓ ✓ ✓✓ ✓ ✓✓✓ ✓ are we do are we do How can I are we do you help it How can I ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ you help it are we do you help it Query Key Query Key Query Key Figure 6: Diagram of the attention causal masks during training-time test. It sequentially shows a native training step (the first step) and two simulated training steps (the second and third steps). The arrows between tokens represent contextual relationships. The gray tokens represent the training data while the blue and yellow tokens represent the first- and second-round predictions by the draft model, respectively. tion, mathematical reasoning, instruction follow- ing, and summarization„ we chose the MT-bench (Zheng et al., 2023), HumanEval (Chen et al., 2021), GSM8K (Cobbe et al., 2021), Alpaca (Taori et al., 2023), and CNN/Daily Mail (Nallapati et al., 2016) datasets, respectively. Metrics. EAGLE-3 does not modify the target model’s weights and uses strict speculative sam- pling acceptance conditions, ensuring no loss in performance. Therefore, we do not evaluate gener- ation quality. Instead, we use the following metrics to assess the acceleration performance: • Speedup Ratio: The actual test speedup ratio relative to vanilla autoregressive decoding. • Average Acceptance Length τ: The aver- age number of tokens generated per drafting- verification cycle, which corresponds to the number of tokens accepted from the draft. • Acceptance Rate n-α: The proportion of draft tokens accepted, which directly reflects the draft model’s approximation to the tar- get model. Following EAGLE’s setup, we use a chain-like draft rather than a tree-like draft when testing acceptance rates. EA- GLE suffers from error accumulation, mean- ing that the input to the draft model may be its own estimates rather than the exact val- ues from the target model. Therefore, EA- GLE uses n-α to represent the acceptance rate when the input contains n estimated fea- tures, under the condition that the previous estimated tokens are all accepted by the tar- get model. In other words, the acceptance rate for inputs f1, f2, · · · , fi, ˆfi+1, · · · , ˆfi+n, where f is the exact value and ˆf is the draft model’s estimate. Similarly, we use n-α to represent the acceptance rate in EAGLE-3 when the input contains n self-predicted val- ues a, i.e., the acceptance rate for inputs g1, g2,", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "dcd110f89e71a32c016e5de604676833e802d79c1cda4e2dba6c6dcb98a45a13"}
{"doc_id": "arxiv:2503.01840#experiments:part-2", "url": "https://arxiv.org/abs/2503.01840", "anchor": "#experiments:part-2", "type": "paper", "title": "", "section": "Experiments", "text": "fea- tures, under the condition that the previous estimated tokens are all accepted by the tar- get model. In other words, the acceptance rate for inputs f1, f2, · · · , fi, ˆfi+1, · · · , ˆfi+n, where f is the exact value and ˆf is the draft model’s estimate. Similarly, we use n-α to represent the acceptance rate in EAGLE-3 when the input contains n self-predicted val- ues a, i.e., the acceptance rate for inputs g1, g2, · · · , gi, ai+1, · · · , ai+n, where g is the fused feature from the target model. Implementation. We use the AdamW optimizer, with beta values (β1, β2) set to (0.9, 0.95) and im- plemented gradient clipping of 0.5. The learning rate is set to 5e-5. We use ShareGPT and UltraChat- 200K (Ding et al., 2023) as training data, contain- ing approximately 68K and 464K data entries, re- spectively. We call the target model to generate responses rather than using a fixed dataset. For the reasoning model DeepSeek-R1-Distill-LLaMA 8B, we also used the OpenThoughts-114k-math dataset for training. Comparison. We use vanilla autoregressive de- coding as the baseline, which serves as the bench- mark for speedup ratios (1.00x). We compare EAGLE-3 with recent lossless speculative sam- pling methods, including standard speculative sam- pling (Leviathan et al., 2023; Chen et al., 2023; Gante, 2023), PLD (Saxena, 2023), Medusa (Cai Table 1: Speedup ratios and average acceptance lengths τ of different methods. V represents Vicuna, L31 represents LLaMA-Instruct 3.1, L33 represents LLaMA-Instruct 3.3, and DSL represents DeepSeek-R1-Distill-LLaMA. SpS denotes standard speculative sampling, with its draft model being Vicuna-68M. Methods like Medusa relax acceptance conditions under non-greedy settings, which do not guarantee lossless acceleration. Therefore, we do not compare EAGLE-3 with these methods when temperature=1. MT-bench HumanEval GSM8K Alpaca CNN/DM Mean", "source": "arxiv_pdf", "published": "", "tokens": 301, "sha256": "dc323e2483cf0d53755466b6ed04e6ee8296de78f4c074cc9a531769b038387b"}
{"doc_id": "arxiv:2503.01840#method:part-1", "url": "https://arxiv.org/abs/2503.01840", "anchor": "#method:part-1", "type": "paper", "title": "", "section": "Method", "text": "Speedup τ Speedup τ Speedup τ Speedup τ Speedup τ Speedup τ Temperature=0 V 13B SpS 1.93x 2.27 2.23x 2.57 1.77x 2.01 1.76x 2.03 1.93x 2.33 1.92x 2.24 PLD 1.58x 1.63 1.85x 1.93 1.68x 1.73 1.16x 1.19 2.42x 2.50 1.74x 1.80 Medusa 2.07x 2.59 2.50x 2.78 2.23x 2.64 2.08x 2.45 1.71x 2.09 2.12x 2.51 Lookahead 1.65x 1.69 1.71x 1.75 1.81x 1.90 1.46x 1.51 1.46x 1.50 1.62x 1.67 Hydra 2.88x 3.65 3.28x 3.87 2.93x 3.66 2.86x 3.53 2.05x 2.81 2.80x 3.50 EAGLE 3.07x 3.98 3.58x 4.39 3.08x 3.97 3.03x 3.95 2.49x 3.52 3.05x 3.96 EAGLE-2 4.26x 4.83 4.96x 5.41 4.22x 4.79 4.25x 4.89 3.40x 4.21 4.22x 4.83 EAGLE-3 5.58x 6.65 6.47x 7.54 5.32x 6.29 5.16x 6.17 5.01x 6.47 5.51x 6.62 L31 8B EAGLE-2 3.16x 4.05 3.66x 4.71 3.39x 4.24 3.28x 4.12 2.65x 3.45 3.23x 4.11 EAGLE-3 4.40x 6.13 4.85x 6.74 4.48x 6.23 4.82x 6.70 3.65x 5.34 4.44x 6.23 L33 70B EAGLE-2 2.83x 3.67 3.12x 4.09 2.83x 3.69 3.03x 3.92 2.44x 3.55 2.85x 3.78 EAGLE-3 4.11x 5.63 4.79x 6.52 4.34x 6.15 4.30x 6.09 3.27x 5.02 4.12x 5.88 DSL 8B EAGLE-2 2.92x 3.80 3.42x 4.29 3.40x 4.40 3.01x 3.80 3.53x 3.33 3.26x 3.92 EAGLE-3 4.05x 5.58 4.59x 6.38 5.01x 6.93 3.65x 5.37 3.52x 4.92 4.16x 5.84 Temperature=1 V 13B SpS 1.62x 1.84 1.72x 1.97 1.46x 1.73 1.52x 1.78 1.66x 1.89 1.60x 1.84 EAGLE 2.32x 3.20 2.65x 3.63 2.57x 3.60 2.45x 3.57 2.23x 3.26 2.44x 3.45 EAGLE-2 3.80x 4.40 4.22x 4.89 3.77x 4.41 3.78x 4.37 3.25x 3.97 3.76x 4.41 EAGLE-3 4.57x 5.42 5.15x 6.22 4.71x 5.58 4.49x 5.39 4.33x 5.72 4.65x 5.67 L31 8B EAGLE-2 2.44x 3.16 3.39x 4.39 2.86x 3.74 2.83x 3.65 2.44x 3.14 2.80x 3.62 EAGLE-3 3.07x 4.24 4.13x 5.82 3.32x 4.59 3.90x 5.56 2.99x 4.39 3.45x 4.92 L33 70B EAGLE-2 2.73x 3.51 2.89x 3.81 2.52x 3.36 2.77x 3.73 2.32x 3.27 2.65x 3.54 EAGLE-3 3.96x 5.45 4.36x 6.16 4.17x 5.95 4.14x 5.87 3.11x 4.88 3.95x 5.66 DSL 8B EAGLE-2 2.69x 3.41 3.01x 3.82 3.16x 4.05 2.64x 3.29 2.35x 3.13 2.77x 3.54 EAGLE-3 3.20x 4.49 3.77x 5.28 4.38x 6.10 3.16x 4.30 3.08x 4.27 3.52x 4.89 et al., 2024), Lookahead (Fu et al., 2024), Hydra (Ankner et al., 2024), HASS (Zhang et al., 2024), EAGLE (Li et al., 2024c), and EAGLE-2 (Li et al., 2024b). 4.1 Effectiveness Figure 1 and Table 1 demonstrate the acceleration performance of EAGLE-3. On all tasks and target models, EAGLE-3 achieves the highest speedup ratio and average acceptance length. EAGLE-3 pro- vides a speedup of approximately 3.0x-6.5x com- pared to vanilla autoregressive generation, with a 20%-40% improvement over EAGLE-2. Different tasks affect the draft model’s acceptance rate, so both the average acceptance length and speedup ratio are task-dependent. Due to the presence of many fixed templates in code generation tasks, gen- erating drafts is the easiest, which is why EAGLE-3 performs best on HumanEval, achieving a speedup ratio of up to 6.5x and an average acceptance length of up to 7.5. DeepSeek-R1-Distill-LLaMA 8B is an exception, with the highest speedup ratio on the mathematical reasoning dataset GSM8K. This may be because we trained the draft model of DeepSeek- R1-Distill-LLaMA 8B using", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "0c1bb769ff276b68282614d2b4bccbec56b2e77dba900f1d000c9e72865c3b76"}
{"doc_id": "arxiv:2503.01840#method:part-2", "url": "https://arxiv.org/abs/2503.01840", "anchor": "#method:part-2", "type": "paper", "title": "", "section": "Method", "text": "acceptance length and speedup ratio are task-dependent. Due to the presence of many fixed templates in code generation tasks, gen- erating drafts is the easiest, which is why EAGLE-3 performs best on HumanEval, achieving a speedup ratio of up to 6.5x and an average acceptance length of up to 7.5. DeepSeek-R1-Distill-LLaMA 8B is an exception, with the highest speedup ratio on the mathematical reasoning dataset GSM8K. This may be because we trained the draft model of DeepSeek- R1-Distill-LLaMA 8B using the OpenThoughts- 114k-math dataset. Figure 7 shows the acceptance rates of EAGLE and EAGLE-3 on MT-bench with LLaMA-Instruct 3.1 8B as the target model. The acceptance rate of EAGLE-3 is significantly higher than that of EAGLE. As the input from the draft model itself increases, the acceptance rate of EAGLE drops significantly, whereas EAGLE-3’s acceptance rate remains almost unchanged, demonstrating the ef- fectiveness of the Training-time test. 4.2 Ablation Study The improvements of EAGLE-3 mainly come from two aspects: first, the removal of the feature re- gression constraint, and second, the improvement 0- 1- 2- 3- 4- 5- 6- 7- 0.50 0.55 0.60 0.65 0.70 0.75 0.80 Accept rate EAGLE EAGLE-3 Figure 7: Acceptance rate of EAGLE and EAGLE-3 on MT-bench, with the target model being LLaMA- Instruct 3.1 8B. Hereby, n-α refers to the acceptance rate when the input contains n estimated features, under the condition that the previous estimated tokens are all accepted by the target model. from reusing only the top-layer features to reusing a mix of low, middle, and high-level features. We conducted an ablation study on MT-bench with LLaMA-Instruct 3.1 8B as the target model. The results, shown in Table 2, indicate that both im- provements in EAGLE-3 significantly enhance the acceptance length and speedup ratio, demonstrat- ing the rationality of the EAGLE-3 design. Table 2: Ablation study results with LLaMA-Instruct 3.1 8B as the target model. “Remove fea con” refers to the first improvement of EAGLE-3, which removes the feature prediction constraint. “Fused features” refers to the second improvement of EAGLE-3, where low, middle, and high-level feature fusion replaces the use of top-layer features. MT-bench GSM8K", "source": "arxiv_pdf", "published": "", "tokens": 352, "sha256": "4d3c74946a9e84b020decfb825e698c1b19d97492db709347a3646965d182347"}
{"doc_id": "arxiv:2503.01840#method", "url": "https://arxiv.org/abs/2503.01840", "anchor": "#method", "type": "paper", "title": "", "section": "Method", "text": "Speedup τ Speedup τ EAGLE-2 3.16x 4.05 3.39x 4.24 + remove fea con 3.82x 5.37 3.77x 5.22 + fused features (ours) 4.40x 6.13 4.48x 6.23 4.3 EAGLE-3 in SGLang Speculative sampling algorithms like EAGLE-3 reduce memory accesses and lower latency dur- ing memory-bound decoding by leveraging redun- dant computational power. As batch sizes increase, this redundancy decreases, reducing the effective- ness of speculative sampling. Efficiency improve- ments are more challenging in highly optimized production-grade frameworks. The performance of EAGLE-3 for large batches on a single H100 GPU and LLaMA-Instruct 3.1 8B in the SGLang v0.4.4 environment (Zheng et al., 2024) was evaluated by the SGLang team, shown in Table 3. This part of the experiment did not use the tree structure, the chain length was set to 3, and the testing dataset was MT-Bench. EAGLE reduces throughput at a batch size of 24, whereas EAGLE-3 still achieves a 38% throughput improvement at a batch size of 64. Table 3: Throughput improvement under different batch sizes on H100 and LLaMA-Instruct 3.1 8B for the MT- Bench dataset, with SGLang without speculative sam- pling as the baseline (1.00x). The experiments were conducted by the SGLang team. Batch size 2 4 8 16 24 32 48 56 64 EAGLE 1.40x 1.38x 1.23x 1.02x 0.93x 0.94x 0.88x 0.99x 0.99x EAGLE-3 1.81x 1.82x 1.62x 1.48x 1.39x 1.32x 1.38x 1.34x 1.38x The SGLang team also tested the throughput of EAGLE-3 at batch size = 1 on H100 when the target model is LLaMA-Instruct 3.1 8B and the testing dataset is MT-bench. The results are shown in Table 4. Table 4: Throughput at batch size = 1 on H100 when the target model is LLaMA-Instruct 3.1 8B and the testing dataset is MT-bench. The experiments were conducted by the SGLang team.", "source": "arxiv_pdf", "published": "", "tokens": 294, "sha256": "b9ded358f9895bf152a5d548694408790426cfa1d2228cc79bae752bcbdbddf9"}
{"doc_id": "arxiv:2503.01840#related-work", "url": "https://arxiv.org/abs/2503.01840", "anchor": "#related-work", "type": "paper", "title": "", "section": "Related Work", "text": "Many methods have been used to accelerate in- ference in LLMs, such as quantization (Hubara et al., 2018; Shen et al., 2020; Kim et al., 2021; Zadeh et al., 2020; Zafrir et al., 2019) and distilla- tion (Hinton et al., 2015). These methods generally have trade-offs, where there is a need to balance model performance with acceleration benefits. Speculative sampling uses the target model for verification to ensure lossless acceleration. Early speculative decoding methods (Stern et al., 2018; Sun et al., 2021) accelerated generation in greedy settings, while Leviathan et al. (2023); Chen et al. (2023) introduced speculative sampling to extend the draft verification framework to non- greedy generation. Many subsequent works have improved upon speculative sampling. EAGLE (Li et al., 2024c), EAGLE-2 (Li et al., 2024b), Medusa (Cai et al., 2024), and Hydra (Ankner et al., 2024) reused the features of the target model. HASS (Zhang et al., 2024) simulates a multi-step draft process during training to mitigate the issues of training-inference inconsistency and error accu- mulation in EAGLE. GLIDE and CAPE (Du et al., 2024) reuse the target model’s KV cache, while methods (Hooper et al., 2023; Yang et al., 2023; Monea et al., 2023; Li et al., 2024a; Yi et al., 2024; Liu et al., 2024b; Sun et al., 2024a; Elhoushi et al., 2024; Svirschevski et al., 2024) like Draft & Verify (Zhang et al., 2023) use layer skipping or early exits to reuse parts of the target model’s parameters. 6", "source": "arxiv_pdf", "published": "", "tokens": 244, "sha256": "adeecfdcf27704b72bd7b1f1e7b45dd08e20516a57fea8326d3ebdda07aac6e7"}
{"doc_id": "arxiv:2503.01840#conclusion", "url": "https://arxiv.org/abs/2503.01840", "anchor": "#conclusion", "type": "paper", "title": "", "section": "Conclusion", "text": "In this paper, we introduce EAGLE-3. Building upon EAGLE, EAGLE-3 incorporates two key im- provements. First, it removes the feature prediction constraint, instead directly predicting draft tokens through a Training-time test. Second, it replaces the use of the target model’s top-layer features with a fusion of the target model’s lower, middle, and upper-layer features to obtain richer informa- tion. With these improvements, EAGLE-3 contin- ues to benefit from the augmentation of training data, achieving a maximum speedup of 6.5x. Acknowledgement We woud like to thank James Liu, Ke Bao, Yineng Zhang, Lianmin Zheng, Ying Sheng, and many oth- ers in the SGLang team for merging and evaluating EAGLE-3 in the SGLang environment.", "source": "arxiv_pdf", "published": "", "tokens": 112, "sha256": "820dbb8a6057e61c1f222891664bf1406978933fb28701489db24612b5d50a91"}
{"doc_id": "arxiv:2312.07104#abstract", "url": "https://arxiv.org/abs/2312.07104", "anchor": "#abstract", "type": "paper", "title": "", "section": "Abstract", "text": "Large language models (LLMs) are increasingly used for complex tasks that require multiple generation calls, advanced prompting techniques, control flow, and structured inputs/outputs. However, efficient systems are lacking for programming and executing these applications. We introduce SGLang, a system for efficient execution of complex language model programs. SGLang consists of a frontend language and a runtime. The frontend simplifies programming with primitives for generation and parallelism control. The runtime accelerates execution with novel optimizations like RadixAttention for KV cache reuse and compressed finite state machines for faster structured output decoding. Experiments show that SGLang achieves up to 6.4× higher throughput compared to state-of-the-art inference systems on various large language and multi-modal models on tasks including agent control, logical reasoning, few-shot learning benchmarks, JSON decoding, retrieval-augmented generation pipelines, and multi-turn chat. The code is publicly available at https://github.com/sgl-project/sglang. 1", "source": "arxiv_pdf", "published": "", "tokens": 138, "sha256": "2595cebe753e084267b19e531869bc04215f9ce69f244c5a31e9e8ba00f2e3a0"}
{"doc_id": "arxiv:2312.07104#introduction:part-1", "url": "https://arxiv.org/abs/2312.07104", "anchor": "#introduction:part-1", "type": "paper", "title": "", "section": "Introduction", "text": "Recent increases in the capabilities of LLMs have broadened their utility, enabling them to tackle a wider range of general tasks and act as autonomous agents [35, 6, 36, 52, 46]. In such applications, LLMs engage in multi-round planning, reasoning, and interaction with external environments. This is accomplished through tool usage [41, 38], multiple input modalities [47, 2], and a wide range of prompting techniques [30], like few-shot learning [5], self-consistency [53], skeleton-of-thought [33], and tree-of-thought [56]. All of these new use cases require multiple, often dependent, LLM generation calls, showing a trend of using multi-call structures to complete complex tasks [57, 21]. The emergence of these patterns signifies a shift in our interaction with LLMs, moving from simple chatting to a more sophisticated form of programmatic usage of LLMs, which means using a program to schedule and control the generation processes of LLMs. We refer to these programs as \"Language Model Programs\" (LM Programs) [4, 20]. The advanced prompting techniques and agentic workflow mentioned above fall within the scope of LM programs. There are two common properties of LM programs: (1) LM programs typically contain multiple LLM calls interspersed with control flow. This is needed to complete complex tasks and improve overall quality. (2) LM programs receive structured inputs and produce structured outputs. This is needed to enable the composition of LM programs and to integrate LM programs into existing software systems. ∗Equal contribution. Preprint. Under review. arXiv:2312.07104v2 [cs.AI] 6 Jun 2024 Interpreter SGLang Client (Frontend) Language primitives (Sec. 2) SGLang Runtime (Backend) Optimizations: RadixAttention (Sec. 3), Compressed finite state machines (Sec. 4), API speculative execution (Sec. 5) Figure 1: System architecture: An interpreter executes language primitives with optimized runtime. Despite the widespread use of LM programs, current systems for expressing and executing them remain inefficient. We identify two primary challenges associated with the efficient use of LM pro- grams: First, programming LM programs is tedious and difficult due to the non-deterministic nature of LLMs. Developing an LM program often requires extensive string manipulation, experimental tuning of prompts, brittle output parsing, handling multiple input modalities, and implementing parallelism mechanisms. This complexity significantly reduces the readability of even simple programs (Sec. 2). Secondly and importantly, executing LM programs is inefficient due to redundant computation and memory usage. State-of-the-art inference engines (e.g., vLLM [23], TGI [16], and TensorRT- LLM [34]), have been optimized to reduce latency and improve throughput without direct knowledge of the workload. This makes these systems general and robust but also results in significant ineffi- ciencies for any given workload. A prominent example is the reuse of the Key-Value (KV) cache (Sec. 3). The KV cache consists of reusable intermediate tensors that are essential for generative inference. During typical batch executions of LM programs, numerous opportunities exist to reuse the KV cache across multiple different LLM calls that share a common prefix. However, current systems lack effective mechanisms to facilitate this reuse, resulting in unnecessary computations and wasted memory. Another example is constrained decoding for structured outputs (e.g., JSON mode), where the output of LLMs is restricted to follow specific", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "a5a9546bbefedd5127d0acc062928e685aff4ab0d6336ee7fe828bc74975db82"}
{"doc_id": "arxiv:2312.07104#introduction:part-2", "url": "https://arxiv.org/abs/2312.07104", "anchor": "#introduction:part-2", "type": "paper", "title": "", "section": "Introduction", "text": "cache (Sec. 3). The KV cache consists of reusable intermediate tensors that are essential for generative inference. During typical batch executions of LM programs, numerous opportunities exist to reuse the KV cache across multiple different LLM calls that share a common prefix. However, current systems lack effective mechanisms to facilitate this reuse, resulting in unnecessary computations and wasted memory. Another example is constrained decoding for structured outputs (e.g., JSON mode), where the output of LLMs is restricted to follow specific grammatical rules defined by a regular expression (Sec. 4). Under these constraints, multiple tokens can often be decoded once. However, existing systems only decode one token at a time, leading to suboptimal decoding speeds. To address these challenges, we present SGLang, a Structured Generation Language for LLMs. The core idea is to systematically exploit the multi-call structure in LM programs for efficient execution. As shown in Fig. 1, it has two parts: a front-end language and a back-end runtime. The front-end simplifies the programming of LM programs, and the runtime accelerates their execution. The two parts can work together for better performance but can also function independently. We introduce SGLang as a domain-specific language embedded in Python. It provides primitives for generation (e.g., extend, gen, select) and parallelism control (e.g., fork, join). SGLang is compatible with Python’s control flow and libraries, so users can develop advanced prompting workflows easily with native Python syntax. We provide an interpreter and a compiler for SGLang. The interpreter manages the prompt state as a stream and submits primitive operations to the stream for asynchronous execution, ensuring proper control over synchronization and intra-program parallelism. Additionally, SGLang program can be traced and compiled for more optimizations. On the runtime side, we propose several novel optimizations to accelerate the execution of SGLang programs. The first technique, RadixAttention, enables the automatic reuse of the KV cache across multiple generation calls. In existing inference engines, the KV cache of a request is discarded after processing is completed, preventing the KV cache from being reused across multiple calls and significantly slowing down the execution. Instead, our system maintains an LRU cache of the KV cache for all requests within a radix tree. This approach manages the KV cache as a traditional cache and uses a radix tree for efficient matching, insertion, and eviction. It allows the runtime to handle various reuse patterns with a cache-aware scheduling policy efficiently. The second technique is a compressed finite state machine, which enables faster constrained decoding for structured outputs. Existing systems follow the constraints only for the next token by masking probabilities of disallowed tokens, making them able to decode only one token at a time. Instead, our system analyzes the constraints and builds a compressed finite-state machine to represent the constraint. This approach compresses a multi-token path into a single-step path whenever possible, allowing the decoding of multiple tokens at once to achieve faster decoding speed. Lastly, SGLang also supports API-only models like OpenAI’s GPT-4, and we introduce the third technique, API speculative execution, to optimize multi-call programs for API-only models. Using SGLang, we implemented", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "b5456631ecc2f6517edee4cef1059664c6798509beca584831f4e6c80cfb24e1"}
{"doc_id": "arxiv:2312.07104#introduction:part-3", "url": "https://arxiv.org/abs/2312.07104", "anchor": "#introduction:part-3", "type": "paper", "title": "", "section": "Introduction", "text": "them able to decode only one token at a time. Instead, our system analyzes the constraints and builds a compressed finite-state machine to represent the constraint. This approach compresses a multi-token path into a single-step path whenever possible, allowing the decoding of multiple tokens at once to achieve faster decoding speed. Lastly, SGLang also supports API-only models like OpenAI’s GPT-4, and we introduce the third technique, API speculative execution, to optimize multi-call programs for API-only models. Using SGLang, we implemented various LLM applications, including agent control, logical reasoning, few-shot learning benchmarks, JSON decoding, retrieval-augmented generation pipelines, multi- turn chat, and multi-modality processing. We tested the performance on models including Llama- 7B/70B [49], Mistral-8x7B [17], LLaVA-v1.5-7B (image) [28], and LLaVA-NeXT-34B (video) [62] on NVIDIA A10G and A100 GPUs. Experimental results show that SGLang achieves up to 6.4× higher throughput across a wide range of workloads, models, and hardware setups, compared to existing programming and inference systems, including Guidance [13], vLLM [23], and LMQL [4]. 2 dimensions = [\"Clarity\", \"Originality\", \"Evidence\"] @function def multi_dimensional_judge(s, path, essay): s += system(\"Evaluate an essay about an image.\") s += user(image(path) + \"Essay:\" + essay) s += assistant(\"Sure!\") # Return directly if it is not related s += user(\"Is the essay related to the image?\") s += assistant(select(\"related\", choices=[\"yes\", \"no\"])) if s[\"related\"] == \"no\": return # Judge multiple dimensions in parallel forks = s.fork(len(dimensions)) for f, dim in zip(forks, dimensions): f += user(\"Evaluate based on the following dimension:\" + dim + \". End your judgment with the word 'END'\") f += assistant(\"Judgment:\" + gen(\"judgment\", stop=\"END\")) # Merge the judgments judgment = \"\\n\".join(f[\"judgment\"] for f in forks) # Generate a summary and a grade. Return in the JSON format. s += user(\"Provide the judgment, summary, and a letter grade\") s += assistant(judgment + \"In summary,\" + gen(\"summary\", stop=\".\") + \"The grade of it is\" + gen(\"grade\")) schema = r'\\{\"summary\": \"[\\w\\d\\s]+\\.\", \"grade\": \"[ABCD][+-]?\"\\}' s += user(\"Return in the JSON format.\") s += assistant(gen(\"output\", regex=schema)) state = multi_dimensional_judge.run(...) print(state[\"output\"]) Runtime optimization: KV Cache Reuse (Sec. 3) Handle chat template and multi-modal inputs Multiple generation calls run in parallel Select an option with the highest probability Fetch result; Use Python control flow Runtime optimization: API speculative execution (Sec. 5) Fetch generation results Runtime optimization: fast constrained decoding (Sec. 4) Run an SGLang program Figure 2: The implementation of a multi-dimensional essay judge in SGLang utilizes the branch-solve-merge prompting technique [40]. Primitives provided by SGLang are shown in red. 2 Programming Model This section introduces the SGLang programming model with a running example, describes its language primitives and execution modes, and outlines runtime optimization opportunities. This programming model can simplify tedious operations in multi-call workflows (e.g., string manipulation, API calling, constraint specification, parallelism) by providing flexible and composable primitives. A running example. The language is a domain-specific language embedded in Python. Fig. 2 shows a program that evaluates an essay about an image using the branch-solve-merge prompting method [40]. The function multi_dimensional_judge takes three arguments: s, path, and essay. s manages the prompt state, path is the image file path, and essay is", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "68dabab9a46add13277d3e33c09a2839bbd0c7a159bb7198e22ff50d92c39503"}
{"doc_id": "arxiv:2312.07104#introduction:part-4", "url": "https://arxiv.org/abs/2312.07104", "anchor": "#introduction:part-4", "type": "paper", "title": "", "section": "Introduction", "text": "optimization opportunities. This programming model can simplify tedious operations in multi-call workflows (e.g., string manipulation, API calling, constraint specification, parallelism) by providing flexible and composable primitives. A running example. The language is a domain-specific language embedded in Python. Fig. 2 shows a program that evaluates an essay about an image using the branch-solve-merge prompting method [40]. The function multi_dimensional_judge takes three arguments: s, path, and essay. s manages the prompt state, path is the image file path, and essay is the essay text. New strings and SGLang primitives can be appended to the state s for execution using the += operator. First, the function adds the image and essay to the prompt. It then checks if the essay is related to the image using select, storing the result in s[\"related\"]. If related, the prompt is forked into three copies for parallel evaluation from different dimensions, using gen to store results in f[\"judgment\"]. Next, it merges the judgments, generates a summary, and assigns a letter grade. Finally, it returns the results in JSON format, following a schema defined by a regular expression constraint regex. SGLang greatly simplifies this program, as an equivalent program using an OpenAI API-like interface would take 2.1× as many lines of code due to manual string manipulation and parallelism control. Language primitives. SGLang provides primitives for controlling prompt state, generation, and parallelism. They can be used together with Python syntax and libraries. Here are the primitives: “gen” calls a model to generate and stores the results in a variable with the name specified in its first argument. It supports a “regex” argument to constrain the output to follow a grammar defined by a regular expression (e.g., a JSON schema). “select” calls a model to choose the highest probability option from a list. The operator “+=” or “extend” appends a string to the prompt. The operator “[variable_name]” fetches the results of a generation. “fork” creates parallel forks of the prompt state. “join” rejoins the prompt state. “image” and “video” take in image and video inputs. Execution modes. The simplest way to execute an SGLang program is through an interpreter, where a prompt is treated as an asynchronous stream. Primitives like extend, gen, and select are submitted to the stream for asynchronous execution. These non-blocking calls allow Python code to continue running without waiting for the generation to finish. This is similar to launching CUDA kernels asynchronously. Each prompt is managed by a stream executor in a background thread, enabling intra-program parallelism. Fetching generation results will block until they are ready, ensuring correct synchronization. Alternatively, SGLang programs can be compiled as computational graphs and 3 Table 1: Comparison among LMQL, Guidance, and SGLang. System Syntax Language Primitives Runtime Backends LMQL Custom extend, gen, select HF Transformers, llama.cpp, OpenAI Guidance Python extend, gen, select, image HF Transformers, llama.cpp, OpenAI SGLang Python extend, gen, select, image, video, fork, join SGLang Runtime (SRT), OpenAI executed with a graph executor, allowing for more optimizations. This paper uses interpreter mode by default and discusses compiler mode results in Appendix D. SGLang supports open-weight models with its", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "028c572f2341ac04a8288142bb4686ba669199ef1bf6f25209de25d6f1808895"}
{"doc_id": "arxiv:2312.07104#introduction:part-5", "url": "https://arxiv.org/abs/2312.07104", "anchor": "#introduction:part-5", "type": "paper", "title": "", "section": "Introduction", "text": "computational graphs and 3 Table 1: Comparison among LMQL, Guidance, and SGLang. System Syntax Language Primitives Runtime Backends LMQL Custom extend, gen, select HF Transformers, llama.cpp, OpenAI Guidance Python extend, gen, select, image HF Transformers, llama.cpp, OpenAI SGLang Python extend, gen, select, image, video, fork, join SGLang Runtime (SRT), OpenAI executed with a graph executor, allowing for more optimizations. This paper uses interpreter mode by default and discusses compiler mode results in Appendix D. SGLang supports open-weight models with its own SGLang Runtime (SRT), as well as API models such as OpenAI and Anthropic models. Comparison. Programming systems for LLMs can be classified as high-level (e.g., LangChain, DSPy) and low-level (e.g., LMQL, Guidance, SGLang). High-level systems provide predefined or auto-generated prompts, such as DSPy’s prompt optimizer. Low-level systems typically do not alter prompts but allow direct manipulation of prompts and primitives. SGLang is a low-level system similar to LMQL and Guidance. Table 1 compares their features. SGLang focuses more on runtime efficiency and comes with its own co-designed runtime, allowing for novel optimizations introduced later. High-level languages (e.g., DSPy) can be compiled to low-level languages (e.g., SGLang). We demonstrate the integration of SGLang as a backend in DSPy for better runtime efficiency in Sec. 6. Runtime optimizations. Fig. 2 shows three runtime optimization opportunities: KV cache reuse, fast constrained decoding, API speculative execution. We will discuss them in the following sections. 3 Efficient KV Cache Reuse with RadixAttention SGLang programs can chain multiple generation calls and create parallel copies with the \"fork\" primitive. Additionally, different program instances often share some common parts (e.g., system prompts). These scenarios create many shared prompt prefixes during execution, leading to numerous opportunities for reusing the KV cache. During LLM inference, the KV cache stores intermediate tensors from the forward pass, reused for decoding future tokens. They are named after key-value pairs in the self-attention mechanism [51]. KV cache computation depends only on prefix tokens. Therefore, requests with the same prompt prefix can reuse the KV cache, reducing redundant computation and memory usage. More background and some examples are provided in Appendix A. Given the KV cache reuse opportunity, a key challenge in optimizing SGLang programs is reusing the KV cache across multiple calls and instances. While some systems explore certain KV cache reuse cases [23, 58, 18, 12], they often need manual configurations and cannot handle all reuse patterns (e.g., dynamic tree structures). Consequently, most state-of-the-art inference systems recompute the KV cache for each request. We will discuss their limitations and our differences in Sec. 7. This section introduces RadixAttention, a novel technique for automatic and systematic KV cache reuse during runtime. Unlike existing systems that discard the KV cache after a generation request finishes, our system retains the cache for prompts and generation results in a radix tree, enabling efficient prefix search, reuse, insertion, and eviction. We implement an LRU eviction policy and a cache-aware scheduling policy to enhance the cache hit rate. RadixAttention is compatible with techniques like continuous batching [60], paged attention [23], and tensor parallelism [44]. In addition, it introduces only negligible", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "2df625def51c8bdd0c1f4dc2261364c83ad638d00e34f4abb303cabccfa732be"}
{"doc_id": "arxiv:2312.07104#introduction:part-6", "url": "https://arxiv.org/abs/2312.07104", "anchor": "#introduction:part-6", "type": "paper", "title": "", "section": "Introduction", "text": "systematic KV cache reuse during runtime. Unlike existing systems that discard the KV cache after a generation request finishes, our system retains the cache for prompts and generation results in a radix tree, enabling efficient prefix search, reuse, insertion, and eviction. We implement an LRU eviction policy and a cache-aware scheduling policy to enhance the cache hit rate. RadixAttention is compatible with techniques like continuous batching [60], paged attention [23], and tensor parallelism [44]. In addition, it introduces only negligible memory and time overhead when there is no cache hit. RadixAttention. A radix tree is a data structure that serves as a space-efficient alternative to a classical trie (prefix tree). Unlike typical trees, the edges of a radix tree can be labeled not just with single elements but also with sequences of elements of varying lengths, significantly enhancing efficiency. In our system, we utilize a radix tree to manage a mapping between sequences of tokens, and their corresponding KV cache tensors. These KV cache tensors are stored in a non-contiguous, paged layout, where the size of each page is equivalent to one token. Because GPU memory is quickly filled by the KV cahce, we introduce a simple LRU eviction policy that evicts the least recently used leaf first. By evicting leaves first, we enable the re-use of their common ancestors until those ancestors become leaves and are also evicted. In the continuous batching setting, we cannot evict nodes used by the currently running batch. Therefore, each node maintains a reference counter indicating how many running requests are using it. A node is evictable if its reference counter is zero. Note that we do not preallocate a fixed-size memory pool as a cache. Instead, we let the cached tokens and the currently running requests share the same memory pool. Therefore, the system dynamically allocates memory for cache and running requests. When enough waiting requests run, the system will evict all cached tokens in favor of a larger batch size. Fig. 3 shows how the radix tree is maintained for several incoming requests. The 4 (1) You are a helpful assistant. User: Hello! Assistant: Hi! a (2) You are a helpful assistant. User: Hello! Assistant: Hi! b User: Solve this problem … Assistant: Sure! … (3) c d You are a helpful assistant. User: Hello! Assistant: Hi! User: What can you do? Assistant: I can ... User: Solve this question… Assistant: Sure! … (4) X You are a helpful assistant. User: Hello! Assistant: Hi! User: What can you do? Assistant: I can ... (5) evicted User: Write a story … Assistant: Sure! … You are a helpful assistant. User: Hello! Assistant: Hi! User: What can you do? Assistant: I can ... (6) User: Write a story … Assistant: Sure! … e Question 1: … Answer 1: … Question 2: ... Answer 2:… Question 3: .. Answer 3: .. f g You are a helpful assistant. User: Hello! Assistant: Hi! User: What can you do? Assistant: I can ... (7) h User: Write a story … Assistant: Sure! … Question 1: … Answer 1: …", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "79c49363d9f4c5194568821a99555214d0ccb3aa5fb10151d11a0c83063bb467"}
{"doc_id": "arxiv:2312.07104#introduction:part-7", "url": "https://arxiv.org/abs/2312.07104", "anchor": "#introduction:part-7", "type": "paper", "title": "", "section": "Introduction", "text": "are a helpful assistant. User: Hello! Assistant: Hi! User: What can you do? Assistant: I can ... (6) User: Write a story … Assistant: Sure! … e Question 1: … Answer 1: … Question 2: ... Answer 2:… Question 3: .. Answer 3: .. f g You are a helpful assistant. User: Hello! Assistant: Hi! User: What can you do? Assistant: I can ... (7) h User: Write a story … Assistant: Sure! … Question 1: … Answer 1: … Question 2: ... Answer 2:… Question 3: What … Answer 3: ... When ... Answer 3: ... How … Answer 3: ... X You are a helpful assistant. User: Hello! Assistant: Hi! (8) X Question 1: … Answer 1: … Question 2: ... Answer 2:… Question 3: j What … Answer 3: ... k When ... Answer 3: ... How … Answer 3: ... l i User: Solve this question… Assistant: Sure! … User: How about ..? Assistant: It is a … evicted You are a helpful assistant. User: Hello! Assistant: Hi! (9) Question 1: … Answer 1: … Question 2: ... Answer 2:… Question 3: What … Answer 3: X evicted X X evicted This is ... Let us ... We can ... To solve … Figure 3: Examples of RadixAttention operations with an LRU eviction policy, illustrated across nine time points. The figure demonstrates the dynamic evolution of the radix tree in response to various requests. These requests include two chat sessions, a batch of few-shot learning inquiries, and a self-consistency sampling. Each tree edge carries a label denoting a substring or a sequence of tokens. The nodes are color-coded to reflect different states: green for newly added nodes, blue for cached nodes accessed during the time point, and red for nodes that have been evicted. In step (1), the radix tree is initially empty. In step (2), the server processes an incoming user message \"Hello\" and responds with the LLM output \"Hi\". The system prompt \"You are a helpful assistant\", the user message \"Hello!\", and the LLM reply \"Hi!\" are consolidated into the tree as a single edge linked to a new node. In step (3), a new prompt arrives and the server finds the prefix of the prompt (i.e., the first turn of the conversation) in the radix tree and reuses its KV cache. The new turn is appended to the tree as a new node. In step (4), a new chat session begins. The node “b” from (3) is split into two nodes to allow the two chat sessions to share the system prompt. In step (5), the second chat session continues. However, due to the memory limit, node \"c\" from (4) must be evicted. The new turn is appended after node \"d\" in (4). In step (6), the server receives a few-shot learning query, processes it, and inserts it into the tree. The root node is split because the new query does not share any prefix with existing nodes. In step (7), the server receives a batch of additional few-shot learning queries. These queries share the", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "3510b59fb1a9579af599a16f1146c9cd075af666af11863a00c8765b30bb75ff"}
{"doc_id": "arxiv:2312.07104#introduction:part-8", "url": "https://arxiv.org/abs/2312.07104", "anchor": "#introduction:part-8", "type": "paper", "title": "", "section": "Introduction", "text": "the second chat session continues. However, due to the memory limit, node \"c\" from (4) must be evicted. The new turn is appended after node \"d\" in (4). In step (6), the server receives a few-shot learning query, processes it, and inserts it into the tree. The root node is split because the new query does not share any prefix with existing nodes. In step (7), the server receives a batch of additional few-shot learning queries. These queries share the same set of few-shot examples, so we split node ’e’ from (6) to enable sharing. In step (8), the server receives a new message from the first chat session. It evicts all nodes from the second chat session (node \"g\" and \"h\") as they are least recently used. In step (9), the server receives a request to sample more answers for the questions in node \"j\" from (8), likely for self-consistency prompting. To make space for these requests, we evict node \"i\", \"k\", and \"l\" in (8). frontend interpreter sends full prompts to the runtime, and the runtime performs prefix matching and reuse. The tree structure is stored on the CPU with negligible maintenance overhead. During the execution of the fork primitive, the frontend sends the prefix first as a hint, ensuring the prefix is correctly inserted into the tree. It then sends the remaining prompts. This \"Frontend Hint\" simplifies runtime scheduling and matching, exemplifying the benefits of frontend-runtime co-design. Cache-aware scheduling. We define the cache hit rate as number of cached prompt tokens number of prompt tokens . When there are many requests in the waiting queue, the order in which they are executed can significantly impact the cache hit rate. For example, if the request scheduler frequently switches between different, unrelated requests, it can lead to cache thrashing and a low hit rate. We design a cache-aware scheduling algorithm to increase the cache hit rate. In the batch-processing setting we sort the 5 FSM state Token LLM decode (c) Decoding process with normal FSM (d) Decoding process with compressed FSM (a) Normal FSM for regex {\"summary\": \" 1 {\"summary\": \" 0 (b) Compressed FSM for regex {\"summary\": \" {\" summary LLM \": LLM LLM _\" LLM LLM {\" summary \": _\" 1 2 3 5 4 6 7 8 10 9 11 { \" s u m m a r y \" : 0 12 13 \" _ Figure 4: The decoding process of normal and compressed FSMs (the underscore _ means a space). requests by matched prefix length and prioritize requests with longer matched prefixes instead of using a first-come, first-served schedule. Alg. 1 (Appendix) shows the pseudo-code for cache-aware scheduling with contiguous batching. The algorithm uses longest-shared-prefix-first order. In more latency-sensitive settings we may still be able to tolerate limited batch re-ordering to improve cache reuse. Additionally, we prove the following theorem for optimal scheduling in the offline case. 2 Theorem 3.1. For a batch of requests, we can achieve an optimal cache hit rate by visiting the radix tree of the requests in the depth-first search order,", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "9ac99b519f79548ebd1a297c8c3402b448bca2ab523aa8d7907fd56100bea5e9"}
{"doc_id": "arxiv:2312.07104#introduction:part-9", "url": "https://arxiv.org/abs/2312.07104", "anchor": "#introduction:part-9", "type": "paper", "title": "", "section": "Introduction", "text": "first-come, first-served schedule. Alg. 1 (Appendix) shows the pseudo-code for cache-aware scheduling with contiguous batching. The algorithm uses longest-shared-prefix-first order. In more latency-sensitive settings we may still be able to tolerate limited batch re-ordering to improve cache reuse. Additionally, we prove the following theorem for optimal scheduling in the offline case. 2 Theorem 3.1. For a batch of requests, we can achieve an optimal cache hit rate by visiting the radix tree of the requests in the depth-first search order, with a cache size ≥the maximum request length. The longest-shared-prefix-first order is equivalent to a depth-first search order. The proof is in Sec. A.3 (Appendix). In the online case, the DFS order will be disrupted, but our schedule still approximates the DFS behavior on the augmented part of the full radix tree, as described in Sec. A.3. While greedy cache-aware scheduling can achieve high throughput, it can lead to starvation. We leave its integration with other fair scheduling methods [42] as future work. Distributed Cases. RadixAttention can be extended to multiple GPUs. For tensor parallelism, each GPU maintains a sharded KV cache. There is no need for additional synchronization because the tree operations are the same. Data parallelism with multiple workers is discussed in Sec. A.4 (Appendix). 4 Efficient Constrained Decoding with Compressed Finite State Machine In LM programs, users often want to constrain the model’s output to follow specific formats, such as JSON schemas. This can improve controllability and robustness, and make the output easier to parse. SGLang offers a regex argument to enforce such constraints using regular expressions, which are expressive enough for many practical scenarios. Existing systems support this by converting a regular expression into a finite state machine (FSM) [54]. During decoding, they maintain the current FSM state, retrieve allowed tokens from the next states, and set the probability of invalid tokens to zero, decoding token by token. This token-by-token approach, however, is inefficient when there are opportunities to decode multiple tokens at once. For example, the constant sequence {\"summary\": \" in Fig. 2 spans multiple tokens in the normal decoding process as shown in Fig. 4 (c), requiring multiple decoding stages, even though there is only one valid next token when decoding it. Therefore, the whole sequence can be decoded in a single step (i.e., forward pass). However, existing systems can only decode one token at a time because the lack of integration between the FSM and the model runner in existing systems prevents multi-token processing, resulting in slow decoding. SGLang overcomes this limitation by creating a fast constrained decoding runtime with a compressed FSM. This runtime analyzes the FSM and compresses adjacent singular-transition edges in the FSM into single edges as demonstrated in Fig. 4 (b), allowing it to recognize when multiple tokens can be decoded together. In Fig. 4 (d), multiple tokens on the compressed transition edge can be decoded in one forward pass, which greatly accelerates the decoding process. It is also general and applicable to all regular expressions. More details on the background and implementation are in Appendix B. 5 Efficient Endpoint Calling", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "4ece268f9c7ff80a4c12b7ff0eb46a105e02f1666839c58cbdb48001c592ea25"}
{"doc_id": "arxiv:2312.07104#introduction:part-10", "url": "https://arxiv.org/abs/2312.07104", "anchor": "#introduction:part-10", "type": "paper", "title": "", "section": "Introduction", "text": "analyzes the FSM and compresses adjacent singular-transition edges in the FSM into single edges as demonstrated in Fig. 4 (b), allowing it to recognize when multiple tokens can be decoded together. In Fig. 4 (d), multiple tokens on the compressed transition edge can be decoded in one forward pass, which greatly accelerates the decoding process. It is also general and applicable to all regular expressions. More details on the background and implementation are in Appendix B. 5 Efficient Endpoint Calling with API Speculative Execution The previous sections introduced optimizations for open-weight models, which require modifications to the model inference process. Additionally, SGLang works with API-access-only models, such as OpenAI’s GPT-4. However, for these models, we can only call a black-box API endpoint. This section introduces a new optimization for black-box API models that accelerates execution and reduces the API cost of multi-call SGLang programs using speculative execution. For ex- ample, a program may ask the model to generate a description of a character with a multi-call 2In practice, the computation is not the same as what is described in the proof of Theorem 3.1 because the unpredictable number of output tokens can cause the recomputation of the KV cache. 6 MMLU ReAct Agents Generative Agents Tree of Thought Skeleton of Thought LLM Judge HellaSwag JSON Decoding Multi-Turn Chat(short) Multi-Turn Chat(long) DSPy RAG Pipeline 0.0 0.2 0.5 0.8 1.0 Throughput (Normalized) SGLang vLLM Guidance LMQL Figure 5: Normalized throughput on Llama-7B models. Higher is better. pattern: s += context + \"name:\" + gen(\"name\", stop=\"\\n\") + \"job:\" + gen(\"job\", stop=\"\\n\"). Naively, the two gen primitives correspond to two API calls, meaning that the user needs to pay for the input token fee on the context twice. In SGLang, we can enable speculative execution on the first call and let it continue the generation of a few more tokens by ignoring the stop condition. The interpreter keeps the additional generation outputs and matches and reuses them with later primitives. In certain cases, with careful prompt engineering, the model can correctly match the template with high accuracy, saving us the latency and input costs of one API call. 6", "source": "arxiv_pdf", "published": "", "tokens": 355, "sha256": "36f386373245c47a468b66f3ebff8acb5bab1ae704b3cdc10420131f91525f8f"}
{"doc_id": "arxiv:2312.07104#evaluation:part-1", "url": "https://arxiv.org/abs/2312.07104", "anchor": "#evaluation:part-1", "type": "paper", "title": "", "section": "Evaluation", "text": "We evaluate the performance of SGLang across diverse LLM workloads. Subsequently, we conduct ablation studies and case studies to demonstrate the effectiveness of specific components. SGLang is implemented in PyTorch [37] with custom CUDA kernels from FlashInfer [59] and Triton [48]. 6.1 Setup Models. We test dense Llama-2 models [49], sparse mixture of experts Mixtral models [17], multi- modal LLaVA image [27] and video models [62], and API model OpenAI’s GPT-3.5. For open-weight models, the number of parameters ranges from 7 billion to 70 billion, and we use float16 precision. Hardware. We run most experiments on AWS EC2 G5 instances, which are equipped with NVIDIA A10G GPUs (24GB). We run 7B models on a single A10G GPU and larger models on multiple A10G GPUs with tensor parallelism [44]. We run some additional experiments on A100G (80GB) GPUs. Baselines. We compare SGLang against both high-level programming systems with their respective languages and default runtimes, as well as low-level inference engines with standard OpenAI-like Completion APIs. Unless otherwise stated, we do not turn on optimizations that will change the computation results so that all systems compute the same results. The baselines include: • Guidance[13], a language for controlling LLMs. We use Guidance v0.1.8 with llama.cpp backend. • vLLM [23], a high-throughput inference engine. We use vLLM v0.2.5 and its default API server3. • LMQL [4], a query language. We use LMQL v0.7.3 with Hugging Face Transformers backend. Workloads. We test the following: 5-shot MMLU [14] and 20-shot HellaSwag [61] benchmarks. We decode one token for MMLU and use primitive select to select the answer with the highest probability for HellaSwag. For the ReAct agent [57] and generative agents [36], we extract the traces from the original papers and replay them. We use the Tree-of-thought [56] for the GSM-8K problems and Skeleton-of-thought [33] for tip generation. We use LLM judges with the branch-solve- merge [40] technique; JSON decoding with a schema specified by a regular expression; Multi-turn chat with 4 turns, where the input of each turn is randomly sampled between 256-512 tokens. Multi- turn chat (short) means short output (4-8 tokens) and multi-turn chat (long) means long output (256-512 tokens); DSPy retrieval-augmented generation (RAG) pipeline [20] in its official example. Metrics. We report two performance metrics: throughput and latency. For throughput, we run a sufficiently large batch of program instances to compute the maximum throughput, comparing the number of program instances executed per second (programs per second, p/s). For latency, we execute a single program at a time without batching and report the average latency for multiple instances. 6.2 End-to-End Performance Results on open-weight models. The latency and throughput results are shown in Fig. 5 and Fig. 6. SGLang improves throughput by up to 6.4× and reduces latency by up to 3.7×. These improvements 3RadixAttention has been partially integrated as an optional experimental feature into the latest version of vLLM; therefore, we used an earlier version for comparison. 7 MMLU ReAct Agents Generative Agents Tree of Thought Skeleton of Thought LLM Judge HellaSwag JSON Decoding Multi-Turn Chat(short) Multi-Turn Chat(long) DSPy RAG Pipeline 0.0 0.2", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "8d1826009111b8049e118b7ed0cc306de3b19a6c1557a7b5840a85f99142257e"}
{"doc_id": "arxiv:2312.07104#evaluation:part-2", "url": "https://arxiv.org/abs/2312.07104", "anchor": "#evaluation:part-2", "type": "paper", "title": "", "section": "Evaluation", "text": "models. The latency and throughput results are shown in Fig. 5 and Fig. 6. SGLang improves throughput by up to 6.4× and reduces latency by up to 3.7×. These improvements 3RadixAttention has been partially integrated as an optional experimental feature into the latest version of vLLM; therefore, we used an earlier version for comparison. 7 MMLU ReAct Agents Generative Agents Tree of Thought Skeleton of Thought LLM Judge HellaSwag JSON Decoding Multi-Turn Chat(short) Multi-Turn Chat(long) DSPy RAG Pipeline 0.0 0.2 0.5 0.8 1.0 Latency (Normalized) SGLang vLLM Guidance LMQL Figure 6: Normalized latency on Llama-7B models. Lower is better. MMLU ReAct Agents Generative Agents Tree of Thought Skeleton of Thought LLM Judge HellaSwag JSON Decoding Multi-Turn Chat(short) Multi-Turn Chat(long) DSPy RAG Pipeline 0.0 0.2 0.5 0.8 1.0 Throughput (Normalized) SGLang vLLM Figure 7: Normalized throughput on Mixtral-8x7B models with tensor parallelism. Higher is better. result from KV cache reuse, the exploitation of parallelism within a single program, and faster constrained decoding. Next, we explain the reasons for the speedup in each benchmark. On MMLU, SGLang can reuse the KV cache of the 5-shot examples with RadixAttention. RadixAt- tention benefits both throughput and latency. RadixAttention reduces total memory usage by sharing the KV cache, allowing for a larger batch size to improve maximum throughput. RadixAttention also reduces the computation of prefill, thus decreasing the first token latency. On HellaSwag, SGLang reuses the KV cache of both few-shot examples and the common question prefix for multiple choices, resulting in two-level sharing. For the ReAct and generative agents, SGLang reuses the KV cache of the agent template and previous calls. On Tree-of-thought and Skeleton-of-thought, SGLang parallelizes the generation calls within a single program and reuses the KV cache as much as possible. On JSON decoding, SGLang accelerates decoding by decoding multiple tokens at once with a compressed finite state machine. In multi-turn chat, SGLang reuses the KV cache of the chat history. The speedup is more noticeable for short outputs because KV cache reuse mostly helps reduce the prefix time. For long outputs, because there is not much sharing between different chat sessions and the decoding time dominates, there is almost no speedup. In the DSPy RAG pipeline, SGLang reuses the KV cache of the common context example. On these benchmarks, the cache hit rate ranges from 50% to 99%. Fig. 13 (Appendix) lists the achieved and optimal cache hit rates for all of them, showing that our cache-aware scheduling approaches 96% of the optimal hit rate on average. We exclude LMQL and Guidance from some of the last five benchmarks due to slow performance and missing functionalities. LMQL’s issues stem from slow token-level processing and an unoptimized backend, while Guidance lacks batching and parallelism support. Results on larger models with tensor parallelism. We run larger models, Mixtral-8x7B and Llama- 70B, with tensor parallelism on the same set of benchmarks and report the results in Fig. 7 and Fig. 12 (Appendix). The speedup on larger models shows a trend similar to that observed on smaller models, indicating that our optimization generalizes well to larger models.", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "f4b8145f5f250d0be556b676ccaaf35e4f27e5d29804892c570881532a0d2f54"}
{"doc_id": "arxiv:2312.07104#evaluation:part-3", "url": "https://arxiv.org/abs/2312.07104", "anchor": "#evaluation:part-3", "type": "paper", "title": "", "section": "Evaluation", "text": "and missing functionalities. LMQL’s issues stem from slow token-level processing and an unoptimized backend, while Guidance lacks batching and parallelism support. Results on larger models with tensor parallelism. We run larger models, Mixtral-8x7B and Llama- 70B, with tensor parallelism on the same set of benchmarks and report the results in Fig. 7 and Fig. 12 (Appendix). The speedup on larger models shows a trend similar to that observed on smaller models, indicating that our optimization generalizes well to larger models. We omit Guidance and LMQL here because they lack efficient implementations of tensor parallelism. Results on multi-modal models. SGLang has native support for multi-modal models with the image and video primitives. The optimizations in this paper are compatible with multi-modal models. For RadixAttention, we compute the hash of the input images and use it as the key in the radix tree, allowing us to reuse the KV cache of the image tokens from the same image. We run LLaVA-v1.5-7B (image) on llava-bench-in-the-wild and LLaVA-NeXT-34B (video) on ActivityNet. Because these models are not well supported by other baseline systems, we use the model authors’ original implementation in Hugging Face Transformers as the baseline. As shown in Table 2, SGLang provides throughput up to 6× higher on these benchmarks. In llava-bench-in-the-wild, there are multiple questions about the same image, and SGLang runtime reuses the KV cache in this case. Production deployment. SGLang has been deployed in Chatbot Arena [8] to serve open-weight models. Due to low traffic for some models, only one SGLang worker serves each. After one month, we observed a 52.4% RadixAttention cache hit rate for LLaVA-Next-34B [28] and 74.1% for Vicuna-33B [7]. Cache hits come from common system messages, frequently reused example images, and multi-turn chat histories. This reduces first-token latency by an average of 1.7× for Vicuna-33B. 8 Table 2: Throughput comparison on multi-modal LLaVA image and video models.", "source": "arxiv_pdf", "published": "", "tokens": 311, "sha256": "6a6dff4d86646d260603224a16f98c5b4e5c96edec28be9ad9b2340ad48f13ae"}
{"doc_id": "arxiv:2312.07104#model", "url": "https://arxiv.org/abs/2312.07104", "anchor": "#model", "type": "paper", "title": "", "section": "Model", "text": "LLaVA-v1.5-7B (image) LLaVA-NeXT-34B (video) Author’s original implementation 0.18 image/s 0.02 frame/s SGLang 1.15 image/s 0.10 frame/s 0 20 40 60 80 100 Cache Hit Rate (%) 20 30 40 (a) Batch Size Throughput (tokens / s) 0 20 40 60 80 100 Cache Hit Rate (%) 200 300 400 (b) Total Latency (s) First Token Latency (s) LLM Judge Tree of Thought MMLU Multi-Turn Chat(short) 0.00 0.25 0.50 0.75 1.00 Throughput (Normalized) (c) No Cache No Tree Structure FCFS Schedule Random Schedule No Frontend Parallelism No Frontend Hint Full Optimization 0.4k 0.6k 0.8k 1.0k 1.2k 10 20 Figure 8: (a)(b) Cache hit rate ablation study. (c) RadixAttention ablation study. Results on API models. We test a prompt that extracts three fields from a Wikipedia page using OpenAI’s GPT-3.5 model. By using few-shot prompting, the accuracy of API speculative execution is high, and it reduces the cost of input tokens by about threefold due to the extraction of three fields. 6.3 Ablation Study Cache hit rate vs. latency/throughput. Fig. 8(a)(b) shows the relationship between cache hit rate and performance metrics (first token latency, total latency, batch size, and throughput) on the tree-of-thought benchmark. The figure is obtained by partially disabling matched tokens at runtime. It shows that a higher cache hit rate leads to a larger batch size, higher throughput, and lower latency. Effectiveness of RadixAttention. We test the effectiveness of RadixAttention and its components on several representative benchmarks. As shown in Fig. 8(c), \"No Cache\" means not using any cache, \"No Tree-Structure\" means using a simple table-based cache instead of a tree-structured cache, \"FCFS Schedule\" means using a first-come-first-serve policy instead of our cache-aware scheduling, \"Random Schedule\" means using a random order to schedule requests, \"No Frontend Parallelism\" means disabling parallelism in the interpreter, \"No Frontend Hint\" means disabling sending the fork hints from the interpreters, and \"Full optimizations\" means we turn on all optimizations. The experimental results show that each of these components is required to achieve the best performance. Disabling parallelism and hints from the frontend interpreter also results in suboptimal runtime performance, highlighting the importance of co-designing the frontend language and runtime. Overhead of RadixAttention. We test the overhead of RadixAttention on a benchmark without any KV cache reuse opportunities. The benchmark measures throughput on the ShareGPT dataset. It takes 74.3 seconds to run 100 requests; however, the time used for managing the RadixAttention data structures is only 0.2 seconds, which is a negligible overhead of less than 0.3%. This is because the complexity of tree operations is linear and small. Thus, we can turn on RadixAttention by default. Effectiveness of the compressed finite state machine. We test the effectiveness of the compressed finite state machine and its components on the JSON decoding benchmark. Experimental results show that the compressed finite state machine increases the throughput by 1.6× because it can decode multiple tokens at once. In addition, we need to preprocess the state machine and reuse it for a batch of requests. Otherwise, redoing the preprocessing for each request makes the throughput 2.4× lower. 7", "source": "arxiv_pdf", "published": "", "tokens": 509, "sha256": "7226f6f3f6363193d94a4ba67a8118f2e428959bd2f4ac6c51c4944c9c309f68"}
{"doc_id": "arxiv:2312.07104#related-work", "url": "https://arxiv.org/abs/2312.07104", "anchor": "#related-work", "type": "paper", "title": "", "section": "Related Work", "text": "Various works have explored the reuse of the KV cache, and many of them are concurrent with our work. Uniquely, our RadixAttention first proposes treating the KV cache as a tree-based LRU cache. It is the first solution that supports multi-level sharing, cache-aware scheduling, frontend-runtime co-scheduling, and distributed cases. vLLM [23] and ChunkedAttention [58] explore some simple reuse cases (e.g., system prompt sharing) but do not cover multi-level tree-structured sharing or LRU caching. PromptCache [12] proposes the modular reuse of the KV cache beyond the prefix but can impact accuracy by up to a 43% drop. HydraGen [18], FlashInfer [59], and ChunkedAttention [58] focus on CUDA kernel optimizations and do not include the concept of an LRU cache. API Serve [1] and LLM-SQL [29] study KV cache reuse for specific applications such as interleaving with external API calls and relational databases, but they do not have our radix tree or cache-aware scheduling. 9 Several LLM programming and agent frameworks exist, such as Guidance [13], LMQL [4], DSPy [20], LangChain [24], AutoGen [55], and LLM Compiler [21]. Guidance and LMQL are most similar to SGLang, and we compare them in Sec. 2. Our innovation lies in novel runtime optimizations for accelerating the proposed programming model. SGLang is compatible with other frameworks and can accelerate them (e.g., the DSPy example in our evaluation). Additionally, SGLang is compatible with many other common inference optimizations [60, 39, 3, 23, 59, 10, 26, 15, 19, 32, 31, 11]. 8 Future Directions and Conclusion Future directions. Despite the progress made with SGLang, several limitations remain that reveal promising directions for future research. These include extending SGLang to support additional output modalities, adapting RadixAttention to operate across multiple levels of the memory hierarchy (e.g., DRAM, Disk) [43], enabling fuzzy semantic matching within RadixAttention, providing higher- level primitives atop SGLang, fixing starvation in cache-aware scheduling [42], and enhancing the SGLang compiler to perform advanced static optimizations such as scheduling and memory planning. Conclusion. We introduce SGLang, a framework for efficient programming and executing structured language model programs. SGLang significantly improves the throughput and latency of complex LM programs through novel optimizations like RadixAttention, compressed finite state machines, and a language interpreter. It is a valuable tool for developing advanced prompting techniques and agent workflows. The source code is publicly available. Acknowledgement This project is supported by the Stanford Center for Automated Reasoning and gifts from Astronomer, Google, IBM, Intel, Lacework, Microsoft, Mohamed Bin Zayed University of Artificial Intelligence, Nexla, Samsung SDS, Uber, and VMware. Lianmin Zheng is supported by a Meta Ph.D. Fellowship. We thank Yuanhan Zhang and Bo Li for the LLaVA-NeXT (video) support.", "source": "arxiv_pdf", "published": "", "tokens": 437, "sha256": "129e7c0bda0b478c546659dcea679a258830823d078fde5b4e2c7975f41d33a4"}
{"doc_id": "arxiv:2401.15077#abstract:part-1", "url": "https://arxiv.org/abs/2401.15077", "anchor": "#abstract:part-1", "type": "paper", "title": "", "section": "Abstract", "text": "Autoregressive decoding makes the inference of Large Language Models (LLMs) time-consuming. In this paper, we reconsider speculative sampling and derive two key observations. Firstly, au- toregression at the feature (second-to-top-layer) level is more straightforward than at the token level. Secondly, the inherent uncertainty in fea- ture (second-to-top-layer) level autoregression constrains its performance. Based on these in- sights, we introduce EAGLE (Extrapolation Al- gorithm for Greater Language-model Efficiency), a simple yet highly efficient speculative sampling framework. By incorporating a token sequence advanced by one time step, EAGLE effectively resolves the uncertainty, enabling precise second- to-top-layer feature prediction with minimal over- head. We conducted comprehensive evaluations of EAGLE, including all models from the Vicuna and LLaMA2-Chat series, the MoE model Mix- tral 8x7B Instruct, and tasks in dialogue, code generation, mathematical reasoning, and instruc- tion following. For LLaMA2-Chat 70B, EAGLE achieved a latency speedup ratio of 2.7x-3.5x, doubled throughput, while maintaining the dis- tribution of the generated text. 1. Introduction Autoregressive decoding, the de facto standard for large language models (LLMs), generates tokens sequentially, leading to slow and costly generation. Speculative sampling (Leviathan et al., 2023; Chen et al., 2023a) based methods address this by dividing the process into a low-cost draft stage and a parallelized verification stage over the drafted tokens, allowing for multiple tokens to be validated in a 1 arXiv:2401.15077v3 [cs.LG] 4 Mar 2025 EAGLE: Speculative Sampling Requires Rethinking Feature Uncertainty single LLM pass. These approaches accelerate generation by producing multiple tokens per pass. More importantly, the verification stage ensures that the text distribution aligns precisely with the decoding results of the original LLM, maintaining the integrity of the generated content. Applying speculative sampling hinges on finding a draft model that mirrors the original LLM’s functionality but with reduced latency, often involving a lower-parameter version from the same LLM series. For instance, in the LLaMA2 (Touvron et al., 2023) series which includes models with 7B, 13B, and 70B parameters, using the 7B model as a draft model of the 70B model is valid, while finding a suitable draft model for the smallest 7B variant is tricky. An alterna- tive could be to use TinyLLaMA (Zhang et al., 2024), but it is not feasible for instruct-tuned models due to the inconsis- tency in instruction templates between LLaMA2-Chat and TinyLLaMA-Chat. Despite the 7B model’s potential as a draft model, its high overhead diminishes acceleration gains. Training a new, appropriately sized draft model specifically for speculative sampling is not an ideal solution either due to the high cost: TinyLLaMA is trained on 3,000B tokens, whereas EAGLE is trained on 2-4B tokens. The key to enhancing acceleration in speculative sampling lies in reducing the time overhead and improving the ac- ceptance rate of the draft by the original LLM (Chen et al., 2023b; Xia et al., 2023; Santilli et al., 2023). Numerous approaches focus on reducing the overhead of the drafting phase. Lookahead (Fu et al., 2023) employs n-gram and Jacobi iteration, while Medusa (Cai et al., 2023) utilizes a set of MLPs that predict tokens based on the second- to-top-layer feature of the original", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "8d459aac5d2b34ea48673ecc60696fbc654e50774716ef46dc87b2addcf944c8"}
{"doc_id": "arxiv:2401.15077#abstract:part-2", "url": "https://arxiv.org/abs/2401.15077", "anchor": "#abstract:part-2", "type": "paper", "title": "", "section": "Abstract", "text": "enhancing acceleration in speculative sampling lies in reducing the time overhead and improving the ac- ceptance rate of the draft by the original LLM (Chen et al., 2023b; Xia et al., 2023; Santilli et al., 2023). Numerous approaches focus on reducing the overhead of the drafting phase. Lookahead (Fu et al., 2023) employs n-gram and Jacobi iteration, while Medusa (Cai et al., 2023) utilizes a set of MLPs that predict tokens based on the second- to-top-layer feature of the original LLM. These strategies significantly decrease the latency in generating drafts, lead- ing to improved acceleration. However, their effectiveness is limited by the lower accuracy of the resulting drafts, with Medusa achieving an accuracy of about 0.6, and Lookahead even lower. In contrast, our method attains an accuracy of approximately 0.8. To overcome these limitations, we introduce EAGLE (Ex- trapolation Algorithm for Greater Language-model Effi- ciency), an efficient speculative sampling method, grounded in the following two observations. Firstly, autoregression at the feature level is simpler than at the token level. In this paper, “features” refer to the second-to-top-layer features of the original LLM, lo- cated before the LM head. Compared to token sequences, which are simple transformations of natural language, fea- ture sequences exhibit more regularity. Autoregressively processing at the feature level and then deriving tokens using the LM head of the original LLM yields better results than directly autoregressively predicting tokens. As illustrated in Figure 4, autoregressively predicting features yields better performance, demonstrated by a higher speedup ratio of Vicuna 7B Vicuna 13B Vicuna 33B LLaMA2-Chat 7B LLaMA2-Chat 13B LLaMA2-Chat 70B Models 0 1 2 3 Speedup 2.13x 1.00x N/A 2.32x 1.00x N/A 2.40x 1.00x 1.22x 1.09x 2.22x 1.00x N/A 2.68x 1.00x N/A 2.67x 1.00x 2.06x 1.84x EAGLE Speculative sampling DistillSpec Vanilla Figure 2: Speedup ratio on the MT-bench for non-greedy (temperature=1) settings. Lookahead is confined to greedy decoding, and the non-greedy generation of Medusa does not guarantee lossless performance. Therefore, EAGLE is not compared with these methods. I 𝑓I 𝑝(am)=0.6 𝑝(always)=0.4 sampling am 𝑓am 𝑝(excited)=0.3 𝑝(ready)=0.7 sampling always 𝑓always 𝑝(begin)=0.8 𝑝(look)=0.2 𝑝always 𝑝I 𝑝am Figure 3: Uncertainty in feature sequences. The next fea- ture following fI is contingent on the sampling outcome and cannot be determined solely based on fI, where both “always” and “am” are possible to follow the token “I” and lead to two branches. 1.9x compared to 1.5x. Secondly, the uncertainty inherent in the sampling pro- cess significantly constrains the performance of predict- ing the next feature. In text generation, the target LLM predicts the distribution of tokens and samples accordingly, introducing randomness. Features, being high-dimensional and continuous, cannot be treated similarly. As depicted in Figure 3, sampling different tokens like “am” or “always” leads to distinct feature sequences, introducing ambiguity into the feature-level autoregression. Medusa faces a simi- lar issue in predicting spaced tokens, where it is uncertain whether the true target for the input fI should be pam or palways. To address this issue, EAGLE inputs the token sequence from one time step ahead, which includes the sampling outcomes, into the draft model. In the example", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "2a6756ab83e5bc32df13d6938cb7f86e8e47d83481eb4f955cd8f906f355d152"}
{"doc_id": "arxiv:2401.15077#abstract:part-3", "url": "https://arxiv.org/abs/2401.15077", "anchor": "#abstract:part-3", "type": "paper", "title": "", "section": "Abstract", "text": "cannot be treated similarly. As depicted in Figure 3, sampling different tokens like “am” or “always” leads to distinct feature sequences, introducing ambiguity into the feature-level autoregression. Medusa faces a simi- lar issue in predicting spaced tokens, where it is uncertain whether the true target for the input fI should be pam or palways. To address this issue, EAGLE inputs the token sequence from one time step ahead, which includes the sampling outcomes, into the draft model. In the example il- lustrated in Figure 3, this involves predicting falways based on fI and talways, and predicting fam based on fI and tam. As illustrated in Figure 4, by addressing the uncertainty, the speedup ratio further increases from 1.9x to 2.8x. We conducted experiments across dialogue, code gener- ation, mathematical reasoning, and instruction following tasks using the MT-bench, HumanEval, GSM8K, and Al- paca datasets, respectively. Tested LLMs included all mod- els from the Vicuna and LLaMA2-Chat series, along with Mixtral 8x7B Instruct. For LLaMA2-Chat 70B, EAGLE 2 EAGLE: Speculative Sampling Requires Rethinking Feature Uncertainty 2 4 6 Epoch 1.5 2.0 2.5 Speedup 2 4 6 Epoch 0.4 0.6 0.8 Acc feature&shifted-token token feature Figure 4: Accuracy and speedup ratio of draft models based on tokens, features and feature&shifted-token at tempera- ture=0, tested on MT-bench with Vicuna 7B as the original LLM. Feature&shifted-token refers to using a feature se- quence and a token sequence advanced by one time step as inputs. achieved a speedup ratio of 2.7x-3.5x, doubled through- put, and theoretically guaranteed the preservation of the generated text’s distribution. Figure 1 and 2 illustrates the performance of EAGLE on the MT-bench (Zheng et al., 2023), a highly realistic benchmark simulating actual ap- plications and real-world scenarios, including multi-turn instructions akin to dialogues with ChatGPT. We have cho- sen to utilize this benchmark as it has been employed by the current state-of-the-art, including Lookahead and Medusa, to demonstrate their speedup ratios. This choice facilitates a fair and direct comparison between our approach and these methods. Compared to the recently proposed specula- tive sampling-based frameworks, Lookahead and Medusa, EAGLE achieves 1.7x-2.1x and 1.5x-1.6x speedups, respec- tively. EAGLE operates in parallel with other acceleration or throughput-improving methods, such as quantization, compilation, etc. Combining EAGLE with these techniques could further reduce the operational costs of LLM systems. For example, with gpt-fast (PyTorch Labs, 2023), EAGLE accelerates LLaMA2-Chat 7B decoding to 160.4 tokens/s on a single RTX 3090 GPU. EAGLE boasts low training costs. For the LLaMA2-Chat 70B model, EAGLE trains a decoder layer with fewer than 1B parameters using no more than 70k dialogues from the ShareGPT dataset. The training is completed in 1-2 days on 4x A100 (40G) GPUs. The training of EAGLE on 7B, 13B and 33B models can even be conducted on a RTX 3090 node in 1-2 days. In practical applications, EAGLE requires only a single training session to provide acceleration for each query. As the number of queries increases, the amortized training cost of EAGLE becomes negligible. Beyond performance, EAGLE offers additional advantages: • Generality: EAGLE is applicable to any autoregres- sive", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "5fbff5cdd85d625304335250d7be5569082099ed92e5642d7911148dbef0185b"}
{"doc_id": "arxiv:2401.15077#abstract:part-4", "url": "https://arxiv.org/abs/2401.15077", "anchor": "#abstract:part-4", "type": "paper", "title": "", "section": "Abstract", "text": "dataset. The training is completed in 1-2 days on 4x A100 (40G) GPUs. The training of EAGLE on 7B, 13B and 33B models can even be conducted on a RTX 3090 node in 1-2 days. In practical applications, EAGLE requires only a single training session to provide acceleration for each query. As the number of queries increases, the amortized training cost of EAGLE becomes negligible. Beyond performance, EAGLE offers additional advantages: • Generality: EAGLE is applicable to any autoregres- sive LLMs (at least in principle). We have applied EAGLE to LLaMA2-Chat (7B, 13B, 70B), Vicuna (7B, 13B, 33B) and Mixtral 8x7B Instruct in a zero- shot way on the MT-bench, GSM8K, HumanEval and alpaca datasets. EAGLE adheres to the commonly used zero-shot/few-shot settings within the LLM com- munity. All experiments employ the same weights, trained exclusively on the ShareGPT dataset, without any additional training on the evaluation datasets. The method adds only a lightweight plug-in (a single trans- former decoder layer) to the LLM, which can be easily deployed in a production environment. • Reliability: EAGLE does not involve any fine-tuning of the original LLM, and the preservation of the output distribution by EAGLE is theoretically guaranteed for both the greedy and non-greedy settings. This is in sharp contrast to Lookahead and Medusa which either focus solely on greedy settings or do not guarantee the preservation of distribution in these settings. 2. Preliminaries Notations. In this paper, “target LLM” denotes the LLM intended for acceleration, while “draft model” refers to the model used for draft generation. “Feature” generally signi- fies the second-to-top-layer feature of a LLM, the hidden state before the LM head. Tokens are denoted by lowercase t, their embeddings by e, features by f, and distributions by p. Sequences are represented in uppercase, for example, Ti:j for (ti, ti+1, . . . , tj). In a LLM, input T1:j is transformed into embeddings E1:j through the embedding layer, then to features F1:j, and the LM Head maps fj to a distribu- tion pj+1 = LM Head(fj), sampling the next token tj+1. Vanilla autoregression at the token level is described by T1:j →E1:j →fj →pj+1 →tj+1 for any integer j ≥1. Speculative sampling. Speculative sampling operates through draft and verification phases, with the drafting phase using a smaller model to generate γ tokens ˆTj+1:j+γ and their distributions ˆPj+1:j+γ. In the verification phase, a single forward pass of the target LLM yields the prob- abilities Pj+1:j+γ. Tokens are then sequentially evalu- ated, with a token ˆtj+i having an acceptance probability min(1, pj+i(ˆtj+i)/ˆpj+i(ˆtj+i)). Upon the rejection of a to- ken ˆtj+i, all subsequent tokens are discarded, and this token is resampled based on a distribution norm(max(0, pj+i − ˆpj+i)). As proven in Appendix A.1 of speculative sampling (Leviathan et al., 2023), this method equates to sampling directly from the target LLM. EAGLE adopts this method, ensuring that the distribution of the generated text re- mains unchanged for both the greedy and non-greedy settings. 3. EAGLE EAGLE, aligning with other speculative sampling-based methods, incorporates both a drafting phase and a verifica- tion phase. 3", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "c148cbf997856999836f19ddc4b26d84bb3d520d5614687295d4e9b858440b6f"}
{"doc_id": "arxiv:2401.15077#abstract:part-5", "url": "https://arxiv.org/abs/2401.15077", "anchor": "#abstract:part-5", "type": "paper", "title": "", "section": "Abstract", "text": "tokens are discarded, and this token is resampled based on a distribution norm(max(0, pj+i − ˆpj+i)). As proven in Appendix A.1 of speculative sampling (Leviathan et al., 2023), this method equates to sampling directly from the target LLM. EAGLE adopts this method, ensuring that the distribution of the generated text re- mains unchanged for both the greedy and non-greedy settings. 3. EAGLE EAGLE, aligning with other speculative sampling-based methods, incorporates both a drafting phase and a verifica- tion phase. 3 EAGLE: Speculative Sampling Requires Rethinking Feature Uncertainty 𝑡1 𝑡2 𝑡3 Smaller LLM 𝑡4 𝑡1 𝑡2 𝑡3 Smaller LLM 𝑡5 𝑡4 Speculative Sampling 𝑡3 2-Gram, Jacobi 𝑡4 2-Gram, Jacobi 𝑡5 𝑡4 Lookahead Medusa Embedding layer & Auto-regression Head 𝑓3 𝑓4 EAGLE 𝑡4 𝑡5 𝑓1 𝑡2 𝑓2 𝑡3 𝑓2 𝑡3 𝑓3 𝑡4 𝑓1 𝑡2 Medusa Head1 𝑡4 𝑓2 𝑡5 Medusa Head2 Embedding layer & Auto-regression Head Figure 5: A comparison of the methods for drafting the fourth and fifth tokens, t4 and t5. t (represented by blue blocks) denotes tokens, and f (orange blocks) signifies the features, with subscripts indicating their positions in the se- quence. The red border indicates the predictions of the draft model. For simplicity, the n in the n-gram for Lookahead, as shown in the figure, has been set to 2. 3.1. Drafting phase The primary distinction between EAGLE and other methods lies predominantly in the drafting phase. Figure 5 illustrates a schematic of the drafting phase for different methods. Speculative sampling (Leviathan et al., 2023; Chen et al., 2023a) and Lookahead (Fu et al., 2023) predict tokens based on tokens. Medusa (Cai et al., 2023) independently predicts t4 and t5 using the feature f2 from the target LLM. EAGLE predicts f3 using the feature sequence (f1, f2) and the token sequence (t2, t3), advanced by one time step. From p4 = LM Head(f3), t4 is sampled. Subsequently, f3 and t4 are concatenated into the input sequence to predict the next feature f4 and sample the subsequent token t5. As illustrated in Figure 6, EAGLE’s draft model comprises three modules: the Embedding layer, LM Head, and Au- toregression Head. The Embedding layer and LM Head employ the parameters of the target LLM and do not neces- sitate additional training. The draft model takes as input a feature sequence of shape (bs, seq len, hidden dim) and an advanced token sequence of shape (bs, seq len). It then con- verts the token sequence into a token embedding sequence of shape (bs, seq len, hidden dim), and concatenates it to form a fused sequence of shape (bs, seq len, 2×hidden dim). The Autoregression Head consisting of an FC layer and a decoder layer. The FC layer reduces the dimensionality of the fused sequence to (bs, seq len, hidden dim) and then we utilize the decoder layer to predict the next feature. The LM Head calculates the distribution based on the feature, from which the next token is sampled. Finally, the predicted fea- ture and the sampled token are concatenated into the input, facilitating the continuation of the autoregressive process. EAGLE creates a", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "4cb2cd9c2fb7a6551f6d3b6e3d48ad3946a15ebacb773ef78bbf28d6ad4ad4d3"}
{"doc_id": "arxiv:2401.15077#abstract:part-6", "url": "https://arxiv.org/abs/2401.15077", "anchor": "#abstract:part-6", "type": "paper", "title": "", "section": "Abstract", "text": "Autoregression Head consisting of an FC layer and a decoder layer. The FC layer reduces the dimensionality of the fused sequence to (bs, seq len, hidden dim) and then we utilize the decoder layer to predict the next feature. The LM Head calculates the distribution based on the feature, from which the next token is sampled. Finally, the predicted fea- ture and the sampled token are concatenated into the input, facilitating the continuation of the autoregressive process. EAGLE creates a tree-structured draft using tree attention, generating a draft tree with depth m and more than m to- kens through m forward passes. For instance, as shown in Figure 6, EAGLE drafts a 10-token tree with just 3 forward passes. The actual tree structure employed by EAGLE is Embedding Transformer Layers How can 𝑒how 𝑒can 𝑓how 𝑓can LM Head can I Embedding One Auto-regression Head I help 𝑒I 𝑒help 𝑓I 𝑓help LM Head make/ help 𝑓can Sampling multiple times Sampling 𝑓I with/ you make 𝑒make 𝑓make 𝑓I a/ our with 𝑒with 𝑓with 𝑓help the/ your you 𝑒you 𝑓you 𝑓help to/ feel target LLM Draft model How can I help make with you the your to feel a our Query Sampling using Original LLM Drafting using FeatExtrapolator Forward 1 Forward 1 Forward 2 Forward 3 Forward 1 Forward 1 Forward 2 Forward 3 can 𝑒can 𝑓how Figure 6: Pipeline of EAGLE. The upper section illustrates the computational process, while the lower section displays the corresponding generation results for each step. In the upper section, green blocks represent token embeddings, or- ange blocks represent features, red boxes indicate the predic- tions of the draft model, and blue modules with snowflake icons represent the use of target LLM parameters, which are not subject to training. detailed in Appendix A.1. 3.2. Training of the draft models Predicting the next feature constitutes a regression task, for which we employ Smooth L1 loss (see Figure 5 EAGLE): Lreg = Smooth L1(fi+1, Draft Model(T2:i+1, F1:i)). Predicting features is an intermediary objective of the draft model, with the ultimate goal being the prediction of tokens to generate a sequence of tokens. Consequently, we also employ classification loss to directly optimize towards this final objective: pi+2 = Softmax(LM Head(fi+1)), ˆpi+2 = Softmax(LM Head( ˆfi+1)), Lcls = Cross Entropy(pi+2, ˆpi+2). By integrating regression loss and classification loss, we train the Autoregression Head using the combined loss func- tion L = Lreg + wclsLcls. Typically, the classification loss 4 EAGLE: Speculative Sampling Requires Rethinking Feature Uncertainty is an order of magnitude larger than the regression loss in numerical terms. Consequently, we set wcls to 0.1. EAGLE’s Autoregression Head is ideally trained with au- toregressively generated text from the target LLM, yet this approach is costly. Fortunately, EAGLE exhibits low sen- sitivity to training data (ablation study in Section 4.3.3). Instead of employing text generated by the target LLM, we utilize a fixed dataset, substantially reducing the overhead. During the drafting phase, EAGLE autoregressively pro- cesses features. Inaccuracies in features can lead to error accumulation. To mitigate this issue, we employ data aug- mentation by", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "6bb8b1bd9559dad4f6b9bc267665b808cd49f751d357bc93b4fd0393a1ed0709"}
{"doc_id": "arxiv:2401.15077#abstract:part-7", "url": "https://arxiv.org/abs/2401.15077", "anchor": "#abstract:part-7", "type": "paper", "title": "", "section": "Abstract", "text": "0.1. EAGLE’s Autoregression Head is ideally trained with au- toregressively generated text from the target LLM, yet this approach is costly. Fortunately, EAGLE exhibits low sen- sitivity to training data (ablation study in Section 4.3.3). Instead of employing text generated by the target LLM, we utilize a fixed dataset, substantially reducing the overhead. During the drafting phase, EAGLE autoregressively pro- cesses features. Inaccuracies in features can lead to error accumulation. To mitigate this issue, we employ data aug- mentation by adding random noise sampled from a uniform distribution U(−0.1, 0.1) to features of the target LLM dur- ing training (Jain et al., 2023). 3.3. Verification phase Employing tree attention, the target LLM computes the probability of each token in the tree-structured draft through a single forward pass. At every node of the draft tree, we recursively apply speculative sampling algorithms to sample or adjust the distribution (details in Appendix A.2), consis- tent with SpecInfer (Miao et al., 2023), ensuring that the distribution of the output text aligns with that of the target LLM. Concurrently, we document accepted tokens and their features for use in the next drafting phase. 4. Experiments Models and tasks. We conducted experiments on Vicuna models (7B, 13B, 33B), LLaMA2-chat models (7B, 13B, 70B), and Mixtral 8x7B Instruct, encompassing the com- mon sizes of current mainstream LLMs. We evaluated EA- GLE across multiple tasks including multi-turn dialogue, code generation, mathematical reasoning, and instruction following, employing the MT-bench (Zheng et al., 2023), HumanEval (Chen et al., 2021), GSM8K (Cobbe et al., 2021), and Alpaca (Taori et al., 2023) datasets, respectively. Speculative sampling (Leviathan et al., 2023) conducted experiments with a batch size of 1, a setting subsequently adopted by other works such as DistillSpec (Zhou et al., 2023) and BiLD (Kim et al., 2023). Similarly, the majority of our experiments also adopted this setting. Experiments with a batch size greater than 1 are presented in Section 4.4. Metrics. Like other speculative sampling-based methods, EAGLE primarily focuses on latency rather than throughput. We assess acceleration effects using the following metrics: • Walltime speedup ratio: The actual test speedup ratio relative to vanilla autoregressive decoding. • Average acceptance length τ: The average number of tokens accepted per forward pass of the target LLM. • Acceptance rate α: The ratio of accepted to generated tokens during drafting, gauges draft accuracy. It’s less applicable for tree drafts due to multiple tokens sam- pled per location with only one accepted. Hence, when measuring this metric, we utilize chain drafts without tree attention, aligning with speculative sampling and DistillSpec. EAGLE’s draft model inputs feature and token sequences. Autoregressive feature processing can propagate errors, so we measure the acceptance rate as n-α, considering n features predicted by the draft model, potentially with inaccuracies. Acceleration of EAGLE theoretically guarantees the preser- vation of the target LLMs’ output distribution. Conse- quently, evaluating the quality of EAGLE’s generated results is both unnecessary and meaningless. Training. We fixed the target LLMs. EAGLE was trained on the ShareGPT dataset, utilizing 68,000 dialogue iterations with a learning rate set at 3e-5. We employed", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "06866de813ee0076871d43ed1ee8121ef60fd3e75604192b3a01fdf8d904acb7"}
{"doc_id": "arxiv:2401.15077#abstract:part-8", "url": "https://arxiv.org/abs/2401.15077", "anchor": "#abstract:part-8", "type": "paper", "title": "", "section": "Abstract", "text": "sequences. Autoregressive feature processing can propagate errors, so we measure the acceptance rate as n-α, considering n features predicted by the draft model, potentially with inaccuracies. Acceleration of EAGLE theoretically guarantees the preser- vation of the target LLMs’ output distribution. Conse- quently, evaluating the quality of EAGLE’s generated results is both unnecessary and meaningless. Training. We fixed the target LLMs. EAGLE was trained on the ShareGPT dataset, utilizing 68,000 dialogue iterations with a learning rate set at 3e-5. We employed the AdamW optimizer with beta values (β1, β2) set to (0.9, 0.95) and implemented gradient clipping of 0.5. The trainable parame- ters of EAGLE corresponding to the 7B, 13B, 33B, and 70B models are 0.24B, 0.37B, 0.56B, and 0.99B, respectively. The trainable parameters of EAGLE for MoE model Mixtral 8x7B is 0.28B. EAGLE is characterized by its low training cost; the Autoregression Head is trainable within 1-2 days on an A100 40G server for the 70B models. 4.1. Effectiveness Figures 1 and 2, along with Table 1, display the speedup ratios of EAGLE. EAGLE demonstrates better acceleration at temperature=0 compared to temperature=1. For instance, for LLaMA2-Chat 13B at temperature=0, the speedup ra- tios range from 3.01x to 3.76x, while at temperature=1, they range from 2.66x to 2.89x. In code generation tasks (HumanEval), EAGLE achieves its best acceleration per- formance. This is attributed to the prevalence of fixed tem- plates in code, making it easier to generate drafts for these templates. Compared to recently introduced speculative sampling-based methods, Lookahead and Medusa, EAGLE is faster by 1.70x-2.08x and 1.47x-1.60x, respectively. Em- ploying speculative sampling in the Vicuna and LLaMA2- Chat series is challenging. For the 7B model, there is no suitable draft model. For other sizes, using the 7B model as the draft model, we iterated through draft lengths from 2 to 10 and reported the highest speedup ratio. For the 13B model, we observed no improvement in speed. For the 33B and 70B models, the speedup ratios were 1.12x and 1.88x, respectively. For DistillSpec, to ensure fairness, we used the same training data as EAGLE. Additionally, the divergence function employed follows the FKL as detailed in Appendix A.1 of the DistillSpec paper. While distillation slightly improved the speedup ratio, the limited enhancement is 5 EAGLE: Speculative Sampling Requires Rethinking Feature Uncertainty Table 1: Speedup ratio and average acceptance length τ on HumanEval, GSM8K, and Alpaca. T denotes temperature, V represents Vicuna, and LC stands for LLaMA2-Chat. HumanEval GSM8K Alpaca", "source": "arxiv_pdf", "published": "", "tokens": 408, "sha256": "240de211b08bb8bfd396b2710f3f48408831870086a2adf1685f1c1614517dd4"}
{"doc_id": "arxiv:2401.15077#model", "url": "https://arxiv.org/abs/2401.15077", "anchor": "#model", "type": "paper", "title": "", "section": "Model", "text": "Speedup τ Speedup τ Speedup τ T=0 V 7B 3.33x 4.29 3.01x 4.00 2.79x 3.86 V13B 3.58x 4.39 3.08x 3.97 3.03x 3.95 V 33B 3.67x 4.28 3.25x 3.94 2.97x 3.61 LC 7B 3.17x 4.24 2.91x 3.82 2.78x 3.71 LC 13B 3.76x 4.52 3.20x 4.03 3.01x 3.83 LC 70B 3.52x 4.42 3.03x 3.93 2.97x 3.77 T=1 V 7B 2.39x 3.43 2.34x 3.29 2.21x 3.30 V13B 2.65x 3.63 2.57x 3.60 2.45x 3.57 V 33B 2.76x 3.62 2.77x 3.60 2.52x 3.32 LC 7B 2.61x 3.79 2.40x 3.52 2.29x 3.33 LC 13B 2.89x 3.78 2.82x 3.67 2.66x 3.55 LC 70B 2.92x 3.76 2.74x 3.58 2.65x 3.47 Table 2: Average acceptance length τ and acceptance rate α on MT-bench. T denotes temperature.", "source": "arxiv_pdf", "published": "", "tokens": 117, "sha256": "34693bd2b7edc0016604d58c83a00a245fab48893738822b8a3e2e4c4274b56a"}
{"doc_id": "arxiv:2401.15077#model:part-1", "url": "https://arxiv.org/abs/2401.15077", "anchor": "#model:part-1", "type": "paper", "title": "", "section": "Model", "text": "τ 0-α 1-α 2-α 3-α 4-α T=0 Vicuna 7B 3.94 0.79 0.74 0.72 0.73 0.67 Vicuna 13B 3.98 0.79 0.74 0.72 0.74 0.70 Vicuna 33B 3.68 0.74 0.69 0.67 0.67 0.66 LLaMA2-Chat 7B 3.62 0.76 0.69 0.67 0.68 0.68 LLaMA2-Chat 13B 3.90 0.77 0.69 0.69 0.70 0.71 LLaMA2-Chat 70B 3.81 0.75 0.69 0.65 0.64 0.64 T=1 Vicuna 7B 3.17 0.71 0.68 0.66 0.66 0.65 Vicuna 13B 3.20 0.73 0.68 0.68 0.67 0.69 Vicuna 33B 3.22 0.71 0.67 0.64 0.64 0.64 LLaMA2-Chat 7B 3.30 0.71 0.66 0.66 0.66 0.64 LLaMA2-Chat 13B 3.45 0.73 0.69 0.66 0.67 0.67 LLaMA2-Chat 70B 3.46 0.73 0.67 0.64 0.66 0.65 because distillation aims to increase the draft model’s ac- ceptance rate, while the bottleneck for speculative sampling performance lies in the high overhead of the draft model. Tables 1 and 2 indicate that in EAGLE, the target LLM gen- erates 3.2-4.5 tokens per forward pass, surpassing vanilla decoding which produces only one token per forward pass, thereby significantly increasing generation speed. As shown in Figure 2 and Appendix B, the acceptance rate for com- pletely accurate feature sequences, 0-α, significantly ex- ceeds that for sequences with a single erroneous feature, 1-α, indicating the impact of feature errors on draft model performance. Yet, the slight variation between 1-α to 4-α underscores EAGLE’s robustness to feature errors and its adept handling of error accumulation. Table 3 reveals that EAGLE achieved a 1.5x speedup with the Mixtral 8x7B Instruct model. This modest acceleration, compared to models like LLaMA, is due to a shorter average acceptance length and the complexity of accelerating MoE models via speculative sampling. MoE models typically require reading the weights of only two experts per token Table 3: Speedup ratio, average acceptance length τ, and acceptance rate α on MT-bench at temperature=0. The target LLM is Mixtral 8x7B Instruct-v0.1. Speedup τ 0-α 1-α 2-α 3-α 4-α 1.50x 3.25 0.67 0.62 0.61 0.64 0.63 Table 4: Generation speed of EAGLE combined with gpt- fast, evaluated on MT-bench with LLaMA2-Chat 7B at temperature=0. Precision FP16 int4 Vanilla (Huggingface) 24.5 tokens/s N/A gpt-fast 55.1 tokens/s 106.9 tokens/s EAGLE + gpt-fast 100.2 tokens/s 160.4 tokens/s during vanilla autoregressive decoding. However, during the verification phase of speculative sampling, processing multiple tokens may necessitate accessing the weights of more than two experts, contrasting with dense decoder-only models where all weights are read regardless of the number of tokens forwarded. 4.2. Case study: EAGLE + gpt-fast EAGLE is compatible with other acceleration technologies. We conducted experiments combining EAGLE with gpt-fast, which employs quantization and compilation for accelera- tion. As shown in Figure 4, by integrating EAGLE with gpt-fast, we increased the generation speed of LLaMA2- Chat 7B on a single RTX 3090 to 160.4 tokens/s. 4.3. Ablation study 4.3.1. TREE ATTENTION EAGLE, similar to SpecInfer and Medusa, employs tree attention, where both the generation and validation of drafts are tree-structured. In contrast, methods like specula- tive sampling do not use tree attention, resulting in chain- structured draft generation and validation. Table 5 and Fig- ure 7 present comparative results indicating the impact of using tree", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "97a349cefa356bf61c114c4eaffd4f769de49e4d0a64a67cc0ccf29468009c95"}
{"doc_id": "arxiv:2401.15077#model:part-2", "url": "https://arxiv.org/abs/2401.15077", "anchor": "#model:part-2", "type": "paper", "title": "", "section": "Model", "text": "EAGLE with gpt-fast, we increased the generation speed of LLaMA2- Chat 7B on a single RTX 3090 to 160.4 tokens/s. 4.3. Ablation study 4.3.1. TREE ATTENTION EAGLE, similar to SpecInfer and Medusa, employs tree attention, where both the generation and validation of drafts are tree-structured. In contrast, methods like specula- tive sampling do not use tree attention, resulting in chain- structured draft generation and validation. Table 5 and Fig- ure 7 present comparative results indicating the impact of using tree attention. The implementation of tree draft and verification in EAGLE results in an approximate increase of 0.6-0.8 in the average acceptance length and about 0.3-0.5 in the speedup ratio. Compared to chain draft and verifica- tion, tree draft and verification do not increase the number of forward passes in the model (both the target LLM and the draft model), but they do increase the number of tokens processed per forward pass. Consequently, the improvement in the speedup ratio is less pronounced than the increase in average acceptance length. Notably, even without em- ploying tree draft and verification, EAGLE demonstrates a 6 EAGLE: Speculative Sampling Requires Rethinking Feature Uncertainty Figure 7: Speedup ratios of EAGLE with and without the use of tree attention. The evaluation dataset is MT-bench, with the temperature parameter set to 0. Table 5: Average acceptance length τ of EAGLE with and without the use of tree attention. The evaluation dataset is MT-bench, with the temperature parameter set to 0. Vicuna LLaMA2-Chat Size Chain Tree Size Chain Tree 7B 3.20 3.94 (+0.74) 7B 3.00 3.62 (+0.62) 13B 3.23 3.98 (+0.75) 13B 3.18 3.90 (+0.68) 33B 2.97 3.68 (+0.71) 70B 3.12 3.81 (+0.69) significant acceleration effect, approximately in the range of 2.3x-2.7x. 4.3.2. INPUTS OF DRAFT MODELS Compared to other speculative sampling-based methods, the key innovation of EAGLE lies in its utilization of fea- tures computed by the target LLM and the incorporation of sampling outcomes into the input of the draft model to address randomness. We conducted an ablation study on Vicuna 7B, assessing draft models with varying inputs. We tested four types of inputs: feature&shifted-token (EA- GLE), feature&unshifted-token, token, and feature. Both feature&shifted-token (EAGLE) and feature&unshifted- token integrate semantic information at different levels. The distinction lies in the fact that feature&shifted-token (EA- GLE) inputs tokens advanced by one time step, equipping it to address randomness effectively. Apart from the use of a FC layer to reduce dimensionality for the feature&token input, the structure of the draft model remains entirely con- sistent. Figure 8 presents the experimental outcomes on the MT-bench with Vicuna 7B as the target LLM. Three observations can be drawn. • First, when the number of parameters of the draft model is limited, utilizing features yields slightly better results than tokens. • Second, merging features and tokens modestly boosts Table 6: The speedup ratios and average acceptance length τ using different training datasets evaluated on the MT-bench, with the target LLM being LLaMA2-Chat 7B and the tem- perature set to 0. “Fixed dataset” refers to both questions and answers originating from the ShareGPT dataset. “Data generated by", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "dc4e7123ddf7ed33bef83b4de4e0326db2aea4244ce35cf15e747ce0e5ccaa8d"}
{"doc_id": "arxiv:2401.15077#model:part-3", "url": "https://arxiv.org/abs/2401.15077", "anchor": "#model:part-3", "type": "paper", "title": "", "section": "Model", "text": "can be drawn. • First, when the number of parameters of the draft model is limited, utilizing features yields slightly better results than tokens. • Second, merging features and tokens modestly boosts Table 6: The speedup ratios and average acceptance length τ using different training datasets evaluated on the MT-bench, with the target LLM being LLaMA2-Chat 7B and the tem- perature set to 0. “Fixed dataset” refers to both questions and answers originating from the ShareGPT dataset. “Data generated by target LLM” denotes that while questions are sourced from the ShareGPT dataset, the answers are gener- ated by the target LLM. Training data Speedup τ Fixed dataset 2.78x 3.62 Data generated by target LLM 2.88x 3.75 performance, mainly as discrete, error-free tokens mit- igate feature error accumulation, evident from the sim- ilar 0-α of feature&unshifted-token and feature-only draft models, with a significantly improved 1-α. • Third, addressing the randomness inherent in the sam- pling process results in the most significant improve- ment. The feature&shifted-token scheme, compared to feature&unshifted-token, adds no complexity yet markedly enhances the draft model’s capability by sim- ply advancing the token by one time step, allowing the draft model to account for the randomness in sampling. 4.3.3. TRAINING DATA EAGLE uses a fixed dataset for training, avoiding increased overhead from using the target LLM for generating training data. Ablation study (see Table 6) shows that data from the target LLM marginally improves performance, indicating EAGLE’s low sensitivity to training data and justifying the fixed dataset approach for cost reduction. 4.4. Batch size and throughput Inference in LLMs is memory-bound (Patterson, 2004; Shazeer, 2019), leaving GPU computational resources un- derutilized. The principle behind the speculative sampling- based approach in enhancing generation speed lies in more effectively utilizing GPU computational resources. As the batch size increases, the available computational capacity of the GPU decreases, leading to a reduction in the ac- celeration effect. In this section, we present experimental results for scenarios where the batch size exceeds 1. As demonstrated in Table 7, the speedup ratio diminishes with increasing batch size. When using Vicuna 7B as the target LLM, the speedup ratio at bs=4 is higher than at bs=3. This is attributed to the fact that, during the verification phase of EAGLE, the target LLM processes multiple tokens in a single forward pass, and the processing at bs=4 is faster than at bs=3. In contrast, with vanilla autoregressive decoding 7 EAGLE: Speculative Sampling Requires Rethinking Feature Uncertainty 2.5 5.0 Epoch 1.5 2.0 2.5 T = 0 Speedup 2.5 5.0 Epoch 2 3 2.5 5.0 Epoch 0.4 0.6 0.8 0- 2.5 5.0 Epoch 0.2 0.4 0.6 1- 2.5 5.0 Epoch 1.0 1.5 2.0 T = 1 2.5 5.0 Epoch 1.5 2.0 2.5 3.0 2.5 5.0 Epoch 0.2 0.4 0.6 2.5 5.0 Epoch 0.2 0.4 0.6 feature&shifted-token feature&unshifted-token token feature Figure 8: Performance of draft models with varying inputs. The target LLM is Vicuna 7B, and the test dataset is MT-bench. Speed refers to the walltime speedup ratio, τ denotes the average acceptance length, 0-α represents the acceptance rate with entirely precise inputs,", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "13287a035dedbf2bafaa01efe8aea2085e5d9dc1702b3f96e37d4a529eb9d3bc"}
{"doc_id": "arxiv:2401.15077#model:part-4", "url": "https://arxiv.org/abs/2401.15077", "anchor": "#model:part-4", "type": "paper", "title": "", "section": "Model", "text": "Epoch 0.2 0.4 0.6 1- 2.5 5.0 Epoch 1.0 1.5 2.0 T = 1 2.5 5.0 Epoch 1.5 2.0 2.5 3.0 2.5 5.0 Epoch 0.2 0.4 0.6 2.5 5.0 Epoch 0.2 0.4 0.6 feature&shifted-token feature&unshifted-token token feature Figure 8: Performance of draft models with varying inputs. The target LLM is Vicuna 7B, and the test dataset is MT-bench. Speed refers to the walltime speedup ratio, τ denotes the average acceptance length, 0-α represents the acceptance rate with entirely precise inputs, 1-α indicates the acceptance rate when the input includes one imprecise feature, and T refers to the temperature. where the target LLM processes one token per forward pass, the speeds at bs=3 and bs=4 are nearly identical. Although speculative sampling-based methods predomi- nantly focus on latency, we also investigated EAGLE’s throughput for batch size > 1, another key metric for LLM systems. Compared to vanilla autoregressive decoding, EA- GLE requires slightly more CUDA memory. For Vicuna 7B as the target LLM, operating under a memory constraint of a single RTX 3090 with 24G of CUDA memory, the maximum batch size (bs) for vanilla autoregressive decod- ing and EAGLE are 8 and 7, respectively. In the case of LLaMA2-Chat 70B, constrained by 4 A100 (40G) GPUs to- taling 160G of CUDA memory, the maximum bs for vanilla autoregressive decoding and EAGLE are 5 and 4, respec- tively. All evaluations were conducted at FP16 precision. We calculated the throughput for different bs and selected the maximum value. Both vanilla autoregressive decoding and EAGLE achieve maximum throughput at their respec- tive maximum bs. Tree attention consumes more computa- tional resources. At bs=7, the computational resources are less abundant, making the non-use of tree attention more advantageous. As illustrated in Table 7, EAGLE achieves a 2x increase in throughput. Table 7: Speedup ratios at different batch sizes and through- put of EAGLE. The evaluation dataset is MT-bench, with the temperature parameter set to 0. Batch size 1 2 3 4 Throughput Vicuna 7B 2.90x 2.87x 2.65x 2.76x 1.97x LLaMA2-Chat 70B 3.01x 2.81x 2.50x 2.40x 1.99x 5. Related Work There has been considerable research into accelerating lan- guage models, involving techniques such as distillation (Hin- ton et al., 2015), quantization (Hubara et al., 2018; Shen et al., 2020; Kim et al., 2021; Zadeh et al., 2020; Zafrir et al., 2019), pruning (Gale et al., 2019; Sanh et al., 2020; Kur- tic et al., 2022; Voita et al., 2019), and innovative network architecture designs (Gu & Dao, 2023; Wu et al., 2020). These methods aim to reduce the latency per forward pass. Similar to our approach are frameworks based on specula- tive sampling. Early works (Stern et al., 2018; Sun et al., 2021) accelerated greedy decoding, while speculative sam- pling (Leviathan et al., 2023; Chen et al., 2023a) extended it to non-greedy sampling, provably maintaining the original output distribution. Ensuring unchanged output distribution makes acceleration more challenging; many studies have explored lossy acceleration as a trade-off. For instance, 8 EAGLE: Speculative Sampling Requires Rethinking Feature Uncertainty DistillSpec (Zhou et al., 2023) modifies acceptance proba- bilities using a lenience", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "34558bbfd98a987256388143979e37a4dfcdab56fbcca9e34ef599957d401dcf"}
{"doc_id": "arxiv:2401.15077#model:part-5", "url": "https://arxiv.org/abs/2401.15077", "anchor": "#model:part-5", "type": "paper", "title": "", "section": "Model", "text": "based on specula- tive sampling. Early works (Stern et al., 2018; Sun et al., 2021) accelerated greedy decoding, while speculative sam- pling (Leviathan et al., 2023; Chen et al., 2023a) extended it to non-greedy sampling, provably maintaining the original output distribution. Ensuring unchanged output distribution makes acceleration more challenging; many studies have explored lossy acceleration as a trade-off. For instance, 8 EAGLE: Speculative Sampling Requires Rethinking Feature Uncertainty DistillSpec (Zhou et al., 2023) modifies acceptance proba- bilities using a lenience function, BiLD (Kim et al., 2023) accepts drafts if the distance metric from the target LLM distribution is below a certain threshold, and Medusa (Cai et al., 2023) uses a minimum of a hard threshold and an entropy-dependent threshold for truncation. In contrast, EA- GLE does not employ any relaxations and maintains the output distribution of the LLM unchanged. The primary differences among speculative sampling-based methods manifest predominantly in the drafting phase. Spec- ulative sampling (Leviathan et al., 2023; Chen et al., 2023a) utilizes a lower-parameter version of the target LLM as the draft model. Self-Speculative Decoding (Zhang et al., 2023) skips some layers of the target LLM during draft generation. SpecInfer (Miao et al., 2023) employs a set of small models to generate drafts in parallel. Cascade Speculative Drafting (Chen et al., 2023b) and Staged Speculative Decoding (Spec- tor & Re, 2023) cascade different overhead draft models. Online Speculative Decoding (Liu et al., 2023) trains the draft model on a distribution of queries. Methods (Hooper et al., 2023; Fu et al., 2023; Yang et al., 2023b) such as Medusa (Cai et al., 2023) do not employ a separate target LLM; instead, they generate drafts by utilizing features or weights from the target LLM. REST (He et al., 2023) gener- ates drafts based on retrieval methods. LLMA (Yang et al., 2023a), used for tasks like grammatical correction where input and output overlap, retrieves drafts directly from the input. 6. Conclusion In this paper, we introduce EAGLE, an efficient framework for speculative sampling. EAGLE conducts the drafting process autoregressively at the more structured (second-to- top-layer) feature level and mitigates sampling uncertainty in predicting the next feature by incorporating tokens from one time step ahead. EAGLE is guaranteed to preserve the output distribution of the LLM while significantly enhancing generation speed. On MT-bench, EAGLE is 2.1x-3.8x faster than vanilla autoregressive decoding, 1.7x-2.1x faster than Lookahead, and 1.5x-1.6x faster than Medusa. Acknowledgements. We acknowledge useful discussions with the Medusa’s team leader Tianle Cai, the Lookahead’s team leader Hao Zhang, the SpecTr’s team leader Ziteng Sun, interactions with the gpt-fast team leaders Horace He and Soumith Chintala on X, and Yihan Wu.", "source": "arxiv_pdf", "published": "", "tokens": 438, "sha256": "ca5cd832584fedad85a6a1ef853a814ce67e9d2df76987c59856934ba66c545a"}
{"doc_id": "arxiv:2205.14135#abstract", "url": "https://arxiv.org/abs/2205.14135", "anchor": "#abstract", "type": "paper", "title": "", "section": "Abstract", "text": "Transformers are slow and memory-hungry on long sequences, since the time and memory complexity of self-attention are quadratic in sequence length. Approximate attention methods have attempted to address this problem by trading oﬀmodel quality to reduce the compute complexity, but often do not achieve wall-clock speedup. We argue that a missing principle is making attention algorithms IO- aware—accounting for reads and writes between levels of GPU memory. We propose FlashAttention, an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM. We analyze the IO complexity of FlashAttention, showing that it requires fewer HBM accesses than standard attention, and is optimal for a range of SRAM sizes. We also extend FlashAttention to block-sparse attention, yielding an approximate attention algorithm that is faster than any existing approximate attention method. FlashAttention trains Transformers faster than existing baselines: 15% end-to-end wall-clock speedup on BERT-large (seq. length 512) compared to the MLPerf 1.1 training speed record, 3× speedup on GPT-2 (seq. length 1K), and 2.4× speedup on long-range arena (seq. length 1K-4K). FlashAttention and block-sparse FlashAttention enable longer context in Transformers, yielding higher quality models (0.7 better perplexity on GPT-2 and 6.4 points of lift on long-document classiﬁcation) and entirely new capabilities: the ﬁrst Transformers to achieve better-than-chance performance on the Path-X challenge (seq. length 16K, 61.4% accuracy) and Path-256 (seq. length 64K, 63.1% accuracy). 1", "source": "arxiv_pdf", "published": "", "tokens": 236, "sha256": "f8164f1a016477a22ce6f3f9b840d6ddc5f15975a4445bae4289d2cd8483bc99"}
{"doc_id": "arxiv:2205.14135#introduction:part-1", "url": "https://arxiv.org/abs/2205.14135", "anchor": "#introduction:part-1", "type": "paper", "title": "", "section": "Introduction", "text": "Transformer models [82] have emerged as the most widely used architecture in applications such as natural language processing and image classiﬁcation. Transformers have grown larger [5] and deeper [83], but equipping them with longer context remains diﬃcult [80], since the self-attention module at their heart has time and memory complexity quadratic in sequence length. An important question is whether making attention faster and more memory-eﬃcient can help Transformer models address their runtime and memory challenges for long sequences. Many approximate attention methods have aimed to reduce the compute and memory requirements of attention. These methods range from sparse-approximation [51, 74] to low-rank approximation [12, 50, 84], and their combinations [3, 9, 92]. Although these methods reduce the compute requirements to linear or near-linear in sequence length, many of them do not display wall-clock speedup against standard attention and have not gained wide adoption. One main reason is that they focus on FLOP reduction (which may not correlate with wall-clock speed) and tend to ignore overheads from memory access (IO). In this paper, we argue that a missing principle is making attention algorithms IO-aware [1]—that is, carefully accounting for reads and writes to diﬀerent levels of fast and slow memory (e.g., between fast GPU on-chip SRAM and relatively slow GPU high bandwidth memory, or HBM [45], Figure 1 left). On modern 1 arXiv:2205.14135v2 [cs.LG] 23 Jun 2022 FlashAttention Memory Hierarchy with Bandwidth & Memory Size Attention on GPT-2 FlashAttention PyTorch Time (ms) Matmul Mask Softmax Dropout Matmul Fused Kernel Q: N x d V: N X d KT: d x N QKT: N x N sm(QKT)V: N x d Outer Loop Copy Block to SRAM Copy Outer Loop Copy Inner Loop Compute Block on SRAM Output to HBM Inner Loop Inner Loop Outer Loop GPU SRAM GPU HBM Main Memory (CPU DRAM) SRAM: 19 TB/s (20 MB) HBM: 1.5 TB/s (40 GB) DRAM: 12.8 GB/s (>1 TB) 0 5 10 15 Figure 1: Left: FlashAttention uses tiling to prevent materialization of the large 𝑁× 𝑁attention matrix (dotted box) on (relatively) slow GPU HBM. In the outer loop (red arrows), FlashAttention loops through blocks of the K and V matrices and loads them to fast on-chip SRAM. In each block, FlashAttention loops over blocks of Q matrix (blue arrows), loading them to SRAM, and writing the output of the attention computation back to HBM. Right: Speedup over the PyTorch implementation of attention on GPT-2. FlashAttention does not read and write the large 𝑁× 𝑁attention matrix to HBM, resulting in an 7.6× speedup on the attention computation. GPUs, compute speed has out-paced memory speed [61, 62, 63], and most operations in Transformers are bottlenecked by memory accesses [43]. IO-aware algorithms have been critical for similar memory-bound operations, when reading and writing data can account for a large portion of the runtime—such as database joins [71], image processing [70], numerical linear algebra [4], and more [40, 85]. However, common Python interfaces to deep learning such as PyTorch and Tensorﬂow do not allow ﬁne-grained control of memory access. We propose FlashAttention, a new attention algorithm that computes", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "7b43f13e470a5cef0c336849227e07e856ec99c209f9fe463bc9d4debd7c11d1"}
{"doc_id": "arxiv:2205.14135#introduction:part-2", "url": "https://arxiv.org/abs/2205.14135", "anchor": "#introduction:part-2", "type": "paper", "title": "", "section": "Introduction", "text": "62, 63], and most operations in Transformers are bottlenecked by memory accesses [43]. IO-aware algorithms have been critical for similar memory-bound operations, when reading and writing data can account for a large portion of the runtime—such as database joins [71], image processing [70], numerical linear algebra [4], and more [40, 85]. However, common Python interfaces to deep learning such as PyTorch and Tensorﬂow do not allow ﬁne-grained control of memory access. We propose FlashAttention, a new attention algorithm that computes exact attention with far fewer memory accesses. Our main goal is to avoid reading and writing the attention matrix to and from HBM. This requires (i) computing the softmax reduction without access to the whole input (ii) not storing the large intermediate attention matrix for the backward pass. We apply two well-established techniques to address these challenges. (i) We restructure the attention computation to split the input into blocks and make several passes over input blocks, thus incrementally performing the softmax reduction (also known as tiling). (ii) We store the softmax normalization factor from the forward pass to quickly recompute attention on-chip in the backward pass, which is faster than the standard approach of reading the intermediate attention matrix from HBM. We implement FlashAttention in CUDA to achieve ﬁne-grained control over memory access and fuse all the attention operations into one GPU kernel. Even with the increased FLOPs due to recomputation, our algorithm both runs faster (up to 7.6x on GPT-2 [67], Figure 1 right) and uses less memory—linear in sequence length—than standard attention, thanks to the massively reduced amount of HBM access. We analyze the IO complexity [1] of FlashAttention, proving that it requires 𝑂(𝑁2𝑑2𝑀−1) HBM accesses where 𝑑is the head dimension and 𝑀is the size of SRAM, as compared to Ω(𝑁𝑑+ 𝑁2) of standard attention. For typical values of 𝑑and 𝑀, FlashAttention requires many times fewer HBM accesses compared to standard attention (up to 9× fewer, as shown in Fig. 2). Moreover, we provide a lower bound, showing that no exact attention algorithm can asymptotically improve on the number of HBM accesses over all SRAM sizes. We also show that FlashAttention can serve as a useful primitive for realizing the potential of approximate attention algorithms by overcoming their issues with memory access overhead. As a proof of concept, we implement block-sparse FlashAttention, a sparse attention algorithm that is 2-4× faster than even FlashAttention, scaling up to sequence length of 64k. We prove that block-sparse FlashAttention has better IO complexity than FlashAttention by a factor proportional to the sparsity ratio. We discuss further extensions to other operations (attention on multi-GPU, kernel regression, block-sparse matrix 2 multiply) in Section 5. We open-source FlashAttention to make it easier to build on this primitive.1 We empirically validate that FlashAttention speeds up model training and improves model quality by modeling longer context. We also benchmark the runtime and memory footprint of FlashAttention and block-sparse FlashAttention compared to prior attention implementations. • Faster Model Training. FlashAttention trains Transformer models faster in wall-clock time. We train BERT-large (seq. length 512) 15% faster than the training speed record", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "b774aae2868d49e1309dd14e6b14b389fb38e3220c575394ab93ba6cbcf959cb"}
{"doc_id": "arxiv:2205.14135#introduction:part-3", "url": "https://arxiv.org/abs/2205.14135", "anchor": "#introduction:part-3", "type": "paper", "title": "", "section": "Introduction", "text": "regression, block-sparse matrix 2 multiply) in Section 5. We open-source FlashAttention to make it easier to build on this primitive.1 We empirically validate that FlashAttention speeds up model training and improves model quality by modeling longer context. We also benchmark the runtime and memory footprint of FlashAttention and block-sparse FlashAttention compared to prior attention implementations. • Faster Model Training. FlashAttention trains Transformer models faster in wall-clock time. We train BERT-large (seq. length 512) 15% faster than the training speed record in MLPerf 1.1 [58], GPT2 (seq. length 1K) 3× faster than baseline implementations from HuggingFace [87] and Megatron-LM [77], and long-range arena (seq. length 1K-4K) 2.4× faster than baselines. • Higher Quality Models. FlashAttention scales Transformers to longer sequences, which improves their quality and enables new capabilities. We observe a 0.7 improvement in perplexity on GPT-2 and 6.4 points of lift from modeling longer sequences on long-document classiﬁcation [13]. FlashAttention enables the ﬁrst Transformer that can achieve better-than-chance performance on the Path-X [80] challenge, solely from using a longer sequence length (16K). Block-sparse FlashAttention enables a Transformer to scale to even longer sequences (64K), resulting in the ﬁrst model that can achieve better-than-chance performance on Path-256. • Benchmarking Attention. FlashAttention is up to 3× faster than the standard attention implemen- tation across common sequence lengths from 128 to 2K and scales up to 64K. Up to sequence length of 512, FlashAttention is both faster and more memory-eﬃcient than any existing attention method, whereas for sequence length beyond 1K, some approximate attention methods (e.g., Linformer) start to become faster. On the other hand, block-sparse FlashAttention is faster than all existing approximate attention methods that we know of. 2", "source": "arxiv_pdf", "published": "", "tokens": 277, "sha256": "2eeddb6768ca62a8a25ca34bcde37504e3dec228c30e46522f85399399070afe"}
{"doc_id": "arxiv:2205.14135#background:part-1", "url": "https://arxiv.org/abs/2205.14135", "anchor": "#background:part-1", "type": "paper", "title": "", "section": "Background", "text": "We provide some background on the performance characteristics of common deep learning operations on modern hardware (GPUs). We also describe the standard implementation of attention. 2.1 Hardware Performance We focus here on GPUs. Performance on other hardware accelerators are similar [46, 48]. GPU Memory Hierarchy. The GPU memory hierarchy (Fig. 1 left) comprises multiple forms of memory of diﬀerent sizes and speeds, with smaller memory being faster. As an example, the A100 GPU has 40-80GB of high bandwidth memory (HBM) with bandwidth 1.5-2.0TB/s and 192KB of on-chip SRAM per each of 108 streaming multiprocessors with bandwidth estimated around 19TB/s [44, 45]. The on-chip SRAM is an order of magnitude faster than HBM but many orders of magnitude smaller in size. As compute has gotten faster relative to memory speed [61, 62, 63], operations are increasingly bottlenecked by memory (HBM) accesses. Thus exploiting fast SRAM becomes more important. Execution Model. GPUs have a massive number of threads to execute an operation (called a kernel). Each kernel loads inputs from HBM to registers and SRAM, computes, then writes outputs to HBM. Performance characteristics. Depending on the balance of computation and memory accesses, op- erations can be classiﬁed as either compute-bound or memory-bound. This is commonly measured by the arithmetic intensity [85], which is the number of arithmetic operations per byte of memory access. 1. Compute-bound: the time taken by the operation is determined by how many arithmetic operations there are, while time accessing HBM is much smaller. Typical examples are matrix multiply with large inner dimension, and convolution with large number of channels. 2. Memory-bound: the time taken by the operation is determined by the number of memory accesses, while time spent in computation is much smaller. Examples include most other operations: elementwise (e.g., activation, dropout), and reduction (e.g., sum, softmax, batch norm, layer norm). Kernel fusion. The most common approach to accelerate memory-bound operations is kernel fusion: if there are multiple operations applied to the same input, the input can be loaded once from HBM, instead of multiple times for each operation. Compilers can automatically fuse many elementwise operations [53, 65, 75]. 1FlashAttention code is available at https://github.com/HazyResearch/flash-attention 3 However, in the context of model training, the intermediate values still need to be written to HBM to save for the backward pass, reducing the eﬀectiveness of naive kernel fusion. 2.2 Standard Attention Implementation Given input sequences Q, K, V ∈R𝑁×𝑑where 𝑁is the sequence length and 𝑑is the head dimension, we want to compute the attention output O ∈R𝑁×𝑑: S = QK⊤∈R𝑁×𝑁, P = softmax(S) ∈R𝑁×𝑁, O = PV ∈R𝑁×𝑑, where softmax is applied row-wise. Standard attention implementations materialize the matrices S and P to HBM, which takes 𝑂(𝑁2) memory. Often 𝑁≫𝑑(e.g., for GPT2, 𝑁= 1024 and 𝑑= 64). We describe the standard attention implementation in Algorithm 0. As some or most of the operations are memory-bound (e.g., softmax), the large number of memory accesses translates to slow wall-clock time. This problem is exacerbated by other elementwise operations applied to the attention matrix, such as masking applied to S or dropout applied to P.", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "5342eb283a63f9625062fb699703403babe5bdc589c775336f75182fd261b0b1"}
{"doc_id": "arxiv:2205.14135#background:part-2", "url": "https://arxiv.org/abs/2205.14135", "anchor": "#background:part-2", "type": "paper", "title": "", "section": "Background", "text": "row-wise. Standard attention implementations materialize the matrices S and P to HBM, which takes 𝑂(𝑁2) memory. Often 𝑁≫𝑑(e.g., for GPT2, 𝑁= 1024 and 𝑑= 64). We describe the standard attention implementation in Algorithm 0. As some or most of the operations are memory-bound (e.g., softmax), the large number of memory accesses translates to slow wall-clock time. This problem is exacerbated by other elementwise operations applied to the attention matrix, such as masking applied to S or dropout applied to P. As a result, there have been many attempts to fuse several elementwise operations, such as fusing masking with softmax [77]. In Section 3.2, we will show that the standard attention implementation performs HBM accesses quadratic in the sequence length 𝑁. We also compare the number of FLOPs and number of HBM accesses of standard attention and of our method (FlashAttention). Algorithm 0 Standard Attention Implementation Require: Matrices Q, K, V ∈R𝑁×𝑑in HBM. 1: Load Q, K by blocks from HBM, compute S = QK⊤, write S to HBM. 2: Read S from HBM, compute P = softmax(S), write P to HBM. 3: Load P and V by blocks from HBM, compute O = PV, write O to HBM. 4: Return O. 3 FlashAttention: Algorithm, Analysis, and Extensions We show how to compute exact attention with fewer HBM reads/writes and without storing large intermediate matrices for the backward pass. This yields an attention algorithm that is both memory eﬃcient and faster in wall-clock time. We analyze its IO complexity, showing that our method requires much fewer HBM accesses compared to standard attention. We further show that FlashAttention can serve as a useful primitive by extending it to handle block-sparse attention. We focus here on the forward pass for ease of exposition; Appendix B contains details for the backward. 3.1 An Eﬃcient Attention Algorithm With Tiling and Recomputation Given the inputs Q, K, V ∈R𝑁×𝑑in HBM, we aim to compute the attention output O ∈R𝑁×𝑑and write it to HBM. Our goal is to reduce the amount of HBM accesses (to sub-quadratic in 𝑁). We apply two established techniques (tiling, recomputation) to overcome the technical challenge of computing exact attention in sub-quadratic HBM accesses. We describe this in Algorithm 1. The main idea is that we split the inputs Q, K, V into blocks, load them from slow HBM to fast SRAM, then compute the attention output with respect to those blocks. By scaling the output of each block by the right normalization factor before adding them up, we get the correct result at the end. Tiling. We compute attention by blocks. Softmax couples columns of K, so we decompose the large softmax with scaling [51, 60, 66]. For numerical stability, the softmax of vector 𝑥∈R𝐵is computed as: 𝑚(𝑥) := max 𝑖 𝑥𝑖, 𝑓(𝑥) := \u0002 𝑒𝑥1−𝑚(𝑥) . . . 𝑒𝑥𝐵−𝑚(𝑥)\u0003 , ℓ(𝑥) := ∑︁ 𝑖 𝑓(𝑥)𝑖, softmax(𝑥) := 𝑓(𝑥) ℓ(𝑥) . 4 For vectors 𝑥(1), 𝑥(2) ∈R𝐵, we can decompose the softmax of the concatenated 𝑥= \u0002 𝑥(1) 𝑥(2)\u0003 ∈R2𝐵as: 𝑚(𝑥) = 𝑚( \u0002 𝑥(1) 𝑥(2)\u0003 ) = max(𝑚(𝑥(1)), 𝑚(𝑥(2))), 𝑓(𝑥) = h 𝑒𝑚(𝑥(1))−𝑚(𝑥)", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "f66dee61891ef1bc4151b4b1d3c46eb9697f6124ed0b06e9fd7a7717bf75b48d"}
{"doc_id": "arxiv:2205.14135#background:part-3", "url": "https://arxiv.org/abs/2205.14135", "anchor": "#background:part-3", "type": "paper", "title": "", "section": "Background", "text": "of K, so we decompose the large softmax with scaling [51, 60, 66]. For numerical stability, the softmax of vector 𝑥∈R𝐵is computed as: 𝑚(𝑥) := max 𝑖 𝑥𝑖, 𝑓(𝑥) := \u0002 𝑒𝑥1−𝑚(𝑥) . . . 𝑒𝑥𝐵−𝑚(𝑥)\u0003 , ℓ(𝑥) := ∑︁ 𝑖 𝑓(𝑥)𝑖, softmax(𝑥) := 𝑓(𝑥) ℓ(𝑥) . 4 For vectors 𝑥(1), 𝑥(2) ∈R𝐵, we can decompose the softmax of the concatenated 𝑥= \u0002 𝑥(1) 𝑥(2)\u0003 ∈R2𝐵as: 𝑚(𝑥) = 𝑚( \u0002 𝑥(1) 𝑥(2)\u0003 ) = max(𝑚(𝑥(1)), 𝑚(𝑥(2))), 𝑓(𝑥) = h 𝑒𝑚(𝑥(1))−𝑚(𝑥) 𝑓(𝑥(1)) 𝑒𝑚(𝑥(2))−𝑚(𝑥) 𝑓(𝑥(2)) i , ℓ(𝑥) = ℓ( \u0002 𝑥(1) 𝑥(2)\u0003 ) = 𝑒𝑚(𝑥(1))−𝑚(𝑥)ℓ(𝑥(1)) + 𝑒𝑚(𝑥(2))−𝑚(𝑥)ℓ(𝑥(2)), softmax(𝑥) = 𝑓(𝑥) ℓ(𝑥) . Therefore if we keep track of some extra statistics (𝑚(𝑥), ℓ(𝑥)), we can compute softmax one block at a time.2 We thus split the inputs Q, K, V into blocks (Algorithm 1 line 3), compute the softmax values along with extra statistics (Algorithm 1 line 10), and combine the results (Algorithm 1 line 12). Recomputation. One of our goals is to not store 𝑂(𝑁2) intermediate values for the backward pass. The backward pass typically requires the matrices S, P ∈R𝑁×𝑁to compute the gradients with respect to Q, K, V. However, by storing the output O and the softmax normalization statistics (𝑚, ℓ), we can recompute the attention matrix S and P easily in the backward pass from blocks of Q, K, V in SRAM. This can be seen as a form of selective gradient checkpointing [10, 34]. While gradient checkpointing has been suggested to reduce the maximum amount of memory required [66], all implementations (that we know oﬀ) have to trade speed for memory. In contrast, even with more FLOPs, our recomputation speeds up the backward pass due to reduced HBM accesses (Fig. 2). The full backward pass description is in Appendix B. Implementation details: Kernel fusion. Tiling enables us to implement our algorithm in one CUDA kernel, loading input from HBM, performing all the computation steps (matrix multiply, softmax, optionally masking and dropout, matrix multiply), then write the result back to HBM (masking and dropout in Appendix B). This avoids repeatedly reading and writing of inputs and outputs from and to HBM. Algorithm 1 FlashAttention Require: Matrices Q, K, V ∈R𝑁×𝑑in HBM, on-chip SRAM of size 𝑀. 1: Set block sizes 𝐵𝑐= \u0006 𝑀 4𝑑 \u0007 , 𝐵𝑟= min \u0000\u0006 𝑀 4𝑑 \u0007 , 𝑑\u0001. 2: Initialize O = (0)𝑁×𝑑∈R𝑁×𝑑, ℓ= (0)𝑁∈R𝑁, 𝑚= (−∞)𝑁∈R𝑁in HBM. 3: Divide Q into 𝑇𝑟= l 𝑁 𝐵𝑟 m blocks Q1, . . . , Q𝑇𝑟of size 𝐵𝑟× 𝑑each, and divide K, V in to 𝑇𝑐= l 𝑁 𝐵𝑐 m blocks K1, . . . , K𝑇𝑐and V1, . . . , V𝑇𝑐, of size 𝐵𝑐× 𝑑each. 4: Divide O into 𝑇𝑟blocks O𝑖, . . . , O𝑇𝑟of size 𝐵𝑟× 𝑑each, divide ℓinto 𝑇𝑟blocks ℓ𝑖, . . . , ℓ𝑇𝑟of size 𝐵𝑟each, divide 𝑚into 𝑇𝑟blocks 𝑚1, . . . , 𝑚𝑇𝑟of size 𝐵𝑟each. 5: for 1 ≤𝑗≤𝑇𝑐do 6: Load K 𝑗, V𝑗from HBM to on-chip SRAM. 7: for 1 ≤𝑖≤𝑇𝑟do 8: Load Q𝑖, O𝑖, ℓ𝑖, 𝑚𝑖from HBM to on-chip SRAM. 9: On chip, compute S𝑖𝑗=", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "049784d039c17467af8d43cf3801dd5ba07c281ad1fbd0612620f041bdcaa087"}
{"doc_id": "arxiv:2205.14135#background:part-4", "url": "https://arxiv.org/abs/2205.14135", "anchor": "#background:part-4", "type": "paper", "title": "", "section": "Background", "text": ", K𝑇𝑐and V1, . . . , V𝑇𝑐, of size 𝐵𝑐× 𝑑each. 4: Divide O into 𝑇𝑟blocks O𝑖, . . . , O𝑇𝑟of size 𝐵𝑟× 𝑑each, divide ℓinto 𝑇𝑟blocks ℓ𝑖, . . . , ℓ𝑇𝑟of size 𝐵𝑟each, divide 𝑚into 𝑇𝑟blocks 𝑚1, . . . , 𝑚𝑇𝑟of size 𝐵𝑟each. 5: for 1 ≤𝑗≤𝑇𝑐do 6: Load K 𝑗, V𝑗from HBM to on-chip SRAM. 7: for 1 ≤𝑖≤𝑇𝑟do 8: Load Q𝑖, O𝑖, ℓ𝑖, 𝑚𝑖from HBM to on-chip SRAM. 9: On chip, compute S𝑖𝑗= Q𝑖K𝑇 𝑗∈R𝐵𝑟×𝐵𝑐. 10: On chip, compute ˜𝑚𝑖𝑗= rowmax(S𝑖𝑗) ∈R𝐵𝑟, ˜P𝑖𝑗= exp(S𝑖𝑗−˜𝑚𝑖𝑗) ∈R𝐵𝑟×𝐵𝑐(pointwise), ˜ℓ𝑖𝑗= rowsum(˜P𝑖𝑗) ∈R𝐵𝑟. 11: On chip, compute 𝑚new 𝑖 = max(𝑚𝑖, ˜𝑚𝑖𝑗) ∈R𝐵𝑟, ℓnew 𝑖 = 𝑒𝑚𝑖−𝑚new 𝑖 ℓ𝑖+ 𝑒˜𝑚𝑖𝑗−𝑚new 𝑖 ˜ℓ𝑖𝑗∈R𝐵𝑟. 12: Write O𝑖←diag(ℓnew 𝑖 )−1(diag(ℓ𝑖)𝑒𝑚𝑖−𝑚new 𝑖 O𝑖+ 𝑒˜𝑚𝑖𝑗−𝑚new 𝑖 ˜P𝑖𝑗V𝑗) to HBM. 13: Write ℓ𝑖←ℓnew 𝑖 , 𝑚𝑖←𝑚new 𝑖 to HBM. 14: end for 15: end for 16: Return O. We show FlashAttention’s correctness, runtime, and memory requirement (proof in Appendix C). Theorem 1. Algorithm 1 returns O = softmax(QK⊤)V with 𝑂(𝑁2𝑑) FLOPs and requires 𝑂(𝑁) additional memory beyond inputs and output. 3.2 Analysis: IO Complexity of FlashAttention We analyze the IO complexity of FlashAttention, showing signiﬁcant reduction in HBM accesses compared to standard attention. We also provide a lower bound, proving that no exact attention algorithm can 2This style of aggregation is called algebraic aggregation [33]. 5 Attention Standard FlashAttention GFLOPs 66.6 75.2 HBM R/W (GB) 40.3 4.4 Runtime (ms) 41.7 7.3 Sparsity Speedup % Non-Zero Blocks 20 60 50 100 150 Fwd + Bwd (ms) Eﬀect of Block Size Block Size 64 128 256 512 Fwd Runtime (ms) 6 2 HBM Accesses (GB) Dense FlashAttention Block-Sparse FlashAttention 2 4 6 Runtime HBM Accesses Figure 2: Left: Forward + backward runtime of standard attention and FlashAttention for GPT-2 medium (seq. length 1024, head dim. 64, 16 heads, batch size 64) on A100 GPU. HBM access is the primary factor aﬀecting runtime. Middle: Forward runtime of FlashAttention (seq. length 1024, head dim. 64, 16 heads, batch size 64) on A100 GPU. Fewer HBM accesses result in faster runtime, up to a point. Right: The runtime (for seq. length 4K) of block-sparse FlashAttention is faster than FlashAttention by a factor proportional to the sparsity. asymptotically improve on HBM accesses over all SRAM sizes. Proofs are in Appendix C. Theorem 2. Let 𝑁be the sequence length, 𝑑be the head dimension, and 𝑀be size of SRAM with 𝑑≤𝑀≤𝑁𝑑. Standard attention (Algorithm 0) requires Θ(𝑁𝑑+ 𝑁2) HBM accesses, while FlashAttention (Algorithm 1) requires Θ(𝑁2𝑑2𝑀−1) HBM accesses. For typical values of 𝑑(64-128) and 𝑀(around 100KB), 𝑑2 is many times smaller than 𝑀, and thus FlashAttention requires many times fewer HBM accesses than standard implementation. This leads to both faster execution and lower memory footprint, which we validate in Section 4.3. The main idea of the proof is that given the SRAM size of 𝑀, we can load blocks of K, V of size Θ(𝑀) each (Algorithm 1 line 6). For each block of K and V, we iterate over all blocks of Q (Algorithm 1 line 8) to compute the intermediate values, resulting", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "341646c21e6f59ecff363742861f5f899221ee048d5fde36925233286e7b0b00"}
{"doc_id": "arxiv:2205.14135#background:part-5", "url": "https://arxiv.org/abs/2205.14135", "anchor": "#background:part-5", "type": "paper", "title": "", "section": "Background", "text": "thus FlashAttention requires many times fewer HBM accesses than standard implementation. This leads to both faster execution and lower memory footprint, which we validate in Section 4.3. The main idea of the proof is that given the SRAM size of 𝑀, we can load blocks of K, V of size Θ(𝑀) each (Algorithm 1 line 6). For each block of K and V, we iterate over all blocks of Q (Algorithm 1 line 8) to compute the intermediate values, resulting in Θ(𝑁𝑑𝑀−1) passes over Q. Each pass loads Θ(𝑁𝑑) elements, which amounts to Θ(𝑁2𝑑2𝑀−1) HBM accesses. We similarly prove that the backward pass of standard attention requires Θ(𝑁𝑑+ 𝑁2) HBM accesses while the backward pass of FlashAttention requires Θ(𝑁2𝑑2𝑀−1) HBM accesses (Appendix B). We prove a lower-bound: one cannot asymptotically improve on the number of HBM accesses for all values of 𝑀(the SRAM size) when computing exact attention. Proposition 3. Let 𝑁be the sequence length, 𝑑be the head dimension, and 𝑀be size of SRAM with 𝑑≤𝑀≤𝑁𝑑. There does not exist an algorithm to compute exact attention with 𝑜(𝑁2𝑑2𝑀−1) HBM accesses for all 𝑀in the range [𝑑, 𝑁𝑑]. The proof relies on the fact that for 𝑀= Θ(𝑁𝑑) any algorithm must perform Ω(𝑁2𝑑2𝑀−1) = Ω(𝑁𝑑) HBM accesses. This type of lower bound over a subrange of 𝑀is common in the streaming algorithms literature [88]. We leave proving parameterized complexity [27] lower bounds in terms of 𝑀as exciting future work. We validate that the number of HBM accesses is the main determining factor of attention run-time. In Fig. 2 (left), we see that even though FlashAttention has higher FLOP count compared to standard attention (due to recomputation in the backward pass), it has much fewer HBM accesses, resulting in much faster runtime. In Fig. 2 (middle), we vary the block size 𝐵𝑐of FlashAttention, which results in diﬀerent amounts of HBM accesses, and measure the runtime of the forward pass. As block size increases, the number of HBM accesses decreases (as we make fewer passes over the input), and runtime decreases. For large enough block size (beyond 256), the runtime is then bottlenecked by other factors (e.g., arithmetic operations). Moreover, larger block size will not ﬁt into the small SRAM size. 3.3 Extension: Block-Sparse FlashAttention We extend FlashAttention to approximate attention: we propose block-sparse FlashAttention, whose IO complexity is smaller than FlashAttention by a factor proportional to the sparsity. Given inputs Q, K, V ∈R𝑁×𝑑and a mask matrix ˜M ∈{0, 1}𝑁×𝑁, we want to compute: S = QK⊤∈R𝑁×𝑁, P = softmax(S ⊙𝟙˜M) ∈R𝑁×𝑁, O = PV ∈R𝑁×𝑑, where (S ⊙𝟙˜M)𝑘𝑙= S𝑘𝑙if ˜M𝑘𝑙= 1 and −∞if M𝑘𝑙= 0. We require ˜M to have block form: for some block sizes 𝐵𝑟, 𝐵𝑐, for all 𝑘, 𝑙, ˜M𝑘,𝑙= M𝑖𝑗with 𝑖= ⌊𝑘/𝐵𝑟⌋, 𝑗= ⌊𝑙/𝐵𝑐⌋for some M ∈{0, 1}𝑁/𝐵𝑟×𝑁/𝐵𝑐. 6 Given a predeﬁned block sparsity mask M ∈{0, 1}𝑁/𝐵𝑟×𝑁/𝐵𝑐we can easily adapt Algorithm 1 to only compute the nonzero blocks of the attention matrix. The algorithm is identical to Algorithm 1, except we skip zero blocks. We reproduce the algorithm description in Algorithm 5 in Appendix B. We also analyze", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "99c8c78d71d163928d1541c59fde8aea83dd566143c07084a29935ddec0ebd62"}
{"doc_id": "arxiv:2205.14135#background:part-6", "url": "https://arxiv.org/abs/2205.14135", "anchor": "#background:part-6", "type": "paper", "title": "", "section": "Background", "text": "M𝑘𝑙= 0. We require ˜M to have block form: for some block sizes 𝐵𝑟, 𝐵𝑐, for all 𝑘, 𝑙, ˜M𝑘,𝑙= M𝑖𝑗with 𝑖= ⌊𝑘/𝐵𝑟⌋, 𝑗= ⌊𝑙/𝐵𝑐⌋for some M ∈{0, 1}𝑁/𝐵𝑟×𝑁/𝐵𝑐. 6 Given a predeﬁned block sparsity mask M ∈{0, 1}𝑁/𝐵𝑟×𝑁/𝐵𝑐we can easily adapt Algorithm 1 to only compute the nonzero blocks of the attention matrix. The algorithm is identical to Algorithm 1, except we skip zero blocks. We reproduce the algorithm description in Algorithm 5 in Appendix B. We also analyze the IO complexity of block-sparse FlashAttention. Proposition 4. Let 𝑁be the sequence length, 𝑑be the head dimension, and 𝑀be size of SRAM with 𝑑≤𝑀≤𝑁𝑑. Block-sparse FlashAttention (Algorithm 5) requires Θ(𝑁𝑑+ 𝑁2𝑑2𝑀−1𝑠) HBM accesses where 𝑠is the fraction of nonzero blocks in the block-sparsity mask. We see that applying block-sparsity yields a direct improvement by the sparsity to the larger term in the IO complexity. For large sequence lengths 𝑁, 𝑠is often set to 𝑁−1/2 [11] or 𝑁−1 log 𝑁[3, 17, 92], resulting in Θ(𝑁 √ 𝑁) or Θ(𝑁log 𝑁) IO complexity. For downstream experiments, we use the ﬁxed butterﬂy sparsity pattern [17], which has been shown to be able to approximate arbitrary sparsity [16]. In Fig. 2 (right), we validate that as the sparsity increases, the runtime of block-sparse FlashAttention improves proportionally. On the LRA benchmark, block-sparse FlashAttention achieves 2.8× speedup, while performing on par with standard attention (Section 4). 4", "source": "arxiv_pdf", "published": "", "tokens": 231, "sha256": "a00efd7d8560fe26c29e2327e5a1b246eec271bdc0a576cd3be149da13750083"}
{"doc_id": "arxiv:2205.14135#experiments:part-1", "url": "https://arxiv.org/abs/2205.14135", "anchor": "#experiments:part-1", "type": "paper", "title": "", "section": "Experiments", "text": "We evaluate the impact of using FlashAttention to train Transformer models. We validate two claims about training time and model accuracy, and report attention runtime and memory benchmarks. • Training Speed. FlashAttention outperforms the MLPerf 1.1 [58] speed record for BERT by 15%, and speeds up GPT-2 up to 3× over HuggingFace [87] and 1.8× over Megatron [77] over standard Transformers. FlashAttention speeds up the long-range arena (LRA) benchmark 2.4×. • Quality. FlashAttention scales Transformers to longer sequences, yielding higher quality. FlashAt- tention trains GPT-2 with context length 4K faster than Megatron trains GPT-2 with context length 1K, while achieving 0.7 better perplexity. Modeling longer sequences yields 6.4 points of lift on two long- document classiﬁcation tasks. Finally, FlashAttention yields the ﬁrst Transformer that can achieve better-than-random performance on the challenging Path-X task (sequence length 16K), and block-sparse FlashAttention yields the ﬁrst sequence model that we know of that can achieve better-than-random performance on Path-256 (sequence length 64K). • Benchmarking Attention. We measure the runtime and memory performance of FlashAttention and block-sparse FlashAttention based on sequence length. We conﬁrm that the memory footprint of FlashAttention scales linearly with seq. length and is up to 3× faster than standard attention for common seq. lengths (up to 2K). We conﬁrm that runtime of block-sparse FlashAttention scales linearly in seq. length and is faster than all existing approximate attention baselines. Additional experiment details are in Appendix E. 4.1 Faster Models with FlashAttention BERT. FlashAttention yields the fastest single-node BERT training speed that we know of. We train a BERT-large [22] model with FlashAttention on Wikipedia. Table 1 compares our training time to the implementation from Nvidia that set the training speed record for MLPerf 1.1 [58]. Our implementation is 15% faster. Table 1: Training time of BERT-large, starting from the same initialization provided by the MLPerf benchmark, to reach the target accuracy of 72.0% on masked language modeling. Averaged over 10 runs on 8×A100 GPUs. BERT Implementation Training time (minutes) Nvidia MLPerf 1.1 [58] 20.0 ± 1.5 FlashAttention (ours) 17.4 ± 1.4 GPT-2. FlashAttention yields faster training times for GPT-2 [67] on the large OpenWebtext dataset [32] than the widely used HuggingFace [87] and Megatron-LM [77] implementations. Table 2 shows up to 3× end- to-end speedup compared to Huggingface and 1.7× speedup compared to Megatron-LM. FlashAttention 7 achieves the same perplexity as the other two implementations, as we do not change the model deﬁnition. Appendix E includes plots of the validation perplexity throughout training, conﬁrming that FlashAttention is as numerically stable as the baselines and produces the same training / validation curves. Table 2: GPT-2 small and medium using FlashAttention achieve up to 3× speed up compared to Huggingface implementation and up to 1.7× compared to Megatron-LM. Training time reported on 8×A100s GPUs. Model implementations OpenWebText (ppl) Training time (speedup) GPT-2 small - Huggingface [87] 18.2 9.5 days (1.0×) GPT-2 small - Megatron-LM [77] 18.2 4.7 days (2.0×) GPT-2 small - FlashAttention 18.2 2.7 days (3.5×) GPT-2 medium - Huggingface [87] 14.2 21.0 days (1.0×) GPT-2 medium - Megatron-LM [77] 14.3 11.5 days (1.8×) GPT-2", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "fdaf0b0915cc12077183699249d46347efac6f99c36af1f631548e0435d3f803"}
{"doc_id": "arxiv:2205.14135#experiments:part-2", "url": "https://arxiv.org/abs/2205.14135", "anchor": "#experiments:part-2", "type": "paper", "title": "", "section": "Experiments", "text": "small and medium using FlashAttention achieve up to 3× speed up compared to Huggingface implementation and up to 1.7× compared to Megatron-LM. Training time reported on 8×A100s GPUs. Model implementations OpenWebText (ppl) Training time (speedup) GPT-2 small - Huggingface [87] 18.2 9.5 days (1.0×) GPT-2 small - Megatron-LM [77] 18.2 4.7 days (2.0×) GPT-2 small - FlashAttention 18.2 2.7 days (3.5×) GPT-2 medium - Huggingface [87] 14.2 21.0 days (1.0×) GPT-2 medium - Megatron-LM [77] 14.3 11.5 days (1.8×) GPT-2 medium - FlashAttention 14.3 6.9 days (3.0×) Long-range Arena. We compare vanilla Transformer (with either standard implementation or FlashAt- tention) on the long-range arena (LRA [80]) benchmark. We measure accuracy, throughput, and training time of all models. Each task has a diﬀerent sequence length varying between 1024 and 4096. We follow the implementation and experimental setting in Tay et al. [80]and Xiong et al. [90].3 Table 3 shows that FlashAt- tention achieves up 2.4× speed-up compared to standard attention. Block-sparse FlashAttention is faster than all of the approximate attention methods that we have tested. Table 3: The performance of standard attention, FlashAttention, block-sparse FlashAttention, and approximate attention baselines on the Long-Range-Arena benchmarks. Models ListOps Text Retrieval Image Pathﬁnder Avg Speedup Transformer 36.0 63.6 81.6 42.3 72.7 59.3 - FlashAttention 37.6 63.9 81.4 43.5 72.7 59.8 2.4× Block-sparse FlashAttention 37.0 63.0 81.3 43.6 73.3 59.6 2.8× Linformer [84] 35.6 55.9 77.7 37.8 67.6 54.9 2.5× Linear Attention [50] 38.8 63.2 80.7 42.6 72.5 59.6 2.3× Performer [12] 36.8 63.6 82.2 42.1 69.9 58.9 1.8× Local Attention [80] 36.1 60.2 76.7 40.6 66.6 56.0 1.7× Reformer [51] 36.5 63.8 78.5 39.6 69.4 57.6 1.3× Smyrf [19] 36.1 64.1 79.0 39.6 70.5 57.9 1.7× 4.2 Better Models with Longer Sequences Language Modeling with Long Context. The runtime and memory-eﬃciency of FlashAttention allow us to increase the context length of GPT-2 by 4× while still running faster than the optimized implementation from Megatron-LM. Table 4 shows that that GPT-2 with FlashAttention and context length 4K is still 30% faster than GPT-2 from Megatron with context length 1K, while achieving 0.7 better perplexity. Table 4: GPT-2 small with FlashAttention, with 4× larger context length compared to Megatron-LM, is still 30% faster while achieving 0.7 better perplexity. Training time on 8×A100 GPUs is reported. Model implementations Context length OpenWebText (ppl) Training time (speedup) GPT-2 small - Megatron-LM 1k 18.2 4.7 days (1.0×) GPT-2 small - FlashAttention 1k 18.2 2.7 days (1.7×) GPT-2 small - FlashAttention 2k 17.6 3.0 days (1.6×) GPT-2 small - FlashAttention 4k 17.5 3.6 days (1.3×) Long Document Classiﬁcation. Training Transformers with longer sequences with FlashAttention improves performance on the MIMIC-III [47] and ECtHR [6, 7] datasets. MIMIC-III contains intensive care unit patient discharge summaries, each annotated with multiple labels. ECtHR contains legal cases from the 3LRA accuracy results are known to be highly dependent on the tuning procedure [90]. Our reproduced baselines perform better than as reported in the original comparison [80]. 8 Attention Memory Usage Sequence Length Attention Runtime (Fwd Pass + Bwd Pass) Sequence Length Runtime (ms) Memory Footprint (GB) 256 8K", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "8c9a26a9723962426ef1431fd30effda3dbbfe9bdb897907bf5e8aeacfa2ab55"}
{"doc_id": "arxiv:2205.14135#experiments:part-3", "url": "https://arxiv.org/abs/2205.14135", "anchor": "#experiments:part-3", "type": "paper", "title": "", "section": "Experiments", "text": "FlashAttention improves performance on the MIMIC-III [47] and ECtHR [6, 7] datasets. MIMIC-III contains intensive care unit patient discharge summaries, each annotated with multiple labels. ECtHR contains legal cases from the 3LRA accuracy results are known to be highly dependent on the tuning procedure [90]. Our reproduced baselines perform better than as reported in the original comparison [80]. 8 Attention Memory Usage Sequence Length Attention Runtime (Fwd Pass + Bwd Pass) Sequence Length Runtime (ms) Memory Footprint (GB) 256 8K 16K 32K 64K 128 256 512 1024 2048 4096 101 102 10 20 FlashAttention Block-Sparse FlashAttention PyTorch Attention Megatron Attention Linformer Attention OpenAI Sparse Attention 8192 100 Crossover Points 20x 2x Figure 3: Left: runtime of forward pass + backward pass. Right: attention memory usage. European Court of Human Rights, each of which is mapped to articles of the Convention of Human Rights that were allegedly violaged. Both of these datasets contain very long text documents; the average number of tokens in MIMIC is 2,395 tokens, and the longest document contains 14,562 tokens, while the average and longest numbers in ECtHR are 2,197 and 49,392, respectively. We evaluate lift from increasing the sequence length of a pretrained RoBERTa model [56] (we repeat the positional embeddings, as in Beltagy et al. [3]). Table 5 shows that sequence length 16K outperforms length 512 by 4.3 points on MIMIC, and that length 8K outperforms length 512 by 8.5 points on ECtHR. The discrepancies may be due to subtle distribution shifts: MIMIC-III contains specialized medical text and thus may be more susceptible to a distribution shift in the document length, whereas ECtHR contains general language. Table 5: Long Document performance (mi- cro 𝐹1) at diﬀerent sequence lengths using FlashAttention. 512 1024 2048 4096 8192 16384 MIMIC-III [47] 52.8 50.7 51.7 54.6 56.4 57.1 ECtHR [6] 72.2 74.3 77.1 78.6 80.7 79.2 Table 6: We report the ﬁrst Transformer model that can achieve non-random perfor- mance on Path-X and Path-256.", "source": "arxiv_pdf", "published": "", "tokens": 325, "sha256": "8de2da468be93f47e8942f6bdb994023865fed27a527037c05854161ede33341"}
{"doc_id": "arxiv:2205.14135#model:part-1", "url": "https://arxiv.org/abs/2205.14135", "anchor": "#model:part-1", "type": "paper", "title": "", "section": "Model", "text": "Path-X Path-256 Transformer \u0017 \u0017 Linformer [84] \u0017 \u0017 Linear Attention [50] \u0017 \u0017 Performer [12] \u0017 \u0017 Local Attention [80] \u0017 \u0017 Reformer [51] \u0017 \u0017 SMYRF [19] \u0017 \u0017 FlashAttention 61.4 \u0017 Block-sparse FlashAttention 56.0 63.1 Path-X and Path-256. The Path-X and Path-256 benchmarks are challenging tasks from the long-range arena benchmark designed to test long context. The task is to classify whether two points in a black and white 128×128 (or 256×256) image have a path connecting them, and the images are fed to the transformer one pixel at a time. In prior work, all transformer models have either run out of memory, or only achieved random performance [80]. There has been a search for alternative architectures that can model such long context [37]. We present here the ﬁrst result of Transformer models being able to solve Path-X and Path-256 (Table 6). We pretrain a transformer on Path-64, and then transfer to Path-X by spatially interpolating the positional embeddings. FlashAttention achieves 61.4 accuracy on Path-X. Additionally, block-sparse FlashAttention enables the Transformers to scale to sequence length 64K, achieving 63.1 accuracy4 on Path-256. 4.3 Benchmarking Attention We vary sequence length and measure runtime and memory usage of FlashAttention and block-sparse FlashAttention against various attention baselines on one A100 GPU with 40 GB HBM, with dropout and a padding mask. We compare against reference implementations for exact attention, approximate attention, and sparse attention. We report a subset of baselines in the main body; Appendix E contains more baselines and full details. 4Path-256 requires longer sequences but has relatively shorter paths than Path-X, so it is easier to obtain a higher accuracy. 9 Runtime. Figure 3 (left) reports the runtime in milliseconds of the forward + backward pass of FlashAt- tention and block-sparse FlashAttention compared to the baselines in exact, approximate, and sparse attention (exact numbers in Appendix E). Runtime grows quadratically with sequence length, but FlashAt- tention runs signiﬁcantly faster than exact attention baselines, up to 3× faster than the PyTorch implementation. The runtimes of many approximate/sparse attention mechanisms grow linearly with se- quence length, but FlashAttention still runs faster than approximate and sparse attention for short sequences due to fewer memory accesses. The approximate attention runtimes begin to cross over with FlashAttention at sequences between 512 and 1024. On the other hand, block-sparse FlashAttention is faster than all implementations of exact, sparse, and approximate attention that we know of, across all sequence lengths. Memory Footprint. Figure 3 (right) shows the memory footprint of FlashAttention and block-sparse FlashAttention compared to various exact, approximate, and sparse attention baselines. FlashAttention and block-sparse FlashAttention have the same memory footprint, which grows linearly with sequence length. FlashAttention is up to 20× more memory eﬃcient than exact attention baselines, and is more memory-eﬃcient than the approximate attention baselines. All other algorithms except for Linformer run out of memory on an A100 GPU before 64K, and FlashAttention is still 2× more eﬃcient than Linformer. 5 Limitations and Future Directions We discuss limitations of our approach and future directions. Related work is given in Appendix A. Compiling to", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "3e1b182f9bcbed67a3ffe1e3282e9b8541b3a6c7bcb252a24e793cc5ae41d78a"}
{"doc_id": "arxiv:2205.14135#model:part-2", "url": "https://arxiv.org/abs/2205.14135", "anchor": "#model:part-2", "type": "paper", "title": "", "section": "Model", "text": "have the same memory footprint, which grows linearly with sequence length. FlashAttention is up to 20× more memory eﬃcient than exact attention baselines, and is more memory-eﬃcient than the approximate attention baselines. All other algorithms except for Linformer run out of memory on an A100 GPU before 64K, and FlashAttention is still 2× more eﬃcient than Linformer. 5 Limitations and Future Directions We discuss limitations of our approach and future directions. Related work is given in Appendix A. Compiling to CUDA. Our current approach to building IO-aware implementations of attention requires writing a new CUDA kernel for each new attention implementation. This requires writing the attention algorithm in a considerably lower-level language than PyTorch, and requires signiﬁcant engineering eﬀort. Implementations may also not be transferrable across GPU architectures. These limitations suggest the need for a method that supports writing attention algorithms in a high-level language (e.g., PyTorch), and compiling to IO-aware implementations in CUDA—similar to eﬀorts such as Halide in image processing [70]. IO-Aware Deep Learning. We believe that the IO-aware approach can extend beyond attention. Attention is the most memory-intensive computation in Transformers, but every layer in a deep network touches GPU HBM. We hope our work inspires IO-aware implementations of additional modules. We discuss these potential extensions in Appendix D. Multi-GPU IO-Aware Methods. Our IO-aware implementation of attention is optimal within con- stants for computing attention on a single GPU. However, the attention computation may be parallelizable across multiple GPUs [72]. Using multiple GPUs adds an additional layer to IO analysis—accounting for data transfer between GPUs. We hope our work inspires future work in this direction. Acknowledgments Our implementation uses Apex’s FMHA code (https://github.com/NVIDIA/apex/tree/master/apex/ contrib/csrc/fmha) as a starting point. We thank Young-Jun Ko for the in-depth explanation of his FMHA implementation and for his thoughtful answers to our questions about CUDA. We thank Sabri Eyuboglu, Megan Leszczynski, Laurel Orr, Yuhuai Wu, Beidi Chen, and Xun Huang for their constructive feedback and suggestions on early drafts of the paper. We thank Markus Rabe and Charles Staats for helpful discussion of their attention algorithm. We gratefully acknowledge the support of NIH under No. U54EB020405 (Mobilize), NSF under Nos. CCF1763315 (Beyond Sparsity), CCF1563078 (Volume to Velocity), and 1937301 (RTML); ARL under No. W911NF-21-2-0251 (Interactive Human-AI Teaming); ONR under No. N000141712266 (Unifying Weak Supervision); ONR N00014-20-1-2480: Understanding and Applying Non-Euclidean Geometry in Machine Learning; N000142012275 (NEPTUNE); NXP, Xilinx, LETI-CEA, Intel, IBM, Microsoft, NEC, Toshiba, TSMC, ARM, Hitachi, BASF, Accenture, Ericsson, Qualcomm, Analog Devices, Google Cloud, Salesforce, Total, the HAI-GCP & HAI-Azure Cloud Credits for Research program, the Stanford Data Science Initiative (SDSI), Department of Defense (DoD) through the National Defense Science and Engineering Graduate Fellowship (NDSEG) Program, and members of the Stanford DAWN project: Facebook, Google, and VMWare. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes 10 notwithstanding any copyright notation thereon. Any opinions, ﬁndings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reﬂect the views, policies, or endorsements, either expressed or implied, of NIH, ONR, or the U.S. Government.", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "83234ac09531d337a00d80d0a19dcc82c820d0f91f4509c51bab28be57eb38c1"}
{"doc_id": "arxiv:2205.14135#model:part-3", "url": "https://arxiv.org/abs/2205.14135", "anchor": "#model:part-3", "type": "paper", "title": "", "section": "Model", "text": "Defense (DoD) through the National Defense Science and Engineering Graduate Fellowship (NDSEG) Program, and members of the Stanford DAWN project: Facebook, Google, and VMWare. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes 10 notwithstanding any copyright notation thereon. Any opinions, ﬁndings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reﬂect the views, policies, or endorsements, either expressed or implied, of NIH, ONR, or the U.S. Government. Atri Rudra’s research is supported by NSF grant CCF-1763481.", "source": "arxiv_pdf", "published": "", "tokens": 89, "sha256": "b0f7121d448f8ff36af0c3a3aaafd0f3493f8423d40358957cc450d30a691658"}
{"doc_id": "arxiv:2501.19393#abstract:part-1", "url": "https://arxiv.org/abs/2501.19393", "anchor": "#abstract:part-1", "type": "paper", "title": "", "section": "Abstract", "text": "Test-time scaling is a promising new approach to language modeling that uses extra test-time com- pute to improve performance. Recently, OpenAI’s o1 model showed this capability but did not pub- licly share its methodology, leading to many repli- cation efforts. We seek the simplest approach to achieve test-time scaling and strong reasoning per- formance. First, we curate a small dataset s1K of 1,000 questions paired with reasoning traces relying on three criteria we validate through abla- tions: difficulty, diversity, and quality. Second, we develop budget forcing to control test-time com- pute by forcefully terminating the model’s think- ing process or lengthening it by appending “Wait” multiple times to the model’s generation when it tries to end. This can lead the model to double- check its answer, often fixing incorrect reasoning steps. After supervised finetuning the Qwen2.5- 32B-Instruct language model on s1K and equip- ping it with budget forcing, our model s1-32B ex- ceeds o1-preview on competition math questions by up to 27% (MATH and AIME24). Further, scaling s1-32B with budget forcing allows extrap- olating beyond its performance without test-time intervention: from 50% to 57% on AIME24. Our model, data, and code are open-source at https: //github.com/simplescaling/s1. 1. Introduction Performance improvements of language models (LMs) over the past years have largely relied on scaling up train-time compute using large-scale self-supervised pretraining (Ka- plan et al., 2020; Hoffmann et al., 2022). The creation of these powerful models has set the stage for a new scaling paradigm built on top of them: test-time scaling. The aim *Equal contribution. ZY and NM started the project. WS, NM and ZY collected the prompts, XL, ZY and NM, built the data pipeline, LZ and WS proposed using a 1K subset and ZY and NM built budget forcing. 1 Stanford University. 2 University of Washington, Seattle. 3 Allen Institute for AI. 4 Contextual AI. 512 2048 65 75 85 95 Accuracy (%) Mathematical Problem Solving (MATH500) 512 2048 8192 0 20 40 60 Competition Math (AIME24) 1024 4096 40 50 60 PhD-Level Science Questions (GPQA Diamond) Average thinking time (tokens) Figure 1. Test-time scaling with s1-32B. We benchmark s1-32B on reasoning-intensive tasks and vary test-time compute. of this approach is to increase the compute at test time to get better results. There has been much work exploring this idea (Snell et al., 2024; Welleck et al., 2024), and the via- bility of this paradigm was recently validated by OpenAI o1 (OpenAI, 2024). o1 has demonstrated strong reasoning performance with consistent gains from scaling test-time compute. OpenAI describes their approach as using large- scale reinforcement learning (RL) implying the use of sizable amounts of data (OpenAI, 2024). This has led to various attempts to replicate their models relying on techniques like Monte Carlo Tree Search (Gao et al., 2024b; Zhang et al., 2024a), multi-agent approaches (Qin et al., 2024), and oth- ers (Wang et al., 2024a; Huang et al., 2024b; 2025). Among these approaches, DeepSeek R1 (DeepSeek-AI et al., 2025) has successfully replicated o1-level performance, also em- ploying reinforcement learning via millions of samples and multiple training stages. However, despite", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "4bfc1a96584457640807ea51b28a57242f30ddd4bbc361cf2cfeefb1bd264dc2"}
{"doc_id": "arxiv:2501.19393#abstract:part-2", "url": "https://arxiv.org/abs/2501.19393", "anchor": "#abstract:part-2", "type": "paper", "title": "", "section": "Abstract", "text": "of sizable amounts of data (OpenAI, 2024). This has led to various attempts to replicate their models relying on techniques like Monte Carlo Tree Search (Gao et al., 2024b; Zhang et al., 2024a), multi-agent approaches (Qin et al., 2024), and oth- ers (Wang et al., 2024a; Huang et al., 2024b; 2025). Among these approaches, DeepSeek R1 (DeepSeek-AI et al., 2025) has successfully replicated o1-level performance, also em- ploying reinforcement learning via millions of samples and multiple training stages. However, despite the large num- ber of o1 replication attempts, none have openly replicated a clear test-time scaling behavior. Thus, we ask: what is the simplest approach to achieve both test-time scaling and strong reasoning performance? We show that training on only 1,000 samples with next-token prediction and controlling thinking duration via a simple test-time technique we refer to as budget forcing leads to a strong reasoning model that scales in performance with more test-time compute. Specifically, we construct s1K, which consists of 1,000 carefully curated questions paired with reasoning traces and answers distilled from Gemini Thinking Experimental (Google, 2024). We perform super- vised fine-tuning (SFT) of an off-the-shelf pretrained model 1 arXiv:2501.19393v3 [cs.CL] 1 Mar 2025 s1: Simple test-time scaling on our small dataset requiring just 26 minutes of training on 16 H100 GPUs. After training, we control the amount of test-time compute our model spends using budget forc- ing: (I) If the model generates more thinking tokens than a desired limit, we forcefully end the thinking process by appending an end-of-thinking token delimiter. Ending the thinking this way makes the model transition to generating its answer. (II) If we want the model to spend more test-time compute on a problem, we suppress the generation of the end-of-thinking token delimiter and instead append “Wait” to the model’s current reasoning trace to encourage more exploration. Equipped with this simple recipe – SFT on 1,000 samples and test-time budget forcing – our model s1- 32B exhibits test-time scaling (Figure 1). Further, s1-32B is the most sample-efficient reasoning model and outperforms closed-source models like OpenAI’s o1-preview (Figure 2). We conduct extensive ablation experiments targeting (a) our selection of 1,000 (1K) reasoning samples and (b) our test- time scaling. For (a), we find that jointly incorporating difficulty, diversity, and quality measures into our selec- tion algorithm is important. Random selection, selecting samples with the longest reasoning traces, or only selecting maximally diverse samples all lead to significantly worse performance (around −30% on AIME24 on average). Train- ing on our full data pool of 59K examples, a superset of s1K, does not offer substantial gains over our 1K selection. This highlights the importance of careful data selection and echoes prior findings for instruction tuning (Zhou et al., 2023). For (b), we define desiderata for test-time scaling methods to compare different approaches. Budget forcing leads to the best scaling as it has perfect controllability with a clear positive slope leading to strong performance. In summary, our contributions are: We develop simple meth- ods for creating a sample-efficient reasoning dataset (§2) and test-time scaling (§3); Based on these we build", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "9a57a492c38fa7372b99e4cfd811e7305cab06ff6efe2f16b9667ac09ba9d765"}
{"doc_id": "arxiv:2501.19393#abstract:part-3", "url": "https://arxiv.org/abs/2501.19393", "anchor": "#abstract:part-3", "type": "paper", "title": "", "section": "Abstract", "text": "selection. This highlights the importance of careful data selection and echoes prior findings for instruction tuning (Zhou et al., 2023). For (b), we define desiderata for test-time scaling methods to compare different approaches. Budget forcing leads to the best scaling as it has perfect controllability with a clear positive slope leading to strong performance. In summary, our contributions are: We develop simple meth- ods for creating a sample-efficient reasoning dataset (§2) and test-time scaling (§3); Based on these we build s1-32B which is competitive with o1-preview (§4); We ablate sub- tleties of data (§5.1) and test-time scaling (§5.2). We end with a discussion to motivate future work on simple rea- soning (§6). Our code, model, and data are open-source at https://github.com/simplescaling/s1. 2. Reasoning data curation to create s1K In this section, we describe our process for creating a large dataset first in §2.1 and then filtering it down to s1K in §2.2. 2.1. Initial collection of 59K samples We collect an initial 59,029 questions from 16 sources follow- ing three guiding principles. Quality: Datasets should be high-quality; we always inspect samples and ignore datasets with, e.g., poor formatting; Difficulty: Datasets should be challenging and require significant reasoning effort; Diver- sity: Datasets should stem from various fields to cover differ- ent reasoning tasks. We collect datasets of two categories: Curation of existing datasets Our largest source is Nu- minaMATH (LI et al., 2024) with 30,660 mathematical problems from online websites. We also include histori- cal AIME problems (1983-2021). To enhance diversity, we add OlympicArena (Huang et al., 2024a) with 4,250 questions spanning Astronomy, Biology, Chemistry, Com- puter Science, Geography, Mathematics, and Physics from various Olympiads. OmniMath (Gao et al., 2024a) adds 4,238 competition-level mathematics problems. We also include 2,385 problems from AGIEval (Zhong et al., 2023), which features questions from standardized tests like SAT and LSAT, covering English, Law, and Logic. We refer to Table 7 in §C for our other sources. New datasets in quantitative reasoning To comple- ment these existing datasets, we create two original datasets. s1-prob consists of 182 questions from the prob- ability section of Stanford University’s Statistics Depart- ment’s PhD Qualifying Exams (https://statistics. stanford.edu), accompanied by handwritten solutions that cover difficult proofs. The probability qualifying exam is held yearly and requires professional-level mathemati- cal problem-solving. s1-teasers comprises 23 challenging brain-teasers commonly used in interview questions for quantitative trading positions. Each sample consists of a problem and solution taken from PuzzledQuant (https: //www.puzzledquant.com/). We only take examples with the highest difficulty level (\"Hard\"). For each question, we generate a reasoning trace and solu- tion using the Google Gemini Flash Thinking API (Google, 2024) extracting its reasoning trace and response. This yields 59K triplets of a question, generated reasoning trace, and generated solution. Examples from our dataset are in §D.2. We decontaminate all samples against our evaluation ques- tions (MATH500, GPQA Diamond, AIME24; §C.5) using 8-grams and deduplicate the data. 2.2. Final selection of 1K samples We could directly train on our pool of 59K questions, how- ever, our goal is to find the simplest approach with minimal resources. Thus,", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "581b2d3df6210a86e107627f99b66b4838a281b99ddf8a2bb138c1e80d0f87a5"}
{"doc_id": "arxiv:2501.19393#abstract:part-4", "url": "https://arxiv.org/abs/2501.19393", "anchor": "#abstract:part-4", "type": "paper", "title": "", "section": "Abstract", "text": "Thinking API (Google, 2024) extracting its reasoning trace and response. This yields 59K triplets of a question, generated reasoning trace, and generated solution. Examples from our dataset are in §D.2. We decontaminate all samples against our evaluation ques- tions (MATH500, GPQA Diamond, AIME24; §C.5) using 8-grams and deduplicate the data. 2.2. Final selection of 1K samples We could directly train on our pool of 59K questions, how- ever, our goal is to find the simplest approach with minimal resources. Thus, we go through three stages of filtering to arrive at a minimal set of 1,000 samples relying on our three guiding data principles: Quality, Difficulty, and Diversity. Quality We first remove any questions where we ran into any API errors reducing our dataset to 54,116 samples. Next, we filter out low-quality examples by checking if they contain any string patterns with formatting issues, such as ASCII art diagrams, non-existent image references, or inconsistent question numbering reducing our dataset to 51,581 examples. 2 s1: Simple test-time scaling Geometry Number theory Combin− atorics Real functions Biology Complex functions Quantum theory Field theory Calculus of variations Difference equations Electro− dynamics Group theory Linear algebra Probability theory Algebraic systems Mechanics Thermo− dynamics Differential equations Computer science Numerical analysis Calculus Algebraic structures Astronomy Dynamical systems Statistical mechanics Operations research Math− ematics education Measure theory Convex geometry Fluid mechanics Algebraic geometry Statistics General topology Economics Associative rings General relativity Differential geometry Math− ematical logic Partial differential equations Information theory Solid mech −anics Functional analysis Special functions Comm− utative algebra Integral equations Integral transform Approxi− mation theory Potential theory Harmonic analysis Control theory Geo− physics 1000 17000 800000 N/A Number of Examples 80 85 90 95 100 MATH500 Accuracy (%) s1 r1-distill Sky-T1 QwQ Bespoke-Stratos o1-preview Most sample-efficient Figure 2. s1K and s1-32B. (left) s1K is a dataset of 1,000 high-quality, diverse, and difficult questions with reasoning traces. (right) s1-32B, a 32B parameter model finetuned on s1K is on the sample-efficiency frontier. See Table 1 for details on other models. From this pool, we identify 384 samples for our final 1,000 samples from datasets that we perceive as high-quality and not in need of further filtering (see §C.4 for details). Difficulty For difficulty, we use two indicators: model per- formance and reasoning trace length. We evaluate two mod- els on each question: Qwen2.5-7B-Instruct and Qwen2.5- 32B-Instruct (Qwen et al., 2024), with correctness assessed by Claude 3.5 Sonnet comparing each attempt against the reference solution (see §C.3 for the grading protocol). We measure the token length of each reasoning trace to indicate problem difficulty using the Qwen2.5 tokenizer. This relies on the assumption that more difficult problems require more thinking tokens. Based on the grading, we remove questions that either Qwen2.5-7B-Instruct or Qwen2.5-32B-Instruct can solve correctly and thus may be too easy. By using two models we reduce the likelihood of an easy sample slipping through our filtering due to a rare mistake on an easy ques- tion of one of the models. This brings our total samples down to 24,496, setting the stage for the next round of sub-", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "f79774997915e00d39c492fb06be09b2fc24dc5e7cb0d8981ba43c8639ccbf5d"}
{"doc_id": "arxiv:2501.19393#abstract:part-5", "url": "https://arxiv.org/abs/2501.19393", "anchor": "#abstract:part-5", "type": "paper", "title": "", "section": "Abstract", "text": "on the assumption that more difficult problems require more thinking tokens. Based on the grading, we remove questions that either Qwen2.5-7B-Instruct or Qwen2.5-32B-Instruct can solve correctly and thus may be too easy. By using two models we reduce the likelihood of an easy sample slipping through our filtering due to a rare mistake on an easy ques- tion of one of the models. This brings our total samples down to 24,496, setting the stage for the next round of sub- sampling based on diversity. While filtering with these two models may be optimized for our setup as we will also use Qwen2.5-32B-Instruct as our model to finetune, the idea of model-based filtering generalizes to other setups. Diversity To quantify diversity, we classify questions into domains using Claude 3.5 Sonnet based on the Mathematics Subject Classification (MSC) system (e.g., geometry, com- binatorics, etc.) from the American Mathematical Society.1 The taxonomy focuses on topics in mathematics but also includes other sciences such as biology, physics, and eco- 1https://mathscinet.ams.org/mathscinet/ msc/msc2020.html nomics. To select our final examples from the pool of 24,496 questions, we first choose one domain uniformly at random. Then, we sample one problem from this domain according to a distribution that favors longer reasoning traces (see §C.4 for details) as motivated in Difficulty. We repeat this process until we have 1,000 total samples spanning 50 domains. In §5.1, we will show that using our three criteria in combi- nation is important, as only relying on quality, diversity, or difficulty in isolation leads to worse datasets. Some distilled generations are incorrect, which we allow in our data as we focus on capturing the reasoning process rather than entirely correct solutions. Our grader (§C.3) deems 53.6% correct in s1K and 63.0% in our follow-up s1K-1.1 (see §A). 3. Test-time scaling 3.1. Method We classify test-time scaling methods into 1) Sequential, where later computations depend on earlier ones (e.g., a long reasoning trace), and 2) Parallel, where computations run in- dependently (e.g., majority voting) (Snell et al., 2024; Brown et al., 2024). We focus on sequential scaling as intuitively we believe it should scale better, since later computations can build on intermediate results, allowing for deeper reasoning and iterative refinement. We propose new sequential scaling methods and ways to benchmark them. Budget forcing We propose a simple decoding-time in- tervention by forcing a maximum and/or minimum number of thinking tokens. Specifically, we enforce a maximum token count by simply appending the end-of-thinking token delimiter and optionally “Final Answer:” to early exit 3 s1: Simple test-time scaling How many r in raspberry? Let's break down the process of counting the letter 'r' in the word \"raspberry\" ... * First letter: 'r' - This is an 'r', count = 1. * Second letter: 'a' - Not an 'r', count remains 1 ... * Sixth letter: 'e' - Not an 'r', count remains 1. * Seventh letter: 'r' - This is an 'r', count = 2. * Eighth letter: 'y' - Not an 'r', count remains 2 ... The number of 'r's in \"raspberry\" is 2. , let's re-read", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "20f23f615075a12f253da63c49dd326e6b8ddfd300a05d99dfb147cded4c9623"}
{"doc_id": "arxiv:2501.19393#abstract:part-6", "url": "https://arxiv.org/abs/2501.19393", "anchor": "#abstract:part-6", "type": "paper", "title": "", "section": "Abstract", "text": "of counting the letter 'r' in the word \"raspberry\" ... * First letter: 'r' - This is an 'r', count = 1. * Second letter: 'a' - Not an 'r', count remains 1 ... * Sixth letter: 'e' - Not an 'r', count remains 1. * Seventh letter: 'r' - This is an 'r', count = 2. * Eighth letter: 'y' - Not an 'r', count remains 2 ... The number of 'r's in \"raspberry\" is 2. , let's re-read the question carefully. It asks \"How many r in raspberry?\" ... * r - a - s - p - b - e - r - r - y ... * First 'r' ... * Second 'r' ... * Third 'r' ... Count = 3 ... Wait My initial answer of 2 was incorrect due to a quick reading of the word. Final Answer: The final answer is Question Reasoning trace Response Figure 3. Budget forcing with s1-32B. The model tries to stop after “...is 2.”, but we suppress the end-of-thinking token delimiter instead appending “Wait” leading s1-32B to self-correct its answer. the thinking stage and make the model provide its current best answer. To enforce a minimum, we suppress the gener- ation of the end-of-thinking token delimiter and optionally append the string “Wait” to the model’s current reasoning trace to encourage the model to reflect on its current gen- eration. Figure 3 contains an example of how this simple approach can lead the model to arrive at a better answer. Baselines We benchmark budget forcing with: (I) Con- ditional length-control methods, which rely on telling the model in the prompt how long it should generate for. We group them by granularity into (a) Token-conditional con- trol: We specify an upper bound of thinking tokens in the prompt; (b) Step-conditional control: We specify an upper bound of thinking steps, where each step is around 100 to- kens; (c) Class-conditional control: We write two generic prompts that tell the model to either think for a short or long amount of time (see §E.1 for details). (II) Rejection sam- pling, which samples until a generation fits a predetermined compute budget. This oracle captures the posterior over responses conditioned on its length. 3.2. Metrics We establish a set of desiderata as evaluation metrics to measure test-time scaling across methods. Importantly, we do not only care about the accuracy a method can achieve but also its controllability and test-time scaling slope. For each method we consider, we run a set of evaluations 𝑎∈ varying test-time compute on a fixed benchmark, e.g. AIME24. This produces a piece-wise linear function 𝑓 with compute as the x-axis measured in thinking tokens and accuracy as the y-axis (see Figure 1, where the rightmost dot for AIME24 corresponds to 𝑓(7320) = 57%). We measure three metrics: Control = 1 || ∑ 𝑎∈ 𝕀(𝑎min ≤𝑎≤𝑎max) (1) where 𝑎min, 𝑎max refer to a pre-specified minimum and max- imum amount of test-time compute; in our case thinking tokens. We usually only constrain 𝑎max. As tokens gener- ated correspond to the amount of test-time compute", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "67ab00e91bf0285192058c19c1ae8bed90828f8c27c9d4a435b6467fd0737c13"}
{"doc_id": "arxiv:2501.19393#abstract:part-7", "url": "https://arxiv.org/abs/2501.19393", "anchor": "#abstract:part-7", "type": "paper", "title": "", "section": "Abstract", "text": "linear function 𝑓 with compute as the x-axis measured in thinking tokens and accuracy as the y-axis (see Figure 1, where the rightmost dot for AIME24 corresponds to 𝑓(7320) = 57%). We measure three metrics: Control = 1 || ∑ 𝑎∈ 𝕀(𝑎min ≤𝑎≤𝑎max) (1) where 𝑎min, 𝑎max refer to a pre-specified minimum and max- imum amount of test-time compute; in our case thinking tokens. We usually only constrain 𝑎max. As tokens gener- ated correspond to the amount of test-time compute spent, this metric measures the extent to which a method allows controllability over the use of that test-time compute. We report it as a percentage with 100% being perfect control. Scaling = 1 (|| 2 ) ∑ 𝑎,𝑏∈ 𝑏>𝑎 𝑓(𝑏) −𝑓(𝑎) 𝑏−𝑎 (2) Scaling is the average slope of the piece-wise linear function. It must be positive for useful methods and larger is better. Performance = max 𝑎∈𝑓(𝑎) (3) Performance is simply the maximum performance the method achieves on the benchmark. A method with mono- tonically increasing scaling achieves 100% performance on any benchmark in the limit. However, the methods we in- vestigate eventually flatten out or further scaling fails due to control or context window limitations. 4. Results 4.1. Setup Training We perform supervised finetuning on Qwen2.5- 32B-Instruct using s1K to obtain our model s1-32B using basic hyperparameters outlined in §D. Finetuning took 26 minutes on 16 NVIDIA H100 GPUs with PyTorch FSDP.", "source": "arxiv_pdf", "published": "", "tokens": 234, "sha256": "d56d925240a622f521fa288f3466bf06083954fba2aab3b6a22a5e6c03c803d1"}
{"doc_id": "arxiv:2501.19393#evaluation:part-1", "url": "https://arxiv.org/abs/2501.19393", "anchor": "#evaluation:part-1", "type": "paper", "title": "", "section": "Evaluation", "text": "We select three representative reasoning benchmarks widely used in the field: AIME24 (of America, 2024) has 30 problems that were used in the 2024 American Invitational Mathematics Examination (AIME) held from January 31 – February 1, 2024. AIME tests mathematical problem-solving with arithmetic, algebra, counting, geome- try, number theory, probability, and other secondary school math topics. High-scoring high school students in the test are invited to participate in the United States of America Mathe- matics Olympiad (USAMO). All AIME answers are integers ranging from 000 to 999, inclusive. Some AIME problems rely on figures that we provide to our model using the vec- tor graphics language Asymptote as it cannot take image inputs. MATH500 (Hendrycks et al., 2021) is a benchmark of competition math problems of varying difficulty. We evaluate on the same 500 samples selected by OpenAI in prior work (Lightman et al., 2023). GPQA Diamond (Rein et al., 2023) consists of 198 PhD-level science questions 4 s1: Simple test-time scaling Forcing 2048/4096 max thinking tokens Ignoring end-of-thinking 2x/4x/6x and appending “Wait” thus forcing it to continue reasoning when it tries to stop (a) Sequential scaling via budget forcing Parallel scaling via Majority voting with base model (Qwen2.5-32B-Instruct) Sequential scaling via Forcing with s1 60% accuracy (b) Parallel scaling via majority voting Figure 4. Sequential and parallel test-time scaling. (a): Budget forcing shows clear scaling trends and extrapolates to some extent. For the three rightmost dots, we prevent the model from stopping its thinking 2/4/6 times, each time appending “Wait” to its current reasoning trace. (b): For Qwen2.5-32B-Instruct we perform 64 evaluations for each sample with a temperature of 1 and visualize the performance when majority voting across 2, 4, 8, 16, 32, and 64 of these. from Biology, Chemistry and Physics. Experts with PhDs in the corresponding domains only achieved 69.7% on GPQA Diamond (OpenAI, 2024). When we write “GPQA” in the context of evaluation in this work, we always refer to the Diamond subset. We build on the “lm-evaluation-harness” framework (Gao et al., 2021; Biderman et al., 2024). Unless otherwise specified, we evaluate with a temperature of 0 (greedy) and measure accuracy (equivalent to pass@1). Other models We benchmark s1-32B against: OpenAI o1 series (OpenAI, 2024), closed-source models that pop- ularized test-time scaling; DeepSeek r1 series (DeepSeek- AI et al., 2025), open-weight reasoning models with up to o1-level performance; Qwen’s QwQ-32B-preview (Team, 2024), a 32B open-weight reasoning model without dis- closed methodology; Sky-T1-32B-Preview (Team, 2025) and Bespoke-32B (Labs, 2025), open models with open rea- soning data distilled from QwQ-32B-preview and r1; Google Gemini 2.0 Flash Thinking Experimental (Google, 2024), the API that we distill from. As it has no official evaluation scores, we use the Gemini API to benchmark it ourselves. However, the “recitation error” of the Gemini API makes evaluation challenging.2 We circumvent this, by manually inserting all 30 AIME24 questions in its web interface where the error does not appear. However, we leave out MATH500 (500 questions) and GPQA Diamond (198 questions), thus they are N.A. in Table 1. Our model, s1-32B, is fully open including weights,", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "6741e88fdcb5d41f7dfc737dcd6e2cac1d3ebadc17b60e518bfa1443ff1e8c08"}
{"doc_id": "arxiv:2501.19393#evaluation:part-2", "url": "https://arxiv.org/abs/2501.19393", "anchor": "#evaluation:part-2", "type": "paper", "title": "", "section": "Evaluation", "text": "the API that we distill from. As it has no official evaluation scores, we use the Gemini API to benchmark it ourselves. However, the “recitation error” of the Gemini API makes evaluation challenging.2 We circumvent this, by manually inserting all 30 AIME24 questions in its web interface where the error does not appear. However, we leave out MATH500 (500 questions) and GPQA Diamond (198 questions), thus they are N.A. in Table 1. Our model, s1-32B, is fully open including weights, reasoning data, and code. 2https://github.com/google/ generative-ai-docs/issues/257 Table 1. s1-32B is a strong open reasoning model. We evaluate s1-32B, Qwen, and Gemini (some entries are unknown (N.A.), see §4). Other results are from the respective reports (Qwen et al., 2024; Team, 2024; OpenAI, 2024; DeepSeek-AI et al., 2025; Labs, 2025; Team, 2025). # ex. = number examples used for reasoning finetuning; BF = budget forcing. See §A for our better s1.1 model.", "source": "arxiv_pdf", "published": "", "tokens": 151, "sha256": "545ecf8cde9813dc3a7c03c32fc65bc355e41169de73f7f7e8c1c6e0c50f2e8d"}
{"doc_id": "arxiv:2501.19393#model:part-1", "url": "https://arxiv.org/abs/2501.19393", "anchor": "#model:part-1", "type": "paper", "title": "", "section": "Model", "text": "# ex. AIME 2024 MATH 500 GPQA Diamond API only o1-preview N.A. 44.6 85.5 73.3 o1-mini N.A. 70.0 90.0 60.0 o1 N.A. 74.4 94.8 77.3 Gemini 2.0 N.A. 60.0 N.A. N.A. Flash Think. Open Weights Qwen2.5- N.A. 26.7 84.0 49.0 32B-Instruct QwQ-32B N.A. 50.0 90.6 54.5 r1 ≫800K 79.8 97.3 71.5 r1-distill 800K 72.6 94.3 62.1 Open Weights and Open Data Sky-T1 17K 43.3 82.4 56.8 Bespoke-32B 17K 63.3 93.0 58.1 s1 w/o BF 1K 50.0 92.6 56.6 s1-32B 1K 56.7 93.0 59.6 5 s1: Simple test-time scaling 4.2. Performance Test-time scaling Figure 1 shows the performance of s1- 32B with budget forcing scales with more test-time compute. In Figure 4 (left), we expand the plot from Figure 1 (middle) showing that while we can improve AIME24 performance using our budget forcing technique (§3) and more test-time compute it does eventually flatten out at six times. Suppress- ing the end-of-thinking token delimiter too often can lead the model into repetitive loops instead of continued reasoning. In Figure 4 (right), we show that after training Qwen2.5- 32B-Instruct on our 1,000 samples to produce s1-32B and equipping it with the simple budget forcing technique, it operates in a different scaling paradigm. Scaling test-time compute on the base model via majority voting cannot catch up with the performance of s1-32B which validates our intuition from §3 that sequential scaling is more effective than parallel. We provide example generations of s1-32B in Figure 5. Sample-efficiency In Figure 2 (right) and Table 1 we compare s1-32B with other models. We find that s1- 32B is the most sample-efficient open data reasoning model. It performs significantly better than our base model (Qwen2.5-32B-Instruct) despite just training it on an ad- ditional 1,000 samples. The concurrently released r1-32B shows stronger performance than s1-32B while also only using SFT (DeepSeek-AI et al., 2025). However, it is trained on 800 × more reasoning samples. It is an open question whether one can achieve their performance with just 1,000 samples. Finally, our model nearly matches Gemini 2.0 Thinking on AIME24. As the data for s1-32B is distilled from Gemini 2.0, this shows our distillation procedure was likely effective. 5. Ablations 5.1. Data Quantity, Diversity, and Difficulty In §2 we outlined our three guiding principles in curating s1K: Quality, Difficulty, and Diversity. Here we test the importance of combining them and the overall efficacy of our selection. Only Quality (1K-random): After obtaining our high-quality reasoning chains from Gemini, we select 1,000 samples at random; not relying on our difficulty and diversity filtering at all. Table 2 shows this approach performs much worse than s1K across all benchmarks. Only Diversity (1K-diverse): For this dataset, we sample uniformly across domains to maximize diversity disregarding any notion of difficulty. This approach also leads to poor performance similar to 1K-random. Only Difficulty (1K-longest): Here we rely on one of our difficulty indicators introduced in §2 by selecting the 1,000 samples with the longest reasoning traces. This approach significantly boosts GPQA performance but overall still falls short of using s1K. Maximize Quantity: Table 2. s1K data ablations. We budget", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "44f00b0676fbc2b671b2ac20140f2668a2fc88b104c013c4dec95b632357732f"}
{"doc_id": "arxiv:2501.19393#model:part-2", "url": "https://arxiv.org/abs/2501.19393", "anchor": "#model:part-2", "type": "paper", "title": "", "section": "Model", "text": "across all benchmarks. Only Diversity (1K-diverse): For this dataset, we sample uniformly across domains to maximize diversity disregarding any notion of difficulty. This approach also leads to poor performance similar to 1K-random. Only Difficulty (1K-longest): Here we rely on one of our difficulty indicators introduced in §2 by selecting the 1,000 samples with the longest reasoning traces. This approach significantly boosts GPQA performance but overall still falls short of using s1K. Maximize Quantity: Table 2. s1K data ablations. We budget force (BF) a maximum of around 30,000 thinking tokens for all scores in this table. This performs slightly better than the scores without BF (Table 1) as it allows the model to finish with a best guess when stuck in an infinite loop. We report 95% paired bootstrap confidence intervals for differences relative to the s1K model using 10,000 bootstrap samples. E.g., the interval [-13%, 20%] means that, with 95% con- fidence, the true difference between 59K-full and s1K is between -13% and +20%. If the entire interval is negative, e.g. [-27%, -3%], we can confidently say that the performance is worse than s1K.", "source": "arxiv_pdf", "published": "", "tokens": 184, "sha256": "f1d52b4f7fcc0ba251933317c72df82480984e7ab2c6c0e1ce5cc82e9645cff3"}
{"doc_id": "arxiv:2501.19393#model", "url": "https://arxiv.org/abs/2501.19393", "anchor": "#model", "type": "paper", "title": "", "section": "Model", "text": "AIME 2024 MATH 500 GPQA Diamond 1K-random 36.7 90.6 52.0 [-26.7%, -3.3%] [-4.8%, 0.0%] [-12.6%, 2.5%] 1K-diverse 26.7 91.2 54.6 [-40.0%, -10.0%] [-4.0%, 0.2%] [-10.1%, 5.1%] 1K-longest 33.3 90.4 59.6 [-36.7%, 0.0%] [-5.0%, -0.2%] [-5.1%, 10.1%] 59K-full 53.3 92.8 58.1 [-13.3%, 20.0%] [-2.6%, 2.2%] [-6.6%, 8.6%] s1K 50.0 93.0 57.6 Finally, we compare with just training on all of our 59K samples, a superset of all the 1K-sample versions. This leads to a strong model but uses much more resources. To finetune on 59K samples, we use 394 H100 GPU hours while s1-32B only required 7 H100 GPU hours. Moreover, relying only on s1K is extremely competitive as shown in §2. Overall, combining all three criteria – Quality, Difficulty, Diversity – via our methodology in §2 is key for sample- efficient reasoning training. 5.2. Test-time scaling methods Table 3. Ablations on methods to scale test-time compute on AIME24. || refers to the number of evaluation runs used to estimate the properties; thus a higher value indicates more robust- ness. Bold indicates our chosen method and the best values. BF = budget forcing, TCC/SCC/CCC = token/step/class-conditional control, RS = rejection sampling.", "source": "arxiv_pdf", "published": "", "tokens": 190, "sha256": "1a1145abc3afdee885f07fd6327f70546526bf3a668200d211a3d0afe2c8c500"}
{"doc_id": "arxiv:2501.19393#method:part-1", "url": "https://arxiv.org/abs/2501.19393", "anchor": "#method:part-1", "type": "paper", "title": "", "section": "Method", "text": "Control Scaling Performance || BF 100% 15 56.7 5 TCC 40% -24 40.0 5 TCC + BF 100% 13 40.0 5 SCC 60% 3 36.7 5 SCC + BF 100% 6 36.7 5 CCC 50% 25 36.7 2 RS 100% -35 40.0 5 6 s1: Simple test-time scaling [AIME24] Alice and Bob play the following game. A stack of 𝑛tokens lies before them. The players take turns with Alice going first. On each turn, the player removes either 1 token or 4 tokens from the stack. Whoever removes the last token wins. Find the number of positive integers 𝑛less than or equal to 2024 for which there exists a strategy for Bob that guarantees that Bob will win the game regardless of Alice’s play. The problem asks for the number of positive integers 𝑛≤2024 such that Bob has a winning strategy. This is a typical impartial game that can be analyzed using the Sprague-Grundy theorem or by directly finding the winning and losing positions. Let 𝑊be the set of winning positions (number of tokens 𝑛such that the player whose turn it is can force a win) and 𝐿be the set of losing positions. [...] Let 𝑃(𝑛) be the statement \"the player whose turn it is when there are 𝑛tokens has a winning strategy. 𝑃(0) is false (the player whose turn it is and faces 0 tokens has already lost). Consider the small values of 𝑛: 𝑛= 1: The player whose turn it is can take 1 token and win. So 𝑃(1) is true. 1 ∈𝑊. 𝑛= 2: The player whose turn it is can take 1 token, leaving 1 token for the opponent. Since 1 ∈𝑊, the opponent wins. So 𝑃(2) is false. 2 ∈𝐿. 𝑛= 3: The player whose turn it is can take 1 token, leaving 2 tokens for the opponent. Since 2 ∈𝐿, the player whose turn it is wins. So 𝑃(3) is true. 3 ∈ 𝑊.[...] We are looking for the number of positive integers 𝑛≤2024 for which Bob has a winning strategy. This means that if the game starts with 𝑛tokens, and Alice goes first, Bob can win. This happens if and only if 𝑛is a losing position (because if 𝑛∈𝐿, then Alice’s first move must lead to a winning position for Bob). [...] Let’s try to find a pattern in the losing positions 𝐿= {0, 2, 5, 7, 10, 12, 15, 17, 20, 22, ...}. Differences between consecutive terms: 2, 3, 2, 3, 2, 3, 2, 3, 2, ... The pattern of differences is alternating 2, 3. [...] The losing positions are of the form 5𝑚or 5𝑚+ 2, where 𝑚≥0. We need to find the number of positive integers 𝑛≤2024 that are in the set of losing positions 𝐿. [...] Case 1: 𝑛= 5𝑚, where 𝑚≥1. 1 ≤5𝑚≤2024 1∕5 ≤𝑚≤2024∕5 = 404.8 Since 𝑚is an integer, 1 ≤𝑚≤404. The number of values of 𝑚is 404. Case 2: 𝑛= 5𝑚+ 2, where 𝑚≥0. 5𝑚+ 2 ≤2024 5𝑚≤2022 𝑚≤2022∕5 = 404.4 Since 𝑚is an in- teger, 0 ≤𝑚≤404. The number of values of 𝑚is 404 −0 + 1", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "5949f180a703fb1d26fb98332fa91912e0f4f3a0fad90badf048a7824c9a7052"}
{"doc_id": "arxiv:2501.19393#method:part-2", "url": "https://arxiv.org/abs/2501.19393", "anchor": "#method:part-2", "type": "paper", "title": "", "section": "Method", "text": "5𝑚+ 2, where 𝑚≥0. We need to find the number of positive integers 𝑛≤2024 that are in the set of losing positions 𝐿. [...] Case 1: 𝑛= 5𝑚, where 𝑚≥1. 1 ≤5𝑚≤2024 1∕5 ≤𝑚≤2024∕5 = 404.8 Since 𝑚is an integer, 1 ≤𝑚≤404. The number of values of 𝑚is 404. Case 2: 𝑛= 5𝑚+ 2, where 𝑚≥0. 5𝑚+ 2 ≤2024 5𝑚≤2022 𝑚≤2022∕5 = 404.4 Since 𝑚is an in- teger, 0 ≤𝑚≤404. The number of values of 𝑚is 404 −0 + 1 = 405. The total number of positive integers 𝑛≤2024 that are losing positions is 404 + 405 = 809 . [MATH500] Find the projection of 𝑎onto 𝑏= (2 6 3 ) if 𝑎⋅𝑏= 8. The projection of vector 𝐚onto vector 𝐛is given by the formula: proj𝐛𝐚= 𝐚⋅𝐛 ‖𝐛‖2 𝐛 We are given 𝐛= (2 6 3 ) and 𝐚⋅𝐛= 8. First, we need to find the magnitude squared of 𝐛, which is ‖𝐛‖2. ‖𝐛‖2 = 𝐛⋅𝐛= 22+62+32 = 4+36+9 = 49. Now we can plug the values into the projection formula: proj𝐛𝐚= 8 49𝐛 Substitute the vector 𝐛: proj𝐛𝐚= 8 49 (2 6 3 ) Multiply the scalar by each compo- nent of the vector: proj𝐛𝐚= ⎛ ⎜ ⎜ ⎜⎝ 8 49 × 2 8 49 × 6 8 49 × 3 ⎞ ⎟ ⎟ ⎟⎠ = ⎛ ⎜ ⎜ ⎜⎝ 16 49 48 49 24 49 ⎞ ⎟ ⎟ ⎟⎠ Final Answer: The final answer is ⎛ ⎜ ⎜ ⎜⎝ 16 49 48 49 24 49 ⎞ ⎟ ⎟ ⎟⎠ [GPQA] A quantum mechanical particle of mass 𝑚moves in two dimensions in the following potential, as a function of the polar coordinates (𝑟, 𝜃): 𝑉(𝑟, 𝜃) = 1 2𝑘𝑟2 + 3 2𝑘𝑟2 cos2(𝜃) Find the energy spectrum. Hint: Write the potential in Cartesian co- ordinates. The problem asks for the energy spec- trum of a particle in a given 2D po- tential. The potential is given in polar coordinates (𝑟, 𝜃): 𝑉(𝑟, 𝜃) = 1 2𝑘𝑟2 + 3 2𝑘𝑟2 cos2(𝜃). We can rewrite the potential as: 𝑉(𝑟, 𝜃) = 1 2𝑘𝑟2(1 + 3 cos2(𝜃)). The Hamiltonian for a particle in 2D is given by: 𝐻= −ℏ2 2𝑚 ( 𝜕2 𝜕𝑥2 + 𝜕2 𝜕𝑦2 ) + 𝑉(𝑥, 𝑦). In polar coordinates (𝑟, 𝜃), the Lapla- cian operator is: ∇2 = 1 𝑟 𝜕 𝜕𝑟 ( 𝑟𝜕 𝜕𝑟 ) + 1 𝑟2 𝜕2 𝜕𝜃2 . So the Hamiltonian in polar coordi- nates is: 𝐻= −ℏ2 2𝑚 [ 1 𝑟 𝜕 𝜕𝑟 ( 𝑟𝜕 𝜕𝑟 ) + 1 𝑟2 𝜕2 𝜕𝜃2 ] +𝑉(𝑟, 𝜃). The time-independent Schrödinger equation is 𝐻𝜓= 𝐸𝜓. We can try to separate variables by assuming a so- lution of the form 𝜓(𝑟, 𝜃) = 𝑅(𝑟)Θ(𝜃). [...] This confirms the energy spectrum is: 𝐸= ℏ √ 𝑘 𝑚 ( 2𝑛𝑥+ 𝑛𝑦+ 3 2 ) . Final Answer: 𝐸 = ℏ √ 𝑘 𝑚 ( 2𝑛𝑥+ 𝑛𝑦+ 3 2 ) . Figure 5. Example model outputs. We pick a question from AIME24 (left), MATH500 (middle), and GPQA (right), where our model generates the correct answer. The black text is the prompt, the", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "60ca6c860cf2eaec108cccc7c9a49c42f8c9102b3cd410a8f3cd07df3c08c7ff"}
{"doc_id": "arxiv:2501.19393#method:part-3", "url": "https://arxiv.org/abs/2501.19393", "anchor": "#method:part-3", "type": "paper", "title": "", "section": "Method", "text": "to separate variables by assuming a so- lution of the form 𝜓(𝑟, 𝜃) = 𝑅(𝑟)Θ(𝜃). [...] This confirms the energy spectrum is: 𝐸= ℏ √ 𝑘 𝑚 ( 2𝑛𝑥+ 𝑛𝑦+ 3 2 ) . Final Answer: 𝐸 = ℏ √ 𝑘 𝑚 ( 2𝑛𝑥+ 𝑛𝑦+ 3 2 ) . Figure 5. Example model outputs. We pick a question from AIME24 (left), MATH500 (middle), and GPQA (right), where our model generates the correct answer. The black text is the prompt, the light blue text is the reasoning trace, and the blue text is the answer of s1-32B. The gray ellipsis [...] indicates that the text was trimmed to fit this page, but the generated text is actually longer. 7 s1: Simple test-time scaling Budget forcing In Table 3 we compare the test-time scal- ing methods we have introduced in §3. Overall, we find that budget forcing provides perfect control, good scaling, and leads to our best AIME24 score. Thus, this is the method we use for s1-32B in Figure 1 and in §4. In Table 4, we compare different strings for extrapolating performance. We find that “Wait” generally gives the best performance. Class-conditional control We provide benchmark scores for this method in §E.1 and summarize three findings here: (1) Token-conditional control fails without budget forcing, as our model cannot reliably count tokens - even when trained to do so. (2) Under step-conditional control, the model gen- erates a similar total number of tokens when given different step targets, as the model goes from few steps with many tokens per step, to many steps with few tokens in each step. Thus, the model learns to hack its way around the com- pute constraint making the controllability of this method mediocre. (3) Class-conditional control can work - telling a model to simply think longer can increase its test-time com- pute and performance, which leads good scaling in Table 3. Table 4. Budget forcing extrapolation ablations. We compare ignoring the end-of-thinking delimiter twice and appending none or various strings.", "source": "arxiv_pdf", "published": "", "tokens": 335, "sha256": "cb1c8bc7aa0be3ca80a5e61dbcca988e069f9a309da441819e00052b9381cc70"}
{"doc_id": "arxiv:2501.19393#methods:part-1", "url": "https://arxiv.org/abs/2501.19393", "anchor": "#methods:part-1", "type": "paper", "title": "", "section": "Methods", "text": "As we introduce in §3, we differentiate two meth- ods to scale test-time compute: parallel and sequential. The former relies on multiple solution attempts generated in par- allel and selecting the best outcome via specific criteria. These criteria include choosing the most frequent response for majority voting or the best response based on an exter- nal reward for Best-of-N (Brown et al., 2024; Irvine et al., 2023; Levi, 2024). Unlike repeated sampling, previous se- quential scaling methods let the model generate solution attempts sequentially based on previous attempts, allowing it to refine each attempt based on previous outcomes (Snell et al., 2024; Hou et al., 2025; Lee et al., 2025). Tree-based search methods (Gandhi et al., 2024; Wu et al., 2024b) offer a hybrid approach between sequential and parallel scaling, such as Monte-Carlo Tree Search (MCTS) (Liu et al., 2024; Zhang et al., 2023; Zhou et al., 2024; Choi et al., 2023) and guided beam search (Xie et al., 2023). REBASE (Wu et al., 2024b) employs a process reward model to balance exploita- tion and pruning during tree search. Empirically, REBASE has been shown to outperform sampling-based methods and MCTS (Wu et al., 2024b). Reward models (Lightman et al., 2023; Wang et al., 2024b;c) play a key role in these meth- ods. They come in two variants: outcome reward models and process reward models. Outcome reward models (Xin et al., 2024; Ankner et al., 2024) assign a score to complete solutions and are particularly useful in Best-of-N selection, while process reward models (Lightman et al., 2023; Wang et al., 2024b; Wu et al., 2024b) assess individual reasoning steps and are effective in guiding tree-based search methods. Limits to further test-time scaling We have shown that budget forcing allows extrapolating test-time compute in §4, e.g., improving AIME24 performance from 50% to 57%. However, it has two key limitations when scaling further: it eventually flattens out (Figure 4), and the context window of the underlying language model constrains it. Despite these constraints, our work shows test-time scaling across a wide range of accuracies (Figure 1), partly because scaling down test-time compute behaves predictably and does not suffer from these constraints. Continuing test-time scaling will require approaches that can further extrapolate test-time compute. How can we get 2048 8192 32768 131072 Average thinking time (tokens) 30 40 50 60 Accuracy (%) REBASE Majority Sequential scaling Figure 7. Scaling further with parallel scaling methods. All met- rics averaged over the 30 questions in AIME24. Average thinking tokens for REBASE do not account for the additional compute from the reward model. For sequential scaling, we prompt the model to use up to (from left to right) 32, 64, 256, and 512 steps. For REBASE and majority voting we generate 16 parallel trajectories to aggregate across. such extrapolation? There may be improvements to budget forcing such as rotating through different strings, not only “Wait”, or combining it with frequency penalties or higher temperature to avoid repetitive loops. An exciting direc- tion for future work is also researching whether applying budget forcing to a reasoning model trained with reinforce-", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "31054e9c16e6bda36935fbf42eca279c088e77a57bcb1a9fcfaf60f3c1e93aa1"}
{"doc_id": "arxiv:2501.19393#methods:part-2", "url": "https://arxiv.org/abs/2501.19393", "anchor": "#methods:part-2", "type": "paper", "title": "", "section": "Methods", "text": "model to use up to (from left to right) 32, 64, 256, and 512 steps. For REBASE and majority voting we generate 16 parallel trajectories to aggregate across. such extrapolation? There may be improvements to budget forcing such as rotating through different strings, not only “Wait”, or combining it with frequency penalties or higher temperature to avoid repetitive loops. An exciting direc- tion for future work is also researching whether applying budget forcing to a reasoning model trained with reinforce- ment learning yields better extrapolation; or if RL allows for new ways of test-time scaling beyond budget forcing. Our work defines the right metrics (§3.2) – Control, Scaling, and Performance – to enable future research and progress on extrapolating test-time compute. Parallel scaling as a solution Parallel scaling offers one solution to the limits of sequential scaling, thus we aug- ment our sequentially scaled model with two methods: (I) Majority voting: After generating 𝑘solutions, the final solution is the most frequent one across generations; (II) Tree search via REBASE: We use the REBASE process reward model, which is initialized from LLaMA-34B and further finetuned on a synthetic process reward modeling dataset (Wu et al., 2024b). We then aggregate the solutions generated by REBASE via majority voting. As shown in Figure 7, augmenting our model with REBASE scales better than majority voting, and even sequential scaling in this sce- nario. However, REBASE requires an additional forward pass at each step for the reward model adding some computa- tion overhead. For sequential scaling, when prompted to use up to 512 steps, for 12 out of the 30 evaluation questions the model generates a response that exceeds the context window leading to a large performance drop. Overall, we find that these parallel scaling methods complement sequential scal- ing thus they offer an avenue for scaling test-time compute even further; beyond fixed context windows. 9 s1: Simple test-time scaling Impact Statement Language models with strong reasoning capabilities have the potential to greatly enhance human productivity, from assisting in complex decision-making to driving scientific breakthroughs. However, recent advances in reasoning, such as OpenAI’s o1 and DeepSeek’s r1, lack transparency, limit- ing broader research progress. Our work aims to push the frontier of reasoning in a fully open manner, fostering in- novation and collaboration to accelerate advancements that ultimately benefit society. Acknowledgements We thank Ryan Marten for generating traces from DeepSeek r1 for s1.1 using Bespoke Curator (Marten et al., 2025). This work was partly conducted using the Stanford Marlowe GPU cluster (Kapfer et al., 2025), made possible by financial support from Stanford University. We thank Alexander M. Rush, Andrew Ilyas, Banghua Zhu, Chenglei Si, Chunting Zhou, John Yang, Ludwig Schmidt, Samy Jelassi, Suhas Kotha, Tengyu Ma, Xuechen Li, Yu Sun, and Yue Zhang for very constructive discussions.", "source": "arxiv_pdf", "published": "", "tokens": 462, "sha256": "5575e92c8c2299ae6ad86b3f704a20b8159f898a40d97333623374911fd6b074"}
{"doc_id": "arxiv:2303.06865#abstract:part-1", "url": "https://arxiv.org/abs/2303.06865", "anchor": "#abstract:part-1", "type": "paper", "title": "", "section": "Abstract", "text": "The high computational and memory require- ments of large language model (LLM) inference make it feasible only with multiple high-end ac- celerators. Motivated by the emerging demand for latency-insensitive tasks with batched processing, this paper initiates the study of high-throughput LLM inference using limited resources, such as a single commodity GPU. We present FlexGen, a high-throughput generation engine for running LLMs with limited GPU memory. FlexGen can be flexibly configured under various hardware re- source constraints by aggregating memory and computation from the GPU, CPU, and disk. By solving a linear programming problem, it searches for efficient patterns to store and access tensors. FlexGen further compresses the weights and the attention cache to 4 bits with negligible accu- racy loss. These techniques enable FlexGen to have a larger space of batch size choices and thus significantly increase maximum throughput. As a result, when running OPT-175B on a sin- gle 16GB GPU, FlexGen achieves significantly higher throughput compared to state-of-the-art of- floading systems, reaching a generation through- put of 1 token/s for the first time with an effec- tive batch size of 144. On the HELM bench- mark, FlexGen can benchmark a 30B model with a 16GB GPU on 7 representative sub-scenarios in 21 hours. The code is available at https: //github.com/FMInference/FlexGen. 1Stanford University 2UC Berkeley 3ETH Zurich 4Yandex 5HSE University 6Meta 7Carnegie Mellon University. Correspon- dence to: Ying Sheng <ying1123@stanford.edu>. Proceedings of the 40 th International Conference on Machine Learning, Honolulu, Hawaii, USA. PMLR 202, 2023. Copyright 2023 by the author(s). This version has an extended author list compared to the one archived in ICML. 211 213 Latency (s) 20 2 2 2 4 2 6 2 8 OPT-175B FlexGen (c) FlexGen DeepSpeed Accelerate 28 29 210 Latency (s) 23 21 2 1 2 3 OPT-30B Generation throughput (token/s) Figure 1. The total latency for a block and throughput trade-offs of three offloading-based systems for OPT-175B (left) and OPT-30B (right) on a single NVIDIA T4 (16 GB) GPU with 208 GB CPU DRAM and 1.5TB SSD. FlexGen achieves a new Pareto-optimal frontier with 100× higher maximum throughput for OPT-175B. Other systems cannot further increase throughput due to out-of- memory issues. “(c)” denotes compression. 1. Introduction In recent years, large language models (LLMs) have demonstrated strong performance across a wide range of tasks (Brown et al., 2020; Bommasani et al., 2021; Zhang et al., 2022; Chowdhery et al., 2022). Along with these un- precedented capabilities, generative LLM inference comes with unique challenges. These models can have billions, if not trillions of parameters (Chowdhery et al., 2022; Fedus et al., 2022), which leads to extremely high computational and memory requirements to run. For example, GPT-175B requires 325GB of GPU memory simply to load its model weights. Fitting this model onto GPUs would require at least five A100 (80GB) GPUs and complex parallelism strate- gies (Pope et al., 2022; Aminabadi et al., 2022). Thus, lowering LLM inference resource requirements has recently attracted intense interest. In this paper, we focus on a setting that we call throughput- oriented generative inference. In addition to interactive use cases", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "5e37d20767f69a3b565eab9dbbb0e8ffd64edf882d496308af540ae9d101dcab"}
{"doc_id": "arxiv:2303.06865#abstract:part-2", "url": "https://arxiv.org/abs/2303.06865", "anchor": "#abstract:part-2", "type": "paper", "title": "", "section": "Abstract", "text": "extremely high computational and memory requirements to run. For example, GPT-175B requires 325GB of GPU memory simply to load its model weights. Fitting this model onto GPUs would require at least five A100 (80GB) GPUs and complex parallelism strate- gies (Pope et al., 2022; Aminabadi et al., 2022). Thus, lowering LLM inference resource requirements has recently attracted intense interest. In this paper, we focus on a setting that we call throughput- oriented generative inference. In addition to interactive use cases such as chatbots, LLMs are also applied to many “back-of-house” tasks such as benchmarking (Liang et al., 2022), information extraction (Narayan et al., 2018), data wrangling (Narayan et al., 2022), and form processing (Chen et al., 2021). One key characteristic of these tasks is that they often require running LLM inference in batches over a large number of tokens (e.g., all the documents in a company’s 1 arXiv:2303.06865v2 [cs.LG] 12 Jun 2023 FlexGen: High-Throughput Generative Inference of Large Language Models with a Single GPU corpus), and are less sensitive to latency. As a result, it is possible to trade off latency for higher throughput in these workloads, providing opportunities to reduce resource requirements. Prior efforts to lower resource requirements of LLM infer- ence correspond to three directions: (1) model compression to decrease total memory footprint (Dettmers et al., 2022; Yao et al., 2022; Frantar et al., 2022; Xiao et al., 2022); (2) collaborative inference to amortize inference cost via decentralization (Borzunov et al., 2022); and (3) offloading to utilize memory from CPU and disk (Aminabadi et al., 2022; HuggingFace, 2022). These techniques have signifi- cantly lowered the resource requirements for using LLMs, but there are distinct limitations. Research in the first two directions often assume that the model fits into the GPU memory and thereby struggle to run 175B-scale models with a single commodity GPU. On the other hand, state-of-the- art offloading-based systems in the third category do not achieve acceptable throughput on a single GPU due to inef- ficient I/O scheduling and tensor placement. For example, these systems can be bottlenecked by small batch sizes (e.g., batch sizes of only one or two for OPT-175B in some cases). 16 GB 208 GB 1.5 TB GPU CPU Disk 12 GB/s 2 GB/s Our focus is designing efficient offloading strategies for high- throughput generative inference, on a single commodity GPU. To run an LLM with limited GPU memory, we can offload it to sec- ondary storage and perform com- putation part-by-part by partially loading it. On a typical machine, there are three levels of the memory hierarchy, as illustrated in the figure to the right. Higher levels are faster but scarce, while lower levels are slower but abundant. In throughput-oriented scenarios, we can sacrifice latency by using a large batch size, and amortize the expensive I/O operations among different memory hierarchies over a large batch of inputs, overlapped with computation. Fig. 1 shows the latency-throughput trade-off of three inference systems with offloading on a single NVIDIA T4 (16 GB) GPU. Note that the performance in terms of latency and throughput on limited resources", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "0729e8fb0670230c1ec6b14c4e12db9f7c9ec7e9f8434c695d28b75c808b9c61"}
{"doc_id": "arxiv:2303.06865#abstract:part-3", "url": "https://arxiv.org/abs/2303.06865", "anchor": "#abstract:part-3", "type": "paper", "title": "", "section": "Abstract", "text": "the right. Higher levels are faster but scarce, while lower levels are slower but abundant. In throughput-oriented scenarios, we can sacrifice latency by using a large batch size, and amortize the expensive I/O operations among different memory hierarchies over a large batch of inputs, overlapped with computation. Fig. 1 shows the latency-throughput trade-off of three inference systems with offloading on a single NVIDIA T4 (16 GB) GPU. Note that the performance in terms of latency and throughput on limited resources is significantly inferior to that of the cases with sufficient resources. Achieving high-throughput generative inference with lim- ited GPU memory is challenging even if we can sacrifice the latency. The first challenge is to design an efficient of- floading strategy. During generative inference, there are three kinds of tensors: weights, activations, and key-value (KV) cache. The strategy should specify what tensors to of- fload, where to offload them within the three-level memory hierarchy, and when to offload them during inference. The batch-by-batch, token-by-token, and layer-by-layer struc- ture of the computation forms a complex dependency graph where there are multiple ways to conduct computation. To- gether, these choices form a complex design space. Existing offloading-based inference systems (Aminabadi et al., 2022; HuggingFace, 2022) inherit strategies from training, which turn out to be some suboptimal points for inference, per- forming excessive I/O and achieving throughput far below theoretical hardware limits. The second challenge is to develop effective compression strategies. Previous works have demonstrated promising results in compressing the weights and activations of LLMs. However, when combining compression with offloading for high-throughput inference, the I/O costs and memory reduc- tion of the weights and KV cache become more important, motivating alternative compression schemes. To address these challenges, we present FlexGen, an of- floading framework for high-throughput LLM inference. FlexGen aggregates memory from the GPU, CPU, and disk, and efficiently schedules I/O operations, along with possible compression methods and distributed pipeline parallelism. (Contribution 1) We formally define a search space of possible offloading strategies by considering computation schedule, tensor placement, and computation delegation. We prove that our search space captures a computation order with I/O complexity within 2× of optimality. We then develop a linear programming-based search algorithm to optimize the throughput within the search space. This algorithm can be configured for various hardware specifica- tions and can be easily extended to incorporate latency and throughput constraints, thus helping to navigate the trade- off space smoothly. Compared with existing strategies, our solution unifies the placement of weights, activations, and the KV cache, enabling a dramatically higher batch size upper bound, which is key to achieving high throughput. (Contribution 2) We show that it is possible to compress both the weights and KV cache for LLMs like OPT-175B to 4 bits without retraining or calibration, all with negligible accuracy loss. This is achieved through fine-grained group- wise quantization (Shen et al., 2020), which is suitable for reducing I/O costs and memory usage during offloading. (Contribution 3) We demonstrate the efficiency of FlexGen by running OPT-175B on NVIDIA T4 (16GB) GPUs. Com- pared to DeepSpeed Zero-Inference (Aminabadi et", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "fd0ca13704787bd610cb5b0379d2dcc10b826ef85c3318d2f6f0f2e3d62d8a09"}
{"doc_id": "arxiv:2303.06865#abstract:part-4", "url": "https://arxiv.org/abs/2303.06865", "anchor": "#abstract:part-4", "type": "paper", "title": "", "section": "Abstract", "text": "throughput. (Contribution 2) We show that it is possible to compress both the weights and KV cache for LLMs like OPT-175B to 4 bits without retraining or calibration, all with negligible accuracy loss. This is achieved through fine-grained group- wise quantization (Shen et al., 2020), which is suitable for reducing I/O costs and memory usage during offloading. (Contribution 3) We demonstrate the efficiency of FlexGen by running OPT-175B on NVIDIA T4 (16GB) GPUs. Com- pared to DeepSpeed Zero-Inference (Aminabadi et al., 2022) and Hugging Face Accelerate (HuggingFace, 2022), two state-of-the-art offloading-based inference systems, FlexGen often allows a batch size that is orders of mag- nitude larger. As a result, FlexGen can achieve much higher throughputs. On a single T4 GPU with 208 GB CPU DRAM and 1.5 TB SSD, input sequence length 512, and output se- quence length 32: • With the same latency of 5000 seconds, FlexGen (effec- tive batch size 64, or 2048 tokens in total) can achieve 2 FlexGen: High-Throughput Generative Inference of Large Language Models with a Single GPU more than 40× higher throughput than DeepSpeed Zero-Inference (batch size 1, or 32 tokens in total), while Hugging Face Accelerate cannot complete a sin- gle batch. • By allowing a higher latency of 12000 seconds, FlexGen achieves 69× higher maximum throughput compared to baselines because it can enlarge the effec- tive batch size to 256 (8192 tokens generated in total), while DeepSpeed Zero-Inference and Hugging Face Accelerate cannot use a batch size larger than 2 due to out-of-memory issues. • If allowing 4-bit compression, FlexGen can reach 100× higher maximum throughput with effective batch size 144 (4608 tokens generated in total) with latency 4000 seconds by holding all weights in CPU and get- ting rid of disk offloading. We also compare offloading and decentralized collective inference based on FlexGen and Petals (Borzunov et al., 2022) as two representative systems. We conduct compar- isons between the two systems from the aspects of delay and bandwidth of the decentralized network and output se- quence length. The results show that FlexGen outperforms a decentralized Petals cluster in terms of per-GPU throughput and can even achieve lower latency in certain cases. 2. Related Work Given the recent advances of LLMs, LLM inference has become an important workload, encouraging active research from both the system side and the algorithm side. Recent years have witnessed the emergence of systems specialized for LLM inference, such as FasterTrans- former (NVIDIA, 2022), Orca (Yu et al., 2022), Light- Seq (Wang et al., 2021), PaLM inference (Pope et al., 2022), TurboTransformers (Fang et al., 2021), DeepSpeed Inference (Aminabadi et al., 2022), and Hugging Face Accelerate (HuggingFace, 2022). Unfortunately, most of these systems focus on latency-oriented scenarios with high- end accelerators, limiting their deployment for throughput- oriented inference on easily accessible hardware. To enable LLM inference on such commodity hardware, offloading is an essential technique — as far as we know, among current systems, only DeepSpeed Zero-Inference and Hugging Face Accelerate support offloading. These inference systems typically inherit the offloading techniques from training sys- tems (Rajbhandari et al., 2021; Ren", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "a151b978e6086a1ab3a56904b3f6587d22fc819f437af593179a7b765f2fdfd2"}
{"doc_id": "arxiv:2303.06865#abstract:part-5", "url": "https://arxiv.org/abs/2303.06865", "anchor": "#abstract:part-5", "type": "paper", "title": "", "section": "Abstract", "text": "al., 2022), and Hugging Face Accelerate (HuggingFace, 2022). Unfortunately, most of these systems focus on latency-oriented scenarios with high- end accelerators, limiting their deployment for throughput- oriented inference on easily accessible hardware. To enable LLM inference on such commodity hardware, offloading is an essential technique — as far as we know, among current systems, only DeepSpeed Zero-Inference and Hugging Face Accelerate support offloading. These inference systems typically inherit the offloading techniques from training sys- tems (Rajbhandari et al., 2021; Ren et al., 2021; Li et al., 2022; Huang et al., 2020; Wang et al., 2018) but ignore the special computational property of generative inference. They fail to exploit the structure of the throughput-oriented LLM inference computation and miss great opportunities for efficient scheduling of I/O traffic. Another attempt to en- able LLM inference on accessible hardware is collaborative computing proposed by Petals (Borzunov et al., 2022). There are also many algorithm-oriented works that relax cer- tain aspects of computation in LLM inference to accelerate the computation or reduce the memory footprint. Both spar- sification (Hoefler et al., 2021; Frantar & Alistarh, 2023) and quantization (Kwon et al., 2022; Yao et al., 2022; Park et al., 2022; Xiao et al., 2022; Frantar et al., 2022; Dettmers et al., 2022) have been adopted for LLM inference. On the quantization side, prior works have shown weights can be compressed down to 3 bits without compressing activa- tions (Frantar et al., 2022), or both weights and activations can be compressed to 8 bits (Yao et al., 2022; Dettmers et al., 2022; Xiao et al., 2022). In FlexGen, we compress both the weights and KV cache to 4 bits and show how to combine the compression with offloading to make further improvements. Within broader domains, memory optimizations and offload- ing have been studied for training (Huang et al., 2020; Ren et al., 2021; Steiner et al., 2022) and linear algebra (Jia-Wei & Kung, 1981; Demmel, 2013). 3. Background: LLM Inference In this section, we describe the LLM inference workflow and its memory footprint. Generative Inference. A typical LLM generative inference task consists of two stages: i) the prefill stage which takes a prompt sequence to generate the key-value cache (KV cache) for each transformer layer of the LLM; and ii) the decoding stage which utilizes and updates the KV cache to generate tokens step-by-step, where the current token generation depends on previously generated tokens. For a particular inference computation, denote the batch size by b, the input sequence length by s, the output sequence length by n, the hidden dimension of the transformer by h1, the hidden dimension of the second MLP layer by h2, and the total number of transformer layers by l. Given the weight matrices of a transformer layer specified by wi K, wi Q, wi V , wi O, wi 1, wi 2, where wi K, wi Q, wi V , wi O ∈Rh1×h1, w1 ∈Rh1×h2, and w2 ∈Rh2×h1. During the prefill phase, the input of the i-th layer is speci- fied by xi, and key, value, query, and output of the attention", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "ea3684bad418aeea6cbe63e9c7f9b15713961538621e7b87f7d341a9d0ee5d04"}
{"doc_id": "arxiv:2303.06865#abstract:part-6", "url": "https://arxiv.org/abs/2303.06865", "anchor": "#abstract:part-6", "type": "paper", "title": "", "section": "Abstract", "text": "dimension of the second MLP layer by h2, and the total number of transformer layers by l. Given the weight matrices of a transformer layer specified by wi K, wi Q, wi V , wi O, wi 1, wi 2, where wi K, wi Q, wi V , wi O ∈Rh1×h1, w1 ∈Rh1×h2, and w2 ∈Rh2×h1. During the prefill phase, the input of the i-th layer is speci- fied by xi, and key, value, query, and output of the attention layer is specified by xi K, xi V , xi Q, xi Out, where xi, xi K, xi V , xi Q, xi Out ∈Rb×s×h1. Then, the cached key, value can be computed by: xi K = xi · wi K; xi V = xi · wi V The rest of the computation in the i-th layer is: xi Q = xi · wi Q xi Out = fSoftmax \u0012 xi Qxi K T √ h \u0013 · xi V · wi O + xi xi+1 = frelu \u0000xi Out · w1 \u0001 · w2 + xi Out 3 FlexGen: High-Throughput Generative Inference of Large Language Models with a Single GPU During the decode phase, given ti ∈Rb×1×h1 as the em- bedding of the current generated token in the i-th layer, the inference computation needs to i) update the KV cache: xi K ←Concat \u0000xi K, ti · wi K \u0001 xi V ←Concat \u0000xi V , ti · wi V \u0001 and ii) compute the output of the current layer: ti Q = ti · wi Q ti Out = fSoftmax \u0012 ti Qxi K T √ h \u0013 · xi V · wi O + ti ti+1 = frelu \u0000ti Out · w1 \u0001 · w2 + ti Out Memory Analysis. The memory footprint of LLM infer- ence mainly comes from the model weights and the KV cache. Considering the OPT-175B model in FP16, the total number of bytes to store the parameters can be roughly 1 calculated by l(8h2 1 + 4h1h2). The total number of bytes to store the KV cache in peak is 4 × blh1(s + n). In a realistic setting with a sufficient number of GPUs, the OPT-175B model (l = 96, h1 = 12288, h2 = 49152) takes 325 GB. With a batch size of b = 512, an input sequence length s = 512, and an output sequence length of n = 32, the total memory required to store the KV cache is 1.2 TB, which is 3.8× the model weights, making the KV cache a new bottleneck of large-batch high-throughput inference. In FlexGen, for OPT-175B, we enlarge the effective batch size to 256 to achieve the throughput at 0.69 token/s. Throughput and Latency. Considering an effective batch size b, an input sequence length s, and an output sequence length of n, the latency t is defined as the total number of seconds spent to process the prompts and generate all the bn tokens. The generation throughput is defined as bn/t. Token 0 Token 1 Token 2 layer Dataset (infinite) batch Figure 2.", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "7f4c44f1e4f747e3ab020e97c5c7238269af73dfe95cb3d784dff3ca43bbd8c9"}
{"doc_id": "arxiv:2303.06865#abstract:part-7", "url": "https://arxiv.org/abs/2303.06865", "anchor": "#abstract:part-7", "type": "paper", "title": "", "section": "Abstract", "text": "FlexGen, for OPT-175B, we enlarge the effective batch size to 256 to achieve the throughput at 0.69 token/s. Throughput and Latency. Considering an effective batch size b, an input sequence length s, and an output sequence length of n, the latency t is defined as the total number of seconds spent to process the prompts and generate all the bn tokens. The generation throughput is defined as bn/t. Token 0 Token 1 Token 2 layer Dataset (infinite) batch Figure 2. Computational graph of LLM inference. 4. Offloading Strategy In this section, we do not relax any computation of LLM inference and illustrate how to formalize the offloading procedure under the GPU, CPU, and disk memory hierarchy. We first formulate the problem and then construct the search space of the possible offloading strategies in FlexGen. To find an efficient strategy, FlexGen builds an analytical cost model and searches for configurations with an optimizer based on linear programming. 1We ignore the embedding layer(s), which is relatively small. 4.1. Problem Formulation Consider a machine with three devices: a GPU, a CPU, and a disk. The GPU and CPU can perform computation while the disk cannot. The three devices form a three-level mem- ory hierarchy where the GPU has the smallest but fastest memory and the disk has the largest but slowest memory. When an LLM cannot fit entirely within the GPU, we need to offload it to secondary storage and perform computation part-by-part by partially loading the LLM. We formulate the generative inference with offloading as a graph traversal problem. Fig. 2 shows an example computa- tional graph, where the model has 4 layers and we generate 3 tokens per prompt. As our focus is throughput-oriented scenarios, we assume a given dataset with an infinite number of prompts that need to be processed. In the figure, a square means the computation of a GPU batch for a layer. The squares with the same color share the same layer weights. We define a valid path as a path that traverses (i.e., computes) all squares, while subject to the following constraints: • A square can only be computed if all squares to its left on the same row were computed. • To compute a square on a device, all its inputs (weights, activations, cache) must be loaded to the same device. • After being computed, a square produces two outputs: activations and KV cache. The activations should be stored until its right sibling is computed. The KV cache should be stored until the rightmost square on the same row is computed. • At any time, the total size of tensors stored on a device cannot exceed its memory capacity. The goal is to find a valid path that minimizes the total execution time, which includes the compute cost and I/O cost when moving tensors between devices. 4.2. Search Space Given the formulation above, we construct a search space for possible valid strategies in FlexGen. Compute schedule. Intuitively, there are two orders to traverse the graph in Fig. 2: row-by-row and column-by- column. All existing systems (Aminabadi et", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "e79c94b456b217ad746cc32e55ed7ec41e00c96fd6f91efba1b4af22232badd4"}
{"doc_id": "arxiv:2303.06865#abstract:part-8", "url": "https://arxiv.org/abs/2303.06865", "anchor": "#abstract:part-8", "type": "paper", "title": "", "section": "Abstract", "text": "size of tensors stored on a device cannot exceed its memory capacity. The goal is to find a valid path that minimizes the total execution time, which includes the compute cost and I/O cost when moving tensors between devices. 4.2. Search Space Given the formulation above, we construct a search space for possible valid strategies in FlexGen. Compute schedule. Intuitively, there are two orders to traverse the graph in Fig. 2: row-by-row and column-by- column. All existing systems (Aminabadi et al., 2022; Hug- gingFace, 2022) traverse the graph row-by-row, as shown in Fig. 3(a). This is reasonable because it is the fastest way to finish the generation for one batch and the KV cache can be freed immediately after a row. However, because every two contiguous squares do not share weights, this schedule has to repeatedly load the weights and incurs huge I/O costs. To reduce the I/O costs of the weights, we can traverse the graph column-by-column. All squares in a column share weights, so we can let the weights stay on GPU for reusing and only load/unload the activations and KV cache. How- 4 FlexGen: High-Throughput Generative Inference of Large Language Models with a Single GPU Token 0 Token 1 Token 2 layer Dataset (infinite) batch (a) Row-by-row schedule Dataset (infinite) batch block (b) Zig-zag block schedule Figure 3. Two different schedules. The red arrows denote the com- putation order. Algorithm 1 Block Schedule with Overlapping for i = 1 to generation length do for j = 1 to num layers do // Compute a block with multiple GPU batches for k = 1 to num GPU batches do // Load the weight of the next layer load weight(i, j + 1, k) // Store the cache and activation of the prev batch store activation(i, j, k −1) store cache(i, j, k −1) // Load the cache and activation of the next batch load cache(i, j, k + 1) load activation(i, j, k + 1) // Compute this batch compute(i, j, k) // Synchronize all devices synchronize() end for end for end for ever, we cannot traverse a column all the way to the end because the activations and KV cache still need to be stored. Hence, we have to stop when they fill the CPU and disk memory. Taking all this into consideration, we converge to a zig-zag block schedule, as shown in Fig. 3(b). Besides, we propose another more advanced and I/O-optimal sched- ule, but only implement the simpler block schedule due to the practical implementation difficulty of the optimal one. However, we prove that the block schedule is at most twice worse than the optimal schedule in Appendix A.2. Theorem 4.1. The I/O complexity of the zig-zag block schedule is within 2× of the optimal solution. Another typical optimization is overlapping. We can overlap the weights load of the next layer, cache/activation load of the next batch, cache/activation store of the previous batch, and the computation of the current batch. Adding overlap- ping to the block schedule results in Algorithm 1. The first six functions in the", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "cf1a988a94085d8d55a1a254a5646dcff222488db03683c98b8b74171939f474"}
{"doc_id": "arxiv:2303.06865#abstract:part-9", "url": "https://arxiv.org/abs/2303.06865", "anchor": "#abstract:part-9", "type": "paper", "title": "", "section": "Abstract", "text": "is at most twice worse than the optimal schedule in Appendix A.2. Theorem 4.1. The I/O complexity of the zig-zag block schedule is within 2× of the optimal solution. Another typical optimization is overlapping. We can overlap the weights load of the next layer, cache/activation load of the next batch, cache/activation store of the previous batch, and the computation of the current batch. Adding overlap- ping to the block schedule results in Algorithm 1. The first six functions in the innermost loop can be seen as launched in parallel with six logical threads because there are no de- pendencies. The last function then synchronizes these six logical threads. We rely on operating systems and CUDA drivers to resolve the schedule of the underlying hardware resources. As a conclusion, the algorithm introduces two parameters into our search space: the GPU batch size and the number of GPU batches in a block. The product of the GPU batch size and the number of GPU batches is called block size (or effective batch size). Tensor placement. Besides compute schedule, a strategy should specify how to store these tensors within the memory hierarchy. We use three variables wg, wc, and wd to define the percentages of weights stored on GPU, CPU, and disk respectively. Similarly, we use three variables hg, hc, hd to define the percentages of activations and use cg, cc, cd for the KV cache. Given the percentages, there are still multiple ways to partition the tensors. Taking weight tensors as an example, from coarse grain to fine grain, we can partition the weights at the model granularity (e.g., assign 50% of the layers in a model to the GPU), at the layer granularity (e.g., assign 50% of the tensors in a layer to the GPU), or at the tensor granularity (e.g., assign 50% of the elements in a tensor to the GPU). Coarser granularity leads to lower runtime overhead but it is less flexible and its cost is difficult to analyze. Considering both the runtime overhead and desired flexibility, we use layer granularity for weights, and tensor granularity for activations and the KV cache. Computation delegation. While CPUs are much slower than GPUs, we find using CPU compute can still be ben- eficial in some cases. This is because the computation of attention scores during decoding is I/O-bounded. Consider a case where the KV cache is stored on the CPU. Computing the attention scores on the GPU requires moving the entire KV cache to the GPU, which incurs a substantial I/O cost as the KV cache is huge. In contrast, computing the attention score on the CPU does not require moving the KV cache. It only requires moving the activations from the GPU to the CPU. Quantitatively, let b be the GPU batch size, s be the sequence length, and h1 be the hidden size. The size of the moved KV cache is b × s × h1 × 4 bytes, and the size of the moved activation is b×h1 ×4 bytes, so computing attention score on CPU reduces I/O by s×. For", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "5ffbad7b4aa572ee3568d8ea4a15b907baa9a299d3028dc97fa16f04aeee68e5"}
{"doc_id": "arxiv:2303.06865#abstract:part-10", "url": "https://arxiv.org/abs/2303.06865", "anchor": "#abstract:part-10", "type": "paper", "title": "", "section": "Abstract", "text": "score on the CPU does not require moving the KV cache. It only requires moving the activations from the GPU to the CPU. Quantitatively, let b be the GPU batch size, s be the sequence length, and h1 be the hidden size. The size of the moved KV cache is b × s × h1 × 4 bytes, and the size of the moved activation is b×h1 ×4 bytes, so computing attention score on CPU reduces I/O by s×. For long sequences (e.g., s ≥512), it is better to compute the attention scores on the CPU if the associated KV cache is not stored on the GPU. 4.3. Cost Model and Policy Search The schedule and placement in Section 4.2 constructs a search space with several parameters. Now we develop an analytical cost model to estimate the execution time given these algorithm parameters and hardware specifications. Cost Model. The cost model predicts the latency during prefill for one layer denoted as Tpre, and the averaged la- 5 FlexGen: High-Throughput Generative Inference of Large Language Models with a Single GPU tency during decoding for one layer denoted as Tgen in one block. The total latency for computing a block can then be estimated as T = Tpre · l + Tgen · (n −1) · l, where l is the number of layers and n is the number of tokens to generate. Assuming perfect overlapping, Tpre can be estimated as Tpre = max(ctogp, gtocp, dtocp, ctodp, compp), where ctogp, gtocp, dtocp, ctodp, compp denote the latency of read from CPU to GPU, write from GPU to CPU, read from disk to CPU, write from CPU to disk, computation, respectively, during prefill for one layer. Similarly, Tgen can be estimated as Tgen = max(ctogg, gtocg, dtocg, ctodg, compg), with ctogg, gtocg, dtocg, ctodg, compg denoting the latency of read from CPU to GPU, write from GPU to CPU, read from disk to CPU, write from CPU to disk, computation, respectively, during decoding for one layer. For I/O terms like dtocg, it is estimated by summing up the I/O events, which contain weights, activations, and cache reads. The size of FP16 weights for one transformer layer is 8h2 1 + 4h1 · h2 bytes, with h1 denoting the hidden size, and h2 denoting the hidden size of the second MLP layer. Let bls be the block size and s be the prompt length; then the size of activations for one layer is 2·bls·h1. The size of the KV cache for one layer on average is 4·bls·(s+ n 2 )·h1. We have to load wd, hd, cd percent of weights, activations, and the KV cache from the disk respectively so that the total latency of disk read is dtocg = 1 disk to cpu bandwidth((8h2 1 + 4h1 · h2) · wd + 4 · bls · (s + n 2 ) · h1 · cd + 2 · bls · h1 · hd). Similarly for computation terms, we sum up all computation events, including matrix multiplications and batched matrix multiplications on the CPU and the", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "be8116db0718a997d30fb1250a4864bfbd21f46c5d444c75eb52843d5eecd469"}
{"doc_id": "arxiv:2303.06865#abstract:part-11", "url": "https://arxiv.org/abs/2303.06865", "anchor": "#abstract:part-11", "type": "paper", "title": "", "section": "Abstract", "text": "cd percent of weights, activations, and the KV cache from the disk respectively so that the total latency of disk read is dtocg = 1 disk to cpu bandwidth((8h2 1 + 4h1 · h2) · wd + 4 · bls · (s + n 2 ) · h1 · cd + 2 · bls · h1 · hd). Similarly for computation terms, we sum up all computation events, including matrix multiplications and batched matrix multiplications on the CPU and the GPU. Besides latency estimation, we also estimate the peak mem- ory usage of the GPU, CPU, and disk, and then we add memory constraints. The full cost model is in Appendix A.3. Policy Search. A policy includes 11 variables: block size bls, GPU batch size gbs, weight placement wg, wc, wd, activation placement hg, hc, hd, and KV cache placement cg, cc, cd. In practice, the percentage cannot be an arbitrary real number between 0 and 1, because the tensor cannot be split arbitrarily. However, we relax the percentage vari- ables in the cost model to be any real number between 0 and 1 since it is changing gradually. We solve the problem as a two-level optimization problem. We first enumerate a few choices of (bls, gbs) tuple. Typically, gbs is a multi- ple of 4, and bls is less than 20 so there are not too many choices. Then with the fixed bls, gbs, finding the best place- ment p = (wg, wc, wd, cg, cc, cd, hg, hc, hd) becomes a linear programming problem shown in Eq. (1). The linear programming problem can be solved very quickly because there are only 9 variables. This formulation can also be flexibly extended to include latency constraints and model approximate methods such as compression. min p T/bls s.t. gpu peak memory < gpu mem capacity cpu peak memory < cpu mem capacity disk peak memory < disk mem capacity wg + wc + wd = 1 cg + cc + cd = 1 hg + hc + hd = 1 (1) To use the cost model, we run profiling on the hardware to sample some data points and fit the hardware parameters. We then call the optimizer to get an offloading policy. Due to our relaxation and the hardness of accurately modeling peak memory usage (e.g., fragmentation), sometimes a strategy from the policy search can run out of memory. In this case, we manually adjust the policy slightly. The cost model can usually return a good policy, but it is common that a better policy can be obtained by tuning manually. 4.4. Extension to Multiple GPUs We discuss how to extend the offloading strategy in FlexGen if there are multiple GPUs. Although we can find a nearly optimal strategy for one GPU, the strategy is still heavily limited by I/O and has a low GPU utilization. If we are given more GPUs and more CPUs, model parallelism can be utilized to reduce the memory pressure of each GPU, which can potentially lead to a super-linear scaling in decoding. There are two kinds of", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "484613238e101546e86ac86c48119ce5df95f497758f979fdc208f4aa31bef17"}
{"doc_id": "arxiv:2303.06865#abstract:part-12", "url": "https://arxiv.org/abs/2303.06865", "anchor": "#abstract:part-12", "type": "paper", "title": "", "section": "Abstract", "text": "to Multiple GPUs We discuss how to extend the offloading strategy in FlexGen if there are multiple GPUs. Although we can find a nearly optimal strategy for one GPU, the strategy is still heavily limited by I/O and has a low GPU utilization. If we are given more GPUs and more CPUs, model parallelism can be utilized to reduce the memory pressure of each GPU, which can potentially lead to a super-linear scaling in decoding. There are two kinds of model parallelisms: tensor and pipeline parallelism (Narayanan et al., 2021; Zheng et al., 2022). Tensor parallelism can reduce the single-query la- tency but pipeline parallelism can achieve good scaling on throughput due to its low communication costs. Since we target throughput, FlexGen implements pipeline parallelism. We use pipeline parallelism by equally partitioning an l- layer LLM on m GPUs, and then the execution of all GPUs follows the same pattern. The problem is reduced to run- ning an n/m-layer transformer on one GPU. We can di- rectly reuse the policy search developed for one GPU. To achieve micro-batch pipelining, a new for-loop is added to Algorithm 1 to combine the iteration-level pipeline parallel execution schedule (Huang et al., 2019; Yu et al., 2022) with our single-device offloading runtime. 5. Approximate Methods The previous section focuses on the exact computation. However, the inference throughput can be greatly boosted with negligible accuracy loss by allowing some approxima- tions, because LLMs are typically robust to careful approxi- mations. This section introduces two such approximations: group-wise quantization and sparse attention. 6 FlexGen: High-Throughput Generative Inference of Large Language Models with a Single GPU Group-wise Quantization. We show that both the weights and KV cache can be directly quantized into 4-bit integers without any retraining or calibration on OPT-175B, all while preserving similar accuracy (Section 6.2). When compared to some related works (Yao et al., 2022; Dettmers et al., 2022; Xiao et al., 2022) that try to use integer matrix mul- tiplication mainly for accelerated computation, the goal of quantization in our case is primarily for compression and reducing I/O costs. Therefore, we can choose a fine-grained quantization format in favor of a high compression ratio and dequantize the tensors back to FP16 before computation. We use a fine-grained group-wise asymmetric quantization method (Shen et al., 2020). Given a tensor, we choose g contiguous elements along a certain dimension as a group. For each group, we compute the min and max of the group elements and quantize each element x into b-bit integers by xquant = round \u0010 x−min max−min × (2b −1) \u0011 . The tensors are stored in the quantized format and converted back to FP16 before computation. Since both the weights and KV cache consume a significant amount of memory, we compress both to 4 bits with a group size of 64. There are multiple ways to choose which dimension to group on. We find that grouping the weights along the output channel di- mension and the KV cache along the hidden dimension pre- serves the accuracy while being runtime-efficient in practice. One thing", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "08d52460335801a1960f30160ef913b0daae039b16e1761d967f9120247f2547"}
{"doc_id": "arxiv:2303.06865#abstract:part-13", "url": "https://arxiv.org/abs/2303.06865", "anchor": "#abstract:part-13", "type": "paper", "title": "", "section": "Abstract", "text": "are stored in the quantized format and converted back to FP16 before computation. Since both the weights and KV cache consume a significant amount of memory, we compress both to 4 bits with a group size of 64. There are multiple ways to choose which dimension to group on. We find that grouping the weights along the output channel di- mension and the KV cache along the hidden dimension pre- serves the accuracy while being runtime-efficient in practice. One thing to mention is that such a fine-grained group-wise quantization in FlexGen causes some overhead in compres- sion and decompression. Such an overhead could be very significant if run on a CPU which makes the CPU delegation useless, so we turn off the CPU delegation when enabling quantization. A concurrent work (Dettmers & Zettlemoyer, 2022) also finds that 4-bit precision is almost optimal for total model bits and zero-shot accuracy on OPT models. Compared to this previous work, we first propose to com- press the KV cache and present the results on OPT-175B. Sparse Attention. We demonstrate that the sparsity of self-attention can be exploited by only loading the top 10% attention value cache on OPT-175B, all while maintaining the model quality. We present one simple Top-K sparse approximation. After computing the attention matrices, for each query, we calculate the indices of its Top-K tokens from the K cache. We then simply drop the other tokens and only load a subset of the V cache according to the indices. The application of these approximations is straightforward. We present these preliminary but interesting results and intend to emphasize that FlexGen is a general framework that can seamlessly plug in many approximation methods. 6. Evaluation Hardware. We run experiments on the NVIDIA T4 GPU in- stances from Google Cloud. The hardware specifications are Table 1. Hardware Specs Device", "source": "arxiv_pdf", "published": "", "tokens": 305, "sha256": "00bb44c60b10a05c6a5640f94e9df080f3529c1ce5cbef36c4e5fc47204acd1b"}
{"doc_id": "arxiv:2303.06865#model:part-1", "url": "https://arxiv.org/abs/2303.06865", "anchor": "#model:part-1", "type": "paper", "title": "", "section": "Model", "text": "Memory GPU NVIDIA T4 16 GB CPU Intel Xeon @ 2.00GHz 208 GB Disk Cloud default SSD (NVMe) 1.5 TB listed in Table 1. The read bandwidth of SSD is about 2GB/s and the write bandwidth is about 1GB/s. Our methods and implementations do not depend on specific hardware archi- tectures. Some architecture (e.g. unified memory) could be more friendly to our method. See Appendix A.4 for discussions and experiments on different hardware setups. Model. OPT models (Zhang et al., 2022) with 6.7B to 175B parameters are used in the evaluation. Although we do not evaluate other models, the offloading in FlexGen can be ap- plied to other transformer LLMs, e.g., GPT-3 (Brown et al., 2020), PaLM (Chowdhery et al., 2022), and BLOOM (Scao et al., 2022) because they all share a similar structure. Workload. Our focus is high-throughput generation on a given dataset. We use synthetic datasets where all prompts are padded to the same length. The system is required to generate 32 tokens for each prompt. We test two prompt lengths: 512 and 1024 (for experiments in more settings, see Appendix A.4). The evaluation metric is generation throughput, defined as the number of generated tokens / (prefill time + decoding time). Sometimes running a full batch takes too long for certain systems — in this cases, we generate fewer tokens and project the final throughput. We use dummy model weights in throughput benchmarks for all systems and real weights for accuracy evaluations. Baseline. We use DeepSpeed ZeRO-Inference (Aminabadi et al., 2022) and Hugging Face Accelerate (HuggingFace, 2022) as baselines. They are the only systems that can run LLMs with offloading when there is not enough GPU mem- ory. DeepSpeed supports offloading the whole weights to the CPU or disk. It uses ZeRO data parallelism if there are multiple GPUs. Accelerate supports offloading a fraction of the weights. It does not support distributed GPUs on differ- ent machines. Both of them use the row-by-row schedule and can only put cache/activations on GPU. These systems support different quantization methods. However, the quan- tization in Accelerate is not compatible with offloading, and the quantization in DeepSpeed cannot preserve accuracy up to 175B, so we do not enable quantization on these systems. In addition to offloading, decentralized collaborative infer- ence is another option to lower the resource requirement for LLM inference. Thus, we also include Petals (Borzunov et al., 2022; Ryabinin et al., 2023) as an additional baseline. Implementation. FlexGen is implemented on top of PyTorch (Paszke et al., 2019). FlexGen manages multi- ple CUDA streams and CPU threads to overlap I/O with compute. FlexGen creates files for tensors stored on the disk and maps them as virtual memory to access them. 7 FlexGen: High-Throughput Generative Inference of Large Language Models with a Single GPU 6.1. Offloading Maximum throughput benchmark. We first evaluate the maximum generation throughput the systems can achieve with one GPU on two prompt lengths. As shown in Table 2, FlexGen outperforms all baselines in all cases. On OPT- 6.7B, Accelerate and FlexGen can successfully fit the whole model into a", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "22247e0637623450b85291d4a69e24ec5fed1b8dd1607db9864000901207d86a"}
{"doc_id": "arxiv:2303.06865#model:part-2", "url": "https://arxiv.org/abs/2303.06865", "anchor": "#model:part-2", "type": "paper", "title": "", "section": "Model", "text": "FlexGen creates files for tensors stored on the disk and maps them as virtual memory to access them. 7 FlexGen: High-Throughput Generative Inference of Large Language Models with a Single GPU 6.1. Offloading Maximum throughput benchmark. We first evaluate the maximum generation throughput the systems can achieve with one GPU on two prompt lengths. As shown in Table 2, FlexGen outperforms all baselines in all cases. On OPT- 6.7B, Accelerate and FlexGen can successfully fit the whole model into a single GPU, so they choose to only use the GPU. DeepSpeed has a higher memory overhead and cannot fit OPT-6.7B into the GPU, so it uses slower CPU offload- ing. On OPT-30B, all systems switch to CPU offloading. DeepSpeed and Accelerate store the KV cache on the GPU, so they cannot use a very large batch size, while FlexGen offloads most weights and all KV cache to the CPU and en- ables a larger GPU batch size. In addition, FlexGen reuses the weights by block scheduling. On OPT-175B, all systems start to offload the weights to the disk. Baseline systems can only use a maximum batch size of 2, but FlexGen can use a GPU batch size of 32 and a block size of 32 × 8, achiev- ing a 69× higher throughput. With compression enabled, FlexGen achieves a 112× higher generation throughput on a single GPU for prompt sequence length 512. This huge improvement is because FlexGen uses an effective batch size of 144 and compresses the weights and KV cache to fit into CPU memory to avoid slow disk swapping. More details on the policy setups and effective batch sizes can be found in Appendix A.4. More experiments on how disk specification affects the throughput see Appendix A.4. Table 3 shows the results on 4 machines, with one GPU on each machine. OPT-30B or OPT-175B still cannot fit into 4 GPUs. Naively, we can run 4 independent FlexGen in a data-parallel fashion to get a linear scaling on through- put. But here we show that pipeline parallelism can achieve super-linear scaling on decoding throughput. With pipeline parallelism, the memory pressure of each machine is re- duced so we can switch from small batch sizes to larger batch sizes, or switch from disk offloading to CPU-only offloading. In Table 3, FlexGen does not achieve linear scaling on generation throughput (which counts both prefill and decoding time costs). This is because there are pipeline bubbles during the prefill stage and our workload settings only generate 32 tokens. However, FlexGen achieves super- linear scaling on decoding throughput (which only counts decoding time costs assuming the prefill is done). This means if we generate more tokens, pipeline parallelism will show its benefits as decoding time will dominate. Latency-throughput trade-off. We configure these sys- tems to achieve maximum throughput under various la- tency constraints and draw their latency-throughput trade- off curves in Fig. 1. FlexGen sets a new Pareto-optimal frontier that significantly outperforms baselines. On the low-latency side, FlexGen supports partial offloading and uses more space for weights. On the high-throughput side, Table 2. Generation", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "659b55adcb2dd64df9d4399c4f29d018ca1ad3d7da9ed9d3235d05736e7d54aa"}
{"doc_id": "arxiv:2303.06865#model:part-3", "url": "https://arxiv.org/abs/2303.06865", "anchor": "#model:part-3", "type": "paper", "title": "", "section": "Model", "text": "assuming the prefill is done). This means if we generate more tokens, pipeline parallelism will show its benefits as decoding time will dominate. Latency-throughput trade-off. We configure these sys- tems to achieve maximum throughput under various la- tency constraints and draw their latency-throughput trade- off curves in Fig. 1. FlexGen sets a new Pareto-optimal frontier that significantly outperforms baselines. On the low-latency side, FlexGen supports partial offloading and uses more space for weights. On the high-throughput side, Table 2. Generation throughput (token/s) of different systems. Ac- celerate, DeepSpeed, and FlexGen use 1 GPU. Petals uses 1 GPU for OPT-6.7B, 4 GPUs for OPT-30B, and 24 GPUs for OPT-175B, but reports per-GPU throughput. We benchmark Petals under a good network assumption with a delay of less than 10ms and band- width of 1 Gbps. The models are run in INT8 as the default for Petals. See Section 6.3 for more details about Petals. FlexGen is our system without compression; FlexGen (c) uses 4-bit compres- sion. “OOM” means out-of-memory. Seq. length 512 1024 Model size 6.7B 30B 175B 6.7B 30B 175B Accelerate 25.12 0.62 0.01 13.01 0.31 0.01 DeepSpeed 9.28 0.60 0.01 4.59 0.29 OOM Petals 8.25 2.84 0.08 6.56 1.51 0.06 FlexGen 25.26 7.32 0.69 13.72 3.50 0.35 FlexGen (c) 29.12 8.70 1.12 13.18 3.98 0.42 Table 3. The scaling performance on 4 GPUs. The prompt se- quence length is 512. The number of GPUs is denoted in the parenthesis. Generation throughput (token/s) counts the time cost of both prefill and decoding while decoding throughput only counts the time cost of decoding assuming prefill is done. Metric Generation Throughput Decoding Throughput Model size 6.7B 30B 175B 6.7B 30B 175B FlexGen (1) 25.26 7.32 0.69 38.28 11.52 0.83 FlexGen (4) 201.12 23.61 2.33 764.65 48.94 3.86 DeepSpeed (4) 50.00 6.40 0.05 50.20 6.40 0.05 FlexGen aggressively offloads all things out of the GPU to achieve a large GPU batch size and block size. Given the same latency requirement of 5000 seconds, FlexGen without compression can achieve a 40× higher through- put compared to DeepSpeed and Accelerate. If allowing a higher latency and compression, FlexGen can further boost throughput and reach a 100× improvement by using an ef- fective batch size of 144. In this case, compression enables FlexGen to fit all things in the CPU memory and avoid disk I/O. The detailed latency, throughput, and policy setup can be found in Appendix A.4. Runtime breakdown. We shows the runtime breakdown of OPT-175B on FlexGen in Table 8 in Appendix A.4. We disable overlapping and profile the time used for major components. The GPU compute utilization is 82% and 13% for prefill and decoding, respectively. Ablation study. We then isolate the improvement brought by each individual technique. Table 4 lists the throughput FlexGen can achieve if disabling one technique at a time. On OPT-30B, with all optimizations enabled, we put 20% weights on GPU, 80% weights on CPU, and all activations and KV cache to CPU. We also choose a GPU batch size of 48 and a block size of 48 × 3. “No policy", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "0ed26964fe2e4ff4951ca5a409763a89acbe33bab11f32f7690f56f261c7888d"}
{"doc_id": "arxiv:2303.06865#model:part-4", "url": "https://arxiv.org/abs/2303.06865", "anchor": "#model:part-4", "type": "paper", "title": "", "section": "Model", "text": "utilization is 82% and 13% for prefill and decoding, respectively. Ablation study. We then isolate the improvement brought by each individual technique. Table 4 lists the throughput FlexGen can achieve if disabling one technique at a time. On OPT-30B, with all optimizations enabled, we put 20% weights on GPU, 80% weights on CPU, and all activations and KV cache to CPU. We also choose a GPU batch size of 48 and a block size of 48 × 3. “No policy search” illustrates the performance of worse strategies, showing the importance of a good policy. On both models, using CPU compute and overlapping brings non-trivial improvement. We also 8 FlexGen: High-Throughput Generative Inference of Large Language Models with a Single GPU Table 4. Ablation study of proposed techniques. The numbers are generation throughput on 1 GPU with prompt length 512. The gray tuple denotes a policy (GPU batch size × #GPU-batch, wg, wc). More see Appendix A.4. Model size 30B 175B All optimizations 7.32 (48×3, 20, 80) 0.69 (32×8, 0, 50) No policy search 7.26 (48×3, 0, 100) 0.27 (32×1, 0, 50) No overlapping 5.86 0.59 No CPU compute 4.03 0.62 No disk 7.32 OOM w/ DeepSpeed policy 1.57 0.01 Table 5. The accuracy (higher is better) and perplexity (lower is better) with approximate methods. Dataset Lambada (acc) WikiText (ppl) Config FP16 4-bit 4-bit-S FP16 4-bit 4-bit-S OPT-30B 0.725 0.724 0.718 12.72 12.90 12.90 OPT-175B 0.758 0.756 0.756 10.82 10.94 10.94 port the policy used in DeepSpeed/Accelerate into FlexGen runtime, showing the suboptimality of their policy. A more detailed ablation study can be found in Appendix A.4. HELM and Data wrangling. We tested the interaction of FlexGen and HELM (Liang et al., 2022) by evaluating a new model OPT-IML-30B (Iyer et al., 2022), which has not been included in the official release of HELM. FlexGen finishes the benchmark of 7 representative sub-scenarios in 21 hours , with all system overhead included, under the hard- ware setup described in Table 1. Table 9 in Appendix A.4 shows the details of the tasks and the corresponding run- ning time. We also use FlexGen to run the data wrangling tasks (Narayan et al., 2022) with OPT models. The detailed task configurations and running time are in Appendix A.4. 6.2. Approximations We use two tasks to show that our approximation methods exhibit negligible accuracy loss: next-word prediction on Lambada (Paperno et al., 2016) and language modeling on WikiText (Merity et al., 2016). As shown in Table 5, “4- bit” means using group-wise quantization to compress both weights and KV cache into 4-bit integers. “4-bit-S” means combining the quantization and sparse attention with a 10% sparsity on the value cache. Both methods show negligible accuracy loss compared to FP16. The results reveal the robustness of LLMs against these approximations. We also tried 3-bit compression but it cannot preserve accuracy. 6.3. Offloading vs. Collaborative Inference We compare FlexGen and Petals under different network conditions by setting a private Petals cluster on GCP with 4 nodes having one T4 GPU per node. We use Linux traffic control to constrain the", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "f5f1fef591e7f0e04c74c8d82eb88cf42e028f70d680e8591a51d156dcae4f37"}
{"doc_id": "arxiv:2303.06865#model:part-5", "url": "https://arxiv.org/abs/2303.06865", "anchor": "#model:part-5", "type": "paper", "title": "", "section": "Model", "text": "quantization and sparse attention with a 10% sparsity on the value cache. Both methods show negligible accuracy loss compared to FP16. The results reveal the robustness of LLMs against these approximations. We also tried 3-bit compression but it cannot preserve accuracy. 6.3. Offloading vs. Collaborative Inference We compare FlexGen and Petals under different network conditions by setting a private Petals cluster on GCP with 4 nodes having one T4 GPU per node. We use Linux traffic control to constrain the connections between instances to simulate a realistic decentralized network and benchmark the performance of an OPT-30B model (input sequence length: 512, output sequence length: 32). We tune the batch size of each request to be 2 and issue requests by 6 paral- lel client processes to achieve the maximum throughput2. In addition, we normalize the throughput of Petals by the number of used GPUs. As shown in Fig. 4, we find that the throughput of FlexGen with a single T4 outperforms the per-GPU throughput of the Petals cluster under all tested network conditions. Petals does not utilize offloading, so it cannot use a very large batch size, which limits its scaling on throughput. Thus, we believe offloading could be a more efficient solution for throughput than communicating a large volume of activations in a long decentralized pipeline; on the other hand, collaborative inference can be a more viable option in more latency-sensitive scenarios. Interestingly, we find that FlexGen can achieve lower latency than Petals in slow networks with short generation. We speculate this is because the network bandwidth becomes the bottleneck for activation transfer, and a large delay incurs a significant overhead on each communication step in the pipeline. For the curve of a 100ms delay network, we can observe a cross point between FlexGen and Petals. This is because the activations during prefill are larger than the activations during decoding by a factor of the input sequence length. Thus, the communication overhead is proportionally larger, which significantly slows down Petals during prefill. 0 5 10 15 20 25 30 Output sequence length 0 20 40 60 80 100 120 140 160 Full generation latency (s) FlexGen 1xT4 Petals 4xT4 10ms 1Gbps Petals 4xT4 10ms 0.1Gbps Petals 4xT4 100ms 0.1Gbps 0 5 10 15 20 25 30 Output sequence length 0 1 2 3 4 5 6 7 Throughput per GPU (token/s) Figure 4. Full latency and per-GPU throughput of FlexGen and Petals in different network delay and bandwidth. 7. Conclusion We introduce FlexGen, a high-throughput generation engine for LLM inference, which focuses on latency-insensitive batch-processing tasks for resource-constrained scenarios. Acknowledgements We would like to thank Clark Barrett and Joseph E. Gon- zalez for funding support, and Zhiqiang Xie, Daniel Y. Fu, Hao Zhang, Nick Chow, Benjamin Spector, Guangxuan Xiao, Jue Wang, Arjun Desai, Yao Fu, Anjiang Wei, and Zihao Ye for their insightful review and discussions. 2The batch size of 1 did not result in a noticeably better latency. 9 FlexGen: High-Throughput Generative Inference of Large Language Models with a Single GPU", "source": "arxiv_pdf", "published": "", "tokens": 503, "sha256": "d757f99a2eb03885e8d2de7b2d16c712ada4df7654227ca7b9b054138118eeb6"}
{"doc_id": "arxiv:2305.09781#abstract", "url": "https://arxiv.org/abs/2305.09781", "anchor": "#abstract", "type": "paper", "title": "", "section": "Abstract", "text": "This paper introduces SpecInfer, a system that accelerates generative large language model (LLM) serving with tree- based speculative inference and verification. The key idea behind SpecInfer is leveraging small speculative models to predict the LLM’s outputs; the predictions are organized as a token tree, whose nodes each represent a candidate token sequence. The correctness of all candidate token sequences represented by a token tree is verified against the LLM in par- allel using a novel tree-based parallel decoding mechanism. SpecInfer uses an LLM as a token tree verifier instead of an incremental decoder, which significantly reduces the end-to- end latency and computational requirement for serving gen- erative LLMs while provably preserving model quality. Our ∗Equal contribution. Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third- party components of this work must be honored. For all other uses, contact the owner/author(s). ASPLOS ’24, April 27-May 1, 2024, La Jolla, CA, USA © 2024 Copyright held by the owner/author(s). ACM ISBN 979-8-4007-0386-7/24/04. https://doi.org/10.1145/3620666.3651335 evaluation shows that SpecInfer outperforms existing LLM serving systems by 1.5-2.8× for distributed LLM inference and by 2.6-3.5× for offloading-based LLM inference, while preserving the same generative performance. SpecInfer is publicly available at https://github.com/flexflow/FlexFlow/ Keywords: large language model serving, speculative decod- ing, token tree verification ACM Reference Format: Xupeng Miao, Gabriele Oliaro, Zhihao Zhang, Xinhao Cheng, Zeyu Wang, Zhengxin Zhang, Rae Ying Yee Wong, Alan Zhu, Lijie Yang, Xiaoxiang Shi, Chunan Shi, Zhuoming Chen, Daiyaan Arfeen, Reyna Abhyankar, and Zhihao Jia. 2024. SpecInfer: Accelerating Large Language Model Serving with Tree-based Speculative Inference and Verification. In 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 3 (ASPLOS ’24), April 27-May 1, 2024, La Jolla, CA, USA. ACM, New York, NY, USA, 18 pages. https://doi.org/10.1145/3620666.3651335 1", "source": "arxiv_pdf", "published": "", "tokens": 334, "sha256": "21d84f19cce6222684879a9231511954cc178db68939c4f061eafc163dda762a"}
{"doc_id": "arxiv:2305.09781#introduction:part-1", "url": "https://arxiv.org/abs/2305.09781", "anchor": "#introduction:part-1", "type": "paper", "title": "", "section": "Introduction", "text": "Generative large language models (LLMs), such as Chat- GPT [3] and GPT-4 [33], have proven to be powerful in various application domains, including question answering, program synthesis, and task automation [26, 56]. However, it is challenging to quickly and cheaply serve these LLMs arXiv:2305.09781v4 [cs.CL] 1 Apr 2024 due to their large volume of parameters, complex architec- tures, and high computational requirements. For example, the largest GPT-3 architecture has 175 billion parameters, which requires more than eight NVIDIA 40GB A100 GPUs to store in half-precision floating points, and takes several seconds to serve a single inference request [3]. An LLM generally takes as input a sequence of tokens, called prompt, and generates subsequent tokens one at a time, as shown in Figure 1a. The generation of each token in the sequence is conditioned on the input prompt and previously generated tokens and does not consider future tokens. This approach is also called autoregressive decoding because each generated token is also used as input for generating future tokens. This dependency between tokens is crucial for many NLP tasks that require preserving the order and context of the generated tokens, such as text completion [55]. Existing LLM systems generally use an incremental decod- ing approach to serving a request where the system computes the activations for all prompt tokens in a single step and then iteratively decodes one new token using the input prompt and all previously generated tokens [27]. This approach re- spects data dependencies between tokens, but achieves sub- optimal runtime performance and limited GPU utilization, since the degree of parallelism within each request is greatly limited in the incremental phase. In addition, the attention mechanism of Transformer [48] requires accessing the keys and values of all previous tokens to compute the attention output of a new token. To avoid recomputing the keys and values for all preceding tokens, today’s LLM systems use a caching mechanism to store their keys and values for reuse in future iterations. For long-sequence generative tasks (e.g., GPT-4 supports up to 32K tokens in a request), caching keys and values introduces significant memory overhead, which prevents existing systems from serving a large number of re- quests in parallel due to the memory requirement of caching their keys and values. Motivated by the idea of speculative execution in proces- sor optimizations [13, 42], recent work introduces sequence- based speculative inference, which leverages a small specula- tive model (SSM) to generate a sequence of tokens and uses an LLM to examine their correctness in parallel [5, 22, 25, 44, 51]. These attempts only consider a token sequence generated by a single SSM for speculation, which cannot align well with an LLM due to the model capacity gap between them, since SSMs are generally orders of magnitude smaller than the LLM to maintain low memory and runtime overheads. This paper introduces SpecInfer, a system that improves the end-to-end latency and computational efficiency of LLM serving with tree-based speculative inference and verifica- tion. Figure 1b illustrates a comparison between existing incremental decoding, sequence-based speculative inference, and our tree-based speculative inference. A", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "9edf916ceb3f46abfe2b06baf8c1304a84b66ebf4f72d65c22ed8e5c58f600e9"}
{"doc_id": "arxiv:2305.09781#introduction:part-2", "url": "https://arxiv.org/abs/2305.09781", "anchor": "#introduction:part-2", "type": "paper", "title": "", "section": "Introduction", "text": "a single SSM for speculation, which cannot align well with an LLM due to the model capacity gap between them, since SSMs are generally orders of magnitude smaller than the LLM to maintain low memory and runtime overheads. This paper introduces SpecInfer, a system that improves the end-to-end latency and computational efficiency of LLM serving with tree-based speculative inference and verifica- tion. Figure 1b illustrates a comparison between existing incremental decoding, sequence-based speculative inference, and our tree-based speculative inference. A key insight be- hind SpecInfer is to simultaneously consider a diversity of speculation candidates (instead of just one as in existing approaches) to maximize speculative performance. These candidates are organized as a token tree, whose nodes each represents a sequence of speculated tokens. The correctness of all candidate token sequences is verified against the LLM in parallel, which allows SpecInfer to significantly increase the number of generated tokens in an LLM decoding step. Compared with sequence-based speculative inference, lever- aging tree structures can significantly improve the success rate of verifying a token (e.g., from 52-57% to 96-97% for stochastic decoding as shown in Table 1). However, realizing this improvement requires addressing two unique challenges. Next, we elaborate on these challenges and the main ideas SpecInfer uses to address them. First, SpecInfer must explore an extremely large search space of candidate token sequences to maximize speculative performance. While the idea of speculative execution has been widely deployed in a variety of optimization tasks in computer architecture and systems, including branch pre- diction in modern pipelined processors and value prediction for pre-fetching memory and files [13, 42], the search space considered by SpecInfer is significantly larger due to two reasons: (1) modern LLMs generally involve very large vocab- ularies, and (2) maximizing speculative performance requires predicting multiple future tokens (instead of just the next token). For example, all LLMs in the OPT model family con- sider 50,272 different possible tokens in their vocabulary, while SpecInfer can correctly predict the next 4 tokens on average. Achieving this goal requires considering a search space of 502724 ≈6 × 1018 different combinations of tokens. SpecInfer leverages existing distilled, quantized, and/or pruned variants of an LLM, which we call small specula- tive models (SSMs), to guide speculation. A key challenging of using SSMs for speculative inference is that the align- ment between an SSM and an LLM is inherently bounded by the model capacity gap, since an SSM is generally 100- 1000× smaller than an LLM. Instead of using a single SSM for sequence-based speculation, SpecInfer maximizes spec- ulative performance by simultaneously considering a vari- ety of token sequences organized in a tree structure for a given input prompt. SpecInfer introduces an expansion- and a merge-based mechanism for constructing token trees by exploiting diversity within a single SSM and across multiple SSMs, respectively. A second challenge SpecInfer must address is verifying the speculated tokens. Many LLM applications perform stochas- tic decoding, which samples the next token from a probability distribution instead of deterministically generating a token. To preserve an LLM’s generative performance, SpecInfer must guarantee that its tree-based", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "ae0130d5d605d02ea4e415bf7c2ee1b69907c8945b7f0ecce51fd390ad59e0f7"}
{"doc_id": "arxiv:2305.09781#introduction:part-3", "url": "https://arxiv.org/abs/2305.09781", "anchor": "#introduction:part-3", "type": "paper", "title": "", "section": "Introduction", "text": "sequences organized in a tree structure for a given input prompt. SpecInfer introduces an expansion- and a merge-based mechanism for constructing token trees by exploiting diversity within a single SSM and across multiple SSMs, respectively. A second challenge SpecInfer must address is verifying the speculated tokens. Many LLM applications perform stochas- tic decoding, which samples the next token from a probability distribution instead of deterministically generating a token. To preserve an LLM’s generative performance, SpecInfer must guarantee that its tree-based speculative inference and verification mechanism generates the next token by follow- ing the exact same probability distribution as incremental decoding. To achieve this goal, we propose multi-step spec- ulative sampling, a new sampling approach for SSMs that guarantees equivalence while maximizing the number of 2 LLM machine learning system optimization learning system optimization is Iterations: 0 1 2 3 Outputs: !! !\" !# !$ Incremental Decoding (a) Incremental decoding. IncDecode IncDecode IncDecode IncDecode ... !$ !! !\" !# !% TreeVerify TreeVerify !$ !! →!\" →!# →!% !% !& →!' →!( →!) ... Incremental Decoding Timeline Tree-based Speculative Inference Timeline Spec Spec Spec Spec ... ... Spec Spec ... Sequence-based Speculative Inference Timeline SequenceVerify SequenceVerify !$ !! →!\" !\" !# →!% ... Spec Spec ... (b) Timeline Comparison. Figure 1. Comparing the incremental decoding approach used by existing LLM serving systems, the sequence-based speculative inference approach, and the tree-based speculative inference approach used by SpecInfer. speculated tokens that can be verified. To minimize the to- ken tree verification cost, SpecInfer introduces a tree-based parallel decoding mechanism, simultaneously verifying all tokens of a token tree against the LLM’s output in a single LLM decoding step. By leveraging tree-based speculative inference and verifi- cation, SpecInfer accelerates both distributed LLM inference across multiple GPUs and offloading-based LLM inference on one GPU. Our evaluation shows that SpecInfer outperforms existing LLM serving systems by 1.5-2.8× for distributed LLM inference and by 2.6-3.5× for offloading-based LLM inference, while preserving the same generative accuracy. To summarize, we make the following contributions: • We present SpecInfer, a tree-based speculative infer- ence and verification system for LLM serving. • To maximize speculative performance, we propose a merge- and an expansion-based method to construct token trees by exploiting diversity within and across SSMs, respectively. • To minimize verification cost, we introduce a tree- based parallel decoding mechanism to simultaneously verify all tokens of a token tree. • We evaluate SpecInfer and show that it outperforms existing systems by up to 2.8× for distributed inference and by up to 3.5× for offloading-based inference. 2 SpecInfer’s Overview Figure 2 shows an overview of SpecInfer, which includes a learning-based speculator that takes as input a sequence of tokens, and produces a speculated token tree. The goal of the speculator is to predict the LLM’s output by maximizing the overlap between the speculated token tree and the tokens generated by the LLM using incremental decoding (Alg. 1). Algorithm 1 The incremental decoding algorithm used in existing LLM serving systems. 1: Input: A sequence of input tokens ℐ 2: Output: A sequence of generated tokens 3: 𝒮= ℐ", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "1f48ebd70adf6f2dcbd524f3c292510f6dd43f3604d6f98a0293d46517ba35dc"}
{"doc_id": "arxiv:2305.09781#introduction:part-4", "url": "https://arxiv.org/abs/2305.09781", "anchor": "#introduction:part-4", "type": "paper", "title": "", "section": "Introduction", "text": "includes a learning-based speculator that takes as input a sequence of tokens, and produces a speculated token tree. The goal of the speculator is to predict the LLM’s output by maximizing the overlap between the speculated token tree and the tokens generated by the LLM using incremental decoding (Alg. 1). Algorithm 1 The incremental decoding algorithm used in existing LLM serving systems. 1: Input: A sequence of input tokens ℐ 2: Output: A sequence of generated tokens 3: 𝒮= ℐ 4: while true do 5: 𝑡= Decode(LLM, 𝒮) 6: 𝒮.append(𝑡) 7: if 𝑡= ⟨EOS⟩then 8: Return 𝒮 There are several ways to prepare SSMs for speculative inference. First, modern LLMs generally have many smaller architectures pre-trained together with the LLM using the same datasets. For example, in addition to the OPT-175B model with 175 billion parameters, the OPT model family also includes OPT-125M and OPT-350M, two variants with 125 million and 350 million parameters, which were pre- trained using the same datasets as OPT-175B [57]. These pre- trained small models can be directly used as SSMs. Second, to improve the coverage of speculated tokens from SSMs, SpecInfer takes an expansion-based and a merge-based spec- ulation method as shown at the top of Figure 2. The specu- lated tokens are organized in a token tree structure. SpecInfer’s usage of an LLM is also different from that of existing LLM serving systems. Instead of using the LLM as an incremental decoder that predicts the next single token, SpecInfer uses the LLM as a token tree verifier that verifies a speculated token tree against the LLM’s output. For each token, SpecInfer computes its activations by considering all of its ancestors in the token tree as its preceding tokens. For example, in Figure 2, the attention output of the token 𝑡3,0 is calculated based on sequence (𝑡0,𝑡1,0,𝑡2,1,𝑡3,0), where 𝑡0, 3 Learning-based Speculator (§\") !! machine !\",! learning !\",\" translation !#,! algorithm !#,\" system !$,! design !#,#", "source": "arxiv_pdf", "published": "", "tokens": 322, "sha256": "cdc086c9d38ba596f66e24a1a86e3c65b7cd6e63992c0ec6dd5a4fbd86d6bc5e"}
{"doc_id": "arxiv:2305.09781#model:part-1", "url": "https://arxiv.org/abs/2305.09781", "anchor": "#model:part-1", "type": "paper", "title": "", "section": "model", "text": "[machine] learning algorithm system design translation model LLM Tree-based Parallel Decoding learning system design optimization [EOS] model [EOS] Linearization SSM 0 SSM 1 SSM 2 !$,# optimization !$,\" design !%,! [EOS] !$,$ [EOS] Merge-based Token Tree Construction Sequence Representation of Speculated Tokens SSM Expansion-based Token Tree Construction Output 0 Output 1 Output 2 Expansion Expansion Output 0 Output 1 Output 2 Output 0: [machine] learning algorithm Output 1: [machine] learning system design Output 2: [machine] translation model OR Figure 2. An overview of SpecInfer’s tree-based speculative inference and verification mechanism. 𝑡1,0, and 𝑡2,1 are 𝑡3,0’s ancestors in the token tree. SpecInfer includes a novel tree-based parallel decoding mechanism to simultaneously verify all tokens of a token tree in a single LLM decoding step. SpecInfer’s speculative inference and token tree verifi- cation provide two key advantages over the incremental decoding approach of existing LLM inference systems. Reduced memory accesses to LLM parameters. The per- formance of LLM inference is largely limited by accesses to GPU memory. In the existing incremental decoding approach, generating a single token requires accessing all parameters of an LLM. The problem is exacerbated for offloading-based LLM inference systems, which use limited computational resources such as a single commodity GPU to serve LLMs by utilizing CPU DRAM and persistent storage to save model parameters and loading these parameters to GPU’s high bandwidth memory (HBM) for computation. Compared to the incremental decoding approach, SpecInfer significantly reduces accesses to LLM parameters whenever the overlap between a speculated token tree and the LLM’s actual out- put is not empty. Reduced accesses to GPU device memory and reduced data transfers between GPU and CPU memory can also directly translate to decreased energy consumption, since accessing GPU HBM consumes two or three orders of magnitude more energy than floating point arithmetic operations. Reduced end-to-end inference latency. Serving LLMs suffers from long end-to-end inference latency. For exam- ple, the GPT-3 architecture includes 175 billion parameters and requires many seconds to serve a request. In the ex- isting incremental decoding approach, the computation for generating each token depends on the keys and values of all previously generated tokens, which introduces sequen- tial dependencies between tokens and requires modern LLM serving systems to serialize the generation of different tokens for each request. In SpecInfer, LLMs are used as a verifier that takes a speculated token tree as an input and can simul- taneously examine all tokens in the token tree by making a single verification pass over the LLM. This approach enables parallelization across different tokens in a single request and reduces the LLM’s end-to-end inference latency. 3 Learning-based Speculator Existing speculative decoding methods perform sequence- based speculation, where an SSM predicts a single sequence of tokens to be verified by an LLM. However, a key limitation of a single speculated sequence is that the probability of a suc- cessful alignment between the LLM and the speculated token sequence decays exponentially with the expected alignment length. This can be further exacerbated by the fact that the speculation only includes a single candidate token to verify per step, resulting", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "670f745cc8f3746229081be9a2a6345fc6d08480d17789980c917572031cdfb4"}
{"doc_id": "arxiv:2305.09781#model:part-2", "url": "https://arxiv.org/abs/2305.09781", "anchor": "#model:part-2", "type": "paper", "title": "", "section": "model", "text": "Learning-based Speculator Existing speculative decoding methods perform sequence- based speculation, where an SSM predicts a single sequence of tokens to be verified by an LLM. However, a key limitation of a single speculated sequence is that the probability of a suc- cessful alignment between the LLM and the speculated token sequence decays exponentially with the expected alignment length. This can be further exacerbated by the fact that the speculation only includes a single candidate token to verify per step, resulting in suboptimal speculative performance. On the other hand, by encouraging more diverse speculated candidates per step, the probability of a successful match per 4 Algorithm 2 The speculation and verification algorithm used by SpecInfer. Speculate takes the current token se- quence 𝒮as an input and generates a speculated token tree 𝒩. TreeParallelDecode generates a token 𝒪(𝑢) for each node 𝑢∈𝒩. VerifyGreedy and VerifyStochastic exam- ine 𝒩against 𝒪and produce a sequence of verified tokens 𝒱using greedy or stochastic sampling, respectively. 1: Input: A sequence of input tokens ℐ 2: Output: A sequence of generated tokens 3: 𝒮= ℐ 4: while true do 5: 𝒩= Speculate(𝒮) 6: 𝒪= TreeParallelDecode(LLM, 𝒩) 7: if use greedy decoding then 8: 𝒱= VerifyGreedy(𝒪, 𝒩) 9: else 10: 𝒱= VerifyStochastic(𝒪, 𝒩) 11: for 𝑡∈𝒱do 12: 𝒮.append(𝑡) 13: if 𝑡= ⟨EOS⟩then 14: return 𝒮 15: 16: function VerifyGreedy(𝒪, 𝒩) 17: 𝒱= ∅, 𝑢←the root of token tree 𝒩 18: while ∃𝑣∈𝒩.𝑝𝑣= 𝑢and 𝑡𝑣= 𝒪(𝑢) do 19: 𝒱.append(𝑡𝑣) 20: 𝑢= 𝑣 21: 𝒱.append(𝒪(𝑢)) 22: return 𝒱 23: 24: function VerifyStochastic(𝒪, 𝒩) 25: 𝒱= ∅, 𝑢←the root of token tree 𝒩 26: while 𝑢is a non-leaf node do 27: ℋ= child(𝑢) ⊲The set of child nodes for 𝑢 28: while ℋis not empty do 29: 𝑠∼rand(ℋ),𝑟∼𝑈(0, 1),𝑥𝑠= ℋ[𝑠] 30: if 𝑟 ≤ 𝑃(𝑥𝑠| 𝑢, Θ𝐿𝐿𝑀)/𝑃(𝑥𝑠| 𝑢, Θ𝑆𝑆𝑀𝑠) then 31: ⊲Token 𝑥𝑠passes verification. 32: 𝒱.append(𝑥𝑠) 33: 𝑢= 𝑠 34: break 35: else 36: ⊲Normalize the residual 𝑃(𝑥| 𝑢, Θ𝐿𝐿𝑀) 37: 𝑃(𝑥| 𝑢, ΘLLM) B norm(max(0, 𝑃(𝑥| 𝑢, ΘLLM) −𝑃(𝑥| 𝑢, ΘSSM𝑠))) 38: ℋ.pop(𝑠) 39: if ℋis empty then 40: break 41: ⊲All SSMs fail verification; sample the next token 42: 𝑥next ∼𝑃(𝑥| 𝑢, Θ𝐿𝐿𝑀) 43: 𝒱.append(𝑥next) 44: return 𝒱 Table 1. The success rate of verifying a token for LLaMA-7B using the top-𝑘tokens derived from LLaMA-68M. The five prompt datasets are described in Section 6.1. Dataset 𝑘= 1 𝑘= 2 𝑘= 3 𝑘= 4 𝑘= 5 Greedy decoding Alpaca 68% 77% 81% 84% 85% CP 69% 79% 83% 86% 87% WebQA 62% 72% 77% 80% 82% CIP 70% 81% 85% 88% 89% PIQA 63% 75% 79% 83% 85% Stochastic decoding Alpaca 54% 81% 91% 95% 97% CP 56% 82% 92% 95% 97% WebQA 52% 80% 90% 94% 96% CIP 57% 84% 92% 95% 97% PIQA 55% 82% 91% 95% 97% step (i.e., the token decoded by the LLM is in this candidate pool) can be greatly improved. To this end, SpecInfer aims to construct a tree of speculated candidates by exploiting diversity within a single SSM and across multiple SSMs. In particular, SpecInfer’s learning-based speculator aggregates the predictions of one or multiple SSMs", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "b48b1b90733785e69e394710b6db4eecd56a0eb103dd12daa48c9b7d8901308b"}
{"doc_id": "arxiv:2305.09781#model:part-3", "url": "https://arxiv.org/abs/2305.09781", "anchor": "#model:part-3", "type": "paper", "title": "", "section": "model", "text": "91% 95% 97% CP 56% 82% 92% 95% 97% WebQA 52% 80% 90% 94% 96% CIP 57% 84% 92% 95% 97% PIQA 55% 82% 91% 95% 97% step (i.e., the token decoded by the LLM is in this candidate pool) can be greatly improved. To this end, SpecInfer aims to construct a tree of speculated candidates by exploiting diversity within a single SSM and across multiple SSMs. In particular, SpecInfer’s learning-based speculator aggregates the predictions of one or multiple SSMs to maximize specula- tive performance while maintaining low memory overhead and inference latency. SpecInfer uses a token tree to orga- nize the tokens produced by the speculator and introduces two methods for constructing token trees: expansion- and merge-based tree constructions. Definition 3.1 (Token Tree). A token tree 𝒩is a tree struc- ture, where each node 𝑢∈𝒩is labeled with a token 𝑡𝑢, and 𝑝𝑢represents 𝑢’s parent node in the token tree. For each node 𝑢, 𝑆𝑢represents a sequence of tokens identified by concatenating 𝑆𝑝𝑢and {𝑡𝑢}1. Expansion-based token tree construction. One approach to creating a token tree involves deriving multiple tokens from an SSM within a single decoding step. This approach is motivated by an important observation that when an SSM misaligns with an LLM (i.e., the two models select different top-1 tokens), the token selected by the LLM is generally among the top-𝑘tokens from the SSM for very small values of 𝑘. Table 1 shows the success rate of verifying a token using the top-𝑘tokens derived from an SSM, where a verification is successful if the token selected by the LLM is among the top-𝑘tokens from the SSM. Compared to only using the top- 1 token from an SSM, using the top-5 tokens can increase the success rate from 70% to 89% for greedy decoding and from 57% to 97% for stochastic decoding. Directly selecting the top-𝑘tokens at each step leads to an exponential increase in the number of potential token se- quences, which substantially elevates inference latency and 1For the root node 𝑟, 𝑆𝑟represents the token sequence {𝑡𝑟}. 5 memory overhead. Consequently, we adopt a static strategy that expands the token tree following a preset expansion con- figuration represented as a vector of integers ⟨𝑘1,𝑘2, ...,𝑘𝑚⟩, where𝑚denotes the maximum number of speculative decod- ing steps, and𝑘𝑖indicates the number of tokens to expand for each token in the 𝑖-th step. For example, Figure 3 illustrates the expansion configuration ⟨2, 2, 1⟩, leading to four token sequences. Our evaluation (see Section 6.4) shows that even a simple strategy can generate highly accurate speculative results. We acknowledge that dynamically expanding a token tree from an SSM is an opening research problem beyond the scope of this paper, which we leave as future work. Merge-based token tree construction. In addition to us- ing a single SSM, SpecInfer can also combine multiple SSMs to jointly predict an LLM’s output. SpecInfer uses an unsu- pervised method to collectively boost-tune a pool of SSMs to align their outputs with that of the LLM by leveraging adaptive boosting [12]. SpecInfer uses SSMs to predict the next few tokens that an", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "2113e5410f3c1a09625f63b95d3a06f598bb0169faa63ab9ef0961454610fd1c"}
{"doc_id": "arxiv:2305.09781#model:part-4", "url": "https://arxiv.org/abs/2305.09781", "anchor": "#model:part-4", "type": "paper", "title": "", "section": "model", "text": "SSM is an opening research problem beyond the scope of this paper, which we leave as future work. Merge-based token tree construction. In addition to us- ing a single SSM, SpecInfer can also combine multiple SSMs to jointly predict an LLM’s output. SpecInfer uses an unsu- pervised method to collectively boost-tune a pool of SSMs to align their outputs with that of the LLM by leveraging adaptive boosting [12]. SpecInfer uses SSMs to predict the next few tokens that an LLM will generate, and uses gen- eral text datasets (e.g., the OpenWebText corpus [15] in our evaluation) to adaptively align the aggregated output of mul- tiple SSMs with the LLM in a fully unsupervised fashion. In particular, SpecInfer converts a text corpus into a collection of prompt samples and use the LLM to generate a token se- quence for each prompt. SpecInfer first fine-tunes one SSM at a time to the fullest and marks all prompt samples where the SSM and LLM generate identical subsequent tokens. Next, SpecInfer filters all marked prompt samples and uses all re- maining samples in the corpus to fine-tune the next SSM to the fullest. By repeating this process for every SSM in the pool, SpecIn- fer obtains a diverse set of SSMs whose aggregated output largely overlaps with the LLM’s output on the training cor- pus. All SSMs have identical inference latency, and therefore running all SSMs on different GPUs in parallel does not in- crease the latency of speculative inference compared to using a single SSM. In addition, SpecInfer uses data parallelism to serve SSMs across multiple GPUs, and therefore using multi- ple SSMs does not increase the memory overhead on each GPU. In the case where multiple SSMs are employed, the out- put of each SSM is considered as a token tree, and SpecInfer performs token tree merge to aggregate all speculated tokens in a single tree structure. Definition 3.2 (Token Tree Merge). ℳis the tree merge of 𝑚token trees {𝒩𝑖} (1 ≤𝑖≤𝑚) if and only if ∀1 ≤𝑖≤ 𝑚, ∀𝑢∈𝒩𝑖, ∃𝑣∈ℳsuch that 𝑆𝑣= 𝑆𝑢and vice versa. Intuitively, each token tree represents a set of token se- quences. Merging multiple token trees produces a new tree that includes all token sequences of the original trees. For example, Figure 3 shows the token tree derived by merging four sequences of tokens. Each token sequence is identified by a node in the merged token tree. !# !$ !% !& !' !( !) machine learning algorithm system models translation design Speculated Token Sequences Expanded Token Tree (Depth=3) !* system !$# design Sequence 1: machine learning algorithm is Sequence 2: machine learning system design Sequence 3: machine translation models are Sequence 4: machine translation system design Step 0 Width=2 Step 1 Width=2 Step 2 Width=1 !+ is !, are Figure 3. Illustration of token tree expansion. Note that, in addition to boosting, there are several other ensemble learning methods (e.g., voting, bagging, and stack- ing) [14] that can be used to combine the outputs from mul- tiple SSMs, and we leave the exploration as future work. 4 Token", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "a430d09bced63ef54e9fa6837696554832047c548699fd4c5fab713851cfcc61"}
{"doc_id": "arxiv:2305.09781#model:part-5", "url": "https://arxiv.org/abs/2305.09781", "anchor": "#model:part-5", "type": "paper", "title": "", "section": "model", "text": "Sequence 2: machine learning system design Sequence 3: machine translation models are Sequence 4: machine translation system design Step 0 Width=2 Step 1 Width=2 Step 2 Width=1 !+ is !, are Figure 3. Illustration of token tree expansion. Note that, in addition to boosting, there are several other ensemble learning methods (e.g., voting, bagging, and stack- ing) [14] that can be used to combine the outputs from mul- tiple SSMs, and we leave the exploration as future work. 4 Token Tree Verifier This section introduces SpecInfer’s token tree verifier, which takes as input a token tree generated by the speculator and verifies the correctness of its tokens against an LLM’s output. A key idea behind the design of SpecInfer is simultaneously verifying all sequences of a token tree against the original LLM’s output by making a single pass over the LLM’s parame- ters. This functionality allows SpecInfer to opportunistically decode multiple tokens (instead of a single token in incre- mental decoding), resulting in reduced memory accesses to the LLM’s parameters. A challenge SpecInfer must address in token tree verification is efficiently computing the atten- tion scores for all sequences of a token tree. To this end, we introduce tree attention, which generalizes the attention mechanism [48] from sequence to tree structure. In addition, we develop a tree-based parallel decoding mechanism that can decode all tokens in a token tree in parallel. §4.1 and §4.2 describe tree attention and tree-based parallel decoding. §4.3 introduces the mechanism to verify a token tree against the LLM’s output. 4.1 Tree Attention Transformer-based language models use the attention mech- anism to reason about sequential information [48]. LLMs generally use decoder-only, multi-head self-attention, which takes a single input tensor 𝑋and computes an output tensor 𝑂via scaled multiplicative formulations as follows. 𝑄𝑖= 𝑋×𝑊𝑄 𝑖, 𝐾𝑖= 𝑋×𝑊𝐾 𝑖, (1) 𝑉𝑖= 𝑋×𝑊𝑉 𝑖, 𝐴𝑖= (𝑄𝑖×𝐾𝑇 𝑖) √ 𝑑 , (2) 𝐻𝑖= softmax\u0000mask(𝐴𝑖)\u0001𝑉𝑖, 𝑂= (𝐻1, ..., 𝐻ℎ)𝑊𝑂 (3) where 𝑄𝑖, 𝐾𝑖, and 𝑉𝑖denote the query, key, and value tensors of the 𝑖-th attention head (1 ≤𝑖≤ℎ), 𝑊𝑄 𝑖, 𝑊𝐾 𝑖, and 𝑊𝑉 𝑖 are the corresponding weight matrices. 𝐴𝑖is an 𝑙× 𝑙matrix that represents the attention scores between different tokens in the input sequence, where 𝑙is the sequence length. To pre- serve causality when generating tokens (i.e., a token in the 6 sequence should not affect the hidden states of any preceding tokens), the following causal mask function is applied: mask(𝐴)𝑗𝑘= ( 𝐴𝑗𝑘 𝑗≥𝑘 −∞ 𝑗< 𝑘. (4) Intuitively, when computing the attention output of the 𝑗-th token in the sequence, all subsequent tokens should have an attention score of −∞to indicate that the subsequent tokens will not affect the attention output of the 𝑗-th token2. In Equation 3, 𝐻𝑖represents the output of the 𝑖-th attention head, and𝑊𝑂is a weight matrix used for computing the final output of the attention layer. Note that the attention mechanism described above ap- plies only to a sequence of tokens. We generalize the atten- tion mechanism to arbitrary tree structures. Definition 4.1 (Tree Attention). For a token tree 𝒩and an arbitrary node 𝑢∈𝒩, its tree attention", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "e1303f713dde0389e109717e021f7575b380a7234bc871a23c03463d1e7de38d"}
{"doc_id": "arxiv:2305.09781#model:part-6", "url": "https://arxiv.org/abs/2305.09781", "anchor": "#model:part-6", "type": "paper", "title": "", "section": "model", "text": "that the subsequent tokens will not affect the attention output of the 𝑗-th token2. In Equation 3, 𝐻𝑖represents the output of the 𝑖-th attention head, and𝑊𝑂is a weight matrix used for computing the final output of the attention layer. Note that the attention mechanism described above ap- plies only to a sequence of tokens. We generalize the atten- tion mechanism to arbitrary tree structures. Definition 4.1 (Tree Attention). For a token tree 𝒩and an arbitrary node 𝑢∈𝒩, its tree attention is defined as the out- put of computing the original Transformer-based sequence attention on 𝑆𝑢(i.e., the token sequence represented by 𝑢): TreeAttention(𝑢) = Attention(𝑆𝑢)∀𝑢∈𝒩 (5) For a given set of token sequences, since each sequence 𝑆 is covered by a node of the merged token tree, performing tree attention on the token tree allows SpecInfer to obtain the attention output for all token sequences. 4.2 Tree-based Parallel Decoding This section describes SpecInfer’s tree-based parallel decoding mechanism for computing tree attention for all tokens in a to- ken tree in parallel. A key challenge SpecInfer must address in computing tree attention is managing key-value cache. In particular, the attention mechanism of Transformer [48] requires accessing the keys and values of all preceding to- kens to compute the attention output of each new token, as shown in Equation 3. To avoid recomputing these keys and values, today’s LLM inference systems generally cache the keys and values of all tokens for reuse in future iter- ations, since the causal relation guarantees that a token’s key and value remain unchanged in subsequent iterations (i.e., mask(A)jk = −∞for any 𝑗< 𝑘). However, when com- puting tree attention, different sequences in a token tree may include conflicting key-value caches. For example, for the speculated token tree in Figure 4, two token sequences (𝑡2,𝑡3,𝑡4,𝑡5) and (𝑡2,𝑡3,𝑡8,𝑡9) have different keys and values for the third and fourth positions. A straightforward approach to supporting key-value cache is employing the sequence-based decoding of existing LLM inference systems and using a different key-value cache for each sequence of a token tree, as shown on the left of Figure 4. However, this approach is computationally very expensive 2Note that we use −∞(instead of 0) to guarantee that the softmax’s output is 0 for these positions. and involves redundant computation, since two token se- quences sharing a common prefix have the same attention outputs for the common prefix due to the causal mask in Equation 3. In addition, launching one kernel for each token sequence introduces additional kernel launch overhead. SpecInfer introduces two key techniques to realize tree- based parallel decoding. Depth-first search to update key-value cache. Instead of caching the keys and values for individual token sequences of a token tree, SpecInfer reuses the same key-value cache across all token sequences by leveraging a depth-first search mechanism to traverse the token tree, as shown in Figure 4, where SpecInfer visits 𝑡2,𝑡3, ...,𝑡9 by following a depth-first order to traverse the token tree and update the shared key- value cache. This approach allows SpecInfer to maintain the correct keys and values for all preceding tokens", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "8208dd3e071fe41478614538923218aab9b3b8cb79cacf839244b53237f16b7c"}
{"doc_id": "arxiv:2305.09781#model:part-7", "url": "https://arxiv.org/abs/2305.09781", "anchor": "#model:part-7", "type": "paper", "title": "", "section": "model", "text": "key-value cache. Instead of caching the keys and values for individual token sequences of a token tree, SpecInfer reuses the same key-value cache across all token sequences by leveraging a depth-first search mechanism to traverse the token tree, as shown in Figure 4, where SpecInfer visits 𝑡2,𝑡3, ...,𝑡9 by following a depth-first order to traverse the token tree and update the shared key- value cache. This approach allows SpecInfer to maintain the correct keys and values for all preceding tokens when computing the attention output of a new token. Topology-aware causal mask. A straightforward ap- proach to computing tree attention is calculating the tree attention output for individual tokens by following the depth- first order described earlier. However, this approach would result in high GPU kernel launch overhead since each kernel only computes tree attention for one token sequence. In ad- dition, executing these kernels in parallel requires additional GPU memory to store their key-value caches separately due to cache conflict. A key challenge that prevents SpecInfer from batching multiple tokens is that the attention computa- tion for different tokens requires different key-value caches and therefore cannot be processed in parallel. We introduce topology-aware casual mask to fuse tree at- tention computation of all tokens in a single kernel. To batch attention computation, SpecInfer uses a tree topology in- stead of the original sequence topology to store the keys and values of all tokens in a token tree in the key-value cache. For example, to compute tree attention for the speculated token tree shown in Figure 4, SpecInfer takes both verified tokens (i.e., 𝑡2) and all speculated tokens (i.e., 𝑡3,𝑡4, ...,𝑡9) as inputs. This approach allows SpecInfer to fuse the attention compu- tation into a single kernel but also results in attention scores that violate the causal dependency (e.g., 𝑡7’s attention com- putation uses all previous tokens, including 𝑡5 which is not in 𝑡7’s token sequence). To fix the attention scores for these pairs, SpecInfer updates the causal mask based on the token tree’s topology. This approach computes the exact same at- tention output as incremental decoding, while resulting in much fewer kernel launches compared to sequence-based decoding. 4.3 Token Verification For a given speculated token tree 𝒩, SpecInfer uses tree- based parallel decoding (see Section 4.2) to compute its tree attention and generate an output tensor 𝒪that includes a 7 Kernel 1: 𝑡! →𝑡\" →𝑡# →𝑡$ 𝑡! 𝑡\" 𝑡# 𝑡$ 𝑡% 𝑡& 𝑡' 𝑡( Speculated Token Tree Verified tokens 𝑡) Speculated tokens’ KV-cache Available KV-cache slots Sequence-based Parallel Decoding t5 t4 t3 t2 ✓ ✓ ✓ ✓ t2 ✓ ✓ ✓ t3 ✓ ✓ t4 ✓ t5 t9 t8 t7 t6 t5 t4 t3 t2 ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ t2 ✓ ✓ ✓ ✓ ✓ ✓ ✓ t3 ✓ ✓ ✓ ✓ t4 ✓ t5 ✓ ✓ t6 ✓ t7 ✓ ✓ t8 ✓ t9 … t5 t4 t3 t2 KV-cache Causal Mask Kernel 3: 𝑡! →𝑡\" →𝑡# →𝑡% →𝑡& t7 t6 t4 t3 t2 ✓ ✓ ✓ ✓ ✓ t2 ✓ ✓ ✓ ✓ t3 ✓", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "0f16e472f3ab5bbdfc2f24737078f23ceb1e2393bc7e6b709c5fd08b2ab7e755"}
{"doc_id": "arxiv:2305.09781#model:part-8", "url": "https://arxiv.org/abs/2305.09781", "anchor": "#model:part-8", "type": "paper", "title": "", "section": "model", "text": "t3 ✓ ✓ t4 ✓ t5 t9 t8 t7 t6 t5 t4 t3 t2 ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ t2 ✓ ✓ ✓ ✓ ✓ ✓ ✓ t3 ✓ ✓ ✓ ✓ t4 ✓ t5 ✓ ✓ t6 ✓ t7 ✓ ✓ t8 ✓ t9 … t5 t4 t3 t2 KV-cache Causal Mask Kernel 3: 𝑡! →𝑡\" →𝑡# →𝑡% →𝑡& t7 t6 t4 t3 t2 ✓ ✓ ✓ ✓ ✓ t2 ✓ ✓ ✓ ✓ t3 ✓ ✓ ✓ t4 ✓ ✓ t6 ✓ t7 … t7 t6 t4 t3 t2 Kernel 2: 𝑡! →𝑡\" →𝑡' →𝑡( t9 t8 t3 t2 ✓ ✓ ✓ ✓ t2 ✓ ✓ ✓ t3 ✓ ✓ t8 ✓ t8 … t9 t8 t3 t2 Tree-based Parallel Decoding … t9 t8 t7 t6 t5 t4 t3 t2 Kernel 1: 𝑡!, 𝑡\", 𝑡# , 𝑡$, 𝑡%, 𝑡&, 𝑡' , 𝑡( KV-cache Topology- Aware Causal Mask Verified tokens’ KV-cache Figure 4. Comparing SpecInfer’s tree-based parallel decoding with existing sequence-based decoding. Stochastic Decoding With probability : min(1, P(ui ∣U, ΘLLM) P(ui ∣U, ΘSSM3) ) With probability : min(1, P(ui ∣U, ΘLLM) P(ui ∣U, ΘSSM2) ) SSM 1 LLM P(ui ∣U, ΘSSM1) Failed Normalized residual distribution SSM 2 Verification With probability : min(1, P(ui ∣U, ΘLLM) P(ui ∣U, ΘSSM1) ) Verified Verified Verified else SSM 3 Verification Verification else else P(ui ∣U, ΘSSM2) P(ui ∣U, ΘSSM3) P(ui ∣U, ΘLLM) Normalized residual distribution Figure 5. Illustrating the multi-step speculative sampling mechanism for verifying LLMs with stochastic sampling. token for each node 𝑢∈𝒩. Next, SpecInfer’s token tree veri- fier examines the correctness of speculated tokens against the LLM. SpecInfer supports both greedy and stochastic sam- pling as shown in Algorithm 2. Greedy decoding. Many LLM applications generate to- kens using greedy decoding, which greedily selects the to- ken with the highest likelihood in each decoding step. The VerifyGreedy function in Algorithm 2 shows how SpecIn- fer verifies a speculated token tree 𝒩with greedy decoding. SpecInfer starts from the root of 𝒩and iteratively exam- ines a node’s speculated results against the LLM’s original output. For a node 𝑢∈𝒩, SpecInfer successfully specu- lates its next token if 𝑢includes a child node 𝑣(i.e., 𝑝𝑣= 𝑢) whose token matches the LLM’s output (i.e., 𝑡𝑣= 𝒪(𝑢)). In this case, SpecInfer finishes its verification for node 𝑢and moves on to examine its child 𝑣. When the node 𝑢does not include a child that contains the LLM’s output, SpecInfer adds 𝒪(𝑢) as a verified node in 𝒩and terminates the verifi- cation process. Finally, all verified nodes are appended to the current generated token sequence 𝒱. Token tree verification allows SpecInfer to opportunistically decode multiple tokens (instead of a single token in the incremental decoding ap- proach), while preserving the same generative performance as incremental decoding. Stochastic decoding. To improve the diversity of gener- ated tokens, many LLM applications perform stochastic de- coding, which samples a token from a probability distribution 𝑃(𝑢𝑖|𝑢0, ...,𝑢𝑖−1; Θ𝐿𝐿𝑀), where 𝑈= 𝑢0, ...,𝑢𝑖−1 are previously generated tokens, 𝑢𝑖is the next token to generate, and Θ𝐿𝐿𝑀 represents a parameterized LLM. To verify a speculated", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "ca21ff2e4bdf3e414dd3c18e2cdce934e707e3f82f8b389adf44969c6fe8b937"}
{"doc_id": "arxiv:2305.09781#model:part-9", "url": "https://arxiv.org/abs/2305.09781", "anchor": "#model:part-9", "type": "paper", "title": "", "section": "model", "text": "verification allows SpecInfer to opportunistically decode multiple tokens (instead of a single token in the incremental decoding ap- proach), while preserving the same generative performance as incremental decoding. Stochastic decoding. To improve the diversity of gener- ated tokens, many LLM applications perform stochastic de- coding, which samples a token from a probability distribution 𝑃(𝑢𝑖|𝑢0, ...,𝑢𝑖−1; Θ𝐿𝐿𝑀), where 𝑈= 𝑢0, ...,𝑢𝑖−1 are previously generated tokens, 𝑢𝑖is the next token to generate, and Θ𝐿𝐿𝑀 represents a parameterized LLM. To verify a speculated token tree with stochastic decod- ing, we introduce a multi-step speculative sampling (MSS) algorithm to conduct verification, whose pseudocode code is shown in the VerifyStochastic function in Algorithm 2 and illustrated in Figure 5. Our method provably preserves an LLM’s generative performance as incremental decoding while optimizing the number of speculated tokens that can be verified. Theorem 4.2 proves its correctness. Theorem 4.2. For a given 𝐿𝐿𝑀and𝑚SSMs (i.e.,𝑆𝑆𝑀1,...,𝑆𝑆𝑀𝑚, let 𝑃(𝑢𝑖|𝑈; Θ𝐿𝐿𝑀) be the probability distribution of sampling 8 a token using stochastic decoding, where 𝑈= 𝑢0, ...,𝑢𝑖−1 are previously generated tokens, 𝑢𝑖is the next token to generate, Θ𝐿𝐿𝑀represents the parameterized LLM. Let 𝑃SpecInfer(𝑢𝑖|𝑈; Θ𝐿𝐿𝑀, {Θ𝑆𝑆𝑀𝑗}) be the probability dis- tribution of sampling token 𝑢𝑖using SpecInfer’s multi-step speculative sampling (see the VerifyStochastic function in Algorithm 2), where Θ𝑆𝑆𝑀𝑗is the 𝑗-th parameterized SSM. Then ∀𝑈,𝑢𝑖, Θ𝐿𝐿𝑀, Θ𝑆𝑆𝑀𝑗we have 𝑃(𝑢𝑖| 𝑈; Θ𝐿𝐿𝑀) = 𝑃SpecInfer(𝑢𝑖| 𝑈; Θ𝐿𝐿𝑀, {Θ𝑆𝑆𝑀𝑗}) (6) A proof of this theorem is presented in [28]. We acknowledge that a more straightforward approach to preserving the probability distribution of stochastic decoding is directly sampling the next token 𝑥∼𝑃(𝑢𝑖| 𝑈; Θ𝐿𝐿𝑀) and examining whether 𝑥is a child node of𝑢𝑖−1 in the speculated token tree. We call this approach naive sampling (NS) and show that SpecInfer’s multi-step speculative sampling has a uniformly lower rejection probability than naive sampling. Theorem 4.3. Let 𝑃 \u0010 reject | MSS,𝑈, ΘLLM, {ΘSSM𝑗} \u0011 denote the probability of rejecting speculation following multi-step speculative sampling with abbreviation 𝑃(reject | MSS), and 𝑃 \u0010 reject | NS,𝑈, ΘLLM, {ΘSSM𝑗} \u0011 the probability of rejecting speculation following Naive Sampling (NS) with abbreviation 𝑃(reject | NS). Then ∀𝑈, ΘLLM, {ΘSSM𝑗}, we have 𝑃(reject | MSS) ≤𝑃(reject | NS) We present a proof of Theorem 4.3 in [28]. Note that prior work has introduced single-step specula- tive sampling for sequence-based speculative inference [5, 25]. Different from these approaches, SpecInfer leverages token trees for improving speculative performance, which requires a different verification algorithm. As a result, SpecIn- fer performs multi-step verification (see VerifyStochastic in Algorithm 2) across all branches of a token to maximize the success rate while preserving equivalence as incremental decoding. The proposed MSS algorithm not only works for merge-based method with multiple SSMs, but also supports expansion-based method with one SSM and top-𝑘sampling. 5 System Design and Implementation This section describes the design and implementation of SpecInfer’s distributed runtime system (§5.1 and §5.2), ana- lyzes the computation and memory overheads of speculation and verification (§5.3), and introduces potential LLM appli- cations that can benefit from SpecInfer’s techniques (§5.4). 5.1 SpecInfer’s Runtime Design Figure 6 shows the workflow for one iteration of speculative inference and", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "3a596044aa2a528731be0b2734fcfb829f8a38da44426f2b10f3bc80991abdb1"}
{"doc_id": "arxiv:2305.09781#model:part-10", "url": "https://arxiv.org/abs/2305.09781", "anchor": "#model:part-10", "type": "paper", "title": "", "section": "model", "text": "not only works for merge-based method with multiple SSMs, but also supports expansion-based method with one SSM and top-𝑘sampling. 5 System Design and Implementation This section describes the design and implementation of SpecInfer’s distributed runtime system (§5.1 and §5.2), ana- lyzes the computation and memory overheads of speculation and verification (§5.3), and introduces potential LLM appli- cations that can benefit from SpecInfer’s techniques (§5.4). 5.1 SpecInfer’s Runtime Design Figure 6 shows the workflow for one iteration of speculative inference and verification. SpecInfer’s request manager re- ceives LLM serving requests and schedules these requests for serving by adapting the iteration-level scheduling policy from Orca [55]. Specifically, SpecInfer iteratively selects requests from a pool of pending requests and performs one iteration LLM Tree-based Parallel Decoding SSM1 SSM1 SSM2 SSM2 Request Manager Token Tree Merge SSM-generated Tokens Token Tree Verification Speculative Token Trees LLM-generated Tokens Request Scheduling CPU GPU1 GPU2 GPU3 GPU4 r1, r2 r3, r4 r1, r2 r3, r4 Distributing Requests Figure 6. SpecInfer’s workflow for one iteration of specu- lative inference and verification. SpecInfer uses data paral- lelism to serve SSMs, and combine tensor model parallelism and pipeline model parallelism for serving an LLM. of speculative inference and token tree verification for the se- lected requests. Since SSMs are small and can fit in one GPU, SpecInfer equally distributes GPUs across SSMs and serves these SSMs using data parallelism. For example, Figure 6 shows how SpecInfer serves two SSMs and four requests (i.e., 𝑟1, 𝑟2, 𝑟3, and 𝑟4) on four GPUs. The SSM-generated to- kens are sent back to the request manager, which produces a speculated token tree for each request using the tree merge algorithm introduced in §4. SpecInfer serves an LLM using the hybrid parallelization strategy introduced in Megatron-LM [41], which uses tensor model parallelism for parallelizing each Transformer layer across GPUs within a node, and uses pipeline model paral- lelism for partitioning Transformer layers across nodes. All GPUs perform the tree-based parallel decoding (see §4.2) to compute tree attention scores and send the LLM-generated tokens back to the request manager, which finally verifies the speculated tokens against the LLM’s output (see §4.3). Note that the overhead introduced by the request manager (i.e., request scheduling, token tree merge, and verification) is negligible compared to the execution time of LLM inference. In addition, SpecInfer’s request manager and GPU workers only communicate tokens and do not transfer the vector representations of these tokens, which again introduces neg- ligible communication overheads. Continuous batching. SpecInfer uses continuous batch- ing introduced in Orca [55] to serve multiple LLM inference requests in parallel. Specifically, SpecInfer schedules LLM execution at the granularity of iterations instead of requests. After each LLM decoding iteration, SpecInfer checks each re- quest’s status and sends the generated results of all finished requests to the client. This design also allows SpecInfer to start processing newly arrived requests without waiting for all current requests to complete. 9 5.2 SpecInfer’s Implementation SpecInfer was implemented on top of FlexFlow [21, 47], a dis- tributed multi-GPU runtime for DNN computation. FlexFlow exposes an API that allows users to", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "c24dd23bac9f09b28718876834383a455440838660fc626b62cd33eb017c389e"}
{"doc_id": "arxiv:2305.09781#model:part-11", "url": "https://arxiv.org/abs/2305.09781", "anchor": "#model:part-11", "type": "paper", "title": "", "section": "model", "text": "execution at the granularity of iterations instead of requests. After each LLM decoding iteration, SpecInfer checks each re- quest’s status and sends the generated results of all finished requests to the client. This design also allows SpecInfer to start processing newly arrived requests without waiting for all current requests to complete. 9 5.2 SpecInfer’s Implementation SpecInfer was implemented on top of FlexFlow [21, 47], a dis- tributed multi-GPU runtime for DNN computation. FlexFlow exposes an API that allows users to define a DNN model in terms of its layers. It is compatible with PyTorch’s model definition due to the alignment of underlying operators. For example, the open-source LLMs from HuggingFace [19] can be directly imported into SpecInfer for serving without mod- ification. Users can also provide a parallelization strategy, specifying the degree of data, model, and pipeline parallelism for each layer. A DNN is represented as a computational graph where each node is a region of memory, and each edge is an operation on one or more regions. Operations can be represented using three levels of abstraction: lay- ers, operators, and tasks. The FlexFlow compiler transforms the computational graph from the highest abstractions (i.e., layers) to the lowest (i.e., tasks). Tasks are also the unit of parallelization; they are non-preemptible, and are executed asynchronously. CUDA kernel optimizations. Directly launching cuBLAS and cuDNN kernels for calculating attention results in high kernel launch overhead and does not leverage the shared memory available on modern GPUs. To address this ineffi- ciency, SpecInfer uses a customized kernel built on top of FasterTransformer [32] for computing attention. Within this kernel, each thread block computes a single head for a single request. The process begins with loading the query tensor into GPU shared memory accessible by all threads within a thread block. Each thread then performs a segment of the query/key product and broadcasts the result to other threads for computing the max query/key product and exponential sum. To support tree-based parallel decoding, SpecInfer com- putes all tokens within a tree in parallel and leverages the topology-aware causal mask to preserve casuality. 5.3 Overhead of Speculation and Verification SpecInfer accelerates generative LLM inference at the cost of additional memory and computation overheads. This section analyzes these overheads and shows that they are generally one or two orders of magnitude smaller than the memory and computation cost of executing LLM inference. Memory overhead. The memory overhead of SpecInfer’s speculation-verification approach comes from two aspects. First, in addition to serving an LLM, SpecInfer also needs to allocate memory for saving the parameters of one or mul- tiple SSMs, which collectively speculate the LLM’s output. Our evaluation shows that SpecInfer can achieve signifi- cant performance improvement by using SSMs 100-1000× smaller than the LLM. As a result, hosting each SSM increases the overall memory requirement by less than 1%. A second source of memory overhead comes from the token tree veri- fication engine, which verifies an entire token tree instead of decoding a single token. Therefore, additional memory is needed for caching the keys and values, and storing the attention scores", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "9b3d6d141c0069c6fc2466d3398cde7f76a33eb1ce96d9bfbff5a2c9b7315ae7"}
{"doc_id": "arxiv:2305.09781#model:part-12", "url": "https://arxiv.org/abs/2305.09781", "anchor": "#model:part-12", "type": "paper", "title": "", "section": "model", "text": "speculate the LLM’s output. Our evaluation shows that SpecInfer can achieve signifi- cant performance improvement by using SSMs 100-1000× smaller than the LLM. As a result, hosting each SSM increases the overall memory requirement by less than 1%. A second source of memory overhead comes from the token tree veri- fication engine, which verifies an entire token tree instead of decoding a single token. Therefore, additional memory is needed for caching the keys and values, and storing the attention scores for all tokens. Due to the necessity for sup- porting very long sequence length in today’s LLM serving, we observe that the memory overhead associated with token tree is negligible compared to key-value cache. Computation overhead. Similarly, the computation over- head introduced by speculation and verification also comes from two aspects. First, SpecInfer needs to run SSMs in the incremental-decoding mode to generate candidate tokens. When multiple SSMs are employed, SpecInfer processes these SSMs in parallel across GPUs to minimize speculation la- tency. Second, SpecInfer verifies a token tree by comput- ing the attention outputs for an entire token tree, most of which do not match the LLM’s output and therefore are un- necessary in the incremental-decoding inference. However, the key-value cache mechanism of existing LLM inference systems prevents them from serving a large number of re- quests in parallel, resulting in under-utilized computation resources on GPUs when serving LLMs in incremental de- coding. SpecInfer’s token tree verification leverages these under-utilized resources and therefore introduces negligible runtime overhead compared to incremental decoding. 5.4 Applications Our speculative inference and token tree verification tech- niques can be directly applied to a variety of LLM applica- tions. We identify two practical scenarios where LLM infer- ence can significantly benefit from our techniques. Distributed LLM inference. The memory requirements of modern LLMs exceed the capacity of a single compute node with one or multiple GPUs, and the current approach to addressing the high memory requirement is distributing the LLM’s parameters across multiple GPUs [29]. For ex- ample, serving a single inference pipeline for GPT-3 with 175 billion parameters requires more than 16 NVIDIA A100- 40GB GPUs to store the model parameters in single-precision floating points. Distributed LLM inference is largely limited by the latency to transfer intermediate activations between GPUs for each LLM decoding step. While SpecInfer’s ap- proach does not directly reduce the amount of inter-GPU communications, its verification mechanism can increase the communication granularity and reduce the number of decoding steps. Offloading-based LLM inference. Another practical sce- nario that can benefit from SpecInfer’s techniques is offloading- based LLM inference, which leverages CPU DRAM to store an LLM’s parameters and loads a subset of these parame- ters to GPUs for computation in a pipeline fashion [40]. By opportunistically verifying multiple tokens, SpecInfer can reduce the number of LLM decoding steps and the overall communication between CPU DRAM and GPU HBM. 10 6", "source": "arxiv_pdf", "published": "", "tokens": 479, "sha256": "8b1419d2f8e3e8efd824c733666e40e9157e3e9fc3f9a3cc2924ce0b2e1afed2"}
{"doc_id": "arxiv:2305.09781#evaluation:part-1", "url": "https://arxiv.org/abs/2305.09781", "anchor": "#evaluation:part-1", "type": "paper", "title": "", "section": "Evaluation", "text": "6.1 Experimental Setup LLMs. To compare the runtime performance of SpecInfer with existing LLM serving systems, we evaluate these sys- tems using two publicly available LLM families: OPT [57] and LLaMA [46]. More specifically, we select LLaMA-7B, OPT- 13B, OPT-30B, and LLaMA-65B as the LLMs, and LLaMA- 68M and OPT-125M as the SSMs. The pre-trained model parameters for the LLMs and SSMs were obtained from their HuggingFace repositories [19], and we describe how SpecIn- fer collectively boost-tunes multiple SSMs in [28]. Datasets. We evaluate SpecInfer on five datasets: Chatbot Instruction Prompts (CIP) [34], ChatGPT Prompts (CP) [30], WebQA [1], Alpaca [36, 45], and PIQA [2]. We only use the prompts/questions from these datasets to form our input prompts to simulate real-world conversation traces. Platform. The experiments were conducted on two AWS g5.12xlarge instances, each of which is equipped with four NVIDIA A10 24GB GPUs, 48 CPU cores, and 192 GB DRAM. Nodes are connected by 100 Gbps Ethernet. Our experiments use the expansion-based method (see Section 3) for constructing token trees and use the expan- sion configuration ⟨1, 1, 3, 1, 1, 1, 1, 1⟩, which provides good results for our benchmarks. We analyze the impact of ex- pansion configurations in §6.4, evaluate tree-based parallel decoding and multi-step speculative sampling in §6.5 and §6.6, and finally compares the expansion- and merge-based tree construction methods in [28]. 6.2 Distributed LLM Inference We compare the end-to-end distributed LLM inference perfor- mance among SpecInfer, vLLM [24], HuggingFace Text Gen- eration Inference (TGI) [18], and FasterTransformer [32] on LLaMA-7B, OPT-30B, and LLaMA-65B. For LLaMA-7B and OPT-30B, all systems serve the two LLMs in half-precision floating points across one and four A10 GPUs using tensor model parallelism. LLaMA-65B do not fit on four GPUs on a single node, therefore both FasterTransformer and SpecInfer serve it on eight A10 GPUs on two nodes by combining ten- sor model parallelism within each node and pipeline model parallelism across nodes. vLLM and HuggingFace TGI do not support pipeline model parallelism and cannot serve an LLM on multiple nodes. To rule out potential effects of our system implementation, we also evaluate SpecInfer with two additional configura- tions. First, SpecInfer with incremental decoding evaluates the runtime performance of our implementation when the spec- ulator generates empty token trees, and the verifier verifies exactly one token in each decoding step. Second, SpecInfer with sequence-based speculative inference serves as a refer- ence for existing speculative inference system and is enabled by using a single pre-trained SSM and sequence-based de- coding. We use prompts from the five datasets described in §6.1. For each prompt, we let all systems generate up to 128 new tokens and report the average per-token latency in Figure 7. Note that SpecInfer may generate more than 128 new tokens since the verifier can verify multiple tokens in each iteration. In this case, we truncate SpecInfer’s output to 128 tokens. SpecInfer with incremental decoding achieves on-par perfor- mance as existing systems. This is because all systems use the same strategies to parallelize LLM inference across GPUs and use the same kernel libraries", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "d5ad47e3abc09986bc9a27aa7440320081f2d07607776bf4c87cf76a1b766cc5"}
{"doc_id": "arxiv:2305.09781#evaluation:part-2", "url": "https://arxiv.org/abs/2305.09781", "anchor": "#evaluation:part-2", "type": "paper", "title": "", "section": "Evaluation", "text": "let all systems generate up to 128 new tokens and report the average per-token latency in Figure 7. Note that SpecInfer may generate more than 128 new tokens since the verifier can verify multiple tokens in each iteration. In this case, we truncate SpecInfer’s output to 128 tokens. SpecInfer with incremental decoding achieves on-par perfor- mance as existing systems. This is because all systems use the same strategies to parallelize LLM inference across GPUs and use the same kernel libraries (i.e., cuDNN, cuBLAS, and cuTLASS) to execute inference computation on GPUs. With tree-based speculative inference and verification, SpecInfer outperforms incremental decoding systems by 1.5-2.5× for single-node, multi-GPU inference and by 2.4-2.8× for multi- node, multi-GPU inference, while generating the exact same sequence of tokens as incremental decoding for all prompts. The speedup comes from leveraging spare GPU resources to perform tree-based parallel decoding while maintaining the same per-iteration latency as incremental decoding. Compared to sequence-based speculative inference, SpecIn- fer’s tree-based approach further reduces LLM serving la- tency by 1.2-1.5×. The improvement is achieved by (1) lever- aging token trees to optimize speculative performance, (2) using tree-based parallel decoding to verify an entire token tree in parallel, and (3) performing multi-step speculative sampling to improve verification performance. We further evaluates these aspects in §6.4, §6.5, and §6.6. Note that SpecInfer’s performance improvement over ex- isting systems reduces as the batch size (i.e., number of concurrent requests) increases. This is because SpecInfer leverages spare GPU resources to perform tree-based par- allel decoding while maintaining the same per-iteration la- tency as incremental decoding. A larger batch size introduces more parallelizable computation for incremental decoding, and thus less spare GPU resources that can be leveraged by SpecInfer. On the flip side, larger batch sizes also increase the end-to-end latency of each request, as shown in Figure 7. Overall, SpecInfer is most beneficial for low-latency LLM inference. 6.3 Offloading-based LLM Inference Another important application of SpecInfer is offloading- based LLM inference, where the system offloads an LLM’s parameters to CPU DRAM and loads a subset of these param- eters to GPUs for inference computation in a pipeline fashion. We compare the end-to-end offloading-based LLM inference performance between SpecInfer and FlexGen [39] using a single 24GB A10 GPU and two LLMs (i.e., OPT-13B and OPT- 30B), both of which exceed the memory capacity of an A10 GPU and requires offloading for serving. Both SpecInfer and FlexGen retain all model parameters on CPU DRAM. During computation, the demand weights are loaded from the CPU 11 BS=1 BS=2 BS=4 BS=8 BS=16 LLaMA-7B (1 GPU/node, 1 node) 0 5 10 15 20 25 30 35 40 BS=1 BS=2 BS=4 BS=8 BS=16 OPT-30B (4 GPUs/node, 1 node) 0 10 20 30 40 50 60 BS=1 BS=2 BS=4 BS=8 BS=16 LLaMA-65B (4 GPUs/node, 2 nodes) 0 20 40 60 80 100 120 Per-token latency (ms) vLLM HuggingFace TGI FasterTransformer SpecInfer w/ Incremental Decoding SpecInfer w/ Sequence-based Speculative Inference SpecInfer w/ Tree-based Speculative Inference Figure 7. Comparing the end-to-end inference latency of SpecInfer with existing systems. Numbers in parenthesis show the number of GPUs", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "c500e87f70f7d41f0f853b3e588e8d71c715b53177367c83beb2287cd1f6e3c3"}
{"doc_id": "arxiv:2305.09781#evaluation:part-3", "url": "https://arxiv.org/abs/2305.09781", "anchor": "#evaluation:part-3", "type": "paper", "title": "", "section": "Evaluation", "text": "20 25 30 35 40 BS=1 BS=2 BS=4 BS=8 BS=16 OPT-30B (4 GPUs/node, 1 node) 0 10 20 30 40 50 60 BS=1 BS=2 BS=4 BS=8 BS=16 LLaMA-65B (4 GPUs/node, 2 nodes) 0 20 40 60 80 100 120 Per-token latency (ms) vLLM HuggingFace TGI FasterTransformer SpecInfer w/ Incremental Decoding SpecInfer w/ Sequence-based Speculative Inference SpecInfer w/ Tree-based Speculative Inference Figure 7. Comparing the end-to-end inference latency of SpecInfer with existing systems. Numbers in parenthesis show the number of GPUs and compute node used to serve each LLM. All systems parallelize LLM inference by combining tensor model parallelism (within a node) and pipeline parallelism (across nodes). BS=1 BS=2 BS=4 BS=8 BS=16 OPT-13B 0.0 0.5 1.0 1.5 2.0 3.3x 3.3x 3.1x 2.7x 2.6x BS=1 BS=2 BS=4 BS=8 BS=16 OPT-30B 0 1 2 3 4 3.5x 3.3x 3.0x 3.0x 2.7x Per-token latency (seconds) FlexGen SpecInfer Figure 8. Comparing the end-to-end offloading-based infer- ence latency of FlexGen and SpecInfer. Both FlexGen and SpecInfer perform model offloading to serve OPT-13B and OPT-30B models on a single 24GB A10 GPU. to the GPU. Figure 8 shows the results. Compared to FlexGen, SpecInfer reduces the per-token latency by 2.6-3.5×. Since offloading-based LLM inference is mostly bottlenecked by the communication between CPU DRAM and GPU HBM for loading an LLM’s parameters, SpecInfer’s improvement over existing systems is achieved by opportunistically verifying multiple tokens, which in turn reduces the number of LLM decoding steps and data transfers between CPU and GPU. 6.4 Token Tree Construction This section evaluates the expansion-based token tree con- struction mechanism. We first study how token tree width affects SpecInfer’s speculative performance. In this experi- ment, we use LLaMA-7B and LLaMA-68M as the LLM and SSM, and use the expansion configuration ⟨1, 1,𝑘, 1, 1, 1, 1, 1⟩ (i.e., expanding at the third token), where 𝑘is the token tree width. Figure 9 shows the cumulative distribution function (CDF) of the average number of verified tokens per decoding 0.0 0.2 0.4 0.6 0.8 1.0 CDF Greedy decoding 2 4 6 8 Average # verified tokens per decoding step Tree width = 1 Tree width = 2 Tree width = 3 Tree width = 4 Tree width = 5 0.0 0.2 0.4 0.6 0.8 1.0 CDF Stochastic decoding 1.0 1.5 2.0 2.5 3.0 3.5 4.0 Figure 9. Comparing speculative performance of SpecInfer with different token tree structures. BS=1 BS=2 BS=4 BS=8 BS=16 15 20 25 30 35 40 Per-token latency (ms) Tree width = 1 Tree width = 2 Tree width = 3 Tree width = 4 Tree width = 5 Figure 10. SpecInfer’s end-to-end inference latency with different tree widths. We use LLaMA-7B and LLaMA-68M as the LLM and SSM. step for all prompts in the Alpaca dataset [45]. Compared to sequence-based speculation (i.e., tree width = 1), leveraging token trees can reduce LLM decoding steps by 1.2-1.5× for greedy decoding and by 1.3-1.4× for stochastic decoding. A larger token tree width reduces the LLM decoding steps to process a request at the cost of increased verification 12 Table 2. Average number of tokens verified by SpecInfer", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "a784b69ad4da483bd4bd7fa21e066d6838b4d46e06ee3e2fb9e9e4d2938ad82b"}
{"doc_id": "arxiv:2305.09781#evaluation:part-4", "url": "https://arxiv.org/abs/2305.09781", "anchor": "#evaluation:part-4", "type": "paper", "title": "", "section": "Evaluation", "text": "different tree widths. We use LLaMA-7B and LLaMA-68M as the LLM and SSM. step for all prompts in the Alpaca dataset [45]. Compared to sequence-based speculation (i.e., tree width = 1), leveraging token trees can reduce LLM decoding steps by 1.2-1.5× for greedy decoding and by 1.3-1.4× for stochastic decoding. A larger token tree width reduces the LLM decoding steps to process a request at the cost of increased verification 12 Table 2. Average number of tokens verified by SpecInfer in a decoding step. We use LLaMA-7B and LLaMA-68M as the LLM and SSM, and use different tree widths for constructing a token tree. The speculation length is 8. Token tree width Dataset 1 2 3 4 5 Greedy decoding Alpaca 2.95 3.07 3.21 3.33 3.43 CP 2.58 3.24 3.46 3.59 3.69 WebQA 2.27 2.69 2.86 2.98 3.07 CIP 2.73 3.40 3.62 3.79 3.91 PIQA 2.18 2.80 2.97 3.10 3.21 Stochastic decoding Alpaca 1.79 2.11 2.26 2.32 2.38 CP 1.69 1.99 2.15 2.23 2.28 WebQA 1.64 1.93 2.08 2.15 2.21 CIP 1.72 2.05 2.19 2.28 2.29 PIQA 1.67 1.93 2.08 2.15 2.21 BS=1 BS=2 BS=4 BS=8 BS=16 LLM: LLaMA-7B, SSM: LLaMA-68M 0 5 10 15 20 25 30 Per-token latency (ms) Sequence-based Decoding Tree-based Decoding Figure 11. Comparing SpecInfer’s tree-based parallel decod- ing with the sequence-based decoding mechanism employed by existing LLM inference systems. overhead, since SpecInfer must verify more tokens. Figure 10 compares the end-to-end inference latency of SpecInfer using different tree widths. For small batch sizes (i.e., BS = 1 and 2), using a large tree width can consistently reduce per-token latency, since SpecInfer can leverage sparse GPU resources to verify more tokens in parallel while maintaining the same per-iteration latency. For large batch sizes (i.e., BS ≥4), using a large tree width increases the latency to verify a token tree due to less sparse GPU resources that can be leveraged by SpecInfer, and a tree width of 2 or 3 achieves the best performance by striking a perfect balance between speculative performance and verification latency. 6.5 Tree-based Parallel Decoding We now evaluate the effectiveness of SpecInfer’s tree-based parallel decoding mechanism, which decodes all tokens of a Table 3. Average number of tokens verified by SpecInfer in a stochastic decoding step with different sampling algorithms. We use LLaMA-7B and LLaMA-68M as the LLM and SSM. Each token tree has a width of 5 and a depth of 8. Naive Multi-Step Improvement Sampling Spec. Sampling Alpaca 1.87 2.38 1.27× CP 1.80 2.28 1.26× WebQA 1.73 2.21 1.28× CIP 1.79 2.29 1.28× PIQA 1.73 2.21 1.28× token tree in parallel. As a comparison, all existing LLM infer- ence systems use sequence-based decoding, which requires decomposing a token tree into multiple sequences of tokens and processing these sequences using separate resources due to potential key-value cache conflicts (see §4.2). As shown in Figure 11, SpecInfer’s tree-based parallel decoding achieves on-par performance as existing sequence-based decoding mechanism for small batch sizes and outperforms it by up to 1.8× for large batch sizes. The improvement is realized by (1) eliminating redundant attention computation for se-", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "7e7255dffdf08bd63b6c2c32ebdcc352af1f56af57f3617dc8f29200f1c0f44c"}
{"doc_id": "arxiv:2305.09781#evaluation:part-5", "url": "https://arxiv.org/abs/2305.09781", "anchor": "#evaluation:part-5", "type": "paper", "title": "", "section": "Evaluation", "text": "comparison, all existing LLM infer- ence systems use sequence-based decoding, which requires decomposing a token tree into multiple sequences of tokens and processing these sequences using separate resources due to potential key-value cache conflicts (see §4.2). As shown in Figure 11, SpecInfer’s tree-based parallel decoding achieves on-par performance as existing sequence-based decoding mechanism for small batch sizes and outperforms it by up to 1.8× for large batch sizes. The improvement is realized by (1) eliminating redundant attention computation for se- quences with a shared prefix, and (2) fusing tree attention of all tokens in a single kernel through the topology-aware casual mask (see §4.2). 6.6 Multi-Step Speculative Sampling This section evaluates how our multi-step speculative sam- pling (MSS) and the VerifyStochastic algorithm improves the speculative performance of SpecInfer when performing stochastic decoding. We use naive sampling as a baseline where SpecInfer directly samples the next token from the LLM and examines whether the sampled token is included in the speculated token tree (see §4.3). Since different sam- pling algorithms involve identical speculation and verifica- tion overheads, we focus on the average number of tokens that can be verified in each stochastic decoding step in this experiment. Table 3 shows the results. Compared to naive sampling, MSS can consistently improve the number of veri- fied tokens by 1.2-1.3× on average across a variety of prompt datasets, while guaranteeing the same output distribution with the LLM. 7", "source": "arxiv_pdf", "published": "", "tokens": 235, "sha256": "f7b462d84c158f9e4d5150b3943b81d1e75b8688231371aafaca4db2e19d3176"}
{"doc_id": "arxiv:2305.09781#related-work:part-1", "url": "https://arxiv.org/abs/2305.09781", "anchor": "#related-work:part-1", "type": "paper", "title": "", "section": "Related Work", "text": "Transformer-based LLMs have demonstrated significant po- tential in numerous human-level language modeling tasks by continuously increasing their sizes [7, 9, 37, 43, 48]. As GPT- 3 becomes the first model to surpass 100B parameters [3], multiple LLMs (>100B) have been released, including OPT- 175B [57], Bloom-176B [38], and PaLM [7]. Recent work has 13 proposed a variety of approaches to accelerating generative LLM inference, which can be categorized into two classes. Lossless acceleration. Prior work has explored the idea of using an LLM as a verifier instead of a decoder to boost infer- ence. For example, Yang et al. [53] introduced inference with reference, which leverages the overlap between an LLM’s out- put and the references obtained by retrieving documents, and checks each reference’s appropriateness by examining the decoding results of the LLM. Motivated by the idea of specula- tive execution in processor optimizations [4, 16], recent work proposed speculative decoding, which uses a small language model to produce a sequence of tokens and examines the correctness of these tokens using an LLM [5, 22, 25, 44, 51]. There are two key differences between SpecInfer and these prior works. First, instead of only considering a single se- quence of tokens, SpecInfer generates and verifies a token tree, whose nodes each represent a unique token sequence. SpecInfer performs tree attention to compute the attention output of these token sequences in parallel and uses a novel tree-based decoding algorithm to reuse intermediate results shared across these sequences. Second, prior attempts gener- ally consider a single small language model for speculation, which cannot align well with an LLM due to the model ca- pacity gap between them. SpecInfer introduces two novel speculation methods, including 1) expanding from a single SSM and 2) merging from multiple fine-tuned SSMs, and the generated token tree largely increases the coverage of the LLM’s output. Prior work has also introduced a variety of techniques to optimize ML computations on modern hardware platforms. For example, TVM [6] and Ansor [58] automatically gen- erate kernels for a given tensor program. TASO [20] and PET [50] automatically discover graph-level transformations to optimize the computation graph of a DNN. SpecInfer’s techniques are orthogonal and can be combined with these systems to accelerate generative LLM computation, which we believe is a promising avenue for future work. Lossy acceleration. BiLD [23] is a speculative decoding framework that uses a single SSM to accelerate LLM decod- ing. Unlike the systems mentioned above, the acceleration is lossy: speed comes at the cost of a possible degradation in the generated tokens. Another line of research leverages model compression to reduce LLM inference latency while compromising the predictive performance of the LLM. For example, prior work proposed to leverage weight/activation quantization of LLMs to reduce the memory and computa- tion requirements of serving these LLMs [8, 11, 35, 52, 54]. Recent work further explores a variety of structured prun- ing techniques for accelerating Transformer-based architec- tures [10, 17, 49]. A key difference between SpecInfer and these prior works is that SpecInfer does not directly reduce the computation requirement for performing", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "62e8beebead7e110a86bd749c0421b861905b8052382d1b4d17940ae4b10771f"}
{"doc_id": "arxiv:2305.09781#related-work:part-2", "url": "https://arxiv.org/abs/2305.09781", "anchor": "#related-work:part-2", "type": "paper", "title": "", "section": "Related Work", "text": "reduce LLM inference latency while compromising the predictive performance of the LLM. For example, prior work proposed to leverage weight/activation quantization of LLMs to reduce the memory and computa- tion requirements of serving these LLMs [8, 11, 35, 52, 54]. Recent work further explores a variety of structured prun- ing techniques for accelerating Transformer-based architec- tures [10, 17, 49]. A key difference between SpecInfer and these prior works is that SpecInfer does not directly reduce the computation requirement for performing LLM inference, but instead reorganizing LLM inference computation in a more parallelizable way, which reduces memory accesses and inference latency at the cost of manageable memory and computation overheads. Tree-structured attention. Nguyen et al. [31] introduced tree-structured attention, a technique that lets a Transformer model capture the hierarchical composition of input text by running the model on the text’s parse tree. To process with attention, it uses a one-on-one mapping to encode and de- code the tree. There are two key differences from SpecInfer’s tree-based decoding. First, SpecInfer uses a tree to combine candidate sequences to condense prefixes, whereas Nguyen et al. represent a single sequence with its parse tree. SpecInfer does not incorporate parse tree into the LLM, but accelerates inference by verifying decoded sequences in parallel. Second, SpecInfer’s attention outputs a token sequence, not a tree. Multi-sample decoding techniques. Like tree-based spec- ulative inference, beam search, top-k sampling, and top-p sam- pling consider multiple candidate token sequences at each step and can prune low-probability options. However, tree- based decoding in SpecInfer speculatively predicts and veri- fies multiple candidates in parallel against an LLM to reduce decoding iterations and latency, leveraging small specula- tive models (SSMs). In contrast, beam search and top-k/top- p sampling are decoding strategies applied directly to the LLM’s output probabilities to generate high-probability se- quences without reducing decoding steps. SpecInfer supports beam search, top-k sampling, and top-p sampling. These tech- niques are orthogonal decoding optimizations and can be combined with tree-based speculative decoding. 8", "source": "arxiv_pdf", "published": "", "tokens": 329, "sha256": "7122925af2939afa27e43c4e2e082cbf87ad3342f9b6dcc2808a90faf993ac51"}
{"doc_id": "arxiv:2305.09781#conclusion", "url": "https://arxiv.org/abs/2305.09781", "anchor": "#conclusion", "type": "paper", "title": "", "section": "Conclusion", "text": "This paper introduces SpecInfer, a system that accelerates generative LLM inference with tree-based speculative in- ference and verification. A key insight behind SpecInfer is to simultaneously consider a diversity of speculation can- didates to efficiently predict the LLM’s outputs, which are organized as a token tree and verified against the LLM in parallel using a tree-based parallel decoding mechanism. SpecInfer significantly reduces the memory accesses to the LLM’s parameters and the end-to-end LLM inference latency for both distributed and offloading-based LLM inference. Acknowledgement We thank Tianqi Chen, Bohan Hou, Hongyi Jin, the anony- mous ASPLOS reviewers, and our shepherd Shan Lu for their feedback on this work. This research is partially supported by NSF awards CNS-2147909, CNS-2211882, and CNS-2239351, and research awards from Amazon, Cisco, Google, Meta, Oracle, Qualcomm, and Samsung. 14 A Artifact Appendix A.1", "source": "arxiv_pdf", "published": "", "tokens": 136, "sha256": "24c4a9074efb4bd2f1e68edf70fa4a361549a29c2b92b7320fe8547b4fc760cc"}
{"doc_id": "arxiv:2305.09781#abstract:part-1", "url": "https://arxiv.org/abs/2305.09781", "anchor": "#abstract:part-1", "type": "paper", "title": "", "section": "Abstract", "text": "The artifact contains the code to run SpecInfer, as well as the datasets and scripts that can be used to reproduce the experiments in the paper. A.2 Artifact check-list (meta-information) • Algorithm: Tree-based Speculative Inference • Program: spec_infer.cc, incr_decoding.cc • Compilation: CMake • Run-time environment: CUDA, NCCL, MPI, UCX, Python3. • Hardware: Two AWS g5.12xlarge instances, each with 4 NVIDIA A10 24GB GPUs, 48 CPU cores, and 192 GB DRAM. • Metrics: End to-end average latency • Output: End-to-end latency • Experiments: Server-grade GPU inference, Offloading- based inference • How much disk space required (approximately)?: 350GB per node • How much time is needed to prepare workflow (ap- proximately)?: 2h • How much time is needed to complete experiments (approximately)?: 6h • Publicly available?: Yes • Code licenses (if publicly available)?: Apache License v2.0 • Data licenses (if publicly available)?: LLAMA is under the GNU license and OPT is under a Non-commercial license • Archived (provide DOI)?: https://doi.org/10.5281/zenodo. 10854410. A.3 Description A.3.1 How to access. The artifact is released on Github: https://github.com/goliaro/specinfer-ae. The repository con- tains SpecInfer’s source code, and the instructions to build the framework. We also provide scripts to reproduce the experiments from the paper. To clone the repository, use the following command (make sure to pass the –recursive flag): git clone --recursive \\ https :// github.com/goliaro/specinfer -ae.git A.3.2 Hardware dependencies. We run out experiments on two AWS g5.12xlarge instances, each with 4 NVIDIA A10 24GB GPUs, 48 CPU cores, and 192 GB DRAM. We provide instructions to create and setup the instances. A.3.3 Software dependencies. The following software is required: CUDA 12.1, NCCL, Rust, CMake and Python3. Further, UCX and MPI are required for the multinode experi- ments. Additional Python dependencies are listed here: https: //github.com/flexflow/FlexFlow/blob/inference/requirements. txt. We recommend using the Deep Learning OSS Nvidia Driver AMI GPU PyTorch 2.1.0 (Ubuntu 20.04) AMI, and provide scripts and a conda environment to install all the remaining dependencies. A.3.4 Models. We use the following LLM/SSM models for our experiments (for each model, we specify in parentheses the corresponding HuggingFace repository): LLaMA-68M (JackFram/llama-68m), LLaMA-7B (huggyllama/llama-7b), LLaMA-65B (huggyllama/llama-65b), OPT-125M (facebook/opt- 125m), OPT-13B (facebook/opt-13b), OPT-125M (facebook/opt- 30b). You can download all these models with the script: ./ download_models.sh A.4 Installation To reproduce the experiments, you will need access to two AWS g5.12xlarge instances (or other machines with the same GPU/CPU/network specs). If you are using the preconfigured instances we provided, you can skip this step. Launching the instances. Launch two AWS g5.12xlarge instances using the Deep Learning OSS Nvidia Driver AMI GPU PyTorch 2.1.0 (Ubuntu 20.04) AMI. Make sure to place the instances in a placement group that utilizes the cluster strategy to achieve low-latency network performance. Attach the same security group to all instances and add an inbound rule in the security group to allow all incoming traffic from the same security group. For example, you can add the following rule: Type: All TCP, Source: Anywhere- IPv4. Installing the prerequisites. After gaining access to the AWS instances, install the prerequisites by following the steps below. First, activate the conda shell support by running", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "0251baf115fc5cf9d184a1dd67b6c199eb93a891e028b556d4e0c5dca2dbb4b6"}
{"doc_id": "arxiv:2305.09781#abstract:part-2", "url": "https://arxiv.org/abs/2305.09781", "anchor": "#abstract:part-2", "type": "paper", "title": "", "section": "Abstract", "text": "a placement group that utilizes the cluster strategy to achieve low-latency network performance. Attach the same security group to all instances and add an inbound rule in the security group to allow all incoming traffic from the same security group. For example, you can add the following rule: Type: All TCP, Source: Anywhere- IPv4. Installing the prerequisites. After gaining access to the AWS instances, install the prerequisites by following the steps below. First, activate the conda shell support by running conda init bash, and then restarting the shell session. Next, create the conda environment with all the required dependencies by running: conda env create -f FlexFlow/conda/flexflow.yml conda activate flexflow Multinode setup. Download and build UCX by running the install_ucx.sh script. Next, if you are running SpecIn- fer on two AWS instances, you will need to configure MPI so that the two instances are mutually accessible. Pick a main node, and create a SSH key pair with: ssh -keygen -t ed25519 Append the contents of the public key (~/.ssh/id_ed25519.pub) to the ~/.ssh/authorized_keys file on BOTH the main and secondary machine. Note that if the .ssh folder or the authorized_keys file do not exist, you will need to create them manually. Finally, create a file at the path ~/hostfile with the following contents: <main_node_private_ip > slots =4 <secondary_node_private_ip > slots =4 15 replacing <main_node_private_ip> and <secondary_node_private_ip> with the private IP addresses of the two machines, and the number of slots with the number of GPUs available (if you are using the recommended AWS instances, you will use a value of 4). You can find each machine’s private IP address by running the command (and use the first IP value that is printed): hostname -I Install SpecInfer. To install SpecInfer, run the script: ./ install_specinfer.sh A.5 Basic Test To ensure that SpecInfer is installed correctly and is func- tional, run the basic_test.sh script. This script will test the basic incremental decoding and speculative inference functionalities, on both single and multi nodes. It will also test the support for offloading. The test passes if it prints the \"Test passed!\" message. A.6 Experiment workflow The artifact comes with scripts to gather the data that can be used to reproduce the results from the paper. It also comes with scripts that can be used to convert the output data into CSV format for plotting. Running Experiments. We run the following two exper- iments to evaluate SpecInfer under different hardware setups. The output data will be saved to the FlexFlow/inference/output path. • Server-grade GPU evaluation. This experiment tests the performance of SpecInfer on server-grade GPUs. The LLMs and SSMs are loaded in GPU memory, and we measure the end-to-end inference latency using 1 node, and 2 nodes. In the single node case, we measure the performance using 1 GPU, or 4 GPUs. In the multin- ode case, we use 4GPUs per node. The experiments use LLAMA-7B, OPT-30B and LLAMA-65B as the LLMs, and LLAMA-68M and OPT-125M as SSMs. The exper- iment runs SpecInfer in three different modes: incre- mental decoding, sequence-based speculative decod- ing, and tree-based speculative decoding. The", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "f096ab9f24da9b37bae343c6e0fce2f60158b7a9c78418349c7095bdf4e61052"}
{"doc_id": "arxiv:2305.09781#abstract:part-3", "url": "https://arxiv.org/abs/2305.09781", "anchor": "#abstract:part-3", "type": "paper", "title": "", "section": "Abstract", "text": "are loaded in GPU memory, and we measure the end-to-end inference latency using 1 node, and 2 nodes. In the single node case, we measure the performance using 1 GPU, or 4 GPUs. In the multin- ode case, we use 4GPUs per node. The experiments use LLAMA-7B, OPT-30B and LLAMA-65B as the LLMs, and LLAMA-68M and OPT-125M as SSMs. The exper- iment runs SpecInfer in three different modes: incre- mental decoding, sequence-based speculative decod- ing, and tree-based speculative decoding. The former two are used to obtain data for the ablation study, and the latter is the novel inference mode proposed by SpecInfer, and will be deployed by the user. To run the server-grade GPU evaluation, run: ./ server_gpu_experiments.sh • Offloading evaluation. This experiment tests the performance of SpecInfer when loading only a subset of parameters in GPU memory, while offloading the remaining ones on CPU DRAM. This technique is used to perform inference when the target model is larger than the available GPU memory. In the experiment, SpecInfer uses a single GPU and swaps the model’s weights to and from the CPU. To run the offloading evaluation, run: ./ offloading_experiments.sh Third-party frameworks. Please follow the vLLM, Faster- Transformer, and HuggingFace TGI, and FlexGen official doc- umentation to reproduce the performance of the third-party frameworks under the experiment scenarios. Output data. The scripts above will generate data at the FlexFlow/inference/output path. For each scenario, a .txt file contains the generated output for each prompt, and a .out file contains the stdout logs. The quality of the generated output can be evaluated visually and compared with the output from third-party inference frameworks. We provide scripts to parse the raw output data and generate CSV files that can be used to generate the paper’s figures. The README provides all details on the scripts and the mapping between CSV files and figures. A.7 Evaluation and expected results The data from the CSV files should show similar performance to the figures from the paper. Some variability is to be ex- pected, but overall, SpecInfer should behave according to Figures 7-11 from the paper. A.8 Experiment customization Users can edit the configuration parameters from the evalu- ation scripts to change various parameters, such as the num- ber of GPUs/CPUs, GPU/CPU memory, batch size, LLM/SSM models used, prompt dataset, full vs. half-precision, and the maximum number of tokens to generate.", "source": "arxiv_pdf", "published": "", "tokens": 393, "sha256": "83aca19311e9cc1a1cc26daa98ed117f5334b5f4ff6aeae3bd12c2eb2fb08c3c"}
{"doc_id": "arxiv:2412.19437#abstract", "url": "https://arxiv.org/abs/2412.19437", "anchor": "#abstract", "type": "paper", "title": "", "section": "Abstract", "text": "We present DeepSeek-V3, a strong Mixture-of-Experts (MoE) language model with 671B total parameters with 37B activated for each token. To achieve efficient inference and cost-effective training, DeepSeek-V3 adopts Multi-head Latent Attention (MLA) and DeepSeekMoE architec- tures, which were thoroughly validated in DeepSeek-V2. Furthermore, DeepSeek-V3 pioneers an auxiliary-loss-free strategy for load balancing and sets a multi-token prediction training objective for stronger performance. We pre-train DeepSeek-V3 on 14.8 trillion diverse and high-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to fully harness its capabilities. Comprehensive evaluations reveal that DeepSeek-V3 outperforms other open-source models and achieves performance comparable to leading closed-source models. Despite its excellent performance, DeepSeek-V3 requires only 2.788M H800 GPU hours for its full training. In addition, its training process is remarkably stable. Throughout the entire training process, we did not experience any irrecoverable loss spikes or perform any rollbacks. The model checkpoints are available at https://github.com/deepseek-ai/DeepSeek-V3. MMLU-Pro (EM) GPQA-Diamond (Pass@1) MATH 500 (EM) AIME 2024 (Pass@1) Codeforces (Percentile) SWE-bench Verified (Resolved) 0 20 40 60 80 100 Accuracy / Percentile (%) 75.9 59.1 90.2 39.2 51.6 42.0 66.2 41.3 74.7 16.7 35.6 22.6 71.6 49.0 80.0 23.3 24.8 23.8 73.3 51.1 73.8 23.3 25.3 24.5 72.6 49.9 74.6 9.3 23.6 38.8 78.0 65.0 78.3 16.0 20.3 50.8 DeepSeek-V3 DeepSeek-V2.5 Qwen2.5-72B-Inst Llama-3.1-405B-Inst GPT-4o-0513 Claude-3.5-Sonnet-1022 Figure 1 | Benchmark performance of DeepSeek-V3 and its counterparts. arXiv:2412.19437v2 [cs.CL] 18 Feb 2025 Contents 1", "source": "arxiv_pdf", "published": "", "tokens": 232, "sha256": "d43da7427870e4981bd2d893aa6edd81374a3d5b12afeac2c442143511ff9b64"}
{"doc_id": "arxiv:2412.19437#introduction:part-1", "url": "https://arxiv.org/abs/2412.19437", "anchor": "#introduction:part-1", "type": "paper", "title": "", "section": "Introduction", "text": "4 2 Architecture 6 2.1 Basic Architecture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6 2.1.1 Multi-Head Latent Attention . . . . . . . . . . . . . . . . . . . . . . . . . . 7 2.1.2 DeepSeekMoE with Auxiliary-Loss-Free Load Balancing . . . . . . . . . . 8 2.2 Multi-Token Prediction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10 3 Infrastructures 11 3.1 Compute Clusters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11 3.2 Training Framework . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12 3.2.1 DualPipe and Computation-Communication Overlap . . . . . . . . . . . . 12 3.2.2 Efficient Implementation of Cross-Node All-to-All Communication . . . . 13 3.2.3 Extremely Memory Saving with Minimal Overhead . . . . . . . . . . . . . 14 3.3 FP8 Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14 3.3.1 Mixed Precision Framework . . . . . . . . . . . . . . . . . . . . . . . . . . 15 3.3.2 Improved Precision from Quantization and Multiplication . . . . . . . . . 16 3.3.3 Low-Precision Storage and Communication . . . . . . . . . . . . . . . . . 18 3.4 Inference and Deployment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18 3.4.1 Prefilling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19 3.4.2 Decoding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19 3.5 Suggestions on Hardware Design . . . . . . . . .", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "59eb66d078db5de4b44ff08b5abe7e6fa9cc97fa2abc59c96a3b29a374ce728e"}
{"doc_id": "arxiv:2412.19437#introduction:part-2", "url": "https://arxiv.org/abs/2412.19437", "anchor": "#introduction:part-2", "type": "paper", "title": "", "section": "Introduction", "text": ". . . . . . . . . . . . . . . . . . . . . . . . . 19 3.4.2 Decoding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19 3.5 Suggestions on Hardware Design . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20 3.5.1 Communication Hardware . . . . . . . . . . . . . . . . . . . . . . . . . . . 20 3.5.2 Compute Hardware . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20 4 Pre-Training 21 4.1 Data Construction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21 4.2 Hyper-Parameters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22 4.3 Long Context Extension . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23 4.4 Evaluations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24 4.4.1 Evaluation Benchmarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24 4.4.2 Evaluation Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24 4.5 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26 4.5.1 Ablation Studies for Multi-Token Prediction . . . . . . . . . . . . . . . . . 26 4.5.2 Ablation Studies for the Auxiliary-Loss-Free Balancing Strategy . . . . . . 26 2 4.5.3 Batch-Wise Load Balance VS. Sequence-Wise Load Balance . . . . . . . . . 27 5 Post-Training 28 5.1 Supervised Fine-Tuning . . . .", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "7c06d1d8a0650c8ab2303d6b1dcb217236852a1d679c57a35b88b995870cb766"}
{"doc_id": "arxiv:2412.19437#introduction:part-3", "url": "https://arxiv.org/abs/2412.19437", "anchor": "#introduction:part-3", "type": "paper", "title": "", "section": "Introduction", "text": ". . . . . . . . . . . 26 4.5.1 Ablation Studies for Multi-Token Prediction . . . . . . . . . . . . . . . . . 26 4.5.2 Ablation Studies for the Auxiliary-Loss-Free Balancing Strategy . . . . . . 26 2 4.5.3 Batch-Wise Load Balance VS. Sequence-Wise Load Balance . . . . . . . . . 27 5 Post-Training 28 5.1 Supervised Fine-Tuning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28 5.2 Reinforcement Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29 5.2.1 Reward Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29 5.2.2 Group Relative Policy Optimization . . . . . . . . . . . . . . . . . . . . . . 30 5.3 Evaluations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30 5.3.1 Evaluation Settings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30 5.3.2 Standard Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31 5.3.3 Open-Ended Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33 5.3.4 DeepSeek-V3 as a Generative Reward Model . . . . . . . . . . . . . . . . . 33 5.4 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34 5.4.1 Distillation from DeepSeek-R1 . . . . . . . . . . . . . . . . . . . . . . . . . 34 5.4.2 Self-Rewarding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34 5.4.3 Multi-Token Prediction Evaluation . . . . . . . . . . . .", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "b78f9748eb145b8f47c097383ada37202d93ffa9023729045b19dd093836c657"}
{"doc_id": "arxiv:2412.19437#introduction:part-4", "url": "https://arxiv.org/abs/2412.19437", "anchor": "#introduction:part-4", "type": "paper", "title": "", "section": "Introduction", "text": "DeepSeek-R1 . . . . . . . . . . . . . . . . . . . . . . . . . 34 5.4.2 Self-Rewarding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34 5.4.3 Multi-Token Prediction Evaluation . . . . . . . . . . . . . . . . . . . . . . . 35 6 Conclusion, Limitations, and Future Directions 35 A Contributions and Acknowledgments 45 B Ablation Studies for Low-Precision Training 47 B.1 FP8 v.s. BF16 Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47 B.2 Discussion About Block-Wise Quantization . . . . . . . . . . . . . . . . . . . . . . 47 C Expert Specialization Patterns of the 16B Aux-Loss-Based and Aux-Loss-Free Models 48 3 1. Introduction In recent years, Large Language Models (LLMs) have been undergoing rapid iteration and evolution (Anthropic, 2024; Google, 2024; OpenAI, 2024a), progressively diminishing the gap to- wards Artificial General Intelligence (AGI). Beyond closed-source models, open-source models, including DeepSeek series (DeepSeek-AI, 2024a,b,c; Guo et al., 2024), LLaMA series (AI@Meta, 2024a,b; Touvron et al., 2023a,b), Qwen series (Qwen, 2023, 2024a,b), and Mistral series (Jiang et al., 2023; Mistral, 2024), are also making significant strides, endeavoring to close the gap with their closed-source counterparts. To further push the boundaries of open-source model capa- bilities, we scale up our models and introduce DeepSeek-V3, a large Mixture-of-Experts (MoE) model with 671B parameters, of which 37B are activated for each token. With a forward-looking perspective, we consistently strive for strong model performance and economical costs. Therefore, in terms of architecture, DeepSeek-V3 still adopts Multi-head Latent Attention (MLA) (DeepSeek-AI, 2024c) for efficient inference and DeepSeekMoE (Dai et al., 2024) for cost-effective training. These two architectures have been validated in DeepSeek- V2 (DeepSeek-AI, 2024c), demonstrating their capability to maintain robust model performance while achieving efficient training and inference. Beyond the basic architecture, we implement two additional strategies to further enhance the model capabilities. Firstly, DeepSeek-V3 pi- oneers an auxiliary-loss-free strategy (Wang et al., 2024a) for load balancing, with the aim of minimizing the adverse impact on model performance that arises from the effort to encourage load balancing. Secondly, DeepSeek-V3 employs a multi-token prediction training objective, which we have observed to enhance the overall performance on evaluation benchmarks. In order to achieve efficient training, we support the FP8 mixed precision training and implement comprehensive optimizations for the training framework. Low-precision training has emerged as a promising solution for efficient training (Dettmers et al., 2022; Kalamkar et al., 2019; Narang et al., 2017; Peng et al., 2023b), its evolution being closely tied to advancements in hardware capabilities (Luo et al., 2024; Micikevicius et al., 2022; Rouhani et al., 2023a).", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "05d207c732c4e0c08131c78e0095c115442049dde2d3e2979861d0999d7e7fa6"}
{"doc_id": "arxiv:2412.19437#introduction:part-5", "url": "https://arxiv.org/abs/2412.19437", "anchor": "#introduction:part-5", "type": "paper", "title": "", "section": "Introduction", "text": "have observed to enhance the overall performance on evaluation benchmarks. In order to achieve efficient training, we support the FP8 mixed precision training and implement comprehensive optimizations for the training framework. Low-precision training has emerged as a promising solution for efficient training (Dettmers et al., 2022; Kalamkar et al., 2019; Narang et al., 2017; Peng et al., 2023b), its evolution being closely tied to advancements in hardware capabilities (Luo et al., 2024; Micikevicius et al., 2022; Rouhani et al., 2023a). In this work, we introduce an FP8 mixed precision training framework and, for the first time, validate its effectiveness on an extremely large-scale model. Through the support for FP8 computation and storage, we achieve both accelerated training and reduced GPU memory usage. As for the training framework, we design the DualPipe algorithm for efficient pipeline parallelism, which has fewer pipeline bubbles and hides most of the communication during training through computation-communication overlap. This overlap ensures that, as the model further scales up, as long as we maintain a constant computation-to-communication ratio, we can still employ fine-grained experts across nodes while achieving a near-zero all-to-all communication overhead. In addition, we also develop efficient cross-node all-to-all communication kernels to fully utilize InfiniBand (IB) and NVLink bandwidths. Furthermore, we meticulously optimize the memory footprint, making it possible to train DeepSeek-V3 without using costly tensor parallelism. Combining these efforts, we achieve high training efficiency. During pre-training, we train DeepSeek-V3 on 14.8T high-quality and diverse tokens. The pre-training process is remarkably stable. Throughout the entire training process, we did not encounter any irrecoverable loss spikes or have to roll back. Next, we conduct a two-stage context length extension for DeepSeek-V3. In the first stage, the maximum context length is extended to 32K, and in the second stage, it is further extended to 128K. Following this, we conduct post-training, including Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) on the base model of DeepSeek-V3, to align it with human preferences and further unlock its potential. During the post-training stage, we distill the reasoning capability from the DeepSeek- R1 series of models, and meanwhile carefully maintain the balance between model accuracy 4 Training Costs Pre-Training Context Extension Post-Training Total in H800 GPU Hours 2664K 119K 5K 2788K in USD $5.328M $0.238M $0.01M $5.576M Table 1 | Training costs of DeepSeek-V3, assuming the rental price of H800 is $2 per GPU hour. and generation length. We evaluate DeepSeek-V3 on a comprehensive array of benchmarks. Despite its economical training costs, comprehensive evaluations reveal that DeepSeek-V3-Base has emerged as the strongest open-source base model currently available, especially in code and math. Its chat version also outperforms other open-source models and achieves performance comparable to leading closed-source models, including GPT-4o and Claude-3.5-Sonnet, on a series of standard and open-ended benchmarks. Lastly, we emphasize again the economical training costs of DeepSeek-V3, summarized in Table 1, achieved through our optimized co-design of algorithms, frameworks, and hardware. During the pre-training stage, training DeepSeek-V3 on each trillion tokens requires only 180K H800 GPU hours, i.e., 3.7 days on our cluster with 2048 H800 GPUs. Consequently, our pre- training", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "5d70e929024523cfebde34757db573231e6ab6c93807806751bed26846b6c8c7"}
{"doc_id": "arxiv:2412.19437#introduction:part-6", "url": "https://arxiv.org/abs/2412.19437", "anchor": "#introduction:part-6", "type": "paper", "title": "", "section": "Introduction", "text": "version also outperforms other open-source models and achieves performance comparable to leading closed-source models, including GPT-4o and Claude-3.5-Sonnet, on a series of standard and open-ended benchmarks. Lastly, we emphasize again the economical training costs of DeepSeek-V3, summarized in Table 1, achieved through our optimized co-design of algorithms, frameworks, and hardware. During the pre-training stage, training DeepSeek-V3 on each trillion tokens requires only 180K H800 GPU hours, i.e., 3.7 days on our cluster with 2048 H800 GPUs. Consequently, our pre- training stage is completed in less than two months and costs 2664K GPU hours. Combined with 119K GPU hours for the context length extension and 5K GPU hours for post-training, DeepSeek-V3 costs only 2.788M GPU hours for its full training. Assuming the rental price of the H800 GPU is $2 per GPU hour, our total training costs amount to only $5.576M. Note that the aforementioned costs include only the official training of DeepSeek-V3, excluding the costs associated with prior research and ablation experiments on architectures, algorithms, or data. Our main contribution includes: Architecture: Innovative Load Balancing Strategy and Training Objective • On top of the efficient architecture of DeepSeek-V2, we pioneer an auxiliary-loss-free strategy for load balancing, which minimizes the performance degradation that arises from encouraging load balancing. • We investigate a Multi-Token Prediction (MTP) objective and prove it beneficial to model performance. It can also be used for speculative decoding for inference acceleration. Pre-Training: Towards Ultimate Training Efficiency • We design an FP8 mixed precision training framework and, for the first time, validate the feasibility and effectiveness of FP8 training on an extremely large-scale model. • Through the co-design of algorithms, frameworks, and hardware, we overcome the communication bottleneck in cross-node MoE training, achieving near-full computation- communication overlap. This significantly enhances our training efficiency and reduces the training costs, enabling us to further scale up the model size without additional overhead. • At an economical cost of only 2.664M H800 GPU hours, we complete the pre-training of DeepSeek-V3 on 14.8T tokens, producing the currently strongest open-source base model. The subsequent training stages after pre-training require only 0.1M GPU hours. Post-Training: Knowledge Distillation from DeepSeek-R1 • We introduce an innovative methodology to distill reasoning capabilities from the long- Chain-of-Thought (CoT) model, specifically from one of the DeepSeek R1 series models, into standard LLMs, particularly DeepSeek-V3. Our pipeline elegantly incorporates the 5 verification and reflection patterns of R1 into DeepSeek-V3 and notably improves its reasoning performance. Meanwhile, we also maintain control over the output style and length of DeepSeek-V3. Summary of Core Evaluation Results • Knowledge: (1) On educational benchmarks such as MMLU, MMLU-Pro, and GPQA, DeepSeek-V3 outperforms all other open-source models, achieving 88.5 on MMLU, 75.9 on MMLU-Pro, and 59.1 on GPQA. Its performance is comparable to leading closed-source models like GPT-4o and Claude-Sonnet-3.5, narrowing the gap between open-source and closed-source models in this domain. (2) For factuality benchmarks, DeepSeek-V3 demonstrates superior performance among open-source models on both SimpleQA and Chinese SimpleQA. While it trails behind GPT-4o and Claude-Sonnet-3.5 in English factual knowledge (SimpleQA), it surpasses these models in Chinese factual knowledge (Chinese", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "0fd1a1704c40d96438191c59aeaa90e89758cda6064d102453c69f1256f533f7"}
{"doc_id": "arxiv:2412.19437#introduction:part-7", "url": "https://arxiv.org/abs/2412.19437", "anchor": "#introduction:part-7", "type": "paper", "title": "", "section": "Introduction", "text": "and GPQA, DeepSeek-V3 outperforms all other open-source models, achieving 88.5 on MMLU, 75.9 on MMLU-Pro, and 59.1 on GPQA. Its performance is comparable to leading closed-source models like GPT-4o and Claude-Sonnet-3.5, narrowing the gap between open-source and closed-source models in this domain. (2) For factuality benchmarks, DeepSeek-V3 demonstrates superior performance among open-source models on both SimpleQA and Chinese SimpleQA. While it trails behind GPT-4o and Claude-Sonnet-3.5 in English factual knowledge (SimpleQA), it surpasses these models in Chinese factual knowledge (Chinese SimpleQA), highlighting its strength in Chinese factual knowledge. • Code, Math, and Reasoning: (1) DeepSeek-V3 achieves state-of-the-art performance on math-related benchmarks among all non-long-CoT open-source and closed-source models. Notably, it even outperforms o1-preview on specific benchmarks, such as MATH-500, demonstrating its robust mathematical reasoning capabilities. (2) On coding-related tasks, DeepSeek-V3 emerges as the top-performing model for coding competition benchmarks, such as LiveCodeBench, solidifying its position as the leading model in this domain. For engineering-related tasks, while DeepSeek-V3 performs slightly below Claude-Sonnet-3.5, it still outpaces all other models by a significant margin, demonstrating its competitiveness across diverse technical benchmarks. In the remainder of this paper, we first present a detailed exposition of our DeepSeek-V3 model architecture (Section 2). Subsequently, we introduce our infrastructures, encompassing our compute clusters, the training framework, the support for FP8 training, the inference deployment strategy, and our suggestions on future hardware design. Next, we describe our pre-training process, including the construction of training data, hyper-parameter settings, long- context extension techniques, the associated evaluations, as well as some discussions (Section 4). Thereafter, we discuss our efforts on post-training, which include Supervised Fine-Tuning (SFT), Reinforcement Learning (RL), the corresponding evaluations, and discussions (Section 5). Lastly, we conclude this work, discuss existing limitations of DeepSeek-V3, and propose potential directions for future research (Section 6). 2. Architecture We first introduce the basic architecture of DeepSeek-V3, featured by Multi-head Latent Atten- tion (MLA) (DeepSeek-AI, 2024c) for efficient inference and DeepSeekMoE (Dai et al., 2024) for economical training. Then, we present a Multi-Token Prediction (MTP) training objective, which we have observed to enhance the overall performance on evaluation benchmarks. For other minor details not explicitly mentioned, DeepSeek-V3 adheres to the settings of DeepSeek- V2 (DeepSeek-AI, 2024c). 2.1. Basic Architecture The basic architecture of DeepSeek-V3 is still within the Transformer (Vaswani et al., 2017) framework. For efficient inference and economical training, DeepSeek-V3 also adopts MLA and DeepSeekMoE, which have been thoroughly validated by DeepSeek-V2. Compared with DeepSeek-V2, an exception is that we additionally introduce an auxiliary-loss-free load balancing 6 … Router Input Hidden 𝐮𝐮𝑡𝑡 Output Hidden 𝐡𝐡𝑡𝑡 ′ 1 𝑁𝑁𝑠𝑠 1 2 𝑁𝑁𝑟𝑟-1 𝑁𝑁𝑟𝑟 Shared Expert Routed Expert Top-𝐾𝐾𝑟𝑟 Attention Feed-Forward Network … 3 4 RMSNorm RMSNorm Transformer Block ×𝐿𝐿 DeepSeekMoE 0 Input Hidden 𝐡𝐡𝑡𝑡 Multi-Head Latent Attention (MLA) 0 {𝐪𝐪𝑡𝑡,𝑖𝑖 𝐶𝐶} {𝐯𝐯𝑡𝑡,𝑖𝑖 𝐶𝐶} {𝐤𝐤𝑡𝑡,𝑖𝑖 𝐶𝐶} Latent 𝐜𝐜𝑡𝑡 𝐾𝐾𝐾𝐾 Latent 𝐜𝐜𝑡𝑡 𝑄𝑄 {𝐪𝐪𝑡𝑡,𝑖𝑖 𝑅𝑅} 𝐤𝐤𝑡𝑡 𝑅𝑅 Cached During Inference Multi-Head Attention concatenate concatenate {[𝐪𝐪𝑡𝑡,𝑖𝑖 𝐶𝐶; 𝐪𝐪𝑡𝑡,𝑖𝑖 𝑅𝑅]} {[𝐤𝐤𝑡𝑡,𝑖𝑖 𝐶𝐶; 𝐤𝐤𝑡𝑡 𝑅𝑅]} … Output Hidden 𝐮𝐮𝑡𝑡 … … … … … 1 … … … … apply RoPE apply RoPE Figure 2 | Illustration of the basic", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "ca9a6df2f18bb4f22160247bfabf1c838a5d7c54dcc9df20142ac2783a15daf9"}
{"doc_id": "arxiv:2412.19437#introduction:part-8", "url": "https://arxiv.org/abs/2412.19437", "anchor": "#introduction:part-8", "type": "paper", "title": "", "section": "Introduction", "text": "Routed Expert Top-𝐾𝐾𝑟𝑟 Attention Feed-Forward Network … 3 4 RMSNorm RMSNorm Transformer Block ×𝐿𝐿 DeepSeekMoE 0 Input Hidden 𝐡𝐡𝑡𝑡 Multi-Head Latent Attention (MLA) 0 {𝐪𝐪𝑡𝑡,𝑖𝑖 𝐶𝐶} {𝐯𝐯𝑡𝑡,𝑖𝑖 𝐶𝐶} {𝐤𝐤𝑡𝑡,𝑖𝑖 𝐶𝐶} Latent 𝐜𝐜𝑡𝑡 𝐾𝐾𝐾𝐾 Latent 𝐜𝐜𝑡𝑡 𝑄𝑄 {𝐪𝐪𝑡𝑡,𝑖𝑖 𝑅𝑅} 𝐤𝐤𝑡𝑡 𝑅𝑅 Cached During Inference Multi-Head Attention concatenate concatenate {[𝐪𝐪𝑡𝑡,𝑖𝑖 𝐶𝐶; 𝐪𝐪𝑡𝑡,𝑖𝑖 𝑅𝑅]} {[𝐤𝐤𝑡𝑡,𝑖𝑖 𝐶𝐶; 𝐤𝐤𝑡𝑡 𝑅𝑅]} … Output Hidden 𝐮𝐮𝑡𝑡 … … … … … 1 … … … … apply RoPE apply RoPE Figure 2 | Illustration of the basic architecture of DeepSeek-V3. Following DeepSeek-V2, we adopt MLA and DeepSeekMoE for efficient inference and economical training. strategy (Wang et al., 2024a) for DeepSeekMoE to mitigate the performance degradation induced by the effort to ensure load balance. Figure 2 illustrates the basic architecture of DeepSeek-V3, and we will briefly review the details of MLA and DeepSeekMoE in this section. 2.1.1. Multi-Head Latent Attention For attention, DeepSeek-V3 adopts the MLA architecture. Let 𝑑denote the embedding dimen- sion, 𝑛ℎdenote the number of attention heads, 𝑑ℎdenote the dimension per head, and h𝑡∈R𝑑 denote the attention input for the 𝑡-th token at a given attention layer. The core of MLA is the low-rank joint compression for attention keys and values to reduce Key-Value (KV) cache during inference: c𝐾𝑉 𝑡 = 𝑊𝐷𝐾𝑉h𝑡, (1) [k𝐶 𝑡,1; k𝐶 𝑡,2; ...; k𝐶 𝑡,𝑛ℎ] = k𝐶 𝑡= 𝑊𝑈𝐾c𝐾𝑉 𝑡, (2) k𝑅 𝑡 = RoPE(𝑊𝐾𝑅h𝑡), (3) k𝑡,𝑖= [k𝐶 𝑡,𝑖; k𝑅 𝑡], (4) [v𝐶 𝑡,1; v𝐶 𝑡,2; ...; v𝐶 𝑡,𝑛ℎ] = v𝐶 𝑡= 𝑊𝑈𝑉c𝐾𝑉 𝑡, (5) 7 where c𝐾𝑉 𝑡 ∈R𝑑𝑐is the compressed latent vector for keys and values; 𝑑𝑐(≪𝑑ℎ𝑛ℎ) indicates the KV compression dimension; 𝑊𝐷𝐾𝑉∈R𝑑𝑐×𝑑denotes the down-projection matrix; 𝑊𝑈𝐾,𝑊𝑈𝑉∈R𝑑ℎ𝑛ℎ×𝑑𝑐 are the up-projection matrices for keys and values, respectively; 𝑊𝐾𝑅∈R𝑑𝑅 ℎ×𝑑is the matrix used to produce the decoupled key that carries Rotary Positional Embedding (RoPE) (Su et al., 2024); RoPE(·) denotes the operation that applies RoPE matrices; and [·; ·] denotes concatenation. Note that for MLA, only the blue-boxed vectors (i.e., c𝐾𝑉 𝑡 and k𝑅 𝑡) need to be cached during generation, which results in significantly reduced KV cache while maintaining performance comparable to standard Multi-Head Attention (MHA) (Vaswani et al., 2017). For the attention queries, we also perform a low-rank compression, which can reduce the activation memory during training: c𝑄 𝑡= 𝑊𝐷𝑄h𝑡, (6) [q𝐶 𝑡,1; q𝐶 𝑡,2; ...; q𝐶 𝑡,𝑛ℎ] = q𝐶 𝑡= 𝑊𝑈𝑄c𝑄 𝑡, (7) [q𝑅 𝑡,1; q𝑅 𝑡,2; ...; q𝑅 𝑡,𝑛ℎ] = q𝑅 𝑡= RoPE(𝑊𝑄𝑅c𝑄 𝑡), (8) q𝑡,𝑖= [q𝐶 𝑡,𝑖; q𝑅 𝑡,𝑖], (9) where c𝑄 𝑡 ∈R𝑑′ 𝑐is the compressed latent vector for queries; 𝑑′ 𝑐(≪𝑑ℎ𝑛ℎ) denotes the query compression dimension; 𝑊𝐷𝑄∈R𝑑′ 𝑐×𝑑,𝑊𝑈𝑄∈R𝑑ℎ𝑛ℎ×𝑑′ 𝑐are the down-projection and up-projection matrices for queries, respectively; and 𝑊𝑄𝑅∈R𝑑𝑅 ℎ𝑛ℎ×𝑑′ 𝑐is the matrix to produce the decoupled queries that carry RoPE. Ultimately, the attention queries (q𝑡,𝑖), keys (k𝑗,𝑖), and values (v𝐶 𝑗,𝑖) are combined to yield the final attention output u𝑡: o𝑡,𝑖= 𝑡∑︁ 𝑗=1 Softmax𝑗( q𝑇 𝑡,𝑖k𝑗,𝑖 √︃ 𝑑ℎ+ 𝑑𝑅 ℎ )v𝐶 𝑗,𝑖, (10) u𝑡= 𝑊𝑂[o𝑡,1; o𝑡,2; ...; o𝑡,𝑛ℎ], (11) where 𝑊𝑂∈R𝑑×𝑑ℎ𝑛ℎdenotes the output projection matrix. 2.1.2. DeepSeekMoE with Auxiliary-Loss-Free Load Balancing Basic Architecture of DeepSeekMoE. For Feed-Forward Networks (FFNs), DeepSeek-V3 employs the DeepSeekMoE architecture (Dai", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "6359d6be38fda08d2fe5b2a065ac8d342cb1b0820a21199dca8551ffc6207eda"}
{"doc_id": "arxiv:2412.19437#introduction:part-9", "url": "https://arxiv.org/abs/2412.19437", "anchor": "#introduction:part-9", "type": "paper", "title": "", "section": "Introduction", "text": "respectively; and 𝑊𝑄𝑅∈R𝑑𝑅 ℎ𝑛ℎ×𝑑′ 𝑐is the matrix to produce the decoupled queries that carry RoPE. Ultimately, the attention queries (q𝑡,𝑖), keys (k𝑗,𝑖), and values (v𝐶 𝑗,𝑖) are combined to yield the final attention output u𝑡: o𝑡,𝑖= 𝑡∑︁ 𝑗=1 Softmax𝑗( q𝑇 𝑡,𝑖k𝑗,𝑖 √︃ 𝑑ℎ+ 𝑑𝑅 ℎ )v𝐶 𝑗,𝑖, (10) u𝑡= 𝑊𝑂[o𝑡,1; o𝑡,2; ...; o𝑡,𝑛ℎ], (11) where 𝑊𝑂∈R𝑑×𝑑ℎ𝑛ℎdenotes the output projection matrix. 2.1.2. DeepSeekMoE with Auxiliary-Loss-Free Load Balancing Basic Architecture of DeepSeekMoE. For Feed-Forward Networks (FFNs), DeepSeek-V3 employs the DeepSeekMoE architecture (Dai et al., 2024). Compared with traditional MoE architectures like GShard (Lepikhin et al., 2021), DeepSeekMoE uses finer-grained experts and isolates some experts as shared ones. Let u𝑡denote the FFN input of the 𝑡-th token, we compute the FFN output h′ 𝑡as follows: h′ 𝑡= u𝑡+ 𝑁𝑠 ∑︁ 𝑖=1 FFN(𝑠) 𝑖 (u𝑡) + 𝑁𝑟 ∑︁ 𝑖=1 𝑔𝑖,𝑡FFN(𝑟) 𝑖 (u𝑡), (12) 𝑔𝑖,𝑡= 𝑔′ 𝑖,𝑡 Í𝑁𝑟 𝑗=1 𝑔′ 𝑗,𝑡 , (13) 𝑔′ 𝑖,𝑡= ( 𝑠𝑖,𝑡, 𝑠𝑖,𝑡∈Topk({𝑠𝑗,𝑡|1 ⩽𝑗⩽𝑁𝑟}, 𝐾𝑟), 0, otherwise, (14) 𝑠𝑖,𝑡= Sigmoid \u0000u𝑡 𝑇e𝑖 \u0001 , (15) 8 where 𝑁𝑠and 𝑁𝑟denote the numbers of shared experts and routed experts, respectively; FFN(𝑠) 𝑖 (·) and FFN(𝑟) 𝑖 (·) denote the 𝑖-th shared expert and the 𝑖-th routed expert, respectively; 𝐾𝑟denotes the number of activated routed experts; 𝑔𝑖,𝑡is the gating value for the 𝑖-th expert; 𝑠𝑖,𝑡is the token-to-expert affinity; e𝑖is the centroid vector of the 𝑖-th routed expert; and Topk(·, 𝐾) denotes the set comprising 𝐾highest scores among the affinity scores calculated for the 𝑡-th token and all routed experts. Slightly different from DeepSeek-V2, DeepSeek-V3 uses the sigmoid function to compute the affinity scores, and applies a normalization among all selected affinity scores to produce the gating values. Auxiliary-Loss-Free Load Balancing. For MoE models, an unbalanced expert load will lead to routing collapse (Shazeer et al., 2017) and diminish computational efficiency in scenarios with expert parallelism. Conventional solutions usually rely on the auxiliary loss (Fedus et al., 2021; Lepikhin et al., 2021) to avoid unbalanced load. However, too large an auxiliary loss will impair the model performance (Wang et al., 2024a). To achieve a better trade-off between load balance and model performance, we pioneer an auxiliary-loss-free load balancing strategy (Wang et al., 2024a) to ensure load balance. To be specific, we introduce a bias term 𝑏𝑖for each expert and add it to the corresponding affinity scores 𝑠𝑖,𝑡to determine the top-K routing: 𝑔′ 𝑖,𝑡= ( 𝑠𝑖,𝑡, 𝑠𝑖,𝑡+ 𝑏𝑖∈Topk({𝑠𝑗,𝑡+ 𝑏𝑗|1 ⩽𝑗⩽𝑁𝑟}, 𝐾𝑟), 0, otherwise. (16) Note that the bias term is only used for routing. The gating value, which will be multiplied with the FFN output, is still derived from the original affinity score 𝑠𝑖,𝑡. During training, we keep monitoring the expert load on the whole batch of each training step. At the end of each step, we will decrease the bias term by 𝛾if its corresponding expert is overloaded, and increase it by 𝛾if its corresponding expert is underloaded, where 𝛾is a hyper-parameter called bias update speed. Through the dynamic adjustment, DeepSeek-V3 keeps balanced expert load during training, and achieves better performance than models that encourage load balance through pure auxiliary losses. Complementary Sequence-Wise Auxiliary Loss. Although", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "68a5a6073295f6751febc778577e68f7bdf8a8cf6f47bbebda87ba7eeda5fe66"}
{"doc_id": "arxiv:2412.19437#introduction:part-10", "url": "https://arxiv.org/abs/2412.19437", "anchor": "#introduction:part-10", "type": "paper", "title": "", "section": "Introduction", "text": "keep monitoring the expert load on the whole batch of each training step. At the end of each step, we will decrease the bias term by 𝛾if its corresponding expert is overloaded, and increase it by 𝛾if its corresponding expert is underloaded, where 𝛾is a hyper-parameter called bias update speed. Through the dynamic adjustment, DeepSeek-V3 keeps balanced expert load during training, and achieves better performance than models that encourage load balance through pure auxiliary losses. Complementary Sequence-Wise Auxiliary Loss. Although DeepSeek-V3 mainly relies on the auxiliary-loss-free strategy for load balance, to prevent extreme imbalance within any single sequence, we also employ a complementary sequence-wise balance loss: LBal = 𝛼 𝑁𝑟 ∑︁ 𝑖=1 𝑓𝑖𝑃𝑖, (17) 𝑓𝑖= 𝑁𝑟 𝐾𝑟𝑇 𝑇 ∑︁ 𝑡=1 1 \u0000𝑠𝑖,𝑡∈Topk({𝑠𝑗,𝑡|1 ⩽𝑗⩽𝑁𝑟}, 𝐾𝑟)\u0001 , (18) 𝑠′ 𝑖,𝑡= 𝑠𝑖,𝑡 Í𝑁𝑟 𝑗=1 𝑠𝑗,𝑡 , (19) 𝑃𝑖= 1 𝑇 𝑇 ∑︁ 𝑡=1 𝑠′ 𝑖,𝑡, (20) where the balance factor 𝛼is a hyper-parameter, which will be assigned an extremely small value for DeepSeek-V3; 1(·) denotes the indicator function; and 𝑇denotes the number of tokens in a sequence. The sequence-wise balance loss encourages the expert load on each sequence to be balanced. 9 Main Model (Next Token Prediction) Embedding Layer Output Head Output Head Transformer Block Embedding Layer 𝑡𝑡2 𝑡𝑡3 𝑡𝑡4 𝑡𝑡1 𝑡𝑡3 𝑡𝑡4 𝑡𝑡5 𝑡𝑡2 RMSNorm RMSNorm Linear Projection MTP Module 1 (Next2 Token Prediction) Shared Shared concatenation Output Head Transformer Block Embedding Layer Linear Projection MTP Module 2 (Next3 Token Prediction) concatenation Shared Shared 𝑡𝑡3 𝑡𝑡4 𝑡𝑡5 𝑡𝑡2 𝑡𝑡4 𝑡𝑡5 𝑡𝑡6 𝑡𝑡3 𝑡𝑡5 𝑡𝑡6 𝑡𝑡7 𝑡𝑡4 𝑡𝑡4 𝑡𝑡5 𝑡𝑡6 𝑡𝑡3 Transformer Block × 𝐿𝐿 Transformer Block × 𝐿𝐿 Transformer Block × 𝐿𝐿 Transformer Block × 𝐿𝐿 Transformer Block × 𝐿𝐿 ··· Cross-Entropy Loss Cross-Entropy Loss Cross-Entropy Loss Input Tokens Target Tokens RMSNorm RMSNorm ℒMTP 1 ℒMTP 2 ℒ𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀 Figure 3 | Illustration of our Multi-Token Prediction (MTP) implementation. We keep the complete causal chain for the prediction of each token at each depth. Node-Limited Routing. Like the device-limited routing used by DeepSeek-V2, DeepSeek-V3 also uses a restricted routing mechanism to limit communication costs during training. In short, we ensure that each token will be sent to at most 𝑀nodes, which are selected according to the sum of the highest 𝐾𝑟 𝑀affinity scores of the experts distributed on each node. Under this constraint, our MoE training framework can nearly achieve full computation-communication overlap. No Token-Dropping. Due to the effective load balancing strategy, DeepSeek-V3 keeps a good load balance during its full training. Therefore, DeepSeek-V3 does not drop any tokens during training. In addition, we also implement specific deployment strategies to ensure inference load balance, so DeepSeek-V3 also does not drop tokens during inference. 2.2. Multi-Token Prediction Inspired by Gloeckle et al. (2024), we investigate and set a Multi-Token Prediction (MTP) objective for DeepSeek-V3, which extends the prediction scope to multiple future tokens at each position. On the one hand, an MTP objective densifies the training signals and may improve data efficiency. On the other hand, MTP may enable the model to pre-plan its representations for better prediction of future tokens. Figure 3 illustrates our implementation", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "d4bccb0f080e3e9e8d4633bb220764b4148c1c4fb56206960a21ddd5c3cfb169"}
{"doc_id": "arxiv:2412.19437#introduction:part-11", "url": "https://arxiv.org/abs/2412.19437", "anchor": "#introduction:part-11", "type": "paper", "title": "", "section": "Introduction", "text": "DeepSeek-V3 also does not drop tokens during inference. 2.2. Multi-Token Prediction Inspired by Gloeckle et al. (2024), we investigate and set a Multi-Token Prediction (MTP) objective for DeepSeek-V3, which extends the prediction scope to multiple future tokens at each position. On the one hand, an MTP objective densifies the training signals and may improve data efficiency. On the other hand, MTP may enable the model to pre-plan its representations for better prediction of future tokens. Figure 3 illustrates our implementation of MTP. Different from Gloeckle et al. (2024), which parallelly predicts 𝐷additional tokens using independent output heads, we sequentially predict additional tokens and keep the complete causal chain at each prediction depth. We introduce the details of our MTP implementation in this section. MTP Modules. To be specific, our MTP implementation uses 𝐷sequential modules to predict 𝐷 additional tokens. The 𝑘-th MTP module consists of a shared embedding layer Emb(·), a shared output head OutHead(·), a Transformer block TRM𝑘(·), and a projection matrix 𝑀𝑘∈R𝑑×2𝑑. For the 𝑖-th input token 𝑡𝑖, at the 𝑘-th prediction depth, we first combine the representation of the 𝑖-th token at the (𝑘−1)-th depth h𝑘−1 𝑖 ∈R𝑑and the embedding of the (𝑖+ 𝑘)-th token 𝐸𝑚𝑏(𝑡𝑖+𝑘) ∈R𝑑 10 with the linear projection: h′𝑘 𝑖= 𝑀𝑘[RMSNorm(h𝑘−1 𝑖 ); RMSNorm(Emb(𝑡𝑖+𝑘))], (21) where [·; ·] denotes concatenation. Especially, when 𝑘= 1, h𝑘−1 𝑖 refers to the representation given by the main model. Note that for each MTP module, its embedding layer is shared with the main model. The combined h′𝑘 𝑖serves as the input of the Transformer block at the 𝑘-th depth to produce the output representation at the current depth h𝑘 𝑖: h𝑘 1:𝑇−𝑘= TRM𝑘(h′𝑘 1:𝑇−𝑘), (22) where 𝑇represents the input sequence length and 𝑖:𝑗denotes the slicing operation (inclusive of both the left and right boundaries). Finally, taking h𝑘 𝑖as the input, the shared output head will compute the probability distribution for the 𝑘-th additional prediction token 𝑃𝑘 𝑖+1+𝑘∈R𝑉, where 𝑉is the vocabulary size: 𝑃𝑘 𝑖+𝑘+1 = OutHead(h𝑘 𝑖). (23) The output head OutHead(·) linearly maps the representation to logits and subsequently applies the Softmax(·) function to compute the prediction probabilities of the 𝑘-th additional token. Also, for each MTP module, its output head is shared with the main model. Our principle of maintaining the causal chain of predictions is similar to that of EAGLE (Li et al., 2024b), but its primary objective is speculative decoding (Leviathan et al., 2023; Xia et al., 2023), whereas we utilize MTP to improve training. MTP Training Objective. For each prediction depth, we compute a cross-entropy loss L𝑘 MTP: L𝑘 MTP = CrossEntropy(𝑃𝑘 2+𝑘:𝑇+1, 𝑡2+𝑘:𝑇+1) = −1 𝑇 𝑇+1 ∑︁ 𝑖=2+𝑘 log 𝑃𝑘 𝑖[𝑡𝑖], (24) where 𝑇denotes the input sequence length, 𝑡𝑖denotes the ground-truth token at the 𝑖-th position, and 𝑃𝑘 𝑖[𝑡𝑖] denotes the corresponding prediction probability of 𝑡𝑖, given by the 𝑘-th MTP module. Finally, we compute the average of the MTP losses across all depths and multiply it by a weighting factor 𝜆to obtain the overall MTP loss LMTP, which serves as an additional training objective for DeepSeek-V3: LMTP = 𝜆 𝐷 𝐷", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "c52cf53bc81048ee373b196275cf987b4a0647544c5189515fb77b9425eacc89"}
{"doc_id": "arxiv:2412.19437#introduction:part-12", "url": "https://arxiv.org/abs/2412.19437", "anchor": "#introduction:part-12", "type": "paper", "title": "", "section": "Introduction", "text": "= −1 𝑇 𝑇+1 ∑︁ 𝑖=2+𝑘 log 𝑃𝑘 𝑖[𝑡𝑖], (24) where 𝑇denotes the input sequence length, 𝑡𝑖denotes the ground-truth token at the 𝑖-th position, and 𝑃𝑘 𝑖[𝑡𝑖] denotes the corresponding prediction probability of 𝑡𝑖, given by the 𝑘-th MTP module. Finally, we compute the average of the MTP losses across all depths and multiply it by a weighting factor 𝜆to obtain the overall MTP loss LMTP, which serves as an additional training objective for DeepSeek-V3: LMTP = 𝜆 𝐷 𝐷 ∑︁ 𝑘=1 L𝑘 MTP. (25) MTP in Inference. Our MTP strategy mainly aims to improve the performance of the main model, so during inference, we can directly discard the MTP modules and the main model can function independently and normally. Additionally, we can also repurpose these MTP modules for speculative decoding to further improve the generation latency. 3. Infrastructures 3.1. Compute Clusters DeepSeek-V3 is trained on a cluster equipped with 2048 NVIDIA H800 GPUs. Each node in the H800 cluster contains 8 GPUs connected by NVLink and NVSwitch within nodes. Across different nodes, InfiniBand (IB) interconnects are utilized to facilitate communications. 11 Computation MLP(B)▲ MLP(W)▲ MLP(F)△ ATTN(B)▲ ATTN(W)▲ ATTN(F)△ Communication DISPATCH(F)△ DISPATCH(B)▲ COMBINE(F)△ PP COMBINE(B)▲ Time ➔ △ Forward chunk ▲ Backward chunk Figure 4 | Overlapping strategy for a pair of individual forward and backward chunks (the boundaries of the transformer blocks are not aligned). Orange denotes forward, green denotes \"backward for input\", blue denotes \"backward for weights\", purple denotes PP communication, and red denotes barriers. Both all-to-all and PP communication can be fully hidden. 3.2. Training Framework The training of DeepSeek-V3 is supported by the HAI-LLM framework, an efficient and lightweight training framework crafted by our engineers from the ground up. On the whole, DeepSeek-V3 applies 16-way Pipeline Parallelism (PP) (Qi et al., 2023a), 64-way Expert Paral- lelism (EP) (Lepikhin et al., 2021) spanning 8 nodes, and ZeRO-1 Data Parallelism (DP) (Rajb- handari et al., 2020). In order to facilitate efficient training of DeepSeek-V3, we implement meticulous engineering optimizations. Firstly, we design the DualPipe algorithm for efficient pipeline parallelism. Compared with existing PP methods, DualPipe has fewer pipeline bubbles. More importantly, it overlaps the computation and communication phases across forward and backward processes, thereby addressing the challenge of heavy communication overhead introduced by cross-node expert parallelism. Secondly, we develop efficient cross-node all-to-all communication kernels to fully utilize IB and NVLink bandwidths and conserve Streaming Multiprocessors (SMs) dedicated to communication. Finally, we meticulously optimize the memory footprint during training, thereby enabling us to train DeepSeek-V3 without using costly Tensor Parallelism (TP). 3.2.1. DualPipe and Computation-Communication Overlap For DeepSeek-V3, the communication overhead introduced by cross-node expert parallelism results in an inefficient computation-to-communication ratio of approximately 1:1. To tackle this challenge, we design an innovative pipeline parallelism algorithm called DualPipe, which not only accelerates model training by effectively overlapping forward and backward computation- communication phases, but also reduces the pipeline bubbles. The key idea of DualPipe is to overlap the computation and communication within a pair of individual forward and backward chunks. To be specific, we divide each chunk into four compo- nents:", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "523448c09594a8b840d204aad670d8868adcc4adc8c2a93f3422595179f1346d"}
{"doc_id": "arxiv:2412.19437#introduction:part-13", "url": "https://arxiv.org/abs/2412.19437", "anchor": "#introduction:part-13", "type": "paper", "title": "", "section": "Introduction", "text": "introduced by cross-node expert parallelism results in an inefficient computation-to-communication ratio of approximately 1:1. To tackle this challenge, we design an innovative pipeline parallelism algorithm called DualPipe, which not only accelerates model training by effectively overlapping forward and backward computation- communication phases, but also reduces the pipeline bubbles. The key idea of DualPipe is to overlap the computation and communication within a pair of individual forward and backward chunks. To be specific, we divide each chunk into four compo- nents: attention, all-to-all dispatch, MLP, and all-to-all combine. Specially, for a backward chunk, both attention and MLP are further split into two parts, backward for input and backward for weights, like in ZeroBubble (Qi et al., 2023b). In addition, we have a PP communication component. As illustrated in Figure 4, for a pair of forward and backward chunks, we rearrange these components and manually adjust the ratio of GPU SMs dedicated to communication versus computation. In this overlapping strategy, we can ensure that both all-to-all and PP communication can be fully hidden during execution. Given the efficient overlapping strategy, the full DualPipe scheduling is illustrated in Figure 5. It employs a bidirectional pipeline scheduling, which feeds micro-batches from both ends of the pipeline simultaneously and a significant portion of communications can be fully overlapped. This overlap also ensures that, as the model further scales up, as long as we maintain a constant computation-to-communication ratio, we can still employ fine-grained experts across nodes while achieving a near-zero all-to-all communication overhead. 12 Device 0 0 1 2 3 4 5 6 7 0 8 1 9 2 3 4 5 6 6 7 7 8 8 9 9 Device 1 0 1 2 3 4 5 6 0 7 1 8 2 9 3 4 5 6 7 8 7 9 8 9 Device 2 0 1 2 3 4 5 0 6 1 7 2 8 3 9 4 5 6 7 8 7 9 8 9 Device 3 0 1 2 3 4 0 5 1 6 2 7 3 8 4 9 5 6 7 8 9 8 9 Device 4 0 1 2 3 0 4 1 5 2 6 3 7 4 8 5 9 6 7 8 9 8 9 Device 5 0 1 2 0 0 3 1 4 2 5 3 6 4 7 5 8 6 9 7 8 9 9 Device 6 0 1 0 0 2 1 1 3 2 4 3 5 4 6 5 7 6 8 7 9 8 9 9 Device 7 0 0 0 1 1 1 2 2 2 3 3 4 4 5 5 6 6 7 7 8 8 9 9 Time ➔ Forward Backward Backward for input Backward for weights Overlapped forward & Backward Figure 5 | Example DualPipe scheduling for 8 PP ranks and 20 micro-batches in two directions. The micro-batches in the reverse direction are symmetric to those in the forward direction, so we omit their batch ID for illustration simplicity. Two cells enclosed by a shared black border have mutually overlapped computation", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "99e8b5dd7bc015dea356aa9673d33de576a81e46481b7242e1cff3143921292a"}
{"doc_id": "arxiv:2412.19437#introduction:part-14", "url": "https://arxiv.org/abs/2412.19437", "anchor": "#introduction:part-14", "type": "paper", "title": "", "section": "Introduction", "text": "2 3 3 4 4 5 5 6 6 7 7 8 8 9 9 Time ➔ Forward Backward Backward for input Backward for weights Overlapped forward & Backward Figure 5 | Example DualPipe scheduling for 8 PP ranks and 20 micro-batches in two directions. The micro-batches in the reverse direction are symmetric to those in the forward direction, so we omit their batch ID for illustration simplicity. Two cells enclosed by a shared black border have mutually overlapped computation and communication.", "source": "arxiv_pdf", "published": "", "tokens": 82, "sha256": "6c447a6a89d23d12ac4cbaa1b337350f8beff270b01f056bc3e22dd8738b367d"}
{"doc_id": "arxiv:2412.19437#method:part-1", "url": "https://arxiv.org/abs/2412.19437", "anchor": "#method:part-1", "type": "paper", "title": "", "section": "Method", "text": "Bubble Parameter Activation 1F1B (𝑃𝑃−1)(𝐹+ 𝐵) 1× 𝑃𝑃 ZB1P (𝑃𝑃−1)(𝐹+ 𝐵−2𝑊) 1× 𝑃𝑃 DualPipe (Ours) ( 𝑃𝑃 2 −1)(𝐹&𝐵+ 𝐵−3𝑊) 2× 𝑃𝑃+ 1 Table 2 | Comparison of pipeline bubbles and memory usage across different pipeline parallel methods. 𝐹denotes the execution time of a forward chunk, 𝐵denotes the execution time of a full backward chunk, 𝑊denotes the execution time of a \"backward for weights\" chunk, and 𝐹&𝐵 denotes the execution time of two mutually overlapped forward and backward chunks. In addition, even in more general scenarios without a heavy communication burden, Du- alPipe still exhibits efficiency advantages. In Table 2, we summarize the pipeline bubbles and memory usage across different PP methods. As shown in the table, compared with ZB1P (Qi et al., 2023b) and 1F1B (Harlap et al., 2018), DualPipe significantly reduces the pipeline bubbles while only increasing the peak activation memory by 1 𝑃𝑃times. Although DualPipe requires keeping two copies of the model parameters, this does not significantly increase the memory consumption since we use a large EP size during training. Compared with Chimera (Li and Hoefler, 2021), DualPipe only requires that the pipeline stages and micro-batches be divisible by 2, without requiring micro-batches to be divisible by pipeline stages. In addition, for DualPipe, neither the bubbles nor activation memory will increase as the number of micro-batches grows. 3.2.2. Efficient Implementation of Cross-Node All-to-All Communication In order to ensure sufficient computational performance for DualPipe, we customize efficient cross-node all-to-all communication kernels (including dispatching and combining) to conserve the number of SMs dedicated to communication. The implementation of the kernels is co- designed with the MoE gating algorithm and the network topology of our cluster. To be specific, in our cluster, cross-node GPUs are fully interconnected with IB, and intra-node communications are handled via NVLink. NVLink offers a bandwidth of 160 GB/s, roughly 3.2 times that of IB (50 GB/s). To effectively leverage the different bandwidths of IB and NVLink, we limit each token to be dispatched to at most 4 nodes, thereby reducing IB traffic. For each token, when its routing decision is made, it will first be transmitted via IB to the GPUs with the same in-node index on its target nodes. Once it reaches the target nodes, we will endeavor to ensure that it is instantaneously forwarded via NVLink to specific GPUs that host their target experts, without being blocked by subsequently arriving tokens. In this way, communications via IB and NVLink are fully overlapped, and each token can efficiently select an average of 3.2 experts per node without incurring additional overhead from NVLink. This implies that, although DeepSeek-V3 13 selects only 8 routed experts in practice, it can scale up this number to a maximum of 13 experts (4 nodes × 3.2 experts/node) while preserving the same communication cost. Overall, under such a communication strategy, only 20 SMs are sufficient to fully utilize the bandwidths of IB and NVLink. In detail, we employ the warp specialization technique (Bauer et al., 2014) and partition 20 SMs into 10 communication channels. During the dispatching process, (1) IB sending, (2)", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "81265f4e69081f3c715c965eca5faa0f69bfa7514fd081e7273c105a3a98388f"}
{"doc_id": "arxiv:2412.19437#method:part-2", "url": "https://arxiv.org/abs/2412.19437", "anchor": "#method:part-2", "type": "paper", "title": "", "section": "Method", "text": "DeepSeek-V3 13 selects only 8 routed experts in practice, it can scale up this number to a maximum of 13 experts (4 nodes × 3.2 experts/node) while preserving the same communication cost. Overall, under such a communication strategy, only 20 SMs are sufficient to fully utilize the bandwidths of IB and NVLink. In detail, we employ the warp specialization technique (Bauer et al., 2014) and partition 20 SMs into 10 communication channels. During the dispatching process, (1) IB sending, (2) IB-to-NVLink forwarding, and (3) NVLink receiving are handled by respective warps. The number of warps allocated to each communication task is dynamically adjusted according to the actual workload across all SMs. Similarly, during the combining process, (1) NVLink sending, (2) NVLink-to-IB forwarding and accumulation, and (3) IB receiving and accumulation are also handled by dynamically adjusted warps. In addition, both dispatching and combining kernels overlap with the computation stream, so we also consider their impact on other SM computation kernels. Specifically, we employ customized PTX (Parallel Thread Execution) instructions and auto-tune the communication chunk size, which significantly reduces the use of the L2 cache and the interference to other SMs. 3.2.3. Extremely Memory Saving with Minimal Overhead In order to reduce the memory footprint during training, we employ the following techniques. Recomputation of RMSNorm and MLA Up-Projection. We recompute all RMSNorm op- erations and MLA up-projections during back-propagation, thereby eliminating the need to persistently store their output activations. With a minor overhead, this strategy significantly reduces memory requirements for storing activations. Exponential Moving Average in CPU. During training, we preserve the Exponential Mov- ing Average (EMA) of the model parameters for early estimation of the model performance after learning rate decay. The EMA parameters are stored in CPU memory and are updated asynchronously after each training step. This method allows us to maintain EMA parameters without incurring additional memory or time overhead. Shared Embedding and Output Head for Multi-Token Prediction. With the DualPipe strategy, we deploy the shallowest layers (including the embedding layer) and deepest layers (including the output head) of the model on the same PP rank. This arrangement enables the physical sharing of parameters and gradients, of the shared embedding and output head, between the MTP module and the main model. This physical sharing mechanism further enhances our memory efficiency. 3.3. FP8 Training Inspired by recent advances in low-precision training (Dettmers et al., 2022; Noune et al., 2022; Peng et al., 2023b), we propose a fine-grained mixed precision framework utilizing the FP8 data format for training DeepSeek-V3. While low-precision training holds great promise, it is often limited by the presence of outliers in activations, weights, and gradients (Fishman et al., 2024; He et al.; Sun et al., 2024). Although significant progress has been made in in- ference quantization (Frantar et al., 2022; Xiao et al., 2023), there are relatively few studies demonstrating successful application of low-precision techniques in large-scale language model 14 Σ Fprop FP32 Input To FP8 BF16 Weight Σ Dgrad FP32 Input Gradient Output Output Gradient BF16 To FP8 Σ Wgrad FP32 To FP8 To FP8 Weight Gradient", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "f1f6416b529b67415dff1d5c734867f731c22f5e7458ffead49d7960b07cda21"}
{"doc_id": "arxiv:2412.19437#method:part-3", "url": "https://arxiv.org/abs/2412.19437", "anchor": "#method:part-3", "type": "paper", "title": "", "section": "Method", "text": "of outliers in activations, weights, and gradients (Fishman et al., 2024; He et al.; Sun et al., 2024). Although significant progress has been made in in- ference quantization (Frantar et al., 2022; Xiao et al., 2023), there are relatively few studies demonstrating successful application of low-precision techniques in large-scale language model 14 Σ Fprop FP32 Input To FP8 BF16 Weight Σ Dgrad FP32 Input Gradient Output Output Gradient BF16 To FP8 Σ Wgrad FP32 To FP8 To FP8 Weight Gradient Optimizer States To BF16 Master Weight To FP8 To BF16 To BF16 To FP32 FP32 Figure 6 | The overall mixed precision framework with FP8 data format. For clarification, only the Linear operator is illustrated. pre-training (Fishman et al., 2024). To address this challenge and effectively extend the dynamic range of the FP8 format, we introduce a fine-grained quantization strategy: tile-wise grouping with 1 × 𝑁𝑐elements or block-wise grouping with 𝑁𝑐× 𝑁𝑐elements. The associated dequantiza- tion overhead is largely mitigated under our increased-precision accumulation process, a critical aspect for achieving accurate FP8 General Matrix Multiplication (GEMM). Moreover, to further reduce memory and communication overhead in MoE training, we cache and dispatch activa- tions in FP8, while storing low-precision optimizer states in BF16. We validate the proposed FP8 mixed precision framework on two model scales similar to DeepSeek-V2-Lite and DeepSeek- V2, training for approximately 1 trillion tokens (see more details in Appendix B.1). Notably, compared with the BF16 baseline, the relative loss error of our FP8-training model remains consistently below 0.25%, a level well within the acceptable range of training randomness. 3.3.1. Mixed Precision Framework Building upon widely adopted techniques in low-precision training (Kalamkar et al., 2019; Narang et al., 2017), we propose a mixed precision framework for FP8 training. In this frame- work, most compute-density operations are conducted in FP8, while a few key operations are strategically maintained in their original data formats to balance training efficiency and numerical stability. The overall framework is illustrated in Figure 6. Firstly, in order to accelerate model training, the majority of core computation kernels, i.e., GEMM operations, are implemented in FP8 precision. These GEMM operations accept FP8 tensors as inputs and produce outputs in BF16 or FP32. As depicted in Figure 6, all three GEMMs associated with the Linear operator, namely Fprop (forward pass), Dgrad (activation backward pass), and Wgrad (weight backward pass), are executed in FP8. This design theoretically doubles the computational speed compared with the original BF16 method. Additionally, the FP8 Wgrad GEMM allows activations to be stored in FP8 for use in the backward pass. This significantly reduces memory consumption. Despite the efficiency advantage of the FP8 format, certain operators still require a higher precision due to their sensitivity to low-precision computations. Besides, some low-cost opera- tors can also utilize a higher precision with a negligible overhead to the overall training cost. For this reason, after careful investigations, we maintain the original precision (e.g., BF16 or FP32) for the following components: the embedding module, the output head, MoE gating modules, normalization operators, and attention operators. These targeted retentions of high precision ensure", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "bcb8f6d3a5ff2ff8cc572532a7c0789c8bd3cb805da4ca962f9f62edafa5cbc0"}
{"doc_id": "arxiv:2412.19437#method:part-4", "url": "https://arxiv.org/abs/2412.19437", "anchor": "#method:part-4", "type": "paper", "title": "", "section": "Method", "text": "efficiency advantage of the FP8 format, certain operators still require a higher precision due to their sensitivity to low-precision computations. Besides, some low-cost opera- tors can also utilize a higher precision with a negligible overhead to the overall training cost. For this reason, after careful investigations, we maintain the original precision (e.g., BF16 or FP32) for the following components: the embedding module, the output head, MoE gating modules, normalization operators, and attention operators. These targeted retentions of high precision ensure stable training dynamics for DeepSeek-V3. To further guarantee numerical stability, we store the master weights, weight gradients, and optimizer states in higher precision. While 15 Scaling Factor … … … … Tensor Core CUDA Core Input Scaling Factor Weight Scaling Factor Output Tensor Core WGMMA 1 WGMMA 4 Low Prec Acc CUDA Core FP32 Register Interval Output / GEMM Input (b) Increasing accumulation precision (a) Fine-grained quantization Figure 7 | (a) We propose a fine-grained quantization method to mitigate quantization errors caused by feature outliers; for illustration simplicity, only Fprop is illustrated. (b) In conjunction with our quantization strategy, we improve the FP8 GEMM precision by promoting to CUDA Cores at an interval of 𝑁𝐶= 128 elements MMA for the high-precision accumulation. these high-precision components incur some memory overheads, their impact can be minimized through efficient sharding across multiple DP ranks in our distributed training system. 3.3.2. Improved Precision from Quantization and Multiplication Based on our mixed precision FP8 framework, we introduce several strategies to enhance low- precision training accuracy, focusing on both the quantization method and the multiplication process. Fine-Grained Quantization. In low-precision training frameworks, overflows and underflows are common challenges due to the limited dynamic range of the FP8 format, which is constrained by its reduced exponent bits. As a standard practice, the input distribution is aligned to the representable range of the FP8 format by scaling the maximum absolute value of the input tensor to the maximum representable value of FP8 (Narang et al., 2017). This method makes low- precision training highly sensitive to activation outliers, which can heavily degrade quantization accuracy. To solve this, we propose a fine-grained quantization method that applies scaling at a more granular level. As illustrated in Figure 7 (a), (1) for activations, we group and scale elements on a 1x128 tile basis (i.e., per token per 128 channels); and (2) for weights, we group and scale elements on a 128x128 block basis (i.e., per 128 input channels per 128 output channels). This approach ensures that the quantization process can better accommodate outliers by adapting the scale according to smaller groups of elements. In Appendix B.2, we further discuss the training instability when we group and scale activations on a block basis in the same way as weights quantization. One key modification in our method is the introduction of per-group scaling factors along the inner dimension of GEMM operations. This functionality is not directly supported in the standard FP8 GEMM. However, combined with our precise FP32 accumulation strategy, it can 16 be efficiently implemented. Notably, our fine-grained quantization strategy is highly consistent with the", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "9baed9d4828500eca66e49ab068f222f2baff6a04a1c6ffef0c5db28603f85c6"}
{"doc_id": "arxiv:2412.19437#method:part-5", "url": "https://arxiv.org/abs/2412.19437", "anchor": "#method:part-5", "type": "paper", "title": "", "section": "Method", "text": "Appendix B.2, we further discuss the training instability when we group and scale activations on a block basis in the same way as weights quantization. One key modification in our method is the introduction of per-group scaling factors along the inner dimension of GEMM operations. This functionality is not directly supported in the standard FP8 GEMM. However, combined with our precise FP32 accumulation strategy, it can 16 be efficiently implemented. Notably, our fine-grained quantization strategy is highly consistent with the idea of mi- croscaling formats (Rouhani et al., 2023b), while the Tensor Cores of NVIDIA next-generation GPUs (Blackwell series) have announced the support for microscaling formats with smaller quantization granularity (NVIDIA, 2024a). We hope our design can serve as a reference for future work to keep pace with the latest GPU architectures. Increasing Accumulation Precision. Low-precision GEMM operations often suffer from un- derflow issues, and their accuracy largely depends on high-precision accumulation, which is commonly performed in an FP32 precision (Kalamkar et al., 2019; Narang et al., 2017). However, we observe that the accumulation precision of FP8 GEMM on NVIDIA H800 GPUs is limited to retaining around 14 bits, which is significantly lower than FP32 accumulation precision. This problem will become more pronounced when the inner dimension K is large (Wortsman et al., 2023), a typical scenario in large-scale model training where the batch size and model width are increased. Taking GEMM operations of two random matrices with K = 4096 for example, in our preliminary test, the limited accumulation precision in Tensor Cores results in a maximum relative error of nearly 2%. Despite these problems, the limited accumulation precision is still the default option in a few FP8 frameworks (NVIDIA, 2024b), severely constraining the training accuracy. In order to address this issue, we adopt the strategy of promotion to CUDA Cores for higher precision (Thakkar et al., 2023). The process is illustrated in Figure 7 (b). To be specific, during MMA (Matrix Multiply-Accumulate) execution on Tensor Cores, intermediate results are accumulated using the limited bit width. Once an interval of 𝑁𝐶is reached, these partial results will be copied to FP32 registers on CUDA Cores, where full-precision FP32 accumulation is performed. As mentioned before, our fine-grained quantization applies per-group scaling factors along the inner dimension K. These scaling factors can be efficiently multiplied on the CUDA Cores as the dequantization process with minimal additional computational cost. It is worth noting that this modification reduces the WGMMA (Warpgroup-level Matrix Multiply-Accumulate) instruction issue rate for a single warpgroup. However, on the H800 architecture, it is typical for two WGMMA to persist concurrently: while one warpgroup performs the promotion operation, the other is able to execute the MMA operation. This design enables overlapping of the two operations, maintaining high utilization of Tensor Cores. Based on our experiments, setting 𝑁𝐶= 128 elements, equivalent to 4 WGMMAs, represents the minimal accumulation interval that can significantly improve precision without introducing substantial overhead. Mantissa over Exponents. In contrast to the hybrid FP8 format adopted by prior work (NVIDIA, 2024b; Peng et al., 2023b; Sun et al., 2019b), which uses", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "93c514602008e24ff45e0868f55523f52bb315f28f007362de05d35b19c8d262"}
{"doc_id": "arxiv:2412.19437#method:part-6", "url": "https://arxiv.org/abs/2412.19437", "anchor": "#method:part-6", "type": "paper", "title": "", "section": "Method", "text": "warpgroup performs the promotion operation, the other is able to execute the MMA operation. This design enables overlapping of the two operations, maintaining high utilization of Tensor Cores. Based on our experiments, setting 𝑁𝐶= 128 elements, equivalent to 4 WGMMAs, represents the minimal accumulation interval that can significantly improve precision without introducing substantial overhead. Mantissa over Exponents. In contrast to the hybrid FP8 format adopted by prior work (NVIDIA, 2024b; Peng et al., 2023b; Sun et al., 2019b), which uses E4M3 (4-bit exponent and 3-bit mantissa) in Fprop and E5M2 (5-bit exponent and 2-bit mantissa) in Dgrad and Wgrad, we adopt the E4M3 format on all tensors for higher precision. We attribute the feasibility of this approach to our fine-grained quantization strategy, i.e., tile and block-wise scaling. By operating on smaller element groups, our methodology effectively shares exponent bits among these grouped elements, mitigating the impact of the limited dynamic range. Online Quantization. Delayed quantization is employed in tensor-wise quantization frame- works (NVIDIA, 2024b; Peng et al., 2023b), which maintains a history of the maximum absolute 17 values across prior iterations to infer the current value. In order to ensure accurate scales and simplify the framework, we calculate the maximum absolute value online for each 1x128 acti- vation tile or 128x128 weight block. Based on it, we derive the scaling factor and then quantize the activation or weight online into the FP8 format. 3.3.3. Low-Precision Storage and Communication In conjunction with our FP8 training framework, we further reduce the memory consumption and communication overhead by compressing cached activations and optimizer states into lower-precision formats. Low-Precision Optimizer States. We adopt the BF16 data format instead of FP32 to track the first and second moments in the AdamW (Loshchilov and Hutter, 2017) optimizer, without incurring observable performance degradation. However, the master weights (stored by the optimizer) and gradients (used for batch size accumulation) are still retained in FP32 to ensure numerical stability throughout training. Low-Precision Activation. As illustrated in Figure 6, the Wgrad operation is performed in FP8. To reduce the memory consumption, it is a natural choice to cache activations in FP8 format for the backward pass of the Linear operator. However, special considerations are taken on several operators for low-cost high-precision training: (1) Inputs of the Linear after the attention operator. These activations are also used in the backward pass of the attention operator, which makes it sensitive to precision. We adopt a customized E5M6 data format exclusively for these activations. Additionally, these activations will be converted from an 1x128 quantization tile to an 128x1 tile in the backward pass. To avoid introducing extra quantization error, all the scaling factors are round scaled, i.e., integral power of 2. (2) Inputs of the SwiGLU operator in MoE. To further reduce the memory cost, we cache the inputs of the SwiGLU operator and recompute its output in the backward pass. These activations are also stored in FP8 with our fine-grained quantization method, striking a balance between memory efficiency and computational accuracy. Low-Precision Communication. Communication bandwidth is a critical bottleneck in the training of MoE", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "8f4a5f9297f65f8c0219fd1835154552fb0df16cd3c64ab7e0627333a35959a3"}
{"doc_id": "arxiv:2412.19437#method:part-7", "url": "https://arxiv.org/abs/2412.19437", "anchor": "#method:part-7", "type": "paper", "title": "", "section": "Method", "text": "introducing extra quantization error, all the scaling factors are round scaled, i.e., integral power of 2. (2) Inputs of the SwiGLU operator in MoE. To further reduce the memory cost, we cache the inputs of the SwiGLU operator and recompute its output in the backward pass. These activations are also stored in FP8 with our fine-grained quantization method, striking a balance between memory efficiency and computational accuracy. Low-Precision Communication. Communication bandwidth is a critical bottleneck in the training of MoE models. To alleviate this challenge, we quantize the activation before MoE up-projections into FP8 and then apply dispatch components, which is compatible with FP8 Fprop in MoE up-projections. Like the inputs of the Linear after the attention operator, scaling factors for this activation are integral power of 2. A similar strategy is applied to the activation gradient before MoE down-projections. For both the forward and backward combine components, we retain them in BF16 to preserve training precision in critical parts of the training pipeline. 3.4. Inference and Deployment We deploy DeepSeek-V3 on the H800 cluster, where GPUs within each node are interconnected using NVLink, and all GPUs across the cluster are fully interconnected via IB. To simultaneously ensure both the Service-Level Objective (SLO) for online services and high throughput, we employ the following deployment strategy that separates the prefilling and decoding stages. 18 3.4.1. Prefilling The minimum deployment unit of the prefilling stage consists of 4 nodes with 32 GPUs. The attention part employs 4-way Tensor Parallelism (TP4) with Sequence Parallelism (SP), com- bined with 8-way Data Parallelism (DP8). Its small TP size of 4 limits the overhead of TP communication. For the MoE part, we use 32-way Expert Parallelism (EP32), which ensures that each expert processes a sufficiently large batch size, thereby enhancing computational efficiency. For the MoE all-to-all communication, we use the same method as in training: first transferring tokens across nodes via IB, and then forwarding among the intra-node GPUs via NVLink. In particular, we use 1-way Tensor Parallelism for the dense MLPs in shallow layers to save TP communication. To achieve load balancing among different experts in the MoE part, we need to ensure that each GPU processes approximately the same number of tokens. To this end, we introduce a deployment strategy of redundant experts, which duplicates high-load experts and deploys them redundantly. The high-load experts are detected based on statistics collected during the online deployment and are adjusted periodically (e.g., every 10 minutes). After determining the set of redundant experts, we carefully rearrange experts among GPUs within a node based on the observed loads, striving to balance the load across GPUs as much as possible without increasing the cross-node all-to-all communication overhead. For the deployment of DeepSeek-V3, we set 32 redundant experts for the prefilling stage. For each GPU, besides the original 8 experts it hosts, it will also host one additional redundant expert. Furthermore, in the prefilling stage, to improve the throughput and hide the overhead of all-to-all and TP communication, we simultaneously process two micro-batches with similar computational workloads, overlapping the attention and MoE of", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "280ea072f151bb5b59a56cdc92b8691966db3b250894629e32889b32540374c7"}
{"doc_id": "arxiv:2412.19437#method:part-8", "url": "https://arxiv.org/abs/2412.19437", "anchor": "#method:part-8", "type": "paper", "title": "", "section": "Method", "text": "the load across GPUs as much as possible without increasing the cross-node all-to-all communication overhead. For the deployment of DeepSeek-V3, we set 32 redundant experts for the prefilling stage. For each GPU, besides the original 8 experts it hosts, it will also host one additional redundant expert. Furthermore, in the prefilling stage, to improve the throughput and hide the overhead of all-to-all and TP communication, we simultaneously process two micro-batches with similar computational workloads, overlapping the attention and MoE of one micro-batch with the dispatch and combine of another. Finally, we are exploring a dynamic redundancy strategy for experts, where each GPU hosts more experts (e.g., 16 experts), but only 9 will be activated during each inference step. Before the all-to-all operation at each layer begins, we compute the globally optimal routing scheme on the fly. Given the substantial computation involved in the prefilling stage, the overhead of computing this routing scheme is almost negligible. 3.4.2. Decoding During decoding, we treat the shared expert as a routed one. From this perspective, each token will select 9 experts during routing, where the shared expert is regarded as a heavy-load one that will always be selected. The minimum deployment unit of the decoding stage consists of 40 nodes with 320 GPUs. The attention part employs TP4 with SP, combined with DP80, while the MoE part uses EP320. For the MoE part, each GPU hosts only one expert, and 64 GPUs are responsible for hosting redundant experts and shared experts. All-to-all communication of the dispatch and combine parts is performed via direct point-to-point transfers over IB to achieve low latency. Additionally, we leverage the IBGDA (NVIDIA, 2022) technology to further minimize latency and enhance communication efficiency. Similar to prefilling, we periodically determine the set of redundant experts in a certain interval, based on the statistical expert load from our online service. However, we do not need to rearrange experts since each GPU only hosts one expert. We are also exploring the dynamic redundancy strategy for decoding. However, this requires more careful optimization of the algorithm that computes the globally optimal routing scheme and the fusion with the dispatch kernel to reduce overhead. 19 Additionally, to enhance throughput and hide the overhead of all-to-all communication, we are also exploring processing two micro-batches with similar computational workloads simultaneously in the decoding stage. Unlike prefilling, attention consumes a larger portion of time in the decoding stage. Therefore, we overlap the attention of one micro-batch with the dispatch+MoE+combine of another. In the decoding stage, the batch size per expert is relatively small (usually within 256 tokens), and the bottleneck is memory access rather than computation. Since the MoE part only needs to load the parameters of one expert, the memory access overhead is minimal, so using fewer SMs will not significantly affect the overall performance. Therefore, to avoid impacting the computation speed of the attention part, we can allocate only a small portion of SMs to dispatch+MoE+combine. 3.5. Suggestions on Hardware Design Based on our implementation of the all-to-all communication and FP8 training scheme, we propose the following suggestions", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "d00e7e87d43f983f20b76f8b1716da4c623d810c5f03663e2af141883ba7556b"}
{"doc_id": "arxiv:2412.19437#method:part-9", "url": "https://arxiv.org/abs/2412.19437", "anchor": "#method:part-9", "type": "paper", "title": "", "section": "Method", "text": "is memory access rather than computation. Since the MoE part only needs to load the parameters of one expert, the memory access overhead is minimal, so using fewer SMs will not significantly affect the overall performance. Therefore, to avoid impacting the computation speed of the attention part, we can allocate only a small portion of SMs to dispatch+MoE+combine. 3.5. Suggestions on Hardware Design Based on our implementation of the all-to-all communication and FP8 training scheme, we propose the following suggestions on chip design to AI hardware vendors. 3.5.1. Communication Hardware In DeepSeek-V3, we implement the overlap between computation and communication to hide the communication latency during computation. This significantly reduces the dependency on communication bandwidth compared to serial computation and communication. However, the current communication implementation relies on expensive SMs (e.g., we allocate 20 out of the 132 SMs available in the H800 GPU for this purpose), which will limit the computational throughput. Moreover, using SMs for communication results in significant inefficiencies, as tensor cores remain entirely under-utilized. Currently, the SMs primarily perform the following tasks for all-to-all communication: • Forwarding data between the IB (InfiniBand) and NVLink domain while aggregating IB traffic destined for multiple GPUs within the same node from a single GPU. • Transporting data between RDMA buffers (registered GPU memory regions) and in- put/output buffers. • Executing reduce operations for all-to-all combine. • Managing fine-grained memory layout during chunked data transferring to multiple experts across the IB and NVLink domain. We aspire to see future vendors developing hardware that offloads these communication tasks from the valuable computation unit SM, serving as a GPU co-processor or a network co-processor like NVIDIA SHARP Graham et al. (2016). Furthermore, to reduce application programming complexity, we aim for this hardware to unify the IB (scale-out) and NVLink (scale-up) networks from the perspective of the computation units. With this unified interface, computation units can easily accomplish operations such as read, write, multicast, and reduce across the entire IB-NVLink-unified domain via submitting communication requests based on simple primitives. 3.5.2. Compute Hardware Higher FP8 GEMM Accumulation Precision in Tensor Cores. In the current Tensor Core implementation of the NVIDIA Hopper architecture, FP8 GEMM suffers from limited accumula- tion precision. After aligning 32 mantissa products by right-shifting based on the maximum exponent, the Tensor Core only uses the highest 14 bits of each mantissa product for addition, 20 and truncates bits exceeding this range. The accumulation of addition results into registers also employs 14-bit precision. Our implementation partially mitigates the limitation by accumulating the addition results of 128 FP8×FP8 multiplications into registers with FP32 precision in the CUDA core. Although helpful in achieving successful FP8 training, it is merely a compromise due to the Hopper architecture’s hardware deficiency in FP8 GEMM accumulation precision. Future chips need to adopt higher precision. Support for Tile- and Block-Wise Quantization. Current GPUs only support per-tensor quantization, lacking the native support for fine-grained quantization like our tile- and block- wise quantization. In the current implementation, when the 𝑁𝐶interval is reached, the partial results will be copied from Tensor Cores to CUDA", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "f3e852ef33a5e75e812a0a55eef31b7b98ab23284849105996b40fbdd10ab589"}
{"doc_id": "arxiv:2412.19437#method:part-10", "url": "https://arxiv.org/abs/2412.19437", "anchor": "#method:part-10", "type": "paper", "title": "", "section": "Method", "text": "the CUDA core. Although helpful in achieving successful FP8 training, it is merely a compromise due to the Hopper architecture’s hardware deficiency in FP8 GEMM accumulation precision. Future chips need to adopt higher precision. Support for Tile- and Block-Wise Quantization. Current GPUs only support per-tensor quantization, lacking the native support for fine-grained quantization like our tile- and block- wise quantization. In the current implementation, when the 𝑁𝐶interval is reached, the partial results will be copied from Tensor Cores to CUDA cores, multiplied by the scaling factors, and added to FP32 registers on CUDA cores. Although the dequantization overhead is significantly mitigated combined with our precise FP32 accumulation strategy, the frequent data movements between Tensor Cores and CUDA cores still limit the computational efficiency. Therefore, we recommend future chips to support fine-grained quantization by enabling Tensor Cores to receive scaling factors and implement MMA with group scaling. In this way, the whole partial sum accumulation and dequantization can be completed directly inside Tensor Cores until the final result is produced, avoiding frequent data movements. Support for Online Quantization. The current implementations struggle to effectively support online quantization, despite its effectiveness demonstrated in our research. In the existing process, we need to read 128 BF16 activation values (the output of the previous computation) from HBM (High Bandwidth Memory) for quantization, and the quantized FP8 values are then written back to HBM, only to be read again for MMA. To address this inefficiency, we recommend that future chips integrate FP8 cast and TMA (Tensor Memory Accelerator) access into a single fused operation, so quantization can be completed during the transfer of activations from global memory to shared memory, avoiding frequent memory reads and writes. We also recommend supporting a warp-level cast instruction for speedup, which further facilitates the better fusion of layer normalization and FP8 cast. Alternatively, a near-memory computing approach can be adopted, where compute logic is placed near the HBM. In this case, BF16 elements can be cast to FP8 directly as they are read from HBM into the GPU, reducing off-chip memory access by roughly 50%. Support for Transposed GEMM Operations. The current architecture makes it cumbersome to fuse matrix transposition with GEMM operations. In our workflow, activations during the forward pass are quantized into 1x128 FP8 tiles and stored. During the backward pass, the matrix needs to be read out, dequantized, transposed, re-quantized into 128x1 tiles, and stored in HBM. To reduce memory operations, we recommend future chips to enable direct transposed reads of matrices from shared memory before MMA operation, for those precisions required in both training and inference. Combined with the fusion of FP8 format conversion and TMA access, this enhancement will significantly streamline the quantization workflow. 4. Pre-Training 4.1. Data Construction Compared with DeepSeek-V2, we optimize the pre-training corpus by enhancing the ratio of mathematical and programming samples, while expanding multilingual coverage beyond 21 English and Chinese. Also, our data processing pipeline is refined to minimize redundancy while maintaining corpus diversity. Inspired by Ding et al. (2024), we implement the document packing method for data integrity but do", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "fffef49231b8a89c808efad2829879e017a724b0d478a830c3685412a7eabf74"}
{"doc_id": "arxiv:2412.19437#method:part-11", "url": "https://arxiv.org/abs/2412.19437", "anchor": "#method:part-11", "type": "paper", "title": "", "section": "Method", "text": "with the fusion of FP8 format conversion and TMA access, this enhancement will significantly streamline the quantization workflow. 4. Pre-Training 4.1. Data Construction Compared with DeepSeek-V2, we optimize the pre-training corpus by enhancing the ratio of mathematical and programming samples, while expanding multilingual coverage beyond 21 English and Chinese. Also, our data processing pipeline is refined to minimize redundancy while maintaining corpus diversity. Inspired by Ding et al. (2024), we implement the document packing method for data integrity but do not incorporate cross-sample attention masking during training. Finally, the training corpus for DeepSeek-V3 consists of 14.8T high-quality and diverse tokens in our tokenizer. In the training process of DeepSeekCoder-V2 (DeepSeek-AI, 2024a), we observe that the Fill-in-Middle (FIM) strategy does not compromise the next-token prediction capability while enabling the model to accurately predict middle text based on contextual cues. In alignment with DeepSeekCoder-V2, we also incorporate the FIM strategy in the pre-training of DeepSeek-V3. To be specific, we employ the Prefix-Suffix-Middle (PSM) framework to structure data as follows: <|fim_begin|> 𝑓pre<|fim_hole|> 𝑓suf<|fim_end|> 𝑓middle<|eos_token|>. This structure is applied at the document level as a part of the pre-packing process. The FIM strategy is applied at a rate of 0.1, consistent with the PSM framework. The tokenizer for DeepSeek-V3 employs Byte-level BPE (Shibata et al., 1999) with an extended vocabulary of 128K tokens. The pretokenizer and training data for our tokenizer are modified to optimize multilingual compression efficiency. In addition, compared with DeepSeek-V2, the new pretokenizer introduces tokens that combine punctuations and line breaks. However, this trick may introduce the token boundary bias (Lundberg, 2023) when the model processes multi-line prompts without terminal line breaks, particularly for few-shot evaluation prompts. To address this issue, we randomly split a certain proportion of such combined tokens during training, which exposes the model to a wider array of special cases and mitigates this bias. 4.2. Hyper-Parameters Model Hyper-Parameters. We set the number of Transformer layers to 61 and the hidden dimension to 7168. All learnable parameters are randomly initialized with a standard deviation of 0.006. In MLA, we set the number of attention heads 𝑛ℎto 128 and the per-head dimension 𝑑ℎ to 128. The KV compression dimension 𝑑𝑐is set to 512, and the query compression dimension 𝑑′ 𝑐 is set to 1536. For the decoupled queries and key, we set the per-head dimension 𝑑𝑅 ℎto 64. We substitute all FFNs except for the first three layers with MoE layers. Each MoE layer consists of 1 shared expert and 256 routed experts, where the intermediate hidden dimension of each expert is 2048. Among the routed experts, 8 experts will be activated for each token, and each token will be ensured to be sent to at most 4 nodes. The multi-token prediction depth 𝐷is set to 1, i.e., besides the exact next token, each token will predict one additional token. As DeepSeek-V2, DeepSeek-V3 also employs additional RMSNorm layers after the compressed latent vectors, and multiplies additional scaling factors at the width bottlenecks. Under this configuration, DeepSeek-V3 comprises 671B total parameters, of which 37B are activated for each token. Training Hyper-Parameters. We", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "e9daa7bc0168392021172caa59ef3857d1eff6ec5a061c6726da57dedf4e379a"}
{"doc_id": "arxiv:2412.19437#method:part-12", "url": "https://arxiv.org/abs/2412.19437", "anchor": "#method:part-12", "type": "paper", "title": "", "section": "Method", "text": "activated for each token, and each token will be ensured to be sent to at most 4 nodes. The multi-token prediction depth 𝐷is set to 1, i.e., besides the exact next token, each token will predict one additional token. As DeepSeek-V2, DeepSeek-V3 also employs additional RMSNorm layers after the compressed latent vectors, and multiplies additional scaling factors at the width bottlenecks. Under this configuration, DeepSeek-V3 comprises 671B total parameters, of which 37B are activated for each token. Training Hyper-Parameters. We employ the AdamW optimizer (Loshchilov and Hutter, 2017) with hyper-parameters set to 𝛽1 = 0.9, 𝛽2 = 0.95, and weight_decay = 0.1. We set the maximum sequence length to 4K during pre-training, and pre-train DeepSeek-V3 on 14.8T tokens. As for the learning rate scheduling, we first linearly increase it from 0 to 2.2 × 10−4 during the first 2K steps. Then, we keep a constant learning rate of 2.2 × 10−4 until the model consumes 10T training tokens. Subsequently, we gradually decay the learning rate to 2.2 × 10−5 in 4.3T tokens, following a cosine decay curve. During the training of the final 500B tokens, we keep a constant learning rate of 2.2 × 10−5 in the first 333B tokens, and switch to another constant learning rate 22 of 7.3 × 10−6 in the remaining 167B tokens. The gradient clipping norm is set to 1.0. We employ a batch size scheduling strategy, where the batch size is gradually increased from 3072 to 15360 in the training of the first 469B tokens, and then keeps 15360 in the remaining training. We leverage pipeline parallelism to deploy different layers of a model on different GPUs, and for each layer, the routed experts will be uniformly deployed on 64 GPUs belonging to 8 nodes. As for the node-limited routing, each token will be sent to at most 4 nodes (i.e., 𝑀= 4). For auxiliary-loss-free load balancing, we set the bias update speed 𝛾to 0.001 for the first 14.3T tokens, and to 0.0 for the remaining 500B tokens. For the balance loss, we set 𝛼to 0.0001, just to avoid extreme imbalance within any single sequence. The MTP loss weight 𝜆is set to 0.3 for the first 10T tokens, and to 0.1 for the remaining 4.8T tokens. 2K 11K 20K 29K 38K 47K 56K 65K 74K 83K 92K 101K 110K 119K 128K Context Length (#Tokens) 0 7 14 21 29 36 43 50 57 64 71 79 86 93 100 Document Depth Percent (%) Pressure Testing DeepSeek-V3 128K Context via \"Needle In A HayStack\" 1 2 3 4 5 6 7 8 9 10 Score Figure 8 | Evaluation results on the ”Needle In A Haystack” (NIAH) tests. DeepSeek-V3 performs well across all context window lengths up to 128K. 4.3. Long Context Extension We adopt a similar approach to DeepSeek-V2 (DeepSeek-AI, 2024c) to enable long context capabilities in DeepSeek-V3. After the pre-training stage, we apply YaRN (Peng et al., 2023a) for context extension and perform two additional training phases, each comprising 1000 steps, to progressively expand the context window from 4K to 32K and then to 128K.", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "fe5300734e4b9827d68c1fdbc0c9b9d2e3a428bb4d8062b0aa708d2e9a418d8b"}
{"doc_id": "arxiv:2412.19437#method:part-13", "url": "https://arxiv.org/abs/2412.19437", "anchor": "#method:part-13", "type": "paper", "title": "", "section": "Method", "text": "| Evaluation results on the ”Needle In A Haystack” (NIAH) tests. DeepSeek-V3 performs well across all context window lengths up to 128K. 4.3. Long Context Extension We adopt a similar approach to DeepSeek-V2 (DeepSeek-AI, 2024c) to enable long context capabilities in DeepSeek-V3. After the pre-training stage, we apply YaRN (Peng et al., 2023a) for context extension and perform two additional training phases, each comprising 1000 steps, to progressively expand the context window from 4K to 32K and then to 128K. The YaRN configuration is consistent with that used in DeepSeek-V2, being applied exclusively to the decoupled shared key k𝑅 𝑡. The hyper-parameters remain identical across both phases, with the scale 𝑠= 40, 𝛼= 1, 𝛽= 32, and the scaling factor √ 𝑡= 0.1 ln 𝑠+ 1. In the first phase, the sequence length is set to 32K, and the batch size is 1920. During the second phase, the sequence length is increased to 128K, and the batch size is reduced to 480. The learning rate for both phases is set to 7.3 × 10−6, matching the final learning rate from the pre-training stage. Through this two-phase extension training, DeepSeek-V3 is capable of handling inputs up to 128K in length while maintaining strong performance. Figure 8 illustrates that DeepSeek-V3, following supervised fine-tuning, achieves notable performance on the \"Needle In A Haystack\" (NIAH) test, demonstrating consistent robustness across context window lengths up to 128K. 23 4.4. Evaluations 4.4.1. Evaluation Benchmarks The base model of DeepSeek-V3 is pretrained on a multilingual corpus with English and Chinese constituting the majority, so we evaluate its performance on a series of benchmarks primarily in English and Chinese, as well as on a multilingual benchmark. Our evaluation is based on our internal evaluation framework integrated in our HAI-LLM framework. Considered benchmarks are categorized and listed as follows, where underlined benchmarks are in Chinese and double-underlined benchmarks are multilingual ones: Multi-subject multiple-choice datasets include MMLU (Hendrycks et al., 2020), MMLU- Redux (Gema et al., 2024), MMLU-Pro (Wang et al., 2024b), MMMLU (OpenAI, 2024b), C-Eval (Huang et al., 2023), and CMMLU (Li et al., 2023). Language understanding and reasoning datasets include HellaSwag (Zellers et al., 2019), PIQA (Bisk et al., 2020), ARC (Clark et al., 2018), and BigBench Hard (BBH) (Suzgun et al., 2022). Closed-book question answering datasets include TriviaQA (Joshi et al., 2017) and Natu- ralQuestions (Kwiatkowski et al., 2019). Reading comprehension datasets include RACE Lai et al. (2017), DROP (Dua et al., 2019), C3 (Sun et al., 2019a), and CMRC (Cui et al., 2019). Reference disambiguation datasets include CLUEWSC (Xu et al., 2020) and WinoGrande Sakaguchi et al. (2019). Language modeling datasets include Pile (Gao et al., 2020). Chinese understanding and culture datasets include CCPM (Li et al., 2021). Math datasets include GSM8K (Cobbe et al., 2021), MATH (Hendrycks et al., 2021), MGSM (Shi et al., 2023), and CMath (Wei et al., 2023). Code datasets include HumanEval (Chen et al., 2021), LiveCodeBench-Base (0801-1101) (Jain et al., 2024), MBPP (Austin et al., 2021), and CRUXEval (Gu et al., 2024). Standardized exams include AGIEval (Zhong et al., 2023). Note that AGIEval", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "5ddfe75fbec232d6b90e20f2bfa41b2bca457e68cee30cf18b6a451a8d61215e"}
{"doc_id": "arxiv:2412.19437#method:part-14", "url": "https://arxiv.org/abs/2412.19437", "anchor": "#method:part-14", "type": "paper", "title": "", "section": "Method", "text": "Language modeling datasets include Pile (Gao et al., 2020). Chinese understanding and culture datasets include CCPM (Li et al., 2021). Math datasets include GSM8K (Cobbe et al., 2021), MATH (Hendrycks et al., 2021), MGSM (Shi et al., 2023), and CMath (Wei et al., 2023). Code datasets include HumanEval (Chen et al., 2021), LiveCodeBench-Base (0801-1101) (Jain et al., 2024), MBPP (Austin et al., 2021), and CRUXEval (Gu et al., 2024). Standardized exams include AGIEval (Zhong et al., 2023). Note that AGIEval includes both English and Chinese subsets. Following our previous work (DeepSeek-AI, 2024b,c), we adopt perplexity-based eval- uation for datasets including HellaSwag, PIQA, WinoGrande, RACE-Middle, RACE-High, MMLU, MMLU-Redux, MMLU-Pro, MMMLU, ARC-Easy, ARC-Challenge, C-Eval, CMMLU, C3, and CCPM, and adopt generation-based evaluation for TriviaQA, NaturalQuestions, DROP, MATH, GSM8K, MGSM, HumanEval, MBPP, LiveCodeBench-Base, CRUXEval, BBH, AGIEval, CLUEWSC, CMRC, and CMath. In addition, we perform language-modeling-based evaluation for Pile-test and use Bits-Per-Byte (BPB) as the metric to guarantee fair comparison among models using different tokenizers. 4.4.2. Evaluation Results In Table 3, we compare the base model of DeepSeek-V3 with the state-of-the-art open-source base models, including DeepSeek-V2-Base (DeepSeek-AI, 2024c) (our previous release), Qwen2.5 72B Base (Qwen, 2024b), and LLaMA-3.1 405B Base (AI@Meta, 2024b). We evaluate all these models with our internal evaluation framework, and ensure that they share the same evaluation setting. Note that due to the changes in our evaluation framework over the past months, the performance 24 Benchmark (Metric) # Shots DeepSeek-V2 Qwen2.5 LLaMA-3.1 DeepSeek-V3 Base 72B Base 405B Base Base Architecture - MoE Dense Dense MoE # Activated Params - 21B 72B 405B 37B # Total Params - 236B 72B 405B 671B English Pile-test (BPB) - 0.606 0.638 0.542 0.548 BBH (EM) 3-shot 78.8 79.8 82.9 87.5 MMLU (EM) 5-shot 78.4 85.0 84.4 87.1 MMLU-Redux (EM) 5-shot 75.6 83.2 81.3 86.2 MMLU-Pro (EM) 5-shot 51.4 58.3 52.8 64.4 DROP (F1) 3-shot 80.4 80.6 86.0 89.0 ARC-Easy (EM) 25-shot 97.6 98.4 98.4 98.9 ARC-Challenge (EM) 25-shot 92.2 94.5 95.3 95.3 HellaSwag (EM) 10-shot 87.1 84.8 89.2 88.9 PIQA (EM) 0-shot 83.9 82.6 85.9 84.7 WinoGrande (EM) 5-shot 86.3 82.3 85.2 84.9 RACE-Middle (EM) 5-shot 73.1 68.1 74.2 67.1 RACE-High (EM) 5-shot 52.6 50.3 56.8 51.3 TriviaQA (EM) 5-shot 80.0 71.9 82.7 82.9 NaturalQuestions (EM) 5-shot 38.6 33.2 41.5 40.0 AGIEval (EM) 0-shot 57.5 75.8 60.6 79.6 Code HumanEval (Pass@1) 0-shot 43.3 53.0 54.9 65.2 MBPP (Pass@1) 3-shot 65.0 72.6 68.4 75.4 LiveCodeBench-Base (Pass@1) 3-shot 11.6 12.9 15.5 19.4 CRUXEval-I (EM) 2-shot 52.5 59.1 58.5 67.3 CRUXEval-O (EM) 2-shot 49.8 59.9 59.9 69.8 Math GSM8K (EM) 8-shot 81.6 88.3 83.5 89.3 MATH (EM) 4-shot 43.4 54.4 49.0 61.6 MGSM (EM) 8-shot 63.6 76.2 69.9 79.8 CMath (EM) 3-shot 78.7 84.5 77.3 90.7 Chinese CLUEWSC (EM) 5-shot 82.0 82.5 83.0 82.7 C-Eval (EM) 5-shot 81.4 89.2 72.5 90.1 CMMLU (EM) 5-shot 84.0 89.5 73.7 88.8 CMRC (EM) 1-shot 77.4 75.8 76.0 76.3 C3 (EM) 0-shot 77.4 76.7 79.7 78.6 CCPM (EM) 0-shot 93.0 88.5 78.6 92.0 Multilingual MMMLU-non-English (EM) 5-shot 64.0 74.8 73.8 79.4 Table 3 | Comparison among DeepSeek-V3-Base and other representative open-source base models. All", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "8e72065386fd5c9762de6e280452d47b987c70a376410ba4f945acd9e8e661aa"}
{"doc_id": "arxiv:2412.19437#method:part-15", "url": "https://arxiv.org/abs/2412.19437", "anchor": "#method:part-15", "type": "paper", "title": "", "section": "Method", "text": "49.0 61.6 MGSM (EM) 8-shot 63.6 76.2 69.9 79.8 CMath (EM) 3-shot 78.7 84.5 77.3 90.7 Chinese CLUEWSC (EM) 5-shot 82.0 82.5 83.0 82.7 C-Eval (EM) 5-shot 81.4 89.2 72.5 90.1 CMMLU (EM) 5-shot 84.0 89.5 73.7 88.8 CMRC (EM) 1-shot 77.4 75.8 76.0 76.3 C3 (EM) 0-shot 77.4 76.7 79.7 78.6 CCPM (EM) 0-shot 93.0 88.5 78.6 92.0 Multilingual MMMLU-non-English (EM) 5-shot 64.0 74.8 73.8 79.4 Table 3 | Comparison among DeepSeek-V3-Base and other representative open-source base models. All models are evaluated in our internal framework and share the same evaluation setting. Scores with a gap not exceeding 0.3 are considered to be at the same level. DeepSeek- V3-Base achieves the best performance on most benchmarks, especially on math and code tasks. of DeepSeek-V2-Base exhibits a slight difference from our previously reported results. Overall, DeepSeek-V3-Base comprehensively outperforms DeepSeek-V2-Base and Qwen2.5 72B Base, and surpasses LLaMA-3.1 405B Base in the majority of benchmarks, essentially becoming the strongest open-source model. From a more detailed perspective, we compare DeepSeek-V3-Base with the other open-source base models individually. (1) Compared with DeepSeek-V2-Base, due to the improvements in our model architecture, the scale-up of the model size and training tokens, and the enhancement of data quality, DeepSeek-V3-Base achieves significantly better performance as expected. (2) Compared with Qwen2.5 72B Base, the state-of-the-art Chinese open-source model, with only half of the activated parameters, DeepSeek-V3-Base also demonstrates remarkable advantages, 25 especially on English, multilingual, code, and math benchmarks. As for Chinese benchmarks, except for CMMLU, a Chinese multi-subject multiple-choice task, DeepSeek-V3-Base also shows better performance than Qwen2.5 72B. (3) Compared with LLaMA-3.1 405B Base, the largest open-source model with 11 times the activated parameters, DeepSeek-V3-Base also exhibits much better performance on multilingual, code, and math benchmarks. As for English and Chinese language benchmarks, DeepSeek-V3-Base shows competitive or better performance, and is especially good on BBH, MMLU-series, DROP, C-Eval, CMMLU, and CCPM. Due to our efficient architectures and comprehensive engineering optimizations, DeepSeek- V3 achieves extremely high training efficiency. Under our training framework and infrastruc- tures, training DeepSeek-V3 on each trillion tokens requires only 180K H800 GPU hours, which is much cheaper than training 72B or 405B dense models. Benchmark (Metric) # Shots Small MoE Small MoE Large MoE Large MoE Baseline w/ MTP Baseline w/ MTP # Activated Params (Inference) - 2.4B 2.4B 20.9B 20.9B # Total Params (Inference) - 15.7B 15.7B 228.7B 228.7B # Training Tokens - 1.33T 1.33T 540B 540B Pile-test (BPB) - 0.729 0.729 0.658 0.657 BBH (EM) 3-shot 39.0 41.4 70.0 70.7 MMLU (EM) 5-shot 50.0 53.3 67.5 66.6 DROP (F1) 1-shot 39.2 41.3 68.5 70.6 TriviaQA (EM) 5-shot 56.9 57.7 67.0 67.3 NaturalQuestions (EM) 5-shot 22.7 22.3 27.2 28.5 HumanEval (Pass@1) 0-shot 20.7 26.8 44.5 53.7 MBPP (Pass@1) 3-shot 35.8 36.8 61.6 62.2 GSM8K (EM) 8-shot 25.4 31.4 72.3 74.0 MATH (EM) 4-shot 10.7 12.6 38.6 39.8 Table 4 | Ablation results for the MTP strategy. The MTP strategy consistently enhances the model performance on most of the evaluation benchmarks. 4.5. Discussion 4.5.1. Ablation Studies for Multi-Token Prediction In Table 4, we show the ablation results", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "9cf36e60f25c243cb20dd3a98e0b169b2b4031ab5abfeae72749994ac7ed339b"}
{"doc_id": "arxiv:2412.19437#method:part-16", "url": "https://arxiv.org/abs/2412.19437", "anchor": "#method:part-16", "type": "paper", "title": "", "section": "Method", "text": "(EM) 5-shot 56.9 57.7 67.0 67.3 NaturalQuestions (EM) 5-shot 22.7 22.3 27.2 28.5 HumanEval (Pass@1) 0-shot 20.7 26.8 44.5 53.7 MBPP (Pass@1) 3-shot 35.8 36.8 61.6 62.2 GSM8K (EM) 8-shot 25.4 31.4 72.3 74.0 MATH (EM) 4-shot 10.7 12.6 38.6 39.8 Table 4 | Ablation results for the MTP strategy. The MTP strategy consistently enhances the model performance on most of the evaluation benchmarks. 4.5. Discussion 4.5.1. Ablation Studies for Multi-Token Prediction In Table 4, we show the ablation results for the MTP strategy. To be specific, we validate the MTP strategy on top of two baseline models across different scales. At the small scale, we train a baseline MoE model comprising 15.7B total parameters on 1.33T tokens. At the large scale, we train a baseline MoE model comprising 228.7B total parameters on 540B tokens. On top of them, keeping the training data and the other architectures the same, we append a 1-depth MTP module onto them and train two models with the MTP strategy for comparison. Note that during inference, we directly discard the MTP module, so the inference costs of the compared models are exactly the same. From the table, we can observe that the MTP strategy consistently enhances the model performance on most of the evaluation benchmarks. 4.5.2. Ablation Studies for the Auxiliary-Loss-Free Balancing Strategy In Table 5, we show the ablation results for the auxiliary-loss-free balancing strategy. We validate this strategy on top of two baseline models across different scales. At the small scale, we train a baseline MoE model comprising 15.7B total parameters on 1.33T tokens. At the large scale, we train a baseline MoE model comprising 228.7B total parameters on 578B tokens. 26 Benchmark (Metric) # Shots Small MoE Small MoE Large MoE Large MoE Aux-Loss-Based Aux-Loss-Free Aux-Loss-Based Aux-Loss-Free # Activated Params - 2.4B 2.4B 20.9B 20.9B # Total Params - 15.7B 15.7B 228.7B 228.7B # Training Tokens - 1.33T 1.33T 578B 578B Pile-test (BPB) - 0.727 0.724 0.656 0.652 BBH (EM) 3-shot 37.3 39.3 66.7 67.9 MMLU (EM) 5-shot 51.0 51.8 68.3 67.2 DROP (F1) 1-shot 38.1 39.0 67.1 67.1 TriviaQA (EM) 5-shot 58.3 58.5 66.7 67.7 NaturalQuestions (EM) 5-shot 23.2 23.4 27.1 28.1 HumanEval (Pass@1) 0-shot 22.0 22.6 40.2 46.3 MBPP (Pass@1) 3-shot 36.6 35.8 59.2 61.2 GSM8K (EM) 8-shot 27.1 29.6 70.7 74.5 MATH (EM) 4-shot 10.9 11.1 37.2 39.6 Table 5 | Ablation results for the auxiliary-loss-free balancing strategy. Compared with the purely auxiliary-loss-based method, the auxiliary-loss-free strategy consistently achieves better model performance on most of the evaluation benchmarks. Both of the baseline models purely use auxiliary losses to encourage load balance, and use the sigmoid gating function with top-K affinity normalization. Their hyper-parameters to control the strength of auxiliary losses are the same as DeepSeek-V2-Lite and DeepSeek-V2, respectively. On top of these two baseline models, keeping the training data and the other architectures the same, we remove all auxiliary losses and introduce the auxiliary-loss-free balancing strategy for comparison. From the table, we can observe that the auxiliary-loss-free strategy consistently achieves better model performance on most of the evaluation benchmarks. 4.5.3. Batch-Wise", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "4e7de325f9de42e9adb6cdaa8fa42c22583b79e757c1c87761fc9a4a0f3b32db"}
{"doc_id": "arxiv:2412.19437#method:part-17", "url": "https://arxiv.org/abs/2412.19437", "anchor": "#method:part-17", "type": "paper", "title": "", "section": "Method", "text": "and use the sigmoid gating function with top-K affinity normalization. Their hyper-parameters to control the strength of auxiliary losses are the same as DeepSeek-V2-Lite and DeepSeek-V2, respectively. On top of these two baseline models, keeping the training data and the other architectures the same, we remove all auxiliary losses and introduce the auxiliary-loss-free balancing strategy for comparison. From the table, we can observe that the auxiliary-loss-free strategy consistently achieves better model performance on most of the evaluation benchmarks. 4.5.3. Batch-Wise Load Balance VS. Sequence-Wise Load Balance The key distinction between auxiliary-loss-free balancing and sequence-wise auxiliary loss lies in their balancing scope: batch-wise versus sequence-wise. Compared with the sequence-wise auxiliary loss, batch-wise balancing imposes a more flexible constraint, as it does not enforce in-domain balance on each sequence. This flexibility allows experts to better specialize in different domains. To validate this, we record and analyze the expert load of a 16B auxiliary- loss-based baseline and a 16B auxiliary-loss-free model on different domains in the Pile test set. As illustrated in Figure 9, we observe that the auxiliary-loss-free model demonstrates greater expert specialization patterns as expected. To further investigate the correlation between this flexibility and the advantage in model performance, we additionally design and validate a batch-wise auxiliary loss that encourages load balance on each training batch instead of on each sequence. The experimental results show that, when achieving a similar level of batch-wise load balance, the batch-wise auxiliary loss can also achieve similar model performance to the auxiliary-loss-free method. To be specific, in our experiments with 1B MoE models, the validation losses are: 2.258 (using a sequence- wise auxiliary loss), 2.253 (using the auxiliary-loss-free method), and 2.253 (using a batch-wise auxiliary loss). We also observe similar results on 3B MoE models: the model using a sequence- wise auxiliary loss achieves a validation loss of 2.085, and the models using the auxiliary-loss-free method or a batch-wise auxiliary loss achieve the same validation loss of 2.080. In addition, although the batch-wise load balancing methods show consistent performance advantages, they also face two potential challenges in efficiency: (1) load imbalance within 27 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 Wikipedia (en) Github DM Mathematics Aux-Loss-Based Layer 9 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 Wikipedia (en) Github DM Mathematics Aux-Loss-Free Layer 9 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "b86c665638e7195e6aa9c6add1044fac936afdae9d0e981272e73fed0701316c"}
{"doc_id": "arxiv:2412.19437#method:part-18", "url": "https://arxiv.org/abs/2412.19437", "anchor": "#method:part-18", "type": "paper", "title": "", "section": "Method", "text": "13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 Wikipedia (en) Github DM Mathematics Aux-Loss-Free Layer 9 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 Wikipedia (en) Github DM Mathematics Aux-Loss-Based Layer 18 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 Wikipedia (en) Github DM Mathematics Aux-Loss-Free Layer 18 0 2 4 6 8 10 Relative Expert Load Figure 9 | Expert load of auxiliary-loss-free and auxiliary-loss-based models on three domains in the Pile test set. The auxiliary-loss-free model shows greater expert specialization patterns than the auxiliary-loss-based one. The relative expert load denotes the ratio between the actual expert load and the theoretically balanced expert load. Due to space constraints, we only present the results of two layers as an example, with the results of all layers provided in Appendix C. certain sequences or small batches, and (2) domain-shift-induced load imbalance during infer- ence. The first challenge is naturally addressed by our training framework that uses large-scale expert parallelism and data parallelism, which guarantees a large size of each micro-batch. For the second challenge, we also design and implement an efficient inference framework with redundant expert deployment, as described in Section 3.4, to overcome it. 5. Post-Training 5.1. Supervised Fine-Tuning We curate our instruction-tuning datasets to include 1.5M instances spanning multiple domains, with each domain employing distinct data creation methods tailored to its specific requirements. Reasoning Data. For reasoning-related datasets, including those focused on mathematics, code competition problems, and logic puzzles, we generate the data by leveraging an internal DeepSeek-R1 model. Specifically, while the R1-generated data demonstrates strong accuracy, it suffers from issues such as overthinking, poor formatting, and excessive length. Our objective is to balance the high accuracy of R1-generated reasoning data and the clarity and conciseness of regularly formatted reasoning data. To establish our methodology, we begin by developing an expert model tailored to a specific domain, such as code, mathematics, or general reasoning, using a combined Supervised Fine- Tuning (SFT) and Reinforcement Learning (RL) training pipeline. This expert model serves as a data generator for the final model. The training process involves generating two distinct types of SFT samples for each instance: the first", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "5602dfa600d09a5ce5b17bf1a37c88c06fe9de5cdee481867408e6973fab8263"}
{"doc_id": "arxiv:2412.19437#method:part-19", "url": "https://arxiv.org/abs/2412.19437", "anchor": "#method:part-19", "type": "paper", "title": "", "section": "Method", "text": "high accuracy of R1-generated reasoning data and the clarity and conciseness of regularly formatted reasoning data. To establish our methodology, we begin by developing an expert model tailored to a specific domain, such as code, mathematics, or general reasoning, using a combined Supervised Fine- Tuning (SFT) and Reinforcement Learning (RL) training pipeline. This expert model serves as a data generator for the final model. The training process involves generating two distinct types of SFT samples for each instance: the first couples the problem with its original response in the format of <problem, original response>, while the second incorporates a system prompt 28 alongside the problem and the R1 response in the format of <system prompt, problem, R1 response>. The system prompt is meticulously designed to include instructions that guide the model toward producing responses enriched with mechanisms for reflection and verification. During the RL phase, the model leverages high-temperature sampling to generate responses that integrate patterns from both the R1-generated and original data, even in the absence of explicit system prompts. After hundreds of RL steps, the intermediate RL model learns to incorporate R1 patterns, thereby enhancing overall performance strategically. Upon completing the RL training phase, we implement rejection sampling to curate high- quality SFT data for the final model, where the expert models are used as data generation sources. This method ensures that the final training data retains the strengths of DeepSeek-R1 while producing responses that are concise and effective. Non-Reasoning Data. For non-reasoning data, such as creative writing, role-play, and sim- ple question answering, we utilize DeepSeek-V2.5 to generate responses and enlist human annotators to verify the accuracy and correctness of the data. SFT Settings. We fine-tune DeepSeek-V3-Base for two epochs using the SFT dataset, using the cosine decay learning rate scheduling that starts at 5 × 10−6 and gradually decreases to 1 × 10−6. During training, each single sequence is packed from multiple samples. However, we adopt a sample masking strategy to ensure that these examples remain isolated and mutually invisible. 5.2. Reinforcement Learning 5.2.1. Reward Model We employ a rule-based Reward Model (RM) and a model-based RM in our RL process. Rule-Based RM. For questions that can be validated using specific rules, we adopt a rule- based reward system to determine the feedback. For instance, certain math problems have deterministic results, and we require the model to provide the final answer within a designated format (e.g., in a box), allowing us to apply rules to verify the correctness. Similarly, for LeetCode problems, we can utilize a compiler to generate feedback based on test cases. By leveraging rule-based validation wherever possible, we ensure a higher level of reliability, as this approach is resistant to manipulation or exploitation. Model-Based RM. For questions with free-form ground-truth answers, we rely on the reward model to determine whether the response matches the expected ground-truth. Conversely, for questions without a definitive ground-truth, such as those involving creative writing, the reward model is tasked with providing feedback based on the question and the corresponding answer as inputs. The reward model is trained from the DeepSeek-V3", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "7bd4322fb52efd83dfba1a25dbfc13c56305ad4400eb64489e57aac44893887e"}
{"doc_id": "arxiv:2412.19437#method:part-20", "url": "https://arxiv.org/abs/2412.19437", "anchor": "#method:part-20", "type": "paper", "title": "", "section": "Method", "text": "wherever possible, we ensure a higher level of reliability, as this approach is resistant to manipulation or exploitation. Model-Based RM. For questions with free-form ground-truth answers, we rely on the reward model to determine whether the response matches the expected ground-truth. Conversely, for questions without a definitive ground-truth, such as those involving creative writing, the reward model is tasked with providing feedback based on the question and the corresponding answer as inputs. The reward model is trained from the DeepSeek-V3 SFT checkpoints. To enhance its reliability, we construct preference data that not only provides the final reward but also includes the chain-of-thought leading to the reward. This approach helps mitigate the risk of reward hacking in specific tasks. 29 5.2.2. Group Relative Policy Optimization Similar to DeepSeek-V2 (DeepSeek-AI, 2024c), we adopt Group Relative Policy Optimiza- tion (GRPO) (Shao et al., 2024), which foregoes the critic model that is typically with the same size as the policy model, and estimates the baseline from group scores instead. Specifically, for each question 𝑞, GRPO samples a group of outputs {𝑜1, 𝑜2, · · · , 𝑜𝐺} from the old policy model 𝜋𝜃𝑜𝑙𝑑and then optimizes the policy model 𝜋𝜃by maximizing the following objective: J𝐺𝑅𝑃𝑂(𝜃) = E[𝑞∼𝑃(𝑄), {𝑜𝑖}𝐺 𝑖=1 ∼𝜋𝜃𝑜𝑙𝑑(𝑂|𝑞)] 1 𝐺 𝐺 ∑︁ 𝑖=1 \u0012 min \u0012 𝜋𝜃(𝑜𝑖|𝑞) 𝜋𝜃𝑜𝑙𝑑(𝑜𝑖|𝑞) 𝐴𝑖, clip \u0012 𝜋𝜃(𝑜𝑖|𝑞) 𝜋𝜃𝑜𝑙𝑑(𝑜𝑖|𝑞) , 1 −𝜀, 1 + 𝜀 \u0013 𝐴𝑖 \u0013 −𝛽D𝐾𝐿 \u0000𝜋𝜃||𝜋𝑟𝑒𝑓 \u0001\u0013 , (26) D𝐾𝐿 \u0000𝜋𝜃||𝜋𝑟𝑒𝑓 \u0001 = 𝜋𝑟𝑒𝑓(𝑜𝑖|𝑞) 𝜋𝜃(𝑜𝑖|𝑞) −log 𝜋𝑟𝑒𝑓(𝑜𝑖|𝑞) 𝜋𝜃(𝑜𝑖|𝑞) −1, (27) where 𝜀and 𝛽are hyper-parameters; 𝜋𝑟𝑒𝑓is the reference model; and 𝐴𝑖is the advantage, derived from the rewards {𝑟1, 𝑟2, . . . , 𝑟𝐺} corresponding to the outputs within each group: 𝐴𝑖= 𝑟𝑖−mean({𝑟1, 𝑟2, · · · , 𝑟𝐺}) std({𝑟1, 𝑟2, · · · , 𝑟𝐺}) . (28) We incorporate prompts from diverse domains, such as coding, math, writing, role-playing, and question answering, during the RL process. This approach not only aligns the model more closely with human preferences but also enhances performance on benchmarks, especially in scenarios where available SFT data are limited. 5.3. Evaluations 5.3.1. Evaluation Settings Evaluation Benchmarks. Apart from the benchmark we used for base model testing, we further evaluate instructed models on IFEval (Zhou et al., 2023), FRAMES (Krishna et al., 2024), LongBench v2 (Bai et al., 2024), GPQA (Rein et al., 2023), SimpleQA (OpenAI, 2024c), C- SimpleQA (He et al., 2024), SWE-Bench Verified (OpenAI, 2024d), Aider 1, LiveCodeBench (Jain et al., 2024) (questions from August 2024 to November 2024), Codeforces 2, Chinese National High School Mathematics Olympiad (CNMO 2024)3, and American Invitational Mathematics Examination 2024 (AIME 2024) (MAA, 2024). Compared Baselines. We conduct comprehensive evaluations of our chat model against sev- eral strong baselines, including DeepSeek-V2-0506, DeepSeek-V2.5-0905, Qwen2.5 72B Instruct, LLaMA-3.1 405B Instruct, Claude-Sonnet-3.5-1022, and GPT-4o-0513. For the DeepSeek-V2 model series, we select the most representative variants for comparison. For closed-source models, evaluations are performed through their respective APIs. Detailed Evaluation Configurations. For standard benchmarks including MMLU, DROP, GPQA, and SimpleQA, we adopt the evaluation prompts from the simple-evals framework4. 1https://aider.chat 2https://codeforces.com 3https://www.cms.org.cn/Home/comp/comp/cid/12.html 4https://github.com/openai/simple-evals 30 We utilize the Zero-Eval prompt format (Lin,", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "112476cdbcd40801c0954cf03e1a9f81818be0fc8099e444162aa64c67ef6a42"}
{"doc_id": "arxiv:2412.19437#method:part-21", "url": "https://arxiv.org/abs/2412.19437", "anchor": "#method:part-21", "type": "paper", "title": "", "section": "Method", "text": "conduct comprehensive evaluations of our chat model against sev- eral strong baselines, including DeepSeek-V2-0506, DeepSeek-V2.5-0905, Qwen2.5 72B Instruct, LLaMA-3.1 405B Instruct, Claude-Sonnet-3.5-1022, and GPT-4o-0513. For the DeepSeek-V2 model series, we select the most representative variants for comparison. For closed-source models, evaluations are performed through their respective APIs. Detailed Evaluation Configurations. For standard benchmarks including MMLU, DROP, GPQA, and SimpleQA, we adopt the evaluation prompts from the simple-evals framework4. 1https://aider.chat 2https://codeforces.com 3https://www.cms.org.cn/Home/comp/comp/cid/12.html 4https://github.com/openai/simple-evals 30 We utilize the Zero-Eval prompt format (Lin, 2024) for MMLU-Redux in a zero-shot setting. For other datasets, we follow their original evaluation protocols with default prompts as pro- vided by the dataset creators. For code and math benchmarks, the HumanEval-Mul dataset includes 8 mainstream programming languages (Python, Java, Cpp, C#, JavaScript, TypeScript, PHP, and Bash) in total. We use CoT and non-CoT methods to evaluate model performance on LiveCodeBench, where the data are collected from August 2024 to November 2024. The Codeforces dataset is measured using the percentage of competitors. SWE-Bench verified is evaluated using the agentless framework (Xia et al., 2024). We use the “diff” format to evaluate the Aider-related benchmarks. For mathematical assessments, AIME and CNMO 2024 are evaluated with a temperature of 0.7, and the results are averaged over 16 runs, while MATH-500 employs greedy decoding. We allow all models to output a maximum of 8192 tokens for each benchmark. Benchmark (Metric) DeepSeek DeepSeek Qwen2.5 LLaMA-3.1 Claude-3.5- GPT-4o DeepSeek V2-0506 V2.5-0905 72B-Inst. 405B-Inst. Sonnet-1022 0513 V3 Architecture MoE MoE Dense Dense - - MoE # Activated Params 21B 21B 72B 405B - - 37B # Total Params 236B 236B 72B 405B - - 671B English MMLU (EM) 78.2 80.6 85.3 88.6 88.3 87.2 88.5 MMLU-Redux (EM) 77.9 80.3 85.6 86.2 88.9 88.0 89.1 MMLU-Pro (EM) 58.5 66.2 71.6 73.3 78.0 72.6 75.9 DROP (3-shot F1) 83.0 87.8 76.7 88.7 88.3 83.7 91.6 IF-Eval (Prompt Strict) 57.7 80.6 84.1 86.0 86.5 84.3 86.1 GPQA-Diamond (Pass@1) 35.3 41.3 49.0 51.1 65.0 49.9 59.1 SimpleQA (Correct) 9.0 10.2 9.1 17.1 28.4 38.2 24.9 FRAMES (Acc.) 66.9 65.4 69.8 70.0 72.5 80.5 73.3 LongBench v2 (Acc.) 31.6 35.4 39.4 36.1 41.0 48.1 48.7 Code HumanEval-Mul (Pass@1) 69.3 77.4 77.3 77.2 81.7 80.5 82.6 LiveCodeBench (Pass@1-COT) 18.8 29.2 31.1 28.4 36.3 33.4 40.5 LiveCodeBench (Pass@1) 20.3 28.4 28.7 30.1 32.8 34.2 37.6 Codeforces (Percentile) 17.5 35.6 24.8 25.3 20.3 23.6 51.6 SWE Verified (Resolved) - 22.6 23.8 24.5 50.8 38.8 42.0 Aider-Edit (Acc.) 60.3 71.6 65.4 63.9 84.2 72.9 79.7 Aider-Polyglot (Acc.) - 18.2 7.6 5.8 45.3 16.0 49.6 Math AIME 2024 (Pass@1) 4.6 16.7 23.3 23.3 16.0 9.3 39.2 MATH-500 (EM) 56.3 74.7 80.0 73.8 78.3 74.6 90.2 CNMO 2024 (Pass@1) 2.8 10.8 15.9 6.8 13.1 10.8 43.2 Chinese CLUEWSC (EM) 89.9 90.4 91.4 84.7 85.4 87.9 90.9 C-Eval (EM) 78.6 79.5 86.1 61.5 76.7 76.0 86.5 C-SimpleQA (Correct) 48.5 54.1 48.4 50.4 51.3 59.3 64.8 Table 6 | Comparison between DeepSeek-V3 and other representative chat models. All models are evaluated in a configuration that limits the output length to 8K. Benchmarks containing fewer than 1000 samples are tested multiple", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "e1ae89246218133e311548fe9cf9faff73ca3a5fca2dc0ccb825c45c6eca2514"}
{"doc_id": "arxiv:2412.19437#method:part-22", "url": "https://arxiv.org/abs/2412.19437", "anchor": "#method:part-22", "type": "paper", "title": "", "section": "Method", "text": "(EM) 56.3 74.7 80.0 73.8 78.3 74.6 90.2 CNMO 2024 (Pass@1) 2.8 10.8 15.9 6.8 13.1 10.8 43.2 Chinese CLUEWSC (EM) 89.9 90.4 91.4 84.7 85.4 87.9 90.9 C-Eval (EM) 78.6 79.5 86.1 61.5 76.7 76.0 86.5 C-SimpleQA (Correct) 48.5 54.1 48.4 50.4 51.3 59.3 64.8 Table 6 | Comparison between DeepSeek-V3 and other representative chat models. All models are evaluated in a configuration that limits the output length to 8K. Benchmarks containing fewer than 1000 samples are tested multiple times using varying temperature settings to derive robust final results. DeepSeek-V3 stands as the best-performing open-source model, and also exhibits competitive performance against frontier closed-source models. 5.3.2. Standard Evaluation Table 6 presents the evaluation results, showcasing that DeepSeek-V3 stands as the best- performing open-source model. Additionally, it is competitive against frontier closed-source models like GPT-4o and Claude-3.5-Sonnet. 31 English Benchmarks. MMLU is a widely recognized benchmark designed to assess the perfor- mance of large language models, across diverse knowledge domains and tasks. DeepSeek-V3 demonstrates competitive performance, standing on par with top-tier models such as LLaMA- 3.1-405B, GPT-4o, and Claude-Sonnet 3.5, while significantly outperforming Qwen2.5 72B. Moreover, DeepSeek-V3 excels in MMLU-Pro, a more challenging educational knowledge benchmark, where it closely trails Claude-Sonnet 3.5. On MMLU-Redux, a refined version of MMLU with corrected labels, DeepSeek-V3 surpasses its peers. In addition, on GPQA-Diamond, a PhD-level evaluation testbed, DeepSeek-V3 achieves remarkable results, ranking just behind Claude 3.5 Sonnet and outperforming all other competitors by a substantial margin. In long-context understanding benchmarks such as DROP, LongBench v2, and FRAMES, DeepSeek-V3 continues to demonstrate its position as a top-tier model. It achieves an impressive 91.6 F1 score in the 3-shot setting on DROP, outperforming all other models in this category. On FRAMES, a benchmark requiring question-answering over 100k token contexts, DeepSeek- V3 closely trails GPT-4o while outperforming all other models by a significant margin. This demonstrates the strong capability of DeepSeek-V3 in handling extremely long-context tasks. The long-context capability of DeepSeek-V3 is further validated by its best-in-class performance on LongBench v2, a dataset that was released just a few weeks before the launch of DeepSeek V3. On the factual knowledge benchmark, SimpleQA, DeepSeek-V3 falls behind GPT-4o and Claude-Sonnet, primarily due to its design focus and resource allocation. DeepSeek-V3 assigns more training tokens to learn Chinese knowledge, leading to exceptional performance on the C-SimpleQA. On the instruction-following benchmark, DeepSeek-V3 significantly outperforms its predecessor, DeepSeek-V2-series, highlighting its improved ability to understand and adhere to user-defined format constraints. Code and Math Benchmarks. Coding is a challenging and practical task for LLMs, encom- passing engineering-focused tasks like SWE-Bench-Verified and Aider, as well as algorithmic tasks such as HumanEval and LiveCodeBench. In engineering tasks, DeepSeek-V3 trails behind Claude-Sonnet-3.5-1022 but significantly outperforms open-source models. The open-source DeepSeek-V3 is expected to foster advancements in coding-related engineering tasks. By pro- viding access to its robust capabilities, DeepSeek-V3 can drive innovation and improvement in areas such as software engineering and algorithm development, empowering developers and researchers to push the boundaries of what open-source models can achieve in coding tasks. In algorithmic tasks, DeepSeek-V3 demonstrates superior performance, outperforming", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "feab84b1954c7c101029fbdefe1bce1673e4235d2b07412104fd74556a487dcc"}
{"doc_id": "arxiv:2412.19437#method:part-23", "url": "https://arxiv.org/abs/2412.19437", "anchor": "#method:part-23", "type": "paper", "title": "", "section": "Method", "text": "as algorithmic tasks such as HumanEval and LiveCodeBench. In engineering tasks, DeepSeek-V3 trails behind Claude-Sonnet-3.5-1022 but significantly outperforms open-source models. The open-source DeepSeek-V3 is expected to foster advancements in coding-related engineering tasks. By pro- viding access to its robust capabilities, DeepSeek-V3 can drive innovation and improvement in areas such as software engineering and algorithm development, empowering developers and researchers to push the boundaries of what open-source models can achieve in coding tasks. In algorithmic tasks, DeepSeek-V3 demonstrates superior performance, outperforming all baselines on benchmarks like HumanEval-Mul and LiveCodeBench. This success can be attributed to its advanced knowledge distillation technique, which effectively enhances its code generation and problem-solving capabilities in algorithm-focused tasks. On math benchmarks, DeepSeek-V3 demonstrates exceptional performance, significantly surpassing baselines and setting a new state-of-the-art for non-o1-like models. Specifically, on AIME, MATH-500, and CNMO 2024, DeepSeek-V3 outperforms the second-best model, Qwen2.5 72B, by approximately 10% in absolute scores, which is a substantial margin for such challenging benchmarks. This remarkable capability highlights the effectiveness of the distillation technique from DeepSeek-R1, which has been proven highly beneficial for non-o1-like models. Chinese Benchmarks. Qwen and DeepSeek are two representative model series with robust support for both Chinese and English. On the factual benchmark Chinese SimpleQA, DeepSeek- V3 surpasses Qwen2.5-72B by 16.4 points, despite Qwen2.5 being trained on a larger corpus compromising 18T tokens, which are 20% more than the 14.8T tokens that DeepSeek-V3 is 32", "source": "arxiv_pdf", "published": "", "tokens": 233, "sha256": "e3f688d96aca9bdbb8d0a8b1abcf1976c5136712b348542fdb53bd3b4fb79c50"}
{"doc_id": "arxiv:2412.19437#model", "url": "https://arxiv.org/abs/2412.19437", "anchor": "#model", "type": "paper", "title": "", "section": "Model", "text": "Arena-Hard AlpacaEval 2.0 DeepSeek-V2.5-0905 76.2 50.5 Qwen2.5-72B-Instruct 81.2 49.1 LLaMA-3.1 405B 69.3 40.5 GPT-4o-0513 80.4 51.1 Claude-Sonnet-3.5-1022 85.2 52.0 DeepSeek-V3 85.5 70.0 Table 7 | English open-ended conversation evaluations. For AlpacaEval 2.0, we use the length- controlled win rate as the metric. pre-trained on. On C-Eval, a representative benchmark for Chinese educational knowledge evaluation, and CLUEWSC (Chinese Winograd Schema Challenge), DeepSeek-V3 and Qwen2.5-72B exhibit similar performance levels, indicating that both models are well-optimized for challenging Chinese-language reasoning and educational tasks. 5.3.3. Open-Ended Evaluation In addition to standard benchmarks, we also evaluate our models on open-ended generation tasks using LLMs as judges, with the results shown in Table 7. Specifically, we adhere to the original configurations of AlpacaEval 2.0 (Dubois et al., 2024) and Arena-Hard (Li et al., 2024a), which leverage GPT-4-Turbo-1106 as judges for pairwise comparisons. On Arena-Hard, DeepSeek-V3 achieves an impressive win rate of over 86% against the baseline GPT-4-0314, performing on par with top-tier models like Claude-Sonnet-3.5-1022. This underscores the robust capabilities of DeepSeek-V3, especially in dealing with complex prompts, including coding and debugging tasks. Furthermore, DeepSeek-V3 achieves a groundbreaking milestone as the first open-source model to surpass 85% on the Arena-Hard benchmark. This achievement significantly bridges the performance gap between open-source and closed-source models, setting a new standard for what open-source models can accomplish in challenging domains. Similarly, DeepSeek-V3 showcases exceptional performance on AlpacaEval 2.0, outperform- ing both closed-source and open-source models. This demonstrates its outstanding proficiency in writing tasks and handling straightforward question-answering scenarios. Notably, it surpasses DeepSeek-V2.5-0905 by a significant margin of 20%, highlighting substantial improvements in tackling simple tasks and showcasing the effectiveness of its advancements. 5.3.4. DeepSeek-V3 as a Generative Reward Model We compare the judgment ability of DeepSeek-V3 with state-of-the-art models, namely GPT-4o and Claude-3.5. Table 8 presents the performance of these models in RewardBench (Lambert et al., 2024). DeepSeek-V3 achieves performance on par with the best versions of GPT-4o-0806 and Claude-3.5-Sonnet-1022, while surpassing other versions. Additionally, the judgment ability of DeepSeek-V3 can also be enhanced by the voting technique. Therefore, we employ DeepSeek- V3 along with voting to offer self-feedback on open-ended questions, thereby improving the effectiveness and robustness of the alignment process. 33", "source": "arxiv_pdf", "published": "", "tokens": 363, "sha256": "b3626ec585ca960b5a9253cc5a03f4b65d4f44553770ef61b99fb9af69149d1f"}
{"doc_id": "arxiv:2412.19437#model:part-1", "url": "https://arxiv.org/abs/2412.19437", "anchor": "#model:part-1", "type": "paper", "title": "", "section": "Model", "text": "LiveCodeBench-CoT MATH-500 Pass@1 Length Pass@1 Length DeepSeek-V2.5 Baseline 31.1 718 74.6 769 DeepSeek-V2.5 +R1 Distill 37.4 783 83.2 1510 Table 9 | The contribution of distillation from DeepSeek-R1. The evaluation settings of Live- CodeBench and MATH-500 are the same as in Table 6. 5.4. Discussion 5.4.1. Distillation from DeepSeek-R1 We ablate the contribution of distillation from DeepSeek-R1 based on DeepSeek-V2.5. The baseline is trained on short CoT data, whereas its competitor uses data generated by the expert checkpoints described above. Table 9 demonstrates the effectiveness of the distillation data, showing significant improve- ments in both LiveCodeBench and MATH-500 benchmarks. Our experiments reveal an inter- esting trade-off: the distillation leads to better performance but also substantially increases the average response length. To maintain a balance between model accuracy and computational efficiency, we carefully selected optimal settings for DeepSeek-V3 in distillation. Our research suggests that knowledge distillation from reasoning models presents a promis- ing direction for post-training optimization. While our current work focuses on distilling data from mathematics and coding domains, this approach shows potential for broader applications across various task domains. The effectiveness demonstrated in these specific areas indicates that long-CoT distillation could be valuable for enhancing model performance in other cogni- tive tasks requiring complex reasoning. Further exploration of this approach across different domains remains an important direction for future research. 5.4.2. Self-Rewarding Rewards play a pivotal role in RL, steering the optimization process. In domains where verifica- tion through external tools is straightforward, such as some coding or mathematics scenarios, RL demonstrates exceptional efficacy. However, in more general scenarios, constructing a feedback mechanism through hard coding is impractical. During the development of DeepSeek-V3, for these broader contexts, we employ the constitutional AI approach (Bai et al., 2022), leveraging the voting evaluation results of DeepSeek-V3 itself as a feedback source. This method has 34 produced notable alignment effects, significantly enhancing the performance of DeepSeek-V3 in subjective evaluations. By integrating additional constitutional inputs, DeepSeek-V3 can optimize towards the constitutional direction. We believe that this paradigm, which combines supplementary information with LLMs as a feedback source, is of paramount importance. The LLM serves as a versatile processor capable of transforming unstructured information from diverse scenarios into rewards, ultimately facilitating the self-improvement of LLMs. Beyond self-rewarding, we are also dedicated to uncovering other general and scalable rewarding methods to consistently advance the model capabilities in general scenarios. 5.4.3. Multi-Token Prediction Evaluation Instead of predicting just the next single token, DeepSeek-V3 predicts the next 2 tokens through the MTP technique. Combined with the framework of speculative decoding (Leviathan et al., 2023; Xia et al., 2023), it can significantly accelerate the decoding speed of the model. A natural question arises concerning the acceptance rate of the additionally predicted token. Based on our evaluation, the acceptance rate of the second token prediction ranges between 85% and 90% across various generation topics, demonstrating consistent reliability. This high acceptance rate enables DeepSeek-V3 to achieve a significantly improved decoding speed, delivering 1.8 times TPS (Tokens Per Second). 6. Conclusion, Limitations, and Future Directions In this paper, we introduce DeepSeek-V3, a large", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "0639c2be130003621a72961b2c615a62a8abc255b81e97bc7400816cf2c579b4"}
{"doc_id": "arxiv:2412.19437#model:part-2", "url": "https://arxiv.org/abs/2412.19437", "anchor": "#model:part-2", "type": "paper", "title": "", "section": "Model", "text": "can significantly accelerate the decoding speed of the model. A natural question arises concerning the acceptance rate of the additionally predicted token. Based on our evaluation, the acceptance rate of the second token prediction ranges between 85% and 90% across various generation topics, demonstrating consistent reliability. This high acceptance rate enables DeepSeek-V3 to achieve a significantly improved decoding speed, delivering 1.8 times TPS (Tokens Per Second). 6. Conclusion, Limitations, and Future Directions In this paper, we introduce DeepSeek-V3, a large MoE language model with 671B total pa- rameters and 37B activated parameters, trained on 14.8T tokens. In addition to the MLA and DeepSeekMoE architectures, it also pioneers an auxiliary-loss-free strategy for load balancing and sets a multi-token prediction training objective for stronger performance. The training of DeepSeek-V3 is cost-effective due to the support of FP8 training and meticulous engineering op- timizations. The post-training also makes a success in distilling the reasoning capability from the DeepSeek-R1 series of models. Comprehensive evaluations demonstrate that DeepSeek-V3 has emerged as the strongest open-source model currently available, and achieves performance com- parable to leading closed-source models like GPT-4o and Claude-3.5-Sonnet. Despite its strong performance, it also maintains economical training costs. It requires only 2.788M H800 GPU hours for its full training, including pre-training, context length extension, and post-training. While acknowledging its strong performance and cost-effectiveness, we also recognize that DeepSeek-V3 has some limitations, especially on the deployment. Firstly, to ensure efficient inference, the recommended deployment unit for DeepSeek-V3 is relatively large, which might pose a burden for small-sized teams. Secondly, although our deployment strategy for DeepSeek- V3 has achieved an end-to-end generation speed of more than two times that of DeepSeek-V2, there still remains potential for further enhancement. Fortunately, these limitations are expected to be naturally addressed with the development of more advanced hardware. DeepSeek consistently adheres to the route of open-source models with longtermism, aiming to steadily approach the ultimate goal of AGI (Artificial General Intelligence). In the future, we plan to strategically invest in research across the following directions. • We will consistently study and refine our model architectures, aiming to further improve both the training and inference efficiency, striving to approach efficient support for infinite context length. Additionally, we will try to break through the architectural limitations of Transformer, thereby pushing the boundaries of its modeling capabilities. 35 • We will continuously iterate on the quantity and quality of our training data, and explore the incorporation of additional training signal sources, aiming to drive data scaling across a more comprehensive range of dimensions. • We will consistently explore and iterate on the deep thinking capabilities of our models, aiming to enhance their intelligence and problem-solving abilities by expanding their reasoning length and depth. • We will explore more comprehensive and multi-dimensional model evaluation methods to prevent the tendency towards optimizing a fixed set of benchmarks during research, which may create a misleading impression of the model capabilities and affect our foundational assessment.", "source": "arxiv_pdf", "published": "", "tokens": 490, "sha256": "b04529b03b65a4553149a74f82e66a01f49f66b35b70e21099f7ea2d5ad400dd"}
{"doc_id": "arxiv:2309.06180#abstract", "url": "https://arxiv.org/abs/2309.06180", "anchor": "#abstract", "type": "paper", "title": "", "section": "Abstract", "text": "High throughput serving of large language models (LLMs) requires batching sufficiently many requests at a time. How- ever, existing systems struggle because the key-value cache (KV cache) memory for each request is huge and grows and shrinks dynamically. When managed inefficiently, this memory can be significantly wasted by fragmentation and redundant duplication, limiting the batch size. To address this problem, we propose PagedAttention, an attention al- gorithm inspired by the classical virtual memory and pag- ing techniques in operating systems. On top of it, we build vLLM, an LLM serving system that achieves (1) near-zero waste in KV cache memory and (2) flexible sharing of KV cache within and across requests to further reduce mem- ory usage. Our evaluations show that vLLM improves the throughput of popular LLMs by 2-4× with the same level of latency compared to the state-of-the-art systems, such as FasterTransformer and Orca. The improvement is more pronounced with longer sequences, larger models, and more complex decoding algorithms. vLLM’s source code is publicly available at https://github.com/vllm-project/vllm. 1", "source": "arxiv_pdf", "published": "", "tokens": 170, "sha256": "19be7272d9907873ca942215d7cb99a78fe33e28389cafb6af1092d9c059471d"}
{"doc_id": "arxiv:2309.06180#introduction:part-1", "url": "https://arxiv.org/abs/2309.06180", "anchor": "#introduction:part-1", "type": "paper", "title": "", "section": "Introduction", "text": "The emergence of large language models (LLMs) like GPT [5, 37] and PaLM [9] have enabled new applications such as pro- gramming assistants [6, 18] and universal chatbots [19, 35] that are starting to profoundly impact our work and daily routines. Many cloud companies [34, 44] are racing to pro- vide these applications as hosted services. However, running these applications is very expensive, requiring a large num- ber of hardware accelerators such as GPUs. According to recent estimates, processing an LLM request can be 10× more expensive than a traditional keyword query [43]. Given these high costs, increasing the throughput—and hence reducing Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third- party components of this work must be honored. For all other uses, contact the owner/author(s). SOSP ’23, October 23–26, 2023, Koblenz, Germany © 2023 Copyright held by the owner/author(s). ACM ISBN 979-8-4007-0229-7/23/10. https://doi.org/10.1145/3600006.3613165 NVIDIA A100 40GB Parameters (26GB, 65%) KV Cache (>30%) Others 20 30 40 Memory usage (GB) Parameter size Existing systems vLLM 0 10 20 30 40 Batch size (# requests) 0 0.4k 0.8k 1.2k Throughput (token/s) Figure 1. Left: Memory layout when serving an LLM with 13B parameters on NVIDIA A100. The parameters (gray) persist in GPU memory throughout serving. The memory for the KV cache (red) is (de)allocated per serving request. A small amount of memory (yellow) is used ephemerally for activation. Right: vLLM smooths out the rapid growth curve of KV cache memory seen in existing systems [31, 60], leading to a notable boost in serving throughput. the cost per request—of LLM serving systems is becoming more important. At the core of LLMs lies an autoregressive Transformer model [53]. This model generates words (tokens), one at a time, based on the input (prompt) and the previous sequence of the output’s tokens it has generated so far. For each re- quest, this expensive process is repeated until the model out- puts a termination token. This sequential generation process makes the workload memory-bound, underutilizing the com- putation power of GPUs and limiting the serving throughput. Improving the throughput is possible by batching multi- ple requests together. However, to process many requests in a batch, the memory space for each request should be efficiently managed. For example, Fig. 1 (left) illustrates the memory distribution for a 13B-parameter LLM on an NVIDIA A100 GPU with 40GB RAM. Approximately 65% of the mem- ory is allocated for the model weights, which remain static during serving. Close to 30% of the memory is used to store the dynamic states of the requests. For Transformers, these states consist of the key and value tensors associated with the attention mechanism, commonly referred to as KV cache [41], which represent the context from earlier tokens to gener- ate new output tokens in sequence. The remaining small ∗Equal contribution. 1 arXiv:2309.06180v1 [cs.LG]", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "3a8b50cd35889bc375a1bdd3e2e72f5fd0656a2bd56f359c380028ff3fa1643e"}
{"doc_id": "arxiv:2309.06180#introduction:part-2", "url": "https://arxiv.org/abs/2309.06180", "anchor": "#introduction:part-2", "type": "paper", "title": "", "section": "Introduction", "text": "RAM. Approximately 65% of the mem- ory is allocated for the model weights, which remain static during serving. Close to 30% of the memory is used to store the dynamic states of the requests. For Transformers, these states consist of the key and value tensors associated with the attention mechanism, commonly referred to as KV cache [41], which represent the context from earlier tokens to gener- ate new output tokens in sequence. The remaining small ∗Equal contribution. 1 arXiv:2309.06180v1 [cs.LG] 12 Sep 2023 Orca (Max) Orca (Pow2) Orca (Oracle) vLLM 0 20 40 60 80 100 KV cache usage (%) 20.4 13.3 57.3 8.9 26.8 17.9 13.6 41.6 38.2 25.2 36.6 96.3 Token states Reservation Internal frag. External frag. & Others Figure 2. Average percentage of memory wastes in different LLM serving systems during the experiment in §6.2. percentage of memory is used for other data, including ac- tivations – the ephemeral tensors created when evaluating the LLM. Since the model weights are constant and the ac- tivations only occupy a small fraction of the GPU memory, the way the KV cache is managed is critical in determining the maximum batch size. When managed inefficiently, the KV cache memory can significantly limit the batch size and consequently the throughput of the LLM, as illustrated in Fig. 1 (right). In this paper, we observe that existing LLM serving sys- tems [31, 60] fall short of managing the KV cache memory efficiently. This is mainly because they store the KV cache of a request in contiguous memory space, as most deep learning frameworks [33, 39] require tensors to be stored in contigu- ous memory. However, unlike the tensors in the traditional deep learning workloads, the KV cache has unique charac- teristics: it dynamically grows and shrinks over time as the model generates new tokens, and its lifetime and length are not known a priori. These characteristics make the existing systems’ approach significantly inefficient in two ways: First, the existing systems [31, 60] suffer from internal and external memory fragmentation. To store the KV cache of a request in contiguous space, they pre-allocate a contigu- ous chunk of memory with the request’s maximum length (e.g., 2048 tokens). This can result in severe internal frag- mentation, since the request’s actual length can be much shorter than its maximum length (e.g., Fig. 11). Moreover, even if the actual length is known a priori, the pre-allocation is still inefficient: As the entire chunk is reserved during the request’s lifetime, other shorter requests cannot utilize any part of the chunk that is currently unused. Besides, external memory fragmentation can also be significant, since the pre- allocated size can be different for each request. Indeed, our profiling results in Fig. 2 show that only 20.4% - 38.2% of the KV cache memory is used to store the actual token states in the existing systems. Second, the existing systems cannot exploit the opportu- nities for memory sharing. LLM services often use advanced decoding algorithms, such as parallel sampling and beam search, that generate multiple outputs per request. In these scenarios, the", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "0a402188f3e25aa94be63a684b954b7625b02429b4fb1f63090e03200b2397c0"}
{"doc_id": "arxiv:2309.06180#introduction:part-3", "url": "https://arxiv.org/abs/2309.06180", "anchor": "#introduction:part-3", "type": "paper", "title": "", "section": "Introduction", "text": "also be significant, since the pre- allocated size can be different for each request. Indeed, our profiling results in Fig. 2 show that only 20.4% - 38.2% of the KV cache memory is used to store the actual token states in the existing systems. Second, the existing systems cannot exploit the opportu- nities for memory sharing. LLM services often use advanced decoding algorithms, such as parallel sampling and beam search, that generate multiple outputs per request. In these scenarios, the request consists of multiple sequences that can partially share their KV cache. However, memory sharing is not possible in the existing systems because the KV cache of the sequences is stored in separate contiguous spaces. To address the above limitations, we propose PagedAt- tention, an attention algorithm inspired by the operating system’s (OS) solution to memory fragmentation and shar- ing: virtual memory with paging. PagedAttention divides the request’s KV cache into blocks, each of which can contain the attention keys and values of a fixed number of tokens. In PagedAttention, the blocks for the KV cache are not neces- sarily stored in contiguous space. Therefore, we can manage the KV cache in a more flexible way as in OS’s virtual mem- ory: one can think of blocks as pages, tokens as bytes, and requests as processes. This design alleviates internal frag- mentation by using relatively small blocks and allocating them on demand. Moreover, it eliminates external fragmen- tation as all blocks have the same size. Finally, it enables memory sharing at the granularity of a block, across the different sequences associated with the same request or even across the different requests. In this work, we build vLLM, a high-throughput distributed LLM serving engine on top of PagedAttention that achieves near-zero waste in KV cache memory. vLLM uses block-level memory management and preemptive request scheduling that are co-designed with PagedAttention. vLLM supports popular LLMs such as GPT [5], OPT [62], and LLaMA [52] with varying sizes, including the ones exceeding the memory capacity of a single GPU. Our evaluations on various models and workloads show that vLLM improves the LLM serving throughput by 2-4× compared to the state-of-the-art sys- tems [31, 60], without affecting the model accuracy at all. The improvements are more pronounced with longer sequences, larger models, and more complex decoding algorithms (§4.3). In summary, we make the following contributions: • We identify the challenges in memory allocation in serving LLMs and quantify their impact on serving performance. • We propose PagedAttention, an attention algorithm that operates on KV cache stored in non-contiguous paged memory, which is inspired by the virtual memory and paging in OS. • We design and implement vLLM, a distributed LLM serving engine built on top of PagedAttention. • We evaluate vLLM on various scenarios and demonstrate that it substantially outperforms the previous state-of-the- art solutions such as FasterTransformer [31] and Orca [60]. 2", "source": "arxiv_pdf", "published": "", "tokens": 479, "sha256": "ba4b9278f60bf88023276342499bc9d7e4e6f9701f35726586b77a30bf9a4e8c"}
{"doc_id": "arxiv:2309.06180#background:part-1", "url": "https://arxiv.org/abs/2309.06180", "anchor": "#background:part-1", "type": "paper", "title": "", "section": "Background", "text": "In this section, we describe the generation and serving pro- cedures of typical LLMs and the iteration-level scheduling used in LLM serving. 2 2.1 Transformer-Based Large Language Models The task of language modeling is to model the probability of a list of tokens (𝑥1, . . . ,𝑥𝑛). Since language has a natural sequential ordering, it is common to factorize the joint prob- ability over the whole sequence as the product of conditional probabilities (a.k.a. autoregressive decomposition [3]): 𝑃(𝑥) = 𝑃(𝑥1) · 𝑃(𝑥2 | 𝑥1) · · · 𝑃(𝑥𝑛| 𝑥1, . . . ,𝑥𝑛−1). (1) Transformers [53] have become the de facto standard ar- chitecture for modeling the probability above at a large scale. The most important component of a Transformer-based lan- guage model is its self-attention layers. For an input hidden state sequence (𝑥1, . . . ,𝑥𝑛) ∈R𝑛×𝑑, a self-attention layer first applies linear transformations on each position 𝑖to get the query, key, and value vectors: 𝑞𝑖= 𝑊𝑞𝑥𝑖, 𝑘𝑖= 𝑊𝑘𝑥𝑖, 𝑣𝑖= 𝑊𝑣𝑥𝑖. (2) Then, the self-attention layer computes the attention score 𝑎𝑖𝑗by multiplying the query vector at one position with all the key vectors before it and compute the output 𝑜𝑖as the weighted average over the value vectors: 𝑎𝑖𝑗= exp(𝑞⊤ 𝑖𝑘𝑗/ √ 𝑑) Í𝑖 𝑡=1 exp(𝑞⊤ 𝑖𝑘𝑡/ √ 𝑑) , 𝑜𝑖= 𝑖∑︁ 𝑗=1 𝑎𝑖𝑗𝑣𝑗. (3) Besides the computation in Eq. 4, all other components in the Transformer model, including the embedding layer, feed-forward layer, layer normalization [2], residual connec- tion [22], output logit computation, and the query, key, and value transformation in Eq. 2, are all applied independently position-wise in a form of 𝑦𝑖= 𝑓(𝑥𝑖). 2.2 LLM Service & Autoregressive Generation Once trained, LLMs are often deployed as a conditional gen- eration service (e.g., completion API [34] or chatbot [19, 35]). A request to an LLM service provides a list of input prompt tokens (𝑥1, . . . ,𝑥𝑛), and the LLM service generates a list of output tokens (𝑥𝑛+1, . . . ,𝑥𝑛+𝑇) according to Eq. 1. We refer to the concatenation of the prompt and output lists as sequence. Due to the decomposition in Eq. 1, the LLM can only sam- ple and generate new tokens one by one, and the generation process of each new token depends on all the previous tokens in that sequence, specifically their key and value vectors. In this sequential generation process, the key and value vectors of existing tokens are often cached for generating future tokens, known as KV cache. Note that the KV cache of one token depends on all its previous tokens. This means that the KV cache of the same token appearing at different positions in a sequence will be different. Given a request prompt, the generation computation in the LLM service can be decomposed into two phases: The prompt phase takes the whole user prompt (𝑥1, . . . ,𝑥𝑛) as input and computes the probability of the first new to- ken 𝑃(𝑥𝑛+1 | 𝑥1, . . . ,𝑥𝑛). During this process, also gener- ates the key vectors 𝑘1, . . . ,𝑘𝑛and value vectors 𝑣1, . .", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "ca934a153dcf977cdd00f42c974faefc5b1ff0da34494829619c94ddd06548f3"}
{"doc_id": "arxiv:2309.06180#background:part-2", "url": "https://arxiv.org/abs/2309.06180", "anchor": "#background:part-2", "type": "paper", "title": "", "section": "Background", "text": "same token appearing at different positions in a sequence will be different. Given a request prompt, the generation computation in the LLM service can be decomposed into two phases: The prompt phase takes the whole user prompt (𝑥1, . . . ,𝑥𝑛) as input and computes the probability of the first new to- ken 𝑃(𝑥𝑛+1 | 𝑥1, . . . ,𝑥𝑛). During this process, also gener- ates the key vectors 𝑘1, . . . ,𝑘𝑛and value vectors 𝑣1, . . . , 𝑣𝑛. Since prompt tokens 𝑥1, . . . ,𝑥𝑛are all known, the computa- tion of the prompt phase can be parallelized using matrix- matrix multiplication operations. Therefore, this phase can efficiently use the parallelism inherent in GPUs. The autoregressive generation phase generates the re- maining new tokens sequentially. At iteration 𝑡, the model takes one token 𝑥𝑛+𝑡as input and computes the probability 𝑃(𝑥𝑛+𝑡+1 | 𝑥1, . . . ,𝑥𝑛+𝑡) with the key vectors 𝑘1, . . . ,𝑘𝑛+𝑡and value vectors 𝑣1, . . . , 𝑣𝑛+𝑡. Note that the key and value vectors at positions 1 to 𝑛+ 𝑡−1 are cached at previous iterations, only the new key and value vector 𝑘𝑛+𝑡and 𝑣𝑛+𝑡are com- puted at this iteration. This phase completes either when the sequence reaches a maximum length (specified by users or limited by LLMs) or when an end-of-sequence (<eos>) token is emitted. The computation at different iterations cannot be parallelized due to the data dependency and often uses matrix-vector multiplication, which is less efficient. As a re- sult, this phase severely underutilizes GPU computation and becomes memory-bound, being responsible for most portion of the latency of a single request. 2.3 Batching Techniques for LLMs The compute utilization in serving LLMs can be improved by batching multiple requests. Because the requests share the same model weights, the overhead of moving weights is amortized across the requests in a batch, and can be over- whelmed by the computational overhead when the batch size is sufficiently large. However, batching the requests to an LLM service is non-trivial for two reasons. First, the requests may arrive at different times. A naive batching strat- egy would either make earlier requests wait for later ones or delay the incoming requests until earlier ones finish, lead- ing to significant queueing delays. Second, the requests may have vastly different input and output lengths (Fig. 11). A straightforward batching technique would pad the inputs and outputs of the requests to equalize their lengths, wasting GPU computation and memory. To address this problem, fine-grained batching mecha- nisms, such as cellular batching [16] and iteration-level sched- uling [60], have been proposed. Unlike traditional methods that work at the request level, these techniques operate at the iteration level. After each iteration, completed requests are removed from the batch, and new ones are added. There- fore, a new request can be processed after waiting for a single iteration, not waiting for the entire batch to complete. Moreover, with special GPU kernels, these techniques elim- inate the need to pad the inputs and outputs. By reducing the queueing delay and the inefficiencies", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "bec32a3e6b34b3ec15f9d33ad2db596d6e12f2e55381f02b0efd5cac7527e80a"}
{"doc_id": "arxiv:2309.06180#background:part-3", "url": "https://arxiv.org/abs/2309.06180", "anchor": "#background:part-3", "type": "paper", "title": "", "section": "Background", "text": "been proposed. Unlike traditional methods that work at the request level, these techniques operate at the iteration level. After each iteration, completed requests are removed from the batch, and new ones are added. There- fore, a new request can be processed after waiting for a single iteration, not waiting for the entire batch to complete. Moreover, with special GPU kernels, these techniques elim- inate the need to pad the inputs and outputs. By reducing the queueing delay and the inefficiencies from padding, the fine-grained batching mechanisms significantly increase the throughput of LLM serving. 3 Four score and seven years ago our fathers brought forth <eos> <resv> … <resv> You only live once <eos> <resv> … <resv> 2038 slots never used (internal fragmentation) 2 slots future used (reserved) External fragmentation 7 KV cache states for request A’s prompt 3 KV cache states for request B’s prompt 1 slot future used (reserved) 507 slots never used (Internal fragmentation) Request B current iteration Request A current iteration 1 slot for generated token Figure 3. KV cache memory management in existing systems. Three types of memory wastes – reserved, internal fragmentation, and external fragmentation – exist that prevent other requests from fitting into the memory. The token in each memory slot represents its KV cache. Note the same tokens can have different KV cache when at different positions. 3 Memory Challenges in LLM Serving Although fine-grained batching reduces the waste of com- puting and enables requests to be batched in a more flexible way, the number of requests that can be batched together is still constrained by GPU memory capacity, particularly the space allocated to store the KV cache. In other words, the serving system’s throughput is memory-bound. Overcom- ing this memory-bound requires addressing the following challenges in the memory management: Large KV cache. The KV Cache size grows quickly with the number of requests. As an example, for the 13B parameter OPT model [62], the KV cache of a single token demands 800 KB of space, calculated as 2 (key and value vectors) × 5120 (hidden state size) × 40 (number of layers) × 2 (bytes per FP16). Since OPT can generate sequences up to 2048 tokens, the memory required to store the KV cache of one request can be as much as 1.6 GB. Concurrent GPUs have memory capacities in the tens of GBs. Even if all available memory was allocated to KV cache, only a few tens of requests could be accommodated. Moreover, inefficient memory manage- ment can further decrease the batch size, as shown in Fig. 2. Additionally, given the current trends, the GPU’s computa- tion speed grows faster than the memory capacity [17]. For example, from NVIDIA A100 to H100, The FLOPS increases by more than 2x, but the GPU memory stays at 80GB max- imum. Therefore, we believe the memory will become an increasingly significant bottleneck. Complex decoding algorithms. LLM services offer a range of decoding algorithms for users to select from, each with varying implications for memory management complexity. For example, when users request multiple random samples from a", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "f617475b1b58c02cd53949ea0d71d3c6ce8d39a0f22bfc698753b381a44ad064"}
{"doc_id": "arxiv:2309.06180#background:part-4", "url": "https://arxiv.org/abs/2309.06180", "anchor": "#background:part-4", "type": "paper", "title": "", "section": "Background", "text": "the GPU’s computa- tion speed grows faster than the memory capacity [17]. For example, from NVIDIA A100 to H100, The FLOPS increases by more than 2x, but the GPU memory stays at 80GB max- imum. Therefore, we believe the memory will become an increasingly significant bottleneck. Complex decoding algorithms. LLM services offer a range of decoding algorithms for users to select from, each with varying implications for memory management complexity. For example, when users request multiple random samples from a single input prompt, a typical use case in program suggestion [18], the KV cache of the prompt part, which accounts for 12% of the total KV cache memory in our ex- periment (§6.3), can be shared to minimize memory usage. On the other hand, the KV cache during the autoregressive generation phase should remain unshared due to the dif- ferent sample results and their dependence on context and position. The extent of KV cache sharing depends on the specific decoding algorithm employed. In more sophisticated algorithms like beam search [49], different request beams can share larger portions (up to 55% memory saving, see §6.3) of their KV cache, and the sharing pattern evolves as the decoding process advances. Scheduling for unknown input & output lengths. The requests to an LLM service exhibit variability in their input and output lengths. This requires the memory management system to accommodate a wide range of prompt lengths. In addition, as the output length of a request grows at decoding, the memory required for its KV cache also expands and may exhaust available memory for incoming requests or ongoing generation for existing prompts. The system needs to make scheduling decisions, such as deleting or swapping out the KV cache of some requests from GPU memory. 3.1 Memory Management in Existing Systems Since most operators in current deep learning frameworks [33, 39] require tensors to be stored in contiguous memory, previous LLM serving systems [31, 60] also store the KV cache of one request as a contiguous tensor across the differ- ent positions. Due to the unpredictable output lengths from the LLM, they statically allocate a chunk of memory for a request based on the request’s maximum possible sequence length, irrespective of the actual input or eventual output length of the request. Fig. 3 illustrates two requests: request A with 2048 max- imum possible sequence length and request B with a max- imum of 512. The chunk pre-allocation scheme in existing systems has three primary sources of memory wastes: re- served slots for future tokens, internal fragmentation due to over-provisioning for potential maximum sequence lengths, and external fragmentation from the memory allocator like the buddy allocator. The external fragmentation will never be used for generated tokens, which is known before serving a request. Internal fragmentation also remains unused, but this is only realized after a request has finished sampling. They are both pure memory waste. Although the reserved memory is eventually used, reserving this space for the en- tire request’s duration, especially when the reserved space is large, occupies the space that could otherwise be used to process", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "8f7a770cf0d0143a7ee70f35d9fd77d5974ec1ddb8cc3422b6b09bd1cfeb0217"}
{"doc_id": "arxiv:2309.06180#background:part-5", "url": "https://arxiv.org/abs/2309.06180", "anchor": "#background:part-5", "type": "paper", "title": "", "section": "Background", "text": "from the memory allocator like the buddy allocator. The external fragmentation will never be used for generated tokens, which is known before serving a request. Internal fragmentation also remains unused, but this is only realized after a request has finished sampling. They are both pure memory waste. Although the reserved memory is eventually used, reserving this space for the en- tire request’s duration, especially when the reserved space is large, occupies the space that could otherwise be used to process other requests. We visualize the average percentage of memory wastes in our experiments in Fig. 2, revealing that the actual effective memory in previous systems can be as low as 20.4%. 4 KV Cache Manager Scheduler CPU Block Allocator GPU Block Allocator Block tables Worker 0", "source": "arxiv_pdf", "published": "", "tokens": 126, "sha256": "5f9639b98cdf7e4cbdd14917c30f66fc03309a852a4e33aae932091ceb259847"}
{"doc_id": "arxiv:2309.06180#model", "url": "https://arxiv.org/abs/2309.06180", "anchor": "#model", "type": "paper", "title": "", "section": "Model", "text": "Shard N - 1 Cache Engine … Figure 4. vLLM system overview. Although compaction [54] has been proposed as a poten- tial solution to fragmentation, performing compaction in a performance-sensitive LLM serving system is impractical due to the massive KV cache. Even with compaction, the pre-allocated chunk space for each request prevents memory sharing specific to decoding algorithms in existing memory management systems. 4", "source": "arxiv_pdf", "published": "", "tokens": 64, "sha256": "d1c724b4382eec3fd4ad42d4170903b5bd2f3da364ccd9dd9091d1d7217065e5"}
{"doc_id": "arxiv:2309.06180#method:part-1", "url": "https://arxiv.org/abs/2309.06180", "anchor": "#method:part-1", "type": "paper", "title": "", "section": "Method", "text": "In this work, we develop a new attention algorithm, Page- dAttention, and build an LLM serving engine, vLLM, to tackle the challenges outlined in §3. The architecture of vLLM is shown in Fig. 4. vLLM adopts a centralized scheduler to coordinate the execution of distributed GPU workers. The KV cache manager effectively manages the KV cache in a paged fashion, enabled by PagedAttention. Specifically, the KV cache manager manages the physical KV cache memory on the GPU workers through the instructions sent by the centralized scheduler. Next, We describe the PagedAttention algorithm in §4.1. With that, we show the design of the KV cache manager in §4.2 and how it facilitates PagedAttention in §4.3, respec- tively. Then, we show how this design facilitates effective memory management for various decoding methods (§4.4) and handles the variable length input and output sequences (§4.5). Finally, we show how the system design of vLLM works in a distributed setting (§4.6). 4.1 PagedAttention To address the memory challenges in §3, we introduce Page- dAttention, an attention algorithm inspired by the classic idea of paging [25] in operating systems. Unlike the traditional attention algorithms, PagedAttention allows storing continu- ous keys and values in non-contiguous memory space. Specif- ically, PagedAttention partitions the KV cache of each se- quence into KV blocks. Each block contains the key and value vectors for a fixed number of tokens,1 which we denote as KV 1In Transformer, each token has a set of key and value vectors across layers and attention heads within a layer. All the key and value vectors can be managed together within a single KV block, or the key and value vectors at different heads and layers can each have a separate block and be managed in separate block tables. The two designs have no performance difference and we choose the second one for easy implementation. forth Query vector years ago our fathers brought forth Four score and seven Key and value vectors Block 1 Block 2 Block 0 Figure 5. Illustration of the PagedAttention algorithm, where the attention key and values vectors are stored as non-contiguous blocks in the memory. block size (𝐵). Denote the key block 𝐾𝑗= (𝑘(𝑗−1)𝐵+1, . . . ,𝑘𝑗𝐵) and value block 𝑉𝑗= (𝑣(𝑗−1)𝐵+1, . . . , 𝑣𝑗𝐵). The attention com- putation in Eq. 4 can be transformed into the following block- wise computation: 𝐴𝑖𝑗= exp(𝑞⊤ 𝑖𝐾𝑗/ √ 𝑑) Í⌈𝑖/𝐵⌉ 𝑡=1 exp(𝑞⊤ 𝑖𝐾𝑡1/ √ 𝑑) , 𝑜𝑖= ⌈𝑖/𝐵⌉ ∑︁ 𝑗=1 𝑉𝑗𝐴⊤ 𝑖𝑗, (4) where 𝐴𝑖𝑗= (𝑎𝑖,(𝑗−1)𝐵+1, . . . ,𝑎𝑖,𝑗𝐵) is the row vector of atten- tion score on 𝑗-th KV block. During the attention computation, the PagedAttention kernel identifies and fetches different KV blocks separately. We show an example of PagedAttention in Fig. 5: The key and value vectors are spread across three blocks, and the three blocks are not contiguous on the physical memory. At each time, the kernel multiplies the query vector 𝑞𝑖of the query token (“forth”) and the key vectors 𝐾𝑗in a block (e.g., key vectors of “Four score and seven” for block 0) to compute the attention score𝐴𝑖𝑗, and", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "e7beab8eff4db5e7aead52ac608c0772c144c36a8180408df91105e47bc78b72"}
{"doc_id": "arxiv:2309.06180#method:part-2", "url": "https://arxiv.org/abs/2309.06180", "anchor": "#method:part-2", "type": "paper", "title": "", "section": "Method", "text": "attention computation, the PagedAttention kernel identifies and fetches different KV blocks separately. We show an example of PagedAttention in Fig. 5: The key and value vectors are spread across three blocks, and the three blocks are not contiguous on the physical memory. At each time, the kernel multiplies the query vector 𝑞𝑖of the query token (“forth”) and the key vectors 𝐾𝑗in a block (e.g., key vectors of “Four score and seven” for block 0) to compute the attention score𝐴𝑖𝑗, and later multiplies𝐴𝑖𝑗with the value vectors 𝑉𝑗in a block to derive the final attention output 𝑜𝑖. In summary, the PagedAttention algorithm allows the KV blocks to be stored in non-contiguous physical memory, which enables more flexible paged memory management in vLLM. 4.2 KV Cache Manager The key idea behind vLLM’s memory manager is analogous to the virtual memory [25] in operating systems. OS parti- tions memory into fixed-sized pages and maps user programs’ logical pages to physical pages. Contiguous logical pages can correspond to non-contiguous physical memory pages, al- lowing user programs to access memory as though it were contiguous. Moreover, physical memory space needs not to be fully reserved in advance, enabling the OS to dynamically allocate physical pages as needed. vLLM uses the ideas be- hind virtual memory to manage the KV cache in an LLM service. Enabled by PagedAttention, we organize the KV cache as fixed-size KV blocks, like pages in virtual memory. A request’s KV cache is represented as a series of logical KV blocks, filled from left to right as new tokens and their KV cache are generated. The last KV block’s unfilled positions are reserved for future generations. On GPU workers, a block engine allocates a contiguous chunk of GPU DRAM and 5 Request A Four score and seven years ago our fathers brought Prompt: “Four score and seven years ago our” Outputs: “fathers” → “brought” → … Block 0 Block 1 Block 2 Block 3 years ago our fathers brought Four score and seven Physical KV blocks (on GPU DRAM) Block 0 Block 1 Block 2 Block 3 Block 4 Block 5 Block 6 Block 7 Block 8 Logical KV blocks Physical block number # filled 7 4 1 3 → 4 3 1 – – Block Table 1 1 1 1 1 1 1 2 3 1 1 1 1 2 3 3 3 1 1 1 1 3 1 1 1 2 1 1 1 1 3 Figure 6. Block table translation in vLLM. divides it into physical KV blocks (this is also done on CPU RAM for swapping; see §4.5). The KV block manager also maintains block tables—the mapping between logical and physical KV blocks of each request. Each block table entry records the corresponding physical blocks of a logical block and the number of filled positions. Separating logical and physical KV blocks allows vLLM to dynamically grow the KV cache memory without reserving it for all positions in advance, which eliminates most memory waste in existing systems, as in Fig. 2. 4.3 Decoding with PagedAttention and vLLM Next, we walk through an", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "3fe139ca0941ce67514589ae0f7733a4a883e43f77622a1fc61be7e92e5ecf10"}
{"doc_id": "arxiv:2309.06180#method:part-3", "url": "https://arxiv.org/abs/2309.06180", "anchor": "#method:part-3", "type": "paper", "title": "", "section": "Method", "text": "manager also maintains block tables—the mapping between logical and physical KV blocks of each request. Each block table entry records the corresponding physical blocks of a logical block and the number of filled positions. Separating logical and physical KV blocks allows vLLM to dynamically grow the KV cache memory without reserving it for all positions in advance, which eliminates most memory waste in existing systems, as in Fig. 2. 4.3 Decoding with PagedAttention and vLLM Next, we walk through an example, as in Fig. 6, to demon- strate how vLLM executes PagedAttention and manages the memory during the decoding process of a single input se- quence: 1○As in OS’s virtual memory, vLLM does not require reserving the memory for the maximum possible generated sequence length initially. Instead, it reserves only the nec- essary KV blocks to accommodate the KV cache generated during prompt computation. In this case, The prompt has 7 tokens, so vLLM maps the first 2 logical KV blocks (0 and 1) to 2 physical KV blocks (7 and 1, respectively). In the prefill step, vLLM generates the KV cache of the prompts and the first output token with a conventional self-attention algorithm (e.g., [13]). vLLM then stores the KV cache of the first 4 tokens in logical block 0 and the following 3 tokens in logical block 1. The remaining slot is reserved for the subsequent autoregressive generation phase. 2○In the first autoregressive decoding step, vLLM generates the new token with the PagedAttention algorithm on physical blocks 7 and 1. Since one slot remains available in the last logical block, the newly generated KV cache is stored there, and the block table’s #filled record is updated. 3○At the second decoding step, as the last logical block is full, vLLM stores the newly generated KV cache in a new logical block; vLLM allocates a new physical block (physical block 3) for it and stores this mapping in the block table. Globally, for each decoding iteration, vLLM first selects a set of candidate sequences for batching (more in §4.5), and allocates the physical blocks for the newly required logical blocks. Then, vLLM concatenates all the input tokens of the current iteration (i.e., all tokens for prompt phase Four score and seven years ago our fathers brought Block 0 Block 1 Block 2 Block 3 years ago our fathers of times brought It was the best Four score and seven Physical KV blocks Block 0 Block 1 Block 2 Block 3 Block 4 Block 5 Block 6 Block 7 Block 8 Logical KV blocks It was the best of times Block 0 Block 1 Block 2 Logical KV blocks Request A Request B Figure 7. Storing the KV cache of two requests at the same time in vLLM. requests and the latest tokens for generation phase requests) as one sequence and feeds it into the LLM. During LLM’s computation, vLLM uses the PagedAttention kernel to access the previous KV cache stored in the form of logical KV blocks and saves the newly generated KV cache into the physical KV blocks. Storing multiple", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "8108422dee9afcbb6213b3fa27add287981b7541b8df8b0e9b3d92b7bef2fef0"}
{"doc_id": "arxiv:2309.06180#method:part-4", "url": "https://arxiv.org/abs/2309.06180", "anchor": "#method:part-4", "type": "paper", "title": "", "section": "Method", "text": "Block 1 Block 2 Logical KV blocks Request A Request B Figure 7. Storing the KV cache of two requests at the same time in vLLM. requests and the latest tokens for generation phase requests) as one sequence and feeds it into the LLM. During LLM’s computation, vLLM uses the PagedAttention kernel to access the previous KV cache stored in the form of logical KV blocks and saves the newly generated KV cache into the physical KV blocks. Storing multiple tokens within a KV block (block size > 1) enables the PagedAttention kernel to process the KV cache across more positions in parallel, thus increasing the hardware utilization and reducing latency. However, a larger block size also increases memory fragmentation. We study the effect of block size in §7.2. Again, vLLM dynamically assigns new physical blocks to logical blocks as more tokens and their KV cache are gener- ated. As all the blocks are filled from left to right and a new physical block is only allocated when all previous blocks are full, vLLM limits all the memory wastes for a request within one block, so it can effectively utilize all the memory, as shown in Fig. 2. This allows more requests to fit into mem- ory for batching—hence improving the throughput. Once a request finishes its generation, its KV blocks can be freed to store the KV cache of other requests. In Fig. 7, we show an example of vLLM managing the memory for two sequences. The logical blocks of the two sequences are mapped to differ- ent physical blocks within the space reserved by the block engine in GPU workers. The neighboring logical blocks of both sequences do not need to be contiguous in physical GPU memory and the space of physical blocks can be effectively utilized by both sequences. 4.4 Application to Other Decoding Scenarios §4.3 shows how PagedAttention and vLLM handle basic de- coding algorithms, such as greedy decoding and sampling, that take one user prompt as input and generate a single out- put sequence. In many successful LLM applications [18, 34], an LLM service must offer more complex decoding scenarios that exhibit complex accessing patterns and more opportuni- ties for memory sharing. We show the general applicability of vLLM on them in this section. Parallel sampling. In LLM-based program assistants [6, 18], an LLM generates multiple sampled outputs for a single in- put prompt; users can choose a favorite output from various candidates. So far we have implicitly assumed that a request 6 Sample A1 Four score and seven years ago our fathers Block 0 Block 1 years ago our mothers years ago our fathers Four score and seven Physical KV blocks Block 0 Block 1 Block 2 Block 3 Block 4 Block 5 Block 6 Block 7 Block 8 Logical KV blocks Four score and seven years ago our mothers Block 0 Block 1 Logical KV blocks Sample A2 Copy-on-write Ref count: 2 → 1 Figure 8. Parallel sampling example. generates a single sequence. In the remainder of this paper, we assume the more general case", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "65ea444f31510f0dd177b658721a30f3de0fa5fdeb8b41e14295063d3ee288a6"}
{"doc_id": "arxiv:2309.06180#method:part-5", "url": "https://arxiv.org/abs/2309.06180", "anchor": "#method:part-5", "type": "paper", "title": "", "section": "Method", "text": "years ago our mothers years ago our fathers Four score and seven Physical KV blocks Block 0 Block 1 Block 2 Block 3 Block 4 Block 5 Block 6 Block 7 Block 8 Logical KV blocks Four score and seven years ago our mothers Block 0 Block 1 Logical KV blocks Sample A2 Copy-on-write Ref count: 2 → 1 Figure 8. Parallel sampling example. generates a single sequence. In the remainder of this paper, we assume the more general case in which a request gener- ates multiple sequences. In parallel sampling, one request includes multiple samples sharing the same input prompt, allowing the KV cache of the prompt to be shared as well. Via its PagedAttention and paged memory management, vLLM can realize this sharing easily and save memory. Fig. 8 shows an example of parallel decoding for two out- puts. Since both outputs share the same prompt, we only reserve space for one copy of the prompt’s state at the prompt phase; the logical blocks for the prompts of both sequences are mapped to the same physical blocks: the logical block 0 and 1 of both sequences are mapped to physical blocks 7 and 1, respectively. Since a single physical block can be mapped to multiple logical blocks, we introduce a reference count for each physical block. In this case, the reference counts for physical blocks 7 and 1 are both 2. At the generation phase, the two outputs sample different output tokens and need separate storage for KV cache. vLLM implements a copy-on- write mechanism at the block granularity for the physical blocks that need modification by multiple sequences, similar to the copy-on-write technique in OS virtual memory (e.g., when forking a process). Specifically, in Fig. 8, when sample A1 needs to write to its last logical block (logical block 1), vLLM recognizes that the reference count of the correspond- ing physical block (physical block 1) is greater than 1; it allocates a new physical block (physical block 3), instructs the block engine to copy the information from physical block 1, and decreases the reference count to 1. Next, when sample A2 writes to physical block 1, the reference count is already reduced to 1; thus A2 directly writes its newly generated KV cache to physical block 1. In summary, vLLM enables the sharing of most of the space used to store the prompts’ KV cache across multiple output samples, with the exception of the final logical block, which is managed by a copy-on-write mechanism. By sharing physical blocks across multiple samples, memory usage can be greatly reduced, especially for long input prompts. Beam search. In LLM tasks like machine translation [59], the users expect the top-𝑘most appropriate translations out- put by the LLM. Beam search [49] is widely used to decode the most probable output sequence from an LLM, as it miti- gates the computational complexity of fully traversing the Block 10 Block 11 Block 1 Block 3 Block 6 Block 7 Block 5 Block 0 Block 2 Block 4 Block 8 Block 9 Block 12 Beam candidate 0", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "07bb90c62cfb723a9c659970c519490bc7728fb534b5527c6f5d708f51fdd344"}
{"doc_id": "arxiv:2309.06180#method:part-6", "url": "https://arxiv.org/abs/2309.06180", "anchor": "#method:part-6", "type": "paper", "title": "", "section": "Method", "text": "long input prompts. Beam search. In LLM tasks like machine translation [59], the users expect the top-𝑘most appropriate translations out- put by the LLM. Beam search [49] is widely used to decode the most probable output sequence from an LLM, as it miti- gates the computational complexity of fully traversing the Block 10 Block 11 Block 1 Block 3 Block 6 Block 7 Block 5 Block 0 Block 2 Block 4 Block 8 Block 9 Block 12 Beam candidate 0 Beam candidate 1 Beam candidate 2 Beam candidate 3 Figure 9. Beam search example. sample space. The algorithm relies on the beam width pa- rameter 𝑘, which determines the number of top candidates retained at every step. During decoding, beam search ex- pands each candidate sequence in the beam by considering all possible tokens, computes their respective probabilities us- ing the LLM, and retains the top-𝑘most probable sequences out of 𝑘· |𝑉| candidates, where |𝑉| is the vocabulary size. Unlike parallel decoding, beam search facilities sharing not only the initial prompt blocks but also other blocks across different candidates, and the sharing patterns dynamically change as the decoding process advances, similar to the pro- cess tree in the OS created by compound forks. Fig. 9 shows how vLLM manages the KV blocks for a beam search ex- ample with 𝑘= 4. Prior to the iteration illustrated as the dotted line, each candidate sequence has used 4 full logi- cal blocks. All beam candidates share the first block 0 (i.e., prompt). Candidate 3 digresses from others from the second block. Candidates 0-2 share the first 3 blocks and diverge at the fourth block. At subsequent iterations, the top-4 prob- able candidates all originate from candidates 1 and 2. As the original candidates 0 and 3 are no longer among the top candidates, their logical blocks are freed, and the refer- ence counts of corresponding physical blocks are reduced. vLLM frees all physical blocks whose reference counts reach 0 (blocks 2, 4, 5, 8). Then, vLLM allocates new physical blocks (blocks 9-12) to store the new KV cache from the new can- didates. Now, all candidates share blocks 0, 1, 3; candidates 0 and 1 share block 6, and candidates 2 and 3 further share block 7. Previous LLM serving systems require frequent memory copies of the KV cache across the beam candidates. For exam- ple, in the case shown in Fig. 9, after the dotted line, candidate 3 would need to copy a large portion of candidate 2’s KV cache to continue generation. This frequent memory copy overhead is significantly reduced by vLLM’s physical block sharing. In vLLM, most blocks of different beam candidates can be shared. The copy-on-write mechanism is applied only when the newly generated tokens are within an old shared block, as in parallel decoding. This involves only copying one block of data. Shared prefix. Commonly, the LLM user provides a (long) description of the task including instructions and example inputs and outputs, also known as system prompt [36]. The description is concatenated with the actual task input to form", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "d3df979ba77ad50995cf64790ea14a6ee706f0e4c400d967b02078b724708f0a"}
{"doc_id": "arxiv:2309.06180#method:part-7", "url": "https://arxiv.org/abs/2309.06180", "anchor": "#method:part-7", "type": "paper", "title": "", "section": "Method", "text": "physical block sharing. In vLLM, most blocks of different beam candidates can be shared. The copy-on-write mechanism is applied only when the newly generated tokens are within an old shared block, as in parallel decoding. This involves only copying one block of data. Shared prefix. Commonly, the LLM user provides a (long) description of the task including instructions and example inputs and outputs, also known as system prompt [36]. The description is concatenated with the actual task input to form the prompt of the request. The LLM generates outputs based 7 Translate English to French: “sea otter” => “loutre de mer” “peppermint” => “menthe poivrée” “plush girafe” => “girafe en peluche” “cheese” => “fromage” Translate English to French: “sea otter” => “loutre de mer” “peppermint” => “menthe poivrée” “plush girafe” => “girafe en peluche” “I love you” => “Je t’amie” Shared prefix Task input Task output Sequence A Prompt Sequence B Prompt Sequence A LLM output Sequence B LLM output Figure 10. Shared prompt example for machine translation. The examples are adopted from [5]. on the full prompt. Fig. 10 shows an example. Moreover, the shared prefix can be further tuned, via prompt engineering, to improve the accuracy of the downstream tasks [26, 27]. For this type of application, many user prompts share a prefix, thus the LLM service provider can store the KV cache of the prefix in advance to reduce the redundant computa- tion spent on the prefix. In vLLM, this can be conveniently achieved by reserving a set of physical blocks for a set of predefined shared prefixes by the LLM service provider, as how OS handles shared library across processes. A user in- put prompt with the shared prefix can simply map its logi- cal blocks to the cached physical blocks (with the last block marked copy-on-write). The prompt phase computation only needs to execute on the user’s task input. Mixed decoding methods. The decoding methods dis- cussed earlier exhibit diverse memory sharing and access- ing patterns. Nonetheless, vLLM facilitates the simultane- ous processing of requests with different decoding prefer- ences, which existing systems cannot efficiently do. This is because vLLM conceals the complex memory sharing be- tween different sequences via a common mapping layer that translates logical blocks to physical blocks. The LLM and its execution kernel only see a list of physical block IDs for each sequence and do not need to handle sharing pat- terns across sequences. Compared to existing systems, this approach broadens the batching opportunities for requests with different sampling requirements, ultimately increasing the system’s overall throughput. 4.5 Scheduling and Preemption When the request traffic surpasses the system’s capacity, vLLM must prioritize a subset of requests. In vLLM, we adopt the first-come-first-serve (FCFS) scheduling policy for all requests, ensuring fairness and preventing starvation. When vLLM needs to preempt requests, it ensures that the earliest arrived requests are served first and the latest requests are preempted first. LLM services face a unique challenge: the input prompts for an LLM can vary significantly in length, and the resulting output lengths are not known a priori,", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "5ffc9e879869c2038215c516cb1d9468c2525db49806ab1bc861d2aed7f03420"}
{"doc_id": "arxiv:2309.06180#method:part-8", "url": "https://arxiv.org/abs/2309.06180", "anchor": "#method:part-8", "type": "paper", "title": "", "section": "Method", "text": "request traffic surpasses the system’s capacity, vLLM must prioritize a subset of requests. In vLLM, we adopt the first-come-first-serve (FCFS) scheduling policy for all requests, ensuring fairness and preventing starvation. When vLLM needs to preempt requests, it ensures that the earliest arrived requests are served first and the latest requests are preempted first. LLM services face a unique challenge: the input prompts for an LLM can vary significantly in length, and the resulting output lengths are not known a priori, contingent on both the input prompt and the model. As the number of requests and their outputs grow, vLLM can run out of the GPU’s phys- ical blocks to store the newly generated KV cache. There are two classic questions that vLLM needs to answer in this context: (1) Which blocks should it evict? (2) How to recover evicted blocks if needed again? Typically, eviction policies use heuristics to predict which block will be accessed fur- thest in the future and evict that block. Since in our case we know that all blocks of a sequence are accessed together, we implement an all-or-nothing eviction policy, i.e., either evict all or none of the blocks of a sequence. Furthermore, multi- ple sequences within one request (e.g., beam candidates in one beam search request) are gang-scheduled as a sequence group. The sequences within one sequence group are always preempted or rescheduled together due to potential memory sharing across those sequences. To answer the second ques- tion of how to recover an evicted block, we consider two techniques: Swapping. This is the classic technique used by most virtual memory implementations which copy the evicted pages to a swap space on the disk. In our case, we copy evicted blocks to the CPU memory. As shown in Fig. 4, besides the GPU block allocator, vLLM includes a CPU block allocator to manage the physical blocks swapped to CPU RAM. When vLLM exhausts free physical blocks for new tokens, it selects a set of sequences to evict and transfer their KV cache to the CPU. Once it preempts a sequence and evicts its blocks, vLLM stops accepting new requests until all preempted sequences are completed. Once a request completes, its blocks are freed from memory, and the blocks of a preempted sequence are brought back in to continue the processing of that sequence. Note that with this design, the number of blocks swapped to the CPU RAM never exceeds the number of total physical blocks in the GPU RAM, so the swap space on the CPU RAM is bounded by the GPU memory allocated for the KV cache. Recomputation. In this case, we simply recompute the KV cache when the preempted sequences are rescheduled. Note that recomputation latency can be significantly lower than the original latency, as the tokens generated at decoding can be concatenated with the original user prompt as a new prompt—their KV cache at all positions can be generated in one prompt phase iteration. The performances of swapping and recomputation depend on the bandwidth between CPU RAM and GPU memory and the computation power", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "acb8ccbcbfd7b8a18b711af50fce171a501f30a4d634803d33c288ee226b85cc"}
{"doc_id": "arxiv:2309.06180#method:part-9", "url": "https://arxiv.org/abs/2309.06180", "anchor": "#method:part-9", "type": "paper", "title": "", "section": "Method", "text": "cache. Recomputation. In this case, we simply recompute the KV cache when the preempted sequences are rescheduled. Note that recomputation latency can be significantly lower than the original latency, as the tokens generated at decoding can be concatenated with the original user prompt as a new prompt—their KV cache at all positions can be generated in one prompt phase iteration. The performances of swapping and recomputation depend on the bandwidth between CPU RAM and GPU memory and the computation power of the GPU. We examine the speeds of swapping and recomputation in §7.3. 4.6 Distributed Execution Many LLMs have parameter sizes exceeding the capacity of a single GPU [5, 9]. Therefore, it is necessary to partition them across distributed GPUs and execute them in a model parallel fashion [28, 63]. This calls for a memory manager capable of handling distributed memory. vLLM is effective in distributed settings by supporting the widely used Megatron-LM style tensor model parallelism strategy on Transformers [47]. This strategy adheres to an SPMD (Single Program Multiple Data) execution schedule, wherein the linear layers are partitioned 8 Table 1. Model sizes and server configurations. Model size 13B 66B 175B GPUs A100 4×A100 8×A100-80GB Total GPU memory 40 GB 160 GB 640 GB Parameter size 26 GB 132 GB 346 GB Memory for KV cache 12 GB 21 GB 264 GB Max. # KV cache slots 15.7K 9.7K 60.1K to perform block-wise matrix multiplication, and the the GPUs constantly synchronize intermediate results via an all- reduce operation. Specifically, the attention operator is split on the attention head dimension, each SPMD process takes care of a subset of attention heads in multi-head attention. We observe that even with model parallel execution, each model shard still processes the same set of input tokens, thus requiring the KV Cache for the same positions. Therefore, vLLM features a single KV cache manager within the cen- tralized scheduler, as in Fig. 4. Different GPU workers share the manager, as well as the mapping from logical blocks to physical blocks. This common mapping allows GPU workers to execute the model with the physical blocks provided by the scheduler for each input request. Although each GPU worker has the same physical block IDs, a worker only stores a portion of the KV cache for its corresponding attention heads. In each step, the scheduler first prepares the message with input token IDs for each request in the batch, as well as the block table for each request. Next, the scheduler broadcasts this control message to the GPU workers. Then, the GPU workers start to execute the model with the input token IDs. In the attention layers, the GPU workers read the KV cache according to the block table in the control message. During execution, the GPU workers synchronize the intermediate results with the all-reduce communication primitive without the coordination of the scheduler, as in [47]. In the end, the GPU workers send the sampled tokens of this iteration back to the scheduler. In summary, GPU workers do not need to synchronize on memory management as they only need", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "470e0fdec6ee0082bda48b4bdb7bb483782153e789d2e43333d8ee433a699383"}
{"doc_id": "arxiv:2309.06180#method:part-10", "url": "https://arxiv.org/abs/2309.06180", "anchor": "#method:part-10", "type": "paper", "title": "", "section": "Method", "text": "the input token IDs. In the attention layers, the GPU workers read the KV cache according to the block table in the control message. During execution, the GPU workers synchronize the intermediate results with the all-reduce communication primitive without the coordination of the scheduler, as in [47]. In the end, the GPU workers send the sampled tokens of this iteration back to the scheduler. In summary, GPU workers do not need to synchronize on memory management as they only need to receive all the memory management information at the beginning of each decoding iteration along with the step inputs. 5 Implementation vLLM is an end-to-end serving system with a FastAPI [15] frontend and a GPU-based inference engine. The frontend extends the OpenAI API [34] interface, allowing users to customize sampling parameters for each request, such as the maximum sequence length and the beam width 𝑘. The vLLM engine is written in 8.5K lines of Python and 2K lines of C++/CUDA code. We develop control-related components in- cluding the scheduler and the block manager in Python while developing custom CUDA kernels for key operations such as PagedAttention. For the model executor, we implement pop- ular LLMs such as GPT [5], OPT [62], and LLaMA [52] using 0 500 1000 1500 2000 # Tokens 0.0 0.5 1.0 1.5 2.0 Density 1e−2 Input (mean: 161.31) Output (mean: 337.99) (a) ShareGPT 0 500 1000 1500 2000 # Tokens 0 2 4 6 8 Density 1e−2 Input (mean: 19.31) Output (mean: 58.45) (b) Alpaca Figure 11. Input and output length distributions of the (a) ShareGPT and (b) Alpaca datasets. PyTorch [39] and Transformers [58]. We use NCCL [32] for tensor communication across the distributed GPU workers. 5.1 Kernel-level Optimization Since PagedAttention introduces memory access patterns that are not efficiently supported by existing systems, we develop several GPU kernels for optimizing it. (1) Fused re- shape and block write. In every Transformer layer, the new KV cache are split into blocks, reshaped to a memory layout optimized for block read, then saved at positions specified by the block table. To minimize kernel launch overheads, we fuse them into a single kernel. (2) Fusing block read and atten- tion. We adapt the attention kernel in FasterTransformer [31] to read KV cache according to the block table and perform attention operations on the fly. To ensure coalesced memory access, we assign a GPU warp to read each block. More- over, we add support for variable sequence lengths within a request batch. (3) Fused block copy. Block copy operations, issued by the copy-on-write mechanism, may operate on discontinuous blocks. This can lead to numerous invocations of small data movements if we use the cudaMemcpyAsync API. To mitigate the overhead, we implement a kernel that batches the copy operations for different blocks into a single kernel launch. 5.2 Supporting Various Decoding Algorithms vLLM implements various decoding algorithms using three key methods: fork, append, and free. The fork method creates a new sequence from an existing one. The append method appends a new token to the sequence. Finally, the free method deletes the", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "4d3af1cb82e16391e39288ddaf21ea9e9948c2306db9b4fa98424c86c8dced9f"}
{"doc_id": "arxiv:2309.06180#method:part-11", "url": "https://arxiv.org/abs/2309.06180", "anchor": "#method:part-11", "type": "paper", "title": "", "section": "Method", "text": "lead to numerous invocations of small data movements if we use the cudaMemcpyAsync API. To mitigate the overhead, we implement a kernel that batches the copy operations for different blocks into a single kernel launch. 5.2 Supporting Various Decoding Algorithms vLLM implements various decoding algorithms using three key methods: fork, append, and free. The fork method creates a new sequence from an existing one. The append method appends a new token to the sequence. Finally, the free method deletes the sequence. For instance, in paral- lel sampling, vLLM creates multiple output sequences from the single input sequence using the fork method. It then adds new tokens to these sequences in every iteration with append, and deletes sequences that meet a stopping condi- tion using free. The same strategy is also applied in beam search and prefix sharing by vLLM. We believe future decod- ing algorithms can also be supported by combining these methods. 6", "source": "arxiv_pdf", "published": "", "tokens": 154, "sha256": "75a1808cb5523911ace26c85ca4b730c1dd144d300091ed1fc436598f1151e74"}
{"doc_id": "arxiv:2309.06180#evaluation:part-1", "url": "https://arxiv.org/abs/2309.06180", "anchor": "#evaluation:part-1", "type": "paper", "title": "", "section": "Evaluation", "text": "In this section, we evaluate the performance of vLLM under a variety of workloads. 9 0.0 0.5 1.0 1.5 2.0 Request rate (req/s) (a) OPT-13B, 1 GPU, ShareGPT 0.0 0.5 1.0 0.0 0.2 0.4 0.6 0.8 1.0 Request rate (req/s) (b) OPT-66B, 4 GPUs, ShareGPT 0.0 0.5 1.0 0.0 0.5 1.0 1.5 2.0 2.5 Request rate (req/s) (c) OPT-175B, 8 GPUs, ShareGPT 0.0 0.5 1.0 Normalized latency (s/token) FasterTransformer Orca (Max) Orca (Pow2) Orca (Oracle) vLLM 0 10 20 30 Request rate (req/s) (d) OPT-13B, 1 GPU, Alpaca 0.0 0.5 1.0 0 5 10 15 20 Request rate (req/s) (e) OPT-66B, 4 GPUs, Alpaca 0.0 0.5 1.0 0 5 10 15 20 Request rate (req/s) (f) OPT-175B, 8 GPUs, Alpaca 0.0 0.5 1.0 Normalized latency (s/token) Figure 12. Single sequence generation with OPT models on the ShareGPT and Alpaca dataset Orca (Max) Orca (Pow2) Orca (Oracle) vLLM 0 5 10 15 20 25 30 35 # Batched requests 7.00 9.81 13.62 30.42 (a) ShareGPT Orca (Max) Orca (Pow2) Orca (Oracle) vLLM 0 25 50 75 100 125 150 # Batched requests 7.00 43.24 72.75 132.44 (b) Alpaca Figure 13. Average number of batched requests when serv- ing OPT-13B for the ShareGPT (2 reqs/s) and Alpaca (30 reqs/s) traces. 6.1 Experimental Setup Model and server configurations. We use OPT [62] mod- els with 13B, 66B, and 175B parameters and LLaMA [52] with 13B parameters for our evaluation. 13B and 66B are popular sizes for LLMs as shown in an LLM leaderboard [38], while 175B is the size of the famous GPT-3 [5] model. For all of our experiments, we use A2 instances with NVIDIA A100 GPUs on Google Cloud Platform. The detailed model sizes and server configurations are shown in Table 1. Workloads. We synthesize workloads based on ShareGPT [51] and Alpaca [50] datasets, which contain input and output texts of real LLM services. The ShareGPT dataset is a collec- tion of user-shared conversations with ChatGPT [35]. The Alpaca dataset is an instruction dataset generated by GPT- 3.5 with self-instruct [57]. We tokenize the datasets and use their input and output lengths to synthesize client requests. As shown in Fig. 11, the ShareGPT dataset has 8.4× longer input prompts and 5.8× longer outputs on average than the Alpaca dataset, with higher variance. Since these datasets do not include timestamps, we generate request arrival times using Poisson distribution with different request rates. Baseline 1: FasterTransformer. FasterTransformer [31] is a distributed inference engine highly optimized for latency. As FasterTransformer does not have its own scheduler, we implement a custom scheduler with a dynamic batching mechanism similar to the existing serving systems such as Triton [30]. Specifically, we set a maximum batch size 𝐵as large as possible for each experiment, according to the GPU memory capacity. The scheduler takes up to 𝐵number of earliest arrived requests and sends the batch to FasterTrans- former for processing. Baseline 2: Orca. Orca [60] is a state-of-the-art LLM serving system optimized for throughput. Since Orca is not publicly available for use, we implement our own version of Orca. We assume", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "1a57abc37b54f86d8670c63d235f0ae785ce4fc27d03644f736fccbf156d57bd"}
{"doc_id": "arxiv:2309.06180#evaluation:part-2", "url": "https://arxiv.org/abs/2309.06180", "anchor": "#evaluation:part-2", "type": "paper", "title": "", "section": "Evaluation", "text": "similar to the existing serving systems such as Triton [30]. Specifically, we set a maximum batch size 𝐵as large as possible for each experiment, according to the GPU memory capacity. The scheduler takes up to 𝐵number of earliest arrived requests and sends the batch to FasterTrans- former for processing. Baseline 2: Orca. Orca [60] is a state-of-the-art LLM serving system optimized for throughput. Since Orca is not publicly available for use, we implement our own version of Orca. We assume Orca uses the buddy allocation algorithm to deter- mine the memory address to store KV cache. We implement three versions of Orca based on how much it over-reserves the space for request outputs: • Orca (Oracle). We assume the system has the knowledge of the lengths of the outputs that will be actually generated for the requests. This shows the upper-bound performance of Orca, which is infeasible to achieve in practice. • Orca (Pow2). We assume the system over-reserves the space for outputs by at most 2×. For example, if the true output length is 25, it reserves 32 positions for outputs. • Orca (Max). We assume the system always reserves the space up to the maximum sequence length of the model, i.e., 2048 tokens. Key metrics. We focus on serving throughput. Specifically, using the workloads with different request rates, we mea- sure normalized latency of the systems, the mean of every request’s end-to-end latency divided by its output length, as in Orca [60]. A high-throughput serving system should retain low normalized latency against high request rates. For most experiments, we evaluate the systems with 1-hour traces. As an exception, we use 15-minute traces for the OPT-175B model due to the cost limit. 10 0 5 10 15 Request rate (req/s) (a) parallel generation (parallel size = 2) 0.0 0.5 1.0 0 2 4 6 8 10 Request rate (req/s) (b) parallel generation (parallel size = 4) 0.0 0.5 1.0 0 2 4 6 Request rate (req/s) (c) parallel generation (parallel size = 6) 0.0 0.5 1.0 Normalized latency (s/token) Orca (Max) Orca (Pow2) Orca (Oracle) vLLM 0 5 10 15 Request rate (req/s) (d) beam search (beam width = 2) 0.0 0.5 1.0 0 2 4 6 8 10 Request rate (req/s) (e) beam search (beam width = 4) 0.0 0.5 1.0 0 2 4 6 Request rate (req/s) (f) beam search (beam width = 6) 0.0 0.5 1.0 Normalized latency (s/token) Figure 14. Parallel generation and beam search with OPT-13B on the Alpaca dataset. 6.2 Basic Sampling We evaluate the performance of vLLM with basic sampling (one sample per request) on three models and two datasets. The first row of Fig. 12 shows the results on the ShareGPT dataset. The curves illustrate that as the request rate in- creases, the latency initially increases at a gradual pace but then suddenly explodes. This can be attributed to the fact that when the request rate surpasses the capacity of the serv- ing system, the queue length continues to grow infinitely and so does the latency of the requests. On the ShareGPT dataset,", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "fc2da9462f07d8ed32859422225affb3b253fc4512a75567f246ae0e52c9cc5a"}
{"doc_id": "arxiv:2309.06180#evaluation:part-3", "url": "https://arxiv.org/abs/2309.06180", "anchor": "#evaluation:part-3", "type": "paper", "title": "", "section": "Evaluation", "text": "on three models and two datasets. The first row of Fig. 12 shows the results on the ShareGPT dataset. The curves illustrate that as the request rate in- creases, the latency initially increases at a gradual pace but then suddenly explodes. This can be attributed to the fact that when the request rate surpasses the capacity of the serv- ing system, the queue length continues to grow infinitely and so does the latency of the requests. On the ShareGPT dataset, vLLM can sustain 1.7×–2.7× higher request rates compared to Orca (Oracle) and 2.7×–8× compared to Orca (Max), while maintaining similar laten- cies. This is because vLLM’s PagedAttention can efficiently manage the memory usage and thus enable batching more requests than Orca. For example, as shown in Fig. 13a, for OPT-13B vLLM processes 2.2× more requests at the same time than Orca (Oracle) and 4.3× more requests than Orca (Max). Compared to FasterTransformer, vLLM can sustain up to 22× higher request rates, as FasterTransformer does not utilize a fine-grained scheduling mechanism and inefficiently manages the memory like Orca (Max). The second row of Fig. 12 and Fig. 13b shows the results on the Alpaca dataset, which follows a similar trend to the ShareGPT dataset. One exception is Fig. 12 (f), where vLLM’s advantage over Orca (Oracle) and Orca (Pow2) is less pro- nounced. This is because the model and server configuration for OPT-175B (Table 1) allows for large GPU memory space available to store KV cache, while the Alpaca dataset has short sequences. In this setup, Orca (Oracle) and Orca (Pow2) can also batch a large number of requests despite the inef- ficiencies in their memory management. As a result, the performance of the systems becomes compute-bound rather than memory-bound. 2 4 6 # Output sequences 0 4 8 12 Memory saving (%) 6.09 8.53 9.79 (a) Parallel sampling 2 4 6 Beam width 0 20 40 60 Memory saving (%) 37.56 53.13 55.16 (b) Beam search Figure 15. Average amount of memory saving from sharing KV blocks, when serving OPT-13B for the Alpaca trace. 6.3 Parallel Sampling and Beam Search We evaluate the effectiveness of memory sharing in Page- dAttention with two popular sampling methods: parallel sampling and beam search. In parallel sampling, all paral- lel sequences in a request can share the KV cache for the prompt. As shown in the first row of Fig. 14, with a larger number of sequences to sample, vLLM brings more improve- ment over the Orca baselines. Similarly, the second row of Fig. 14 shows the results for beam search with different beam widths. Since beam search allows for more sharing, vLLM demonstrates even greater performance benefits. The im- provement of vLLM over Orca (Oracle) on OPT-13B and the Alpaca dataset goes from 1.3× in basic sampling to 2.3× in beam search with a width of 6. Fig. 15 plots the amount of memory saving, computed by the number of blocks we saved by sharing divided by the number of total blocks without sharing. We show 6.1% - 9.8% memory saving on parallel sampling and", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "8feb2d8c0cf6dbdc702dee0043ec50cc70705fef885c6db9582601b1d7fefea7"}
{"doc_id": "arxiv:2309.06180#evaluation:part-4", "url": "https://arxiv.org/abs/2309.06180", "anchor": "#evaluation:part-4", "type": "paper", "title": "", "section": "Evaluation", "text": "beam search allows for more sharing, vLLM demonstrates even greater performance benefits. The im- provement of vLLM over Orca (Oracle) on OPT-13B and the Alpaca dataset goes from 1.3× in basic sampling to 2.3× in beam search with a width of 6. Fig. 15 plots the amount of memory saving, computed by the number of blocks we saved by sharing divided by the number of total blocks without sharing. We show 6.1% - 9.8% memory saving on parallel sampling and 37.6% - 55.2% on beam search. In the same experiments with the ShareGPT dataset, we saw 16.2% - 30.5% memory saving on parallel sampling and 44.3% - 66.3% on beam search. 6.4 Shared prefix We explore the effectiveness of vLLM for the case a prefix is shared among different input prompts, as illustrated in 11 0 20 40 Request rate (req/s) (a) 1-shot prefix prompt 0.0 0.5 1.0 0 20 40 Request rate (req/s) (b) 5-shot prefix prompt 0.0 0.5 1.0 Normalized latency (s/token) Orca (Oracle) vLLM Figure 16. Translation workload where the input prompts share a common prefix. The prefix includes (a) 1 example with 80 tokens or (b) 5 examples with 341 tokens. 0.0 0.2 0.4 0.6 0.8 Request rate (req/s) 0.0 0.5 1.0 Normalized latency (s/token) Orca (Max) Orca (Pow2) Orca (Oracle) vLLM Figure 17. Performance on chatbot workload. Fig. 10. For the model, we use LLaMA-13B [52], which is mul- tilingual. For the workload, we use the WMT16 [4] English- to-German translation dataset and synthesize two prefixes that include an instruction and a few translation examples. The first prefix includes a single example (i.e., one-shot) while the other prefix includes 5 examples (i.e., few-shot). As shown in Fig. 16 (a), vLLM achieves 1.67× higher through- put than Orca (Oracle) when the one-shot prefix is shared. Furthermore, when more examples are shared (Fig. 16 (b)), vLLM achieves 3.58× higher throughput than Orca (Oracle). 6.5 Chatbot A chatbot [8, 19, 35] is one of the most important applications of LLMs. To implement a chatbot, we let the model generate a response by concatenating the chatting history and the last user query into a prompt. We synthesize the chatting history and user query using the ShareGPT dataset. Due to the limited context length of the OPT-13B model, we cut the prompt to the last 1024 tokens and let the model generate at most 1024 tokens. We do not store the KV cache between different conversation rounds as doing this would occupy the space for other requests between the conversation rounds. Fig. 17 shows that vLLM can sustain 2× higher request rates compared to the three Orca baselines. Since the ShareGPT dataset contains many long conversations, the input prompts for most requests have 1024 tokens. Due to the buddy allo- cation algorithm, the Orca baselines reserve the space for 1024 tokens for the request outputs, regardless of how they predict the output lengths. For this reason, the three Orca baselines behave similarly. In contrast, vLLM can effectively 64 128 256 Context length 0 50 100 150 200 250 Kernel latency (us) vLLM", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "4029bc75d6d336bdfeb697ac47090e7aed580d5fcfb4c8c010698e2b57f15646"}
{"doc_id": "arxiv:2309.06180#evaluation:part-5", "url": "https://arxiv.org/abs/2309.06180", "anchor": "#evaluation:part-5", "type": "paper", "title": "", "section": "Evaluation", "text": "compared to the three Orca baselines. Since the ShareGPT dataset contains many long conversations, the input prompts for most requests have 1024 tokens. Due to the buddy allo- cation algorithm, the Orca baselines reserve the space for 1024 tokens for the request outputs, regardless of how they predict the output lengths. For this reason, the three Orca baselines behave similarly. In contrast, vLLM can effectively 64 128 256 Context length 0 50 100 150 200 250 Kernel latency (us) vLLM (bs 8) FT (bs 8) vLLM (bs 32) FT (bs 32) (a) Latency of attention kernels. 1 2 4 8 16 32 64 128 256 Block size 0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5 Normalized latency (s/token) ShareGPT Alpaca (b) End-to-end latency with dif- ferent block sizes. Figure 18. Ablation experiments. handle the long prompts, as PagedAttention resolves the problem of memory fragmentation and reservation. 7 Ablation Studies In this section, we study various aspects of vLLM and evalu- ate the design choices we make with ablation experiments. 7.1 Kernel Microbenchmark The dynamic block mapping in PagedAttention affects the performance of the GPU operations involving the stored KV cache, i.e., block read/writes and attention. Compared to the existing systems, our GPU kernels (§5) involve extra over- heads of accessing the block table, executing extra branches, and handling variable sequence lengths. As shown in Fig. 18a, this leads to 20–26% higher attention kernel latency, com- pared to the highly-optimized FasterTransformer implemen- tation. We believe the overhead is small as it only affects the attention operator but not the other operators in the model, such as Linear. Despite the overhead, PagedAttention makes vLLM significantly outperform FasterTransformer in end-to-end performance (§6). 7.2 Impact of Block Size The choice of block size can have a substantial impact on the performance of vLLM. If the block size is too small, vLLM may not fully utilize the GPU’s parallelism for reading and processing KV cache. If the block size is too large, inter- nal fragmentation increases and the probability of sharing decreases. In Fig. 18b, we evaluate the performance of vLLM with dif- ferent block sizes, using the ShareGPT and Alpaca traces with basic sampling under fixed request rates. In the ShareGPT trace, block sizes from 16 to 128 lead to the best performance. In the Alpaca trace, while the block size 16 and 32 work well, larger block sizes significantly degrade the performance since the sequences become shorter than the block sizes. In practice, we find that the block size 16 is large enough to efficiently utilize the GPU and small enough to avoid signifi- cant internal fragmentation in most workloads. Accordingly, vLLM sets its default block size as 16. 12 1 2 4 8 16 32 64 128 256 Block size 0 20 40 60 80 100 120 140 Time (ms) Recompute Swap in Swap out Swap in + out (a) Microbenchmark 1 2 4 8 16 32 64 128 256 Block size 0.0 0.5 1.0 1.5 2.0 2.5 Normalized latency (s/token) Recompute Swap (b) End-to-end performance Figure 19. (a) Overhead of recomputation and swapping", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "7b1a80fd3bc4343f080b691bc503327eaa26800df6ab8dbc902dbcc054fc9cf8"}
{"doc_id": "arxiv:2309.06180#evaluation:part-6", "url": "https://arxiv.org/abs/2309.06180", "anchor": "#evaluation:part-6", "type": "paper", "title": "", "section": "Evaluation", "text": "internal fragmentation in most workloads. Accordingly, vLLM sets its default block size as 16. 12 1 2 4 8 16 32 64 128 256 Block size 0 20 40 60 80 100 120 140 Time (ms) Recompute Swap in Swap out Swap in + out (a) Microbenchmark 1 2 4 8 16 32 64 128 256 Block size 0.0 0.5 1.0 1.5 2.0 2.5 Normalized latency (s/token) Recompute Swap (b) End-to-end performance Figure 19. (a) Overhead of recomputation and swapping for different block sizes. (b) Performance when serving OPT-13B with the ShareGPT traces at the same request rate. 7.3 Comparing Recomputation and Swapping vLLM supports both recomputation and swapping as its re- covery mechanisms. To understand the tradeoffs between the two methods, we evaluate their end-to-end performance and microbenchmark their overheads, as presented in Fig. 19. Our results reveal that swapping incurs excessive overhead with small block sizes. This is because small block sizes often result in numerous small data transfers between CPU and GPU, which limits the effective PCIe bandwidth. In contrast, the overhead of recomputation remains constant across dif- ferent block sizes, as recomputation does not utilize the KV blocks. Thus, recomputation is more efficient when the block size is small, while swapping is more efficient when the block size is large, though recomputation overhead is never higher than 20% of swapping’s latency. For medium block sizes from 16 to 64, the two methods exhibit comparable end-to-end performance. 8 Discussion Applying the virtual memory and paging technique to other GPU workloads. The idea of virtual memory and paging is effective for managing the KV cache in LLM serving because the workload requires dynamic memory allocation (since the output length is not known a priori) and its perfor- mance is bound by the GPU memory capacity. However, this does not generally hold for every GPU workload. For exam- ple, in DNN training, the tensor shapes are typically static, and thus memory allocation can be optimized ahead of time. For another example, in serving DNNs that are not LLMs, an increase in memory efficiency may not result in any per- formance improvement since the performance is primarily compute-bound. In such scenarios, introducing the vLLM’s techniques may rather degrade the performance due to the extra overhead of memory indirection and non-contiguous block memory. However, we would be excited to see vLLM’s techniques being applied to other workloads with similar properties to LLM serving. LLM-specific optimizations in applying virtual mem- ory and paging. vLLM re-interprets and augments the idea of virtual memory and paging by leveraging the application- specific semantics. One example is vLLM’s all-or-nothing swap-out policy, which exploits the fact that processing a request requires all of its corresponding token states to be stored in GPU memory. Another example is the recomputa- tion method to recover the evicted blocks, which is not feasi- ble in OS. Besides, vLLM mitigates the overhead of memory indirection in paging by fusing the GPU kernels for memory access operations with those for other operations such as attention. 9", "source": "arxiv_pdf", "published": "", "tokens": 503, "sha256": "855887d826f5e0ac160761883b5dbf9f5242a584291d8fa71fa075d69ed16ce3"}
{"doc_id": "arxiv:2309.06180#related-work", "url": "https://arxiv.org/abs/2309.06180", "anchor": "#related-work", "type": "paper", "title": "", "section": "Related Work", "text": "General model serving systems. Model serving has been an active area of research in recent years, with numerous systems proposed to tackle diverse aspects of deep learning model deployment. Clipper [11], TensorFlow Serving [33], Nexus [45], InferLine [10], and Clockwork [20] are some earlier general model serving systems. They study batch- ing, caching, placement, and scheduling for serving single or multiple models. More recently, DVABatch [12] intro- duces multi-entry multi-exit batching. REEF [21] and Shep- herd [61] propose preemption for serving. AlpaServe [28] utilizes model parallelism for statistical multiplexing. How- ever, these general systems fail to take into account the auto- regressive property and token state of LLM inference, result- ing in missed opportunities for optimization. Specialized serving systems for transformers. Due to the significance of the transformer architecture, numerous specialized serving systems for it have been developed. These systems utilize GPU kernel optimizations [1, 29, 31, 56], ad- vanced batching mechanisms [14, 60], model parallelism [1, 41, 60], and parameter sharing [64] for efficient serving. Among them, Orca [60] is most relevant to our approach. Comparison to Orca. The iteration-level scheduling in Orca [60] and PagedAttention in vLLM are complementary techniques: While both systems aim to increase the GPU utilization and hence the throughput of LLM serving, Orca achieves it by scheduling and interleaving the requests so that more requests can be processed in parallel, while vLLM is doing so by increasing memory utilization so that the working sets of more requests fit into memory. By reducing memory fragmentation and enabling sharing, vLLM runs more requests in a batch in parallel and achieves a 2-4× speedup compared to Orca. Indeed, the fine-grained sched- uling and interleaving of the requests like in Orca makes memory management more challenging, making the tech- niques proposed in vLLM even more crucial. Memory optimizations. The widening gap between the compute capability and memory capacity of accelerators has caused memory to become a bottleneck for both training and inference. Swapping [23, 42, 55], recomputation [7, 24] and their combination [40] have been utilized to reduce the peak memory of training. Notably, FlexGen [46] studies how to swap weights and token states for LLM inference with 13 limited GPU memory, but it does not target the online serv- ing settings. OLLA [48] optimizes the lifetime and location of tensors to reduce fragmentation, but it does not do fine- grained block-level management or online serving. FlashAt- tention [13] applies tiling and kernel optimizations to reduce the peak memory of attention computation and reduce I/O costs. This paper introduces a new idea of block-level mem- ory management in the context of online serving. 10", "source": "arxiv_pdf", "published": "", "tokens": 436, "sha256": "770bdef664ae7de8b8d7b4a8e03aea733083e50b85b705f2fc1873f37575609e"}
{"doc_id": "arxiv:2309.06180#conclusion", "url": "https://arxiv.org/abs/2309.06180", "anchor": "#conclusion", "type": "paper", "title": "", "section": "Conclusion", "text": "This paper proposes PagedAttention, a new attention algo- rithm that allows attention keys and values to be stored in non-contiguous paged memory, and presents vLLM, a high-throughput LLM serving system with efficient mem- ory management enabled by PagedAttention. Inspired by operating systems, we demonstrate how established tech- niques, such as virtual memory and copy-on-write, can be adapted to efficiently manage KV cache and handle various decoding algorithms in LLM serving. Our experiments show that vLLM achieves 2-4× throughput improvements over the state-of-the-art systems. Acknowledgement We would like to thank Xiaoxuan Liu, Zhifeng Chen, Yan- ping Huang, anonymous SOSP reviewers, and our shepherd, Lidong Zhou, for their insightful feedback. This research is partly supported by gifts from Andreessen Horowitz, Anyscale, Astronomer, Google, IBM, Intel, Lacework, Microsoft, Mo- hamed Bin Zayed University of Artificial Intelligence, Sam- sung SDS, Uber, and VMware.", "source": "arxiv_pdf", "published": "", "tokens": 139, "sha256": "6b2c8d28e3b11a103c3c79651f9ab6c3d1a5b21bebc3d8f95a52098e5df53c8f"}
