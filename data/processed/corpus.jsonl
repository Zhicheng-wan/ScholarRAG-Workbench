{"doc_id": "arxiv:2404.10630#model", "url": "https://arxiv.org/abs/2404.10630", "anchor": "#model", "type": "paper", "title": "", "section": "Model", "text": "Sizes Sequence length OpenLLaMA1 7B, 13B 2048 OpenLLaMA2 7B 2048 LLaMA1 7B, 13B, 33B, 65B 2048 LLaMA2 7B, 13B, 70B 4096 HLAT 7B, 70B 4096 Evaluation Tasks: We evaluate HLAT against baselines on 7 groups of tasks including both zero-shot and few-shot tasks [36]. We use HumanEval [37] for coding tasks, and Language Model Evaluation Harness [38] for others. Massive Multitask Language Understanding (MMLU) [15], [39] contains 57 tasks, spanning STEM, social sciences, humanities, and other subjects. The difficulty ranges from elementary to professional levels. The breadth of the dataset tests model’s overall problem solving and knowledge ability. Commonsense Reasoning (CR) consists of 6 datasets: PIQA [40], HellaSwag [41], WinoGrande [42], ARC easy and challenge [43], and OpenBookQA [29]. These multi- choice tasks include carefully crafted riddles, puzzles, and scenarios designed to probe a model’s ability to leverage implicit knowledge, make logical inferences, and navigate the rules of physical and social worlds. World Knowledge (WK) includes NaturalQuestions [44] and TriviaQA [45]. Both tasks are designed to test model’s question-answering ability in closed book setting. The models are not provided documents that may contain information about the question, and it has to rely on information learnt or memorized in pre-training data. Reading Comprehension (RC) uses BoolQ [46] to test model’s open book comprehension ability. BoolQ is a question answering dataset for yes/no questions. Each example is a triplet of (question, passage, answer), with the title of the page as optional additional context. The model is required to answer the question based on the given context in passage. Math ability is evaluated with GSM8K (Grade School Math 8K) [47]. GSM8K contains 8,500 grade school math problems. Both problems and answers are provided in natural language. These problems take between 2 and 8 steps to solve, which is ideal for testing basic multi-step reasoning ability. Code evaluation uses HumanEval [37] dataset including 164 programming problems with a function signature, docstring, body, and several unit tests. They were handwritten to ensure not present in the training set of the models. A. Performance against open-source Models We compare the performance of HLAT with other open- source benchmarks in Table II. The numbers are reported in percentage and for HLAT results, we include both mean and TABLE II: Evaluation of HLAT against 4 open-source models on 6 groups of tasks described in Section V. Numbers in the parentheses represent standard deviation, if available.", "source": "arxiv_pdf", "published": "", "tokens": 397, "sha256": "1f7a04ae12cdbefc36365643cd475c081a310e246a966cd9d7d68b198939e1e1"}
{"doc_id": "arxiv:2404.10630#model:part-1", "url": "https://arxiv.org/abs/2404.10630", "anchor": "#model:part-1", "type": "paper", "title": "", "section": "Model", "text": "Size MMLU CR WK RC Math Code Average - - accuracy accuracy exact match accuracy accuracy pass@1 pass@10 - OpenLLaMA-1 7B 30.5 58.4 40.6 70.5 5.2 4.5 13.4 41.2 OpenLLaMA-2 7B 41.1 61.3 37.9 72.4 6.8 9.7 25 44.9 LLaMA-1 7B 35.1 63.5 43.6 76.5 11 10.5 21.3 47.4 LLaMA-2 7B 45.3 64 45.2 77.4 14.6 12.2 25.2 49.2 HLAT-7B 7B 41.3 (3.6) 59.5 (1.2) 38.8 (0.5) 72.5 (0.8) 9.4 (0.8) 7.6 19.8 44.6 OpenLLaMA-1 13B 43.5 62 45.9 72.3 8.3 7 17 47.1 LLaMA-1 13B 46.9 65.3 49.7 78.1 17.8 15.8 22.6 53.1 LLaMA-2 13B 45.3 66.3 50.5 81.7 28.7 18.3 30.5 54.6 LLaMA-1 33B 57.8 68.9 54.6 83.1 35.6 21.7 38.4 59.2 LLaMA-1 65B 63.4 69.8 57 85.3 50.9 23.7 - 62.1 LLaMA-2 70B 68.9 70.7 59 85 56.8 30.5 59.4 64.7 HLAT-70B 70B 65.1 (3.4) 67.3 (1.2) 54.5 (0.6) 82.6 (0.7) 48.5 (1.4) 21.4 57.9 60.8 standard deviation (in the parentheses, if available). We also report an average score over all tasks in the last column. HLAT-7B performs better than OpenLLaMA-1 and is on- par with OpenLLaMA-2. Both HLAT-7B and OpenLLaMA models have some gap with LLaMA-1 and LLaMA-2, which is likely due to the training data quality. Even though the data composition of RedPajama-1T is similar as those used in LLaMA-1, the data cleaning pipeline and final data quality are different, which therefore affects the model performance [48]. For HLAT-70B, we use the same training dataset as the 7B model for consistency. Although there is no OpenLLaMA baseline for a fair comparison, HLAT-70B performs better than LLaMA-1 and LLaMA-2 models of smaller sizes. The model performance gap with LLaMA-1 (65B) and LLaMA-2 (70B) is also smaller than those on 7B models. We acknowl- edge the lack of effort on data quality improvement, but our main goal is to showcase the effectiveness and efficiency of AWS TRAINIUM. On MMLU (5-shot), both HLAT models perform better than OpenLLaMA-1 and LLaMA-1 models of similar size. The performance is slightly worse than LLaMA-2 family of models, likely due to the difference in training dataset size and composition [8]. On Commonsense Reasoning (0-shot) and World Knowl- edge (5-shot), HLAT-7B performs similar to OpenLLaMA-1 and OpenLLaMA-2 models. By diving deep into performance on each individual task, HLAT-7B excels in 19/29 tasks as compared with OpenLLaMA-1, and 15/29 tasks compared with OpenLLaMA-2. Both HLAT and OpenLLaMA models have some gaps with LLaMA-1 and LLaMA-2 models, which may be due to the training set quality. Nevertheless, the gap (∼3%) is consistent on 7B and 70B models. On Math problems (GSM8K, 8-shot), HLAT-7B performs significantly better than OpenLLaMA-1 and OpenLLaMA-2. As will be discussed in the next section, HLAT-70B has a big improvement of Math ability in later training stage. HLAT- 70B performs similar as LLaMA1-65B, and we observed significant improvement in upsampling training stage. On Coding problems, both HLAT-7B and HLAT-70B perform comparable with LLaMA-1. HLAT-7B performs better than OpenLLaMA-1 and worse than OpenLLaMA- 2 and LLaMA-2. First, for OpenLLaMA-1, the tokenizer merges consecutive spaces which negatively affects the coding performance, as it eliminates important information", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "a7552a7c57daa5102b6e9cb04cf7a777ce27d4c763396756430de988a1e565d1"}
{"doc_id": "arxiv:2404.10630#model:part-2", "url": "https://arxiv.org/abs/2404.10630", "anchor": "#model:part-2", "type": "paper", "title": "", "section": "Model", "text": "OpenLLaMA-1 and OpenLLaMA-2. As will be discussed in the next section, HLAT-70B has a big improvement of Math ability in later training stage. HLAT- 70B performs similar as LLaMA1-65B, and we observed significant improvement in upsampling training stage. On Coding problems, both HLAT-7B and HLAT-70B perform comparable with LLaMA-1. HLAT-7B performs better than OpenLLaMA-1 and worse than OpenLLaMA- 2 and LLaMA-2. First, for OpenLLaMA-1, the tokenizer merges consecutive spaces which negatively affects the coding performance, as it eliminates important information such as indentation and line breaks. This issue is subsequently fixed in OpenLLaMA-2, which explains its better performance. Besides, OpenLLaMA-2 is trained with additional code data from StarCoder which also contributes to performance im- provement. B. Intermediate Model Performance During the model training, we also evaluate the intermediate checkpoints about every 200 billion tokens. Figure 3 and Figure 4 show the model performance of HLAT-7B and HLAT-70B with respect to number of seen training tokens (in billions), respectively. On most benchmarks, the performance improves steadily, and correlates with the training loss. We found that for different tasks, the model converges at different rates. For Commonsense Reasoning, the model accuracy improves quickly at beginning of training, and starts to saturate at later training stages. This is similar as the trends observed in other LLM model trainings [8], [49]. However, for Math task (GSM8K) shown in Figure 3e, the learning curve shows an exponentially increasing trend. It increase very gradually for the initial ∼1 trillion tokens and begins to improve significantly during the later stages of training. Intuitively, this seems to indicate that the model is able to grasp more logical abilities after entering a relatively stable training plateau. We defer further research into this behavior as a future work. For World Knowledge task shown in Figure 3c, the per- formance increases almost linearly with number of training tokens. Since this is a closed book test and mainly evaluates the model’s ability of memorizing facts in pre-training data, the model seems to consistently improve its ability on this domain with more training steps and epochs. In addition, we tested if the trending is related to number of shots used in evaluation. It turns out that the trends are very similar for zero-shot, 3-shot, and 5-shot tests. 250 500 750 1000 1250 1500 1750 Tokens (in Billions) 25 30 35 40 45 Accuracy (norm) MMLU (a) 250 500 750 1000 1250 1500 1750 Tokens (in Billions) 50 52 54 56 58 60 Accuracy (norm) Commonsense Reasoning (b) 250 500 750 1000 1250 1500 1750 Tokens (in Billions) 5 10 15 20 25 30 35 40 Exact match World Knowledge 0 shots 3 shots 5 shots (c) 250 500 750 1000 1250 1500 1750 Tokens (in Billions) 55.0 57.5 60.0 62.5 65.0 67.5 70.0 72.5 Accuracy BoolQ (d) 250 500 750 1000 1250 1500 1750 Tokens (in Billions) 2 4 6 8 10 Accuracy GSM8K (e) 250 500 750 1000 1250 1500 1750 Tokens (in Billions) 5 10 15 20 25 30 35 Percentage Code pass@1 pass@10 pass@100 (f) Fig. 3: Intermediate model performance with number", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "e73cb464adeaf1fc0a25a88e8ec0a68dbd5f59bd15a217af33fa8f0378a3727b"}
{"doc_id": "arxiv:2404.10630#model:part-3", "url": "https://arxiv.org/abs/2404.10630", "anchor": "#model:part-3", "type": "paper", "title": "", "section": "Model", "text": "Exact match World Knowledge 0 shots 3 shots 5 shots (c) 250 500 750 1000 1250 1500 1750 Tokens (in Billions) 55.0 57.5 60.0 62.5 65.0 67.5 70.0 72.5 Accuracy BoolQ (d) 250 500 750 1000 1250 1500 1750 Tokens (in Billions) 2 4 6 8 10 Accuracy GSM8K (e) 250 500 750 1000 1250 1500 1750 Tokens (in Billions) 5 10 15 20 25 30 35 Percentage Code pass@1 pass@10 pass@100 (f) Fig. 3: Intermediate model performance with number of seen tokens for HLAT-7B. 250 500 750 1000 1250 1500 1750 Tokens (in Billions) 30 40 50 60 70 Accuracy MMLU (a) 250 500 750 1000 1250 1500 1750 Tokens (in Billions) 54 56 58 60 62 64 66 68 Accuracy (norm) Commonsense Reasoning (b) 250 500 750 1000 1250 1500 1750 Tokens (in Billions) 35 40 45 50 55 Exact match World Knowledge (c) 250 500 750 1000 1250 1500 1750 Tokens (in Billions) 65 70 75 80 85 Accuracy BoolQ (d) 250 500 750 1000 1250 1500 1750 Tokens (in Billions) 0 10 20 30 40 50 Exact match GSM8K (e) 200 400 600 800 1000 1200 1400 1600 1800 Tokens (in Billions) 10 20 30 40 50 Percentage Code pass@1 pass@10 (f) Fig. 4: Intermediate model performance with number of seen tokens for HLAT-70B. Those observations indicate the necessity of a set of eval- uation tasks covering a wide range of domains for LLM pre-training. A single validation set or evaluation tasks from narrow domains may not fully reflect the actual over- or under- fitting of the model for general downstream performance. C. Upsampling During HLAT-70B training, we upsampled the training dataset in last 400B tokens. Specifically, we use 35.47% web data, 41.27% math data, and 23.26% coding data with more details listed in Table III. In Figure 4, we plot the evalua- tion performance of HLAT-70B with seen training tokens. In upsampling training stage, that is, after 1400B tokens, TABLE III: Upsampling dataset composition for HLAT-70B. Datasets Size Percentage (billions of tokens) Web Data Wikipedia [21] 90 35.47% C4 [21] Domain Specific StackExchange 104.7 41.27% Arxiv [21] Open-Web-Math [23] PeS2o [22] Code Github [21] 59 23.26% Total - 253.7 15.16% we observe significant model performance improvement over math, coding, and MMLU performance. It improved math by 10% and coding by 5%. This is consistent with the findings in LLaMA-3 [10], where the researchers found significant improvement of LLaMA-3 8B model on math problems. However, they mentioned such method did not help much for 405B models. Our experiment fills the model size gap, and shows that upsampling still helps for a 70B model. D. Checkpoint Averaging For HLAT-70B, we average the last two checkpoints used in pre-training to generate a checkpoint for final evaluation [10], [33]. Table IV compares the individual checkpoints with 1740B training tokens and 1800B training tokens, as well as the averaged checkpoints. The averaged checkpoints outperforms individual checkpoints on average performance, as well as some individual tasks. TABLE IV: Evaluation of HLAT-70B with individual and averaged checkpoints. Checkpoint MMLU RC WK CR Math Code", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "a8bb281e58c90887129de05b676493b851f3bec063df775da898963a0c5d5a78"}
{"doc_id": "arxiv:2404.10630#model:part-4", "url": "https://arxiv.org/abs/2404.10630", "anchor": "#model:part-4", "type": "paper", "title": "", "section": "Model", "text": "helps for a 70B model. D. Checkpoint Averaging For HLAT-70B, we average the last two checkpoints used in pre-training to generate a checkpoint for final evaluation [10], [33]. Table IV compares the individual checkpoints with 1740B training tokens and 1800B training tokens, as well as the averaged checkpoints. The averaged checkpoints outperforms individual checkpoints on average performance, as well as some individual tasks. TABLE IV: Evaluation of HLAT-70B with individual and averaged checkpoints. Checkpoint MMLU RC WK CR Math Code Avg. 1740B 64.5 67.5 54 83.1 47.3 18.3 60.3 1800B 64.2 66.9 54 82.1 47.2 21.8 60.2 Average 65.1 67.3 54.5 82.6 48.5 21.4 60.8 E. Truthfulness and Bias We report the model’s truthfulness and bias using Truth- fulQA [50] and CrowS-pairs [51]. TruthfulQA presents a collection of meticulously crafted questions spanning diverse domains such as health, law, finance, and even politics. These queries deliberately target areas where human intuition and personal biases can lead to incorrect responses, and measure an LLM’s resistance to misinformed or erroneous knowledge. CrowS-Pairs is a benchmark designed to probe LLMs for social biases across nine categories, including gender, reli- gion, race/color, sexual orientation, age, nationality, disability, physical appearance and socioeconomic status. Each example is composed of a stereotype and an anti-stereotype. TABLE V: Model Truthfulness and Bias evaluation. CrowS- pairs (CSP) uses percentage of stereotypes as metric and TruthfulQA (TQA) uses multiple choice accuracy as metric. Dataset Size CSP (↓) CSP TQA (↑) TQA Tasks - english french mc1 mc2 OpenLLaMA-1 7B 64.6 50.1 23.1 35.1 OpenLLaMA-2 7B 65.6 51.7 22.6 34.6 LLaMA-1 7B 53.7 47.5 22.0 34.1 LLaMA-2 7B 66.9 54.9 25.2 39.0 HLAT-7B 7B 65.2 54.5 23.6 37.2 LLaMA-1 65B 69.3 58.3 27.9 42.6 LLaMA-2 70B 69.8 63.5 30.6 44.8 HLAT-70B 70B 68.1 59.1 32.3 45.9 We present the results in Table V with 0 shot inference. For TruthfulQA, we measure the multiple-choice score, and higher score shows better truthfulness. For CrowS-Pairs, it measures the percentage of models choosing answers of stereotypes, so lower scores indicates smaller bias. Overall, HLAT performs similar to other open-source models. F. Efficiency and Scalability We describe the training efficiency in terms of Cost per 4-million tokens (CPT) and scalability reported in [52]. The CPT is defined as CPT = C T ×3600 × 4e6, where C is instance cost per hour ($21.50 for Trainium, and $32.77 for GPU), T is training throughput (tokens per second). CPT quantifies both the training speed and also hardware cost. We use this metric to compare training efficiency of Trainium and GPU. 4 8 16 32 64 Number of nodes 0 2 4 6 8 10 12 Normalized CPT trn1, 7B p4d, 7B trn1, 70B p4d, 70B Fig. 5: Normalized cost per 4 million tokens (CPT) for 7B and 70B models on AWS TRAINIUM with various number of nodes. CPT of GPU baseline (p4d, 7B) with 4 nodes is normalized to 100%. 70B models ran into out-of-memory on 4 nodes. For comparison, the GPU baseline is established using p4d.24xlarge instances and NeMo 23.08 [25] (available inside NeMo docker container with tag 23.08) software", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "16290b00b94467bce4ef9dc5eb01a8d0c4f47501d979f21cfe631896e70c702d"}
{"doc_id": "arxiv:2404.10630#model:part-5", "url": "https://arxiv.org/abs/2404.10630", "anchor": "#model:part-5", "type": "paper", "title": "", "section": "Model", "text": "6 8 10 12 Normalized CPT trn1, 7B p4d, 7B trn1, 70B p4d, 70B Fig. 5: Normalized cost per 4 million tokens (CPT) for 7B and 70B models on AWS TRAINIUM with various number of nodes. CPT of GPU baseline (p4d, 7B) with 4 nodes is normalized to 100%. 70B models ran into out-of-memory on 4 nodes. For comparison, the GPU baseline is established using p4d.24xlarge instances and NeMo 23.08 [25] (available inside NeMo docker container with tag 23.08) software stack. Fig- ure 5 plots the normalized CPT of training on TRAINIUM and scaling. The TRAINIUM CPT is normalized, such that the CPT of the GPU baseline (p4d, 7B) on 4 nodes is 100%. Overall, the training cost on trn1 is approximately 60% of GPU, and is consistent with the number of nodes. In addition, the CPTs on 70B models are roughly 10 times of those on 7B models. G. Model Limitation We note some limitations of HLAT in this section. Similar as other LLMs, HLAT suffers a set of limitations such as hallucinations, potential non-factual generations, biases, and toxicity [53]. For example, although comparable with other open-source pre-trained models, the bias of HLAT is still relative high on some subjects such as sexual orientation, physical appearance, religion, and socioeconomic (see Table V). This is partially due to the usage of publicly available datasets. More importantly, as a pre-trained model, HLAT has not gone through a supervised finetuning and human prefer- ence alignment. Those fine-tuning methods have been shown to be able to alleviate some limitations of pre-trained LLMs [9]. Another limitation is that our training is stopped after 1.8 trillion tokens. As is suggested by LLaMA-3 [10], HLAT may be able to further improve on certain tasks, such as math, world knowledge, MMLU, and coding, with more training tokens. VI. BEST PRACTICES & FUTURE DIRECTIONS In this section, we share some best practices we observed for training on AWS TRAINIUM, and raise open questions for future research. Parallelism: NxDT supports TP up to 32 degrees and pipeline parallelism. For a 7B model, we found that the combination of TP=8 and PP=1 provides the highest training throughput, but not for HLAT-70B. So the optimal parallelism configuration varies with model sizes and architectures. To achieve the highest training throughput, parallelism configu- ration needs to be jointly optimized with choice of activation checkpointing method, gradient accumulation steps, and train- ing precision, to balance memory and communication costs. Training Precision: NxDT supports various training pre- cision configurations, including full precision (FP32), BF16 with and without SR, standard mixed precision training, etc. Full precision training is often memory-wise infeasible for multi-billion LLMs. We compared multiple training strategies for HLAT: pure BF16, BF16 with SR and standard mixed precision training. Empirically, we found that training loss of pure BF16 diverges. BF16 with SR shows similar training loss as mixed precision on HLAT-7B model, but converges slower on HLAT-70B. We finally chose BF16 with SR for higher throughput on HLAT-7B, but standard mixed precision on HLAT-70B. For models of other sizes and architecture, warm-up study may be needed", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "2f5e409cfd33be307785bce29ac7af9f1d41871bbc242b54daeb78df32791f87"}
{"doc_id": "arxiv:2404.10630#model:part-6", "url": "https://arxiv.org/abs/2404.10630", "anchor": "#model:part-6", "type": "paper", "title": "", "section": "Model", "text": "often memory-wise infeasible for multi-billion LLMs. We compared multiple training strategies for HLAT: pure BF16, BF16 with SR and standard mixed precision training. Empirically, we found that training loss of pure BF16 diverges. BF16 with SR shows similar training loss as mixed precision on HLAT-7B model, but converges slower on HLAT-70B. We finally chose BF16 with SR for higher throughput on HLAT-7B, but standard mixed precision on HLAT-70B. For models of other sizes and architecture, warm-up study may be needed to decide the optimal training precision. Usually, the divergence can be observed in first few thousands of steps. Choice of β2: We observed that using β2 = 0.99 causes training instability and slower convergence. This is related to the choice of BF16 with SR training precision. A large β2 fails to capture the gradient explosion at current and recent steps, and hence does not effectively reduce the gradients in occurrence of gradient explosion. Switching to β2 = 0.95 addresses the above-mentioned problem. Weight decay: We applied weight decay to all layers. Empirically, weight decay is not applied to normalization and bias layers [54]. In our experiment, we did not found much performance-wise difference of those two methods. Pre-compilation: TRAINIUM requires pre-compiling the scripts to graphs. The compilation takes some time, especially for large models. Debugging on training scripts (e.g., printing out intermediate tensors) may require re-compilation. Instead of directly developing on a large model, we found it more efficient to develop and test on a smaller model and scale up afterwards. VII. RELATED WORK LLM pre-training: After the Transformer architecture [1] was introduced, BERT [54] was proposed to pre-train a language model on a large corpus of unlabeled data. Fol- lowing the success of BERT model on various NLP tasks, many pre-trained language models are later introduced with different architectures and training methods, such as GPT-2 [55], RoBERTa [56], BART [57], and so on [6]. Studies later observed significant performance improvement of language models by increasing model size and training data [58]. Such abilities are further demonstrated in LLMs such as GPT-3 [7], PaLM [59], LLaMA [8]–[10], Falcon [60], Gemini [61], Phi [48], etc. Pre-trained on trillions of tokens, LLMs with tens or hundreds of billions parameters show remarkable ability in generating creative text contents, as well as a variety of downstream tasks, such as question answering, summarization, machine translation, programming, etc. [6]. AI accelerators: Most models are trained on NVIDIA GPU accelerators, such as GPT [7], [62] and LLaMA [8], [9]. Falcon-180B [60] was trained on AWS SageMaker, with up to 4,096 A100 40GB GPUs using p4d instances. However, the landscape of hardware accelerators for deep learning train- ing has blossomed in recent years, with established players like NVIDIA GPUs facing fierce competition from custom offerings like Google’s TPU and AWS TRAINIUM. PaLM- 2 [59] and OpenLLaMA [29] have demonstreated successful LLM pre-training on Google TPU. Recently, OLMo [49] is an open-source model developed by AI2. It has two models trained on AMD and Nvidia GPUs, separately. The two models have nearly identical performance on their evaluation suite by 2T tokens.", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "c96b0fd22b26e5e58be5abed3349e5bb069d60164005e5b9aac25e1774330083"}
{"doc_id": "arxiv:2404.10630#model:part-7", "url": "https://arxiv.org/abs/2404.10630", "anchor": "#model:part-7", "type": "paper", "title": "", "section": "Model", "text": "of hardware accelerators for deep learning train- ing has blossomed in recent years, with established players like NVIDIA GPUs facing fierce competition from custom offerings like Google’s TPU and AWS TRAINIUM. PaLM- 2 [59] and OpenLLaMA [29] have demonstreated successful LLM pre-training on Google TPU. Recently, OLMo [49] is an open-source model developed by AI2. It has two models trained on AMD and Nvidia GPUs, separately. The two models have nearly identical performance on their evaluation suite by 2T tokens. AWS TRAINIUM is a machine learning accelerator developed for deep learning training with high performance and cost-competitiveness. Our work is the first demonstration of end-to-end multi-billion LLM pre-trained on AWS TRAINIUM. Ultimately, the optimal choice depends on the specific needs of the training task, with further research required to fully explore the potential of each accelerator and their possible convergence in future architectures. VIII. CONCLUSION In this paper, we pre-train HLAT, a family of 7 bil- lion and 70 billion parameter large language models, using AWS TRAINIUM over ∼1.8 trillion tokens. HLAT follows the decoder-only architecture and is trained with up to 256 Amazon EC2 trn1.32xlarge instances. We evaluate the per- formance of HLAT against popular open-source baseline models including LLaMA and OpenLLaMA on a variety of popular benchmarking tasks. We find that HLAT achieves model quality on par with these baseline models of similar sizes. This work demonstrates, for the first time, that AWS TRAINIUM with NxDT is able to successfully pre-train high- quality LLMs with high efficiency and low cost.", "source": "arxiv_pdf", "published": "", "tokens": 252, "sha256": "16d93448a27f17f55f281fffad1dc07f8c560e5ec9eb6a999f9690fc6ff65bab"}
{"doc_id": "arxiv:2412.10543#abstract", "url": "https://arxiv.org/abs/2412.10543", "anchor": "#abstract", "type": "paper", "title": "", "section": "Abstract", "text": "RAG (Retrieval Augmented Generation) allows LLMs (large language models) to generate better responses with exter- nal knowledge, but using more external knowledge causes higher response delay. Prior work focuses either on reducing the response delay (e.g., better scheduling of RAG queries) or on maximizing quality (e.g., tuning the RAG workflow), but they fall short in systematically balancing the tradeoff between the delay and quality of RAG responses. To bal- ance both quality and response delay, this paper presents METIS, the first RAG system that jointly schedules queries and adapts the key RAG configurations of each query, such as the number of retrieved text chunks and synthesis meth- ods. Using four popular RAG-QA datasets, we show that compared to the state-of-the-art RAG optimization schemes, METIS reduces the generation latency by 1.64 −2.54× with- out sacrificing generation quality. 1", "source": "arxiv_pdf", "published": "", "tokens": 137, "sha256": "b406a2a0b9db36394819f2e170b6319f504c0bec0ef9ea59bec676bf50e21d4f"}
{"doc_id": "arxiv:2412.10543#introduction:part-1", "url": "https://arxiv.org/abs/2412.10543", "anchor": "#introduction:part-1", "type": "paper", "title": "", "section": "Introduction", "text": "Retrieval-augmented generation (RAG) is a popular LLM inference technique that augments an LLM inference query with relevant text chunks, or “context”, retrieved from a large corpus.1 RAG systems, which include retrieval and LLM inference2, have found many use cases in QA tasks, personal assistants, chatbots, and LLM-powered search [10, 62]. While RAG can enhance the quality (accuracy and relevance) of LLM-generated responses [7, 53, 58, 91, 96], RAG queries are inherently slow as they need more compute and mem- ory resources to process the long input context to answer a query [6, 15, 42]. Thus, it is essential to balance high response quality and low response delays in RAG inference systems. 1RAG vs. long-context models is an active field of research, with the industry widely deploying RAG for its task-focused model inference quality and better resource-sharing capabilities [68]. 2Though RAG sometimes refers to the retrieval step, in this work, a RAG system includes both retrieval and LLM inference based on the retrieved texts, and we aim to optimize the whole pipeline. Past research efforts have optimized RAG, regarding ei- ther response quality or response delay, but they fall short in optimizing the quality-delay tradeoffs of RAG. RAG queries have an associated RAG configuration which de- scribes how and how much data to input for the query (more in §2) [72, 79, 83]. One line of prior work focuses on reducing response delay through better query schedul- ing (e.g., GPU allocation and inference batching) for RAG queries [2, 44, 45, 70, 76], without adapting the RAG con- figuration themselves. An alternate line of work focuses on maximizing generation quality by tuning the configurations of RAG queries [32, 77, 83], but this is often done at the cost of longer response delay. The RAG configuration simultaneously affects generation quality and response delay (e.g., retrieving too many chunks for a simple RAG query may unnecessarily inflate delay with- out increasing quality). Unlike traditional data queries (e.g., SQL) which specify the inputs and operators, RAG queries are inherently under-specified as they consist of a text query written in natural language [27, 32, 57, 64] and do not directly specify the exact RAG configuration of its execution. Moreover, multiple configuration knobs can influence the delay-quality tradeoffs. For instance, besides how many chunks to retrieve, how to use them in the LLM’s input involves two design choices—should the chunks be processed by the LLM jointly, or should the chunks be summarized first before being fed into the LLM together (and how long should a summary be). Recent works also attempt to tune RAG con- figuration [32, 77], but they focus on either tuning individual knobs or maximizing quality at the cost of higher delay. How- ever, tuning configurations across multiple knobs quickly hits a prohibitive combinatorial space (more in §3) and re- quires optimizations to reduce the search cost. What’s more, the RAG configuration should be tuned jointly with scheduling. Consider two configurations: 𝐴feeds all retrieved text chunks in one LLM input, and 𝐵summarizes first each chunk with an LLM and then feeds the summaries 1 arXiv:2412.10543v2 [cs.LG]", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "d9142e9627112b69af8cb5f22978e474158cd1ffcbf75d2f7635604ecdc58a0b"}
{"doc_id": "arxiv:2412.10543#introduction:part-2", "url": "https://arxiv.org/abs/2412.10543", "anchor": "#introduction:part-2", "type": "paper", "title": "", "section": "Introduction", "text": "they focus on either tuning individual knobs or maximizing quality at the cost of higher delay. How- ever, tuning configurations across multiple knobs quickly hits a prohibitive combinatorial space (more in §3) and re- quires optimizations to reduce the search cost. What’s more, the RAG configuration should be tuned jointly with scheduling. Consider two configurations: 𝐴feeds all retrieved text chunks in one LLM input, and 𝐵summarizes first each chunk with an LLM and then feeds the summaries 1 arXiv:2412.10543v2 [cs.LG] 16 Jul 2025 to an LLM input for a final generation. While 𝐴(which calls the LLM once) is seemingly faster than 𝐵(which calls the LLM multiple times), 𝐴could be slower as it requires more GPU memory than 𝐵and thus could be delayed in the sched- uler queue. Without making batching and configuration se- lection jointly, it would be difficult to avoid such pitfalls. Finally, the impact of RAG configurations on quality-delay tradeoffs also varies significantly with queries. For example, to answer “In which country is the Kimbrough Memorial Sta- dium located?”, the RAG may retrieve and analyze one text chunk about the stadium. In contrast, to answer “Compare NVIDIA’s operating cost over the first three quarters of 2024 and identify the highest one”, the RAG may need multiple chunks, each containing the quarter’s operating cost, and process these chunks jointly, instead of reading them separately. The above examples illustrate queries differ in complexity (more in §4), leading to needing different configurations per-query for optimal quality-delay tradeoffs. Empirically, we show that picking RAG configuration per-query achieves 12 −15% higher quality and 2.5 −3× lower delay than using any fixed configuration across all queries in a dataset (§5). Thus, RAG configurations should be adapted on a per-query basis. Yet, existing RAG systems, which hand-pick a static config- uration offline based on a few example queries [1, 21, 39, 85], lose out on quality or response time. This paper presents METIS, the first RAG system that adapts multiple configuration knobs on a per-query basis and jointly makes configuration selections and scheduling decisions (i.e., which LLM inference in a batch) to optimize the delay-quality tradeoffs for RAG. As this would require solving a joint combinatorial prob- lem for every query, which can be prohibitively expensive (§3), METIS tackles the challenge with a two-step approach. First, METIS prunes the massive configuration space for each received query to a smaller yet promising one that con- tains configurations that likely yield high-quality output for the given query. Specifically, METIS uses a separate LLM to estimate the query’s profile, including how many pieces of information are required to answer the query and whether joint reasoning is likely required across these pieces of in- formation (more in §4.1). The intuition of the query profiles is that they can effectively filter out undesirable RAG config- urations. For the earlier query example “Compare NVIDIA’s operating cost over the first three quarters of 2024 and identify the highest one,” the estimated profile would suggest that it involves at least three separate pieces of information, so the number of chunks (one of the configuration knobs)", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "0a67a5bbdd0544d7e6604402f5d0f8df2c7cfcf1ca854e48afbfcffc7ad3a80c"}
{"doc_id": "arxiv:2412.10543#introduction:part-3", "url": "https://arxiv.org/abs/2412.10543", "anchor": "#introduction:part-3", "type": "paper", "title": "", "section": "Introduction", "text": "query and whether joint reasoning is likely required across these pieces of in- formation (more in §4.1). The intuition of the query profiles is that they can effectively filter out undesirable RAG config- urations. For the earlier query example “Compare NVIDIA’s operating cost over the first three quarters of 2024 and identify the highest one,” the estimated profile would suggest that it involves at least three separate pieces of information, so the number of chunks (one of the configuration knobs) should be at least three. It should be noted that the LLM-based profiler is an extra overhead in METIS, but fortunately, its input only contains the RAG query itself and the metadata of the RAG database, which are orders of magnitude shorter than the long contexts in RAG, so the estimation can be relatively fast, about 1/10 of the delay of the execution of the RAG query. METIS (Ours) AdaptiveRAG (ACL 2024) Parrot (OSDI 2024) vLLM : SOTA LLM Engine Better METIS (Ours) AdaptiveRAG (ACL 2024) Parrot (OSDI 2024) vLLM : SOTA LLM Engine Better Figure 1. Performance of METIS on the KG RAG FinSec [50] dataset compared to the baselines. Full results shown in §7. Using the narrowed configuration space, METIS reduces the RAG response delays by jointly deciding the per-query configuration and query scheduling based on available re- sources (§4.3). The insight is that within the pruned configu- ration space, the scheduler can make optimal configuration decisions without exploring the original, large configuration space and the implications on quality. In short, METIS’s two-level design loosely decouples the problem into (1) pruning configuration space to a smaller yet promising range of configurations, which focuses solely on keeping the accuracy high, and (2) jointly optimizing configuration (within the narrowed range) and scheduling to optimize response delay by choosing configurations which best-fit into the GPU memory. We evaluate METIS across four RAG datasets with diverse query profiles (e.g., reasoning vs. domain-specific QA). Fig- ure 1 shows a preview of our results. Our key takeaways are as follows. When achieving the same or higher quality than the baselines, METIS reduces the response delay by 1.6−2.8× compared to the latest vLLM (a state-of-the-art serving en- gine), Parrot (the latest LLM query-scheduling method), as well as AdaptiveRAG (the latest RAG configuration-tuning method). METIS also achieves 1.8 −4.5× higher through- put compared to these baselines when achieving the same response delay and same/higher quality. The general concept of using LLMs to guide system tuning is not exactly new [60, 88], but our key contribution lies in applying the concept to RAG systems, through joint sched- uling with resource-aware configuration selection, leading to significantly better resource sharing (§4.2, §4.3). 2 RAG systems and configurations As an LLM often does not have domain-specific or up-to-date knowledge, LLM applications commonly employ RAG to sup- plement LLM inference with external knowledge to generate high-quality responses. Despite the growth of model context length, using RAG to pinpoint the relevant context is still significantly cheaper in terms of resource cost (GPU require- ment), latency, and memory consumption (KV Cache size). For 2 general-purpose", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "47a76b7d8d9debb103d1a73314c681eb320a0d3d23b207a942306207ed5d05f3"}
{"doc_id": "arxiv:2412.10543#introduction:part-4", "url": "https://arxiv.org/abs/2412.10543", "anchor": "#introduction:part-4", "type": "paper", "title": "", "section": "Introduction", "text": "resource-aware configuration selection, leading to significantly better resource sharing (§4.2, §4.3). 2 RAG systems and configurations As an LLM often does not have domain-specific or up-to-date knowledge, LLM applications commonly employ RAG to sup- plement LLM inference with external knowledge to generate high-quality responses. Despite the growth of model context length, using RAG to pinpoint the relevant context is still significantly cheaper in terms of resource cost (GPU require- ment), latency, and memory consumption (KV Cache size). For 2 general-purpose QA pipelines, RAG is cost-efficient with re- trieving targeted chunks based on semantic similarity to the query. Using LLMs with long-context documents in contrast has much higher GPU memory usage and delay [43, 45, 71]. Before processing queries, a RAG system organizes back- ground documents by splitting them into chunks (each with a fixed number of tokens), embedding each chunk using models like Bert [12, 19], and storing the embeddings with the chunks in a vector database. Processing a RAG query involves two main steps: • Retrieval: The RAG system retrieves one or more rele- vant context chunks from the database by comparing the query’s embedding, (using the same embedding model as for database indexing), with the stored embeddings. • Synthesis: After retrieving the relevant chunks, the RAG system combines these chunks and the RAG query to form a single/multiple LLM call(s) to generate the response. Retrieval is computationally lightweight and much faster than synthesis (> 100×), so the response delay is typically dominated by the synthesis step [90]. RAG configuration: This work focuses on optimizing three configuration knobs, illustrated in Figure 2, which are de- rived from key design questions that affect RAG performance in terms of response delay and quality: • How many chunks to retrieve (num_chunks): The number of context chunks directly affects the delay of the synthesis step, with more computation needed to process the longer sequences with more chunks. In the meantime, retrieving too few chunks risks low response quality if the retrieved chunks do not contain enough useful information. • How to synthesize (synthesis_method): If the LLM should read the chunks separately, RAG uses the LLM to generate one answer for the query using each chunk separately and picks the output with the highest confidence, which is called map_rerank. This often incurs the least compu- tation but can cause low quality if the useful information is scattered in different chunks, in which case the LLM should read the chunks jointly. The RAG system can feed these chunks in the LLM input directly by concatenating them within a single prompt (called stuff) or to create a shorter summary for each chunk first before feeding the summaries and the query into the LLM to generate the final response (called map_reduce). stuff needs less computa- tion than map_reduce, but risks degraded output quality for long inputs due to the lost-in-the-middle problem [47]. • How long is each summary (intermediate_length): Fi- nally, if the LLM produces the summary for each chunk based on the user query, the length of each summary greatly affects the quality and response of map_reduce— shorter", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "3b568ece8ef08daa463a1d90b44e98742e87a8d6b61869bc8b9e9d5ef7a30a9d"}
{"doc_id": "arxiv:2412.10543#introduction:part-5", "url": "https://arxiv.org/abs/2412.10543", "anchor": "#introduction:part-5", "type": "paper", "title": "", "section": "Introduction", "text": "summary for each chunk first before feeding the summaries and the query into the LLM to generate the final response (called map_reduce). stuff needs less computa- tion than map_reduce, but risks degraded output quality for long inputs due to the lost-in-the-middle problem [47]. • How long is each summary (intermediate_length): Fi- nally, if the LLM produces the summary for each chunk based on the user query, the length of each summary greatly affects the quality and response of map_reduce— shorter summaries yield lower delay but also risk not feed- ing enough information to the final LLM inference. How many chunks to retrieve? If jointly, should the LLM summarize each chunk first? If so, how long should each summary be? If multiple chunks, should the LLM read them jointly? Knob 1: num_chunks Knob 2: synthesis_method Knob 3: intermediate_length Key design choices of RAG Figure 2. The configuration knobs adapted by METIS are derived from key design choices of RAG systems. Chunk 1 Chunk 2 Chunk 3 LLM Final Answer Chunk 1 Chunk 2 Chunk 3 Final Answer 1 Confidence : 80% Final Answer 2 Confidence : 99% Final Answer 3 Confidence : 90% Chunk 1 Chunk 2 Chunk 3 S1 S2 S3 Final Answer (a) Stuff (b) Map Rerank (c) Map Reduce LLM LLM LLM Figure 3. Illustration of different RAG synthesis methods, which have various LLM reasoning capabilities. In this work, while we focus on universal RAG knobs which affect quality and delay common to all RAG systems, METIS can be extended to other tunable knobs (e.g., some RAG system may dynamically choose the embedding model, retrieval index or serving LLM). METIS’ design is extensible to any RAG configuration knob based on the query profile. Performance metrics: We evaluate the performance of a RAG system using two metrics: • Response quality calculates the F1 score of the generated response against the ground truth. The F1 score is the harmonic mean of precision (# correctly generated words) and recall (# of correct words successfully generated). This metric is widely used in prior works [10, 69, 72]. • Response delay measures the time elapsed from when the RAG system receives a RAG request to when it completes generating the response. Next, we show that these knobs need to be properly tuned on a per-query basis to achieve optimal tradeoff between quality and delay in §3. 3 Towards better quality-delay tradeoffs Prior work on RAG either optimizes for lower delay or higher quality, i.e., the first picks static configurations and focuses on reducing the delay by smart scheduling and resource allo- cation [44, 70, 76] and the second picks RAG configurations to maximize quality without regard to resource usage or delay [32, 77, 83]. For the first time, we explore the potential of optimizing the quality-delay tradeoffs for RAG. 3 (a) Change: synthesis method from map_rerank (circle) , stuff (plus) and map_reduce (square) (b) Change: number of chunks from 1 to 35 with stuff (c) Change: intermediate length from 1 to 100 with map_reduce Q1 Q1 Q1 Q2 Q2 Q2 Q3 Q3 Q3 Figure 4.", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "aa2a62e6a2676c1bc20827d57ee1fe6cbb65e434a71ff2e9c1182f9a553a14f2"}
{"doc_id": "arxiv:2412.10543#introduction:part-6", "url": "https://arxiv.org/abs/2412.10543", "anchor": "#introduction:part-6", "type": "paper", "title": "", "section": "Introduction", "text": "and the second picks RAG configurations to maximize quality without regard to resource usage or delay [32, 77, 83]. For the first time, we explore the potential of optimizing the quality-delay tradeoffs for RAG. 3 (a) Change: synthesis method from map_rerank (circle) , stuff (plus) and map_reduce (square) (b) Change: number of chunks from 1 to 35 with stuff (c) Change: intermediate length from 1 to 100 with map_reduce Q1 Q1 Q1 Q2 Q2 Q2 Q3 Q3 Q3 Figure 4. Varying each RAG configuration knob leads to different quality-latency tradeoffs, and these tradeoffs differ across queries (Q1 in green, Q2 in blue, and Q3 in red). To improve the delay-quality tradeoff, our insight is that quality and delay should jointly be optimized in this large tradeoff space created by the choice of RAG configuration knobs. Importantly, the configurations with better quality- delay tradeoffs vary significantly across queries. To showcase this observation, we use three queries from Musique [78], a popular reasoning QA dataset (§7.1). • Q1: “In what county was William W. Blair’s born?” • Q2: “Are Alison Skipper, Diane Gilliam Fisher, and Rachel McAdams from the same country?” • Q3: “When and why did the Voyager 1, the spacecraft that detected storms on Neptune, leave our solar system?” We chose queries with different natural language complexity and reasoning, Q1 being relatively less complex than Q2 and Q3. Then, we adjust the value of each configuration knob in order to quantify each knob’s impact on the quality- delay tradeoffs in each of the queries. Impact of synthesis method: Figure 4 (a) changes the syn- thesis method and shows its effect on the quality-delay trade- off, while keeping the other RAG configuration knobs con- stant. We vary the synthesis method as map_rerank, stuff, and map_reduce from left to right. The insight is that the optimal synthesis method that strikes the best quality-delay tradeoff (closest to the top left corner) differs significantly across the different queries. For simple queries like Q1 (green), quality plateaus for more complex synthesis methods (stuff and map_reduce). Because it only needs a single piece of context, map_rerank which processes chunks in isolation suffices, whereas cross- chunk reasoning (stuff and map_reduce) adds undue delay (2×) without improving quality. For queries such as Q2 (blue) that require cross-chunk rea- soning, stuff and map_reduce provide significant quality improvements (35% increase) by processing chunks jointly. For more complex queries, such as Q3 (red), which require even more reasoning and information (why Voyager 1 left has multiple reasons), methods like map_reduce improve quality (30% increase) by removing unnecessary text in the mapper phase, to help the LLM focus on the relevant content. Impact of the number of retrieved chunks: Figure 4 (b) fixes the synthesis method (stuff) and shows the impact of the number of retrieved chunks (1-35) on quality and delay. Simple queries, like Q1 (green), can often be answered using just one or two chunks (needs only birth county). For more complex queries, Q2 (blue) and Q3 (red), increasing the number of chunks (1-15) improves the likelihood of retriev- ing all relevant", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "60112ce6bcfaf4e7f034686d17df1802d9f1667ef6a3a09f8d321c375c38fbed"}
{"doc_id": "arxiv:2412.10543#introduction:part-7", "url": "https://arxiv.org/abs/2412.10543", "anchor": "#introduction:part-7", "type": "paper", "title": "", "section": "Introduction", "text": "help the LLM focus on the relevant content. Impact of the number of retrieved chunks: Figure 4 (b) fixes the synthesis method (stuff) and shows the impact of the number of retrieved chunks (1-35) on quality and delay. Simple queries, like Q1 (green), can often be answered using just one or two chunks (needs only birth county). For more complex queries, Q2 (blue) and Q3 (red), increasing the number of chunks (1-15) improves the likelihood of retriev- ing all relevant context and improves quality. Blindly retrieving more chunks than necessary risks di- luting the relevance of actual important information, due to commonly known problems such as “lost-in-the-middle” [28, 47]. In all three queries, retrieving more chunks beyond a point harms the quality (up to 20% drop) and unnecessar- ily inflates delay (up to 3×). Hence we have a quality-delay tradeoff where increasing chunks up to a point helps quality but beyond that it increases delay while degrading quality. Impact of the intermediate output length: Figure 4 (c) shows the impact of our third configuration knob, vary- ing the intermediate output length (1-100) for map_reduce synthesis methods on the quality-delay tradeoff. For simple queries like Q1 (green), short amounts of intermediate length are enough to answer the query (10-20 words). For more com- plex queries Q2 (blue) and Q3 (red), increasing the amount of intermediate length (70-100 words) provided helps the model with enough information to answer the query. Overall, we see that RAG queries naturally vary in com- plexity, requiring differing levels of inter-chunk reasoning and varying numbers of context chunks. More complex queries, which require more reasoning and context, ben- efit from increased LLM computation, which can come at the cost of increased delay. Adding more context chunks helps to a point beyond which it harms the output quality and delay. Thus, adapting RAG configuration on a per-query basis is crucial. Figures 2, 3, 4 illustrate tuning most popular RAG configuration knobs, however the tuning extends to more RAG configurations with richer tradeoff spaces (§4.2). 4 Pareto Boundary of fixed configuration with vLLM Pareto Boundary of fixed configuration with vLLM Per-Query Configuration Per-Query Configuration Figure 5. Per-query configuration can achieve significantly better quality-delay tradeoffs across queries compared to every fixed configuration choice. Figure 5 uses queries from two datasets (Musique and QM- SUM, see §7.1) and shows that picking the best configuration for each query (the best configuration is the one with the lowest delay that achieves less than 2% drop than the highest achievable quality) achieves superior quality-delay tradeoff than picking any static configuration for all queries. Choos- ing the configuration per-query allows up to 3× delay saving compared to static configurations which are the closest in quality. Every single static configuration choice that achieves comparable delay has at least a 10% quality drop. In spite of the potential benefits, per-query configuration adaptation faces challenges that hinder their real-world adop- tion. Each RAG query comes in plain text with practically no associated RAG configurations. Moreover, the space of configurations grows exponentially with multiple knobs. For example, for a map_reduce configuration,", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "943a60526132d3179fa53f81ee80181bbc3d2be0426a4ab4f1177a0a25cb94b3"}
{"doc_id": "arxiv:2412.10543#introduction:part-8", "url": "https://arxiv.org/abs/2412.10543", "anchor": "#introduction:part-8", "type": "paper", "title": "", "section": "Introduction", "text": "configuration per-query allows up to 3× delay saving compared to static configurations which are the closest in quality. Every single static configuration choice that achieves comparable delay has at least a 10% quality drop. In spite of the potential benefits, per-query configuration adaptation faces challenges that hinder their real-world adop- tion. Each RAG query comes in plain text with practically no associated RAG configurations. Moreover, the space of configurations grows exponentially with multiple knobs. For example, for a map_reduce configuration, with 30 values for num_chunks and 50 values for intermediate_length leads to 1500 configurations for a query. Exhaustively profiling all configurations per-query and choosing the best is infeasible. Alternatively, if we profile periodically, we lose out on the potential configuration selection for each query, as variance in query profile leads to different quality-delay tradeoffs. Pro- filing cost is also prohibitively expensive as the LLM needs to be run with many synthesis methods, number of chunks etc., which require high GPU usage. Additionally, the delay of profiling can be ∼100× the inference delay due to multiple LLM calls during profiling. Online RAG queries have strin- gent requirements for GPU resource usage and end-to-end delay [70, 76]. This makes it hard to systematically decide what an optimal per-input configuration should be. To truly achieve the benefit of per-query configuration adaptation, we need a smart system to drastically reduce to a useful configuration space, in a fast and cheap manner. 4 METIS: Enabling per-query configuration adaptation for RAG We present METIS, a novel system for serving RAG queries focusing on high generation quality and minimal delay. METIS is a RAG controller (Figure 6) with two main components: Configuration Space Pruning (§ 4.1, 4.2 ) Joint scheduler (§ 4.3) RAG Queries Vector Database GPU Memory Serving LLM RAG Configs Text Chunks Check Resource Status Generated Output Retriever RAG Synthesis Chosen Config Figure 6. METIS consists of a RAG controller which per- forms configuration space pruning and joint scheduling. • Pruning configuration space: We estimate each query’s pro- file (§4.1) and reduce the RAG configuration space to a smaller yet promising one that still yields high generation quality (§4.2) (leading to a 50-100× reduction). • RAG scheduler: Within the pruned configuration space for the query, METIS’ scheduler chooses the best config- uration for the query to achieve the best quality-latency trade-off based on the available system resources (§4.3). Once the configuration is chosen, the METIS’ executes the query using the chosen configuration—retrieving the selected number of chunks and uses the selected synthesis method to feed into the LLM’s input. 4.1 Estimating a query’s profile Query profile: To choose the correct RAG configurations, the first step of METIS is to create the profile of the query (as we see in Figure 7) by querying an LLM (we call this LLM query profiler). We ask the query profiler to estimate four high-level dimensions for each query. • Query complexity refers to the intricacy of the query itself. Queries with less complexity are more like simple yes/no questions, while queries with high complexity are more like why questions, which require", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "04052c26574ccf1c6894d4cd522e43fbaa41c43502576fb7ac3a3e6cdeb53d0c"}
{"doc_id": "arxiv:2412.10543#introduction:part-9", "url": "https://arxiv.org/abs/2412.10543", "anchor": "#introduction:part-9", "type": "paper", "title": "", "section": "Introduction", "text": "choose the correct RAG configurations, the first step of METIS is to create the profile of the query (as we see in Figure 7) by querying an LLM (we call this LLM query profiler). We ask the query profiler to estimate four high-level dimensions for each query. • Query complexity refers to the intricacy of the query itself. Queries with less complexity are more like simple yes/no questions, while queries with high complexity are more like why questions, which require deeper reasoning than yes/no questions. As a result, it requires more LLM com- putation to correctly answer complex queries. The output for this dimension is binary “High/Low” • Joint reasoning requirement describes whether multiple pieces of information are needed to answer the query. Even relatively simple queries may require joint reasoning (e.g., checking whether the annual income from two years is the same). The output for this dimension is binary “Yes/No” • Pieces of information required refers to the distinct, stan- dalone pieces of information required to fully answer the query (e.g., the annual income from how many years is required to draw the trend of annual income). The output for this dimension is a number from 1-10. 5 Query Profiler ( LLM ) § 4.1 Estimate the query complexity How many pieces of information? How much can we summarize? Input Prompt Content Query complexity: High/ Low Needs X pieces of information Summary length: X to Y Rule-based Mapping § 4.2 Query Synthesis", "source": "arxiv_pdf", "published": "", "tokens": 243, "sha256": "ced0960f22e0f95a13cb8c636578d5a4be831abe66773826495af351833b97cf"}
{"doc_id": "arxiv:2412.10543#method:part-1", "url": "https://arxiv.org/abs/2412.10543", "anchor": "#method:part-1", "type": "paper", "title": "", "section": "Method", "text": "Number of chunks (value) Intermediat e Length (value) Do we need joint reasoning? Joint reasoning: Yes/No Figure 7. METIS RAG configuration selection workflow. • The length of the summarization: If the query is complex and needs a lot of different information, it is often neces- sary to first summarize the relevant information chunks first (to reduce the noise inside these chunks) and then gen- erate the final answer from these summaries. The output for this dimension is a number from 30-200. METIS is not the first to use query profile as a metric for deciding RAG configurations, it extends upon methods like AdaptiveRAG [32] which have used LLM’s to estimate query profile but they only focus on one dimension (the number of chunks to retrieve). In Section 7, we show the impact of each dimension on the overall improvement. Why the query profile could be estimated: Estimating the aforementioned query profile is feasible, not only be- cause of the reasoning power of LLMs3 in analyzing natural language queries, but also because we provide sufficient in- formation to the LLM-based profiler. METIS feeds the profile estimator with not only the query, but also a metadata of the database that contains the background document. The metadata is a short description about the type of con- tent in the database and its data size (chunk_size). Specif- ically, we use a single-line summaries already attached to the original source datasets as the metadata of the dataset. For example, the metadata for the KG RAG Finsec’s database [50] contains quarterly financial reports and questions of Fortune 500 companies with a chunk_size of 1000. It de- scribes the content topics of the chunks with information such as revenue growth indicators, product release informa- tion, sales etc.,. When presented with a query on financials of such a company, the LLM can use the metadata to decide questions like how much to summarize/how much reasoning is required. We give details on the prompt and the intuition to generate metadata for new datasets in Appendix §A. It is important to acknowledge that for highly under- specified queries, it is hard for any model (even human) to reasonably estimate the query’s profile. For an example 3We have tested both GPT and Llama models as the profile query-profiler, and they yield similarly impressive results (§7). query “Compare current US Stock Market trends,” the query profile here does not provide enough information (e.g., how many years should the trend be derived from). To answer such highly under-specified queries, more information about the dataset will unlikely help.4 Moreover, we observed that extra information does not significantly improve the profiler’s estimates. For instance, in theory, it helps to know the embedding algorithm used by RAG. Yet, the embedding models perform similarly overall across queries and datasets under our consideration. This ex- plains their limited contribution to the profiler, though more future work is needed to understand the wider implications. 4.2 Mapping query profiles to RAG configurations After METIS obtains the query profile using the LLM, it per- forms rule-based mapping to generate values for RAG con-", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "43cbd0071ab281e42dc44526049de45b52c2f8e97c3b2d15842d061cdddffb4d"}
{"doc_id": "arxiv:2412.10543#method:part-2", "url": "https://arxiv.org/abs/2412.10543", "anchor": "#method:part-2", "type": "paper", "title": "", "section": "Method", "text": "significantly improve the profiler’s estimates. For instance, in theory, it helps to know the embedding algorithm used by RAG. Yet, the embedding models perform similarly overall across queries and datasets under our consideration. This ex- plains their limited contribution to the profiler, though more future work is needed to understand the wider implications. 4.2 Mapping query profiles to RAG configurations After METIS obtains the query profile using the LLM, it per- forms rule-based mapping to generate values for RAG con- figuration knobs (e.g., synthesis_method etc. introduced in §2). based on the query profiler’s outputs. How we map and why the profile helps: To understand the role of query profiles, consider the following examples: • “Who is the current CEO of NVIDIA?” This query is not complex and does not require joint reasoning. Due to the query being simple with no reasoning required and one piece of information (name of CEO). • “Which month had the highest NVIDIA’s stock price the six months from January to June 2024?” This query is simple but still needs to read information jointly, specifically six pieces of information (stock price for every month) • “What are the reasons for NVIDIA’s month-on-month stock price change from January to June 2024” This query is complex and needs to read multiple pieces of information jointly (stock prices, reasons for change etc.) As multiple reasons need to be analyzed here, summarizing all of the in- formation first helps narrow down to relevant information and perform clearer reasoning (why the prices changed). 4Maybe some chat history from the same user will help, but that is beyond the scope of this work. 6 Algorithm 1: Rule based mapping algorithm Input: Query complexity, Joint reasoning required Input: Pieces of information , Summarization length range Result: synthesis_method, num_chunks, intermediate_length 1 if Joint reasoning required == “no” then 2 synthesis_method = map_rerank 3 else 4 if Query complexity == “low” then 5 synthesis_method = stuff 6 else 7 synthesis_method = stuff, map_reduce 8 num_chunks = [Pieces of information , 3× Pieces of information] 9 intermediate_length_range = Summarization length range Algorithm 1 outlines the rule-based mapping process. This mapping is significantly helpful, it improves upon raw pro- filer outputs and converts them to usable RAG configurations. It reduces the cost of the profiler LLM by restricting it to provide short binary decisions only. We decide the range of synthesis_method selections based on two of the profile dimensions estimated in §4.1, i.e., the “Query complexity” and the “Joint reasoning require- ment”. Simple queries that don’t need any reasoning can an- swered with map_rerank while queries that require joint rea- soning need stuff or map_reduce. We then decide the range of values for num_chunks based on the profile dimension of the “Pieces of information required”, i.e., 𝑛—specifically, we set the range of num_chunks to be 1−3 times of 𝑛. We do not directly set num_chunks at𝑛, because it (1) gives some leeway for the retrieval logic (e.g., typically Bert-embedding-based)5 to find necessary information, and (2) provides the room for the scheduler to select the configuration that fits in available", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "f265b10e1abcb4c9a7843379ed75bc814f64bf8bb91b83c17d0688ba17448d1d"}
{"doc_id": "arxiv:2412.10543#method:part-3", "url": "https://arxiv.org/abs/2412.10543", "anchor": "#method:part-3", "type": "paper", "title": "", "section": "Method", "text": "rea- soning need stuff or map_reduce. We then decide the range of values for num_chunks based on the profile dimension of the “Pieces of information required”, i.e., 𝑛—specifically, we set the range of num_chunks to be 1−3 times of 𝑛. We do not directly set num_chunks at𝑛, because it (1) gives some leeway for the retrieval logic (e.g., typically Bert-embedding-based)5 to find necessary information, and (2) provides the room for the scheduler to select the configuration that fits in available memory. Finally, we get the intermediate_length range from the “summary length” estimate, which is already a value range (derived from the query, metadata and chunk size). Algorithm 1 is central to METIS’ design to reduce to the space to our useful RAG configurations and this is extendable to other RAG configurations. For instance, a particular RAG pipeline might use an external re-ranker [23, 52], query re- writer [36, 51] or perform an external web-search [73] along with database retrieval. The mapping algorithm can map the profiling LLM’s output (e.g., of Query complexity) and be used to guide such decisions for these newer RAG configurations. Additionally, such mapping algorithms greatly reduce the overall inference cost of RAG inference. Attempting to use 5A typical RAG retriever these days will have to retrieve 2-3× more chunks than minimally required to provide sufficient information for the LLM inference [24, 55]. Used GPU Mem (6GB) Used GPU mem (6GB) time time Map 1 (6GB) Map 2 (6GB) Reduce (6GB) Chunk 1, Query Chunk 2, Query Chunk 1, Chunk 2, Query Stuff (12GB) (a) Baseline Separates configuration selection and scheduling In general, \"Stuff\" is faster than \"MapReduce\" as a RAG config Yet, \"Stuff\" is memory-intensive and thus is slower when available GPU RAM is limited Free mem (6GB) (b) Ours performs configuration selection and scheduling jointly Delay saved We select MapReduce as it can readily fits in the current batch Figure 8. METIS joint schedules RAG configurations with available GPU memory (chosen example - map_reduce) the LLM profiler to directly provide the exact RAG configu- ration values does not work. For this, the LLM needs to be regularly retrained for this task to adapt to new configura- tions and will require significantly greater system resources (e.g., GPUs blocked for this). In contrast, METIS uses the LLM to only analyze natural language properties and provide bi- nary decisions, which the mapping algorithm translates to useful configurations with a significantly lower cost. It is important to note that the concept of METIS belongs to an active research trend in the ML and systems community that leverages LLM outputs and mapping functions to guide real system decisions and optimizations, an example of which is LLM routing [13, 31, 56, 59]. While current LLM routers use trained LLMs to map decisions from query complexity to only choose from families of inference models (outside the realm of RAG), we differ by mapping the output to the configuration knob we run for the RAG queries. Like these prior efforts, METIS is a heuristic to best utilize the LLM-generated information to guide system optimiza- tions.", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "c9ffdbe285143ef221b3746bf25e04ff45fb406a69526c91cfc8cdcedcec9d9d"}
{"doc_id": "arxiv:2412.10543#method:part-4", "url": "https://arxiv.org/abs/2412.10543", "anchor": "#method:part-4", "type": "paper", "title": "", "section": "Method", "text": "functions to guide real system decisions and optimizations, an example of which is LLM routing [13, 31, 56, 59]. While current LLM routers use trained LLMs to map decisions from query complexity to only choose from families of inference models (outside the realm of RAG), we differ by mapping the output to the configuration knob we run for the RAG queries. Like these prior efforts, METIS is a heuristic to best utilize the LLM-generated information to guide system optimiza- tions. While it demonstrates remarkable improvement in practice, more work will be needed to complement it for better interpretability and robustness. 4.3 Joint configuration-scheduling adaptation Once provided with the narrowed range of each RAG con- figuration knob (synthesis_method, num_chunks and intermediate_length), we need to choose a RAG configu- ration, which is aware of the current system resource (GPU memory). If we pick configurations which do not fit in cur- rent memory, it will lead to additional queuing delay waiting for the GPU memory to free up. We have METIS’s pruned configuration space where the quality is high, we now focus on choosing the best configu- ration which fits in memory, without focusing on quality. 7 Why we need to choose the scheduling jointly: We motivate the need for joint scheduling along with the RAG configuration choice in Figure 8. Consider a setup where we tune only one RAG configura- tion knob of synthesis_method. Other knobs num_chunks and intermediate_length are fixed at 20 and 100 respec- tively. Let’s assume both stuff and map_reduce are present in the pruned space. For the scheduling knob, we consider the amount of GPU memory available for the current batch. Consider a baseline system which separates the joint de- cision from the scheduling and picks only the RAG con- figuration knob (synthesis_method). It chooses the stuff configuration knob as it has lower compute requirement, so given enough memory it should be fast. The baseline system in Figure 8 (a) does not consider other jobs in the system and does not evaluate the amount of available resource to make its scheduling decision. Due to its long input length with 20 chunks, stuff turns out to be memory-intensive. If the available GPU memory is low, stuff doesn’t fit in memory and needs to be queued. This ends up with stuff being slow. Jointly considering the available GPU memory with choos- ing the RAG configuration knob avoids this pitfall. For exam- ple, in Figure 8 (b), if the original configuration was stuff, METIS can choose to use map_reduce (based on the current GPU memory available). By doing so, METIS can start putting the mappers which fit in memory, into the current running_batch of requests which fits in the GPU. While map_reduce requires more compute, in this case, it benefits from being able to start execution much faster, as some of the mappers fit in memory. METIS does not need to wait for the GPU memory to free up and changes the configuration aware of system resource, to save delay and achieve a better quality-delay tradeoff. Jointly choosing the configuration knobs: METIS first", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "b418ac55abf8a09f57ef90b9486cc62ef341adac4f029e987777f934d8ea9148"}
{"doc_id": "arxiv:2412.10543#method:part-5", "url": "https://arxiv.org/abs/2412.10543", "anchor": "#method:part-5", "type": "paper", "title": "", "section": "Method", "text": "the mappers which fit in memory, into the current running_batch of requests which fits in the GPU. While map_reduce requires more compute, in this case, it benefits from being able to start execution much faster, as some of the mappers fit in memory. METIS does not need to wait for the GPU memory to free up and changes the configuration aware of system resource, to save delay and achieve a better quality-delay tradeoff. Jointly choosing the configuration knobs: METIS first provides us with a pruned range of configurations. A straw- man solution is to pick a constant value from the across queries. (e.g., the median value of the num_chunks). While this is better than using one static configuration for all queries, it is still sub-optimal as it does not look at the current system resource availability. This prevents us from exploiting the best quality-delay tradeoff across RAG queries. We use a best-fit algorithm to allow for variation in config- urations across queries. We first compute the GPU memory requirement for the RAG query from the RAG configura- tion knobs (e.g., num_chunks) for every configuration in the pruned space. Then, we measure the current available mem- ory on the GPU to see what can fit into the current batch. We then pick the best configuration from the pruned space that fits into the GPU. METIS defines the best configuration as the one with overall highest memory requirement, from all which fit in memory. The insight here is that within the reduced range of good quality configurations, higher mem- ory configurations correspond to expensive configurations (e.g. more number of chunks, higher intermediate length). In general, these configurations should lead to slightly higher quality in the reduced space. For example, if the pruned space says num_chunks is 5-10 and the synthesis_method is stuff and both 5 or 6 chunks can fit in memory, we choose 6 chunks. We don’t pick a configuration that doesn’t fit in GPU, so we would never choose more than 6 chunks. If we do that, the system will queue the request inflating the delay. After choosing the configuration that fits into the current running_batch, the vLLM engine is optimized to perform chunked_prefill. However, even with chunked_prefill, it can only offload parts of long prefill of stuff requests which do not fit in the current batch and still inflates the queuing de- lay. Jointly scheduling RAG configurations enables efficient resource usage, which cannot be obtained by only relying on the output of the LLM profiler. What if none of the configurations fit in the GPU? A main insight for METIS’s design comes from the observation that in general, the RAG-specific focused configurations can be loosely-decoupled from the scheduling-specific configura- tions. METIS tries to fit the best possible configurations into GPU memory after it gets the profiler’s reduced configura- tion space. It can sometimes happen that the current GPU memory availability is too low and none of the profiler’s configurations fit in the currently available GPU. One way we handle this is by falling back to a cheaper fixed configuration and choosing", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "0467499c95c579e49bf99a02f274eeb8ef1eae536adf30d35ef5ee4cdab06f26"}
{"doc_id": "arxiv:2412.10543#method:part-6", "url": "https://arxiv.org/abs/2412.10543", "anchor": "#method:part-6", "type": "paper", "title": "", "section": "Method", "text": "comes from the observation that in general, the RAG-specific focused configurations can be loosely-decoupled from the scheduling-specific configura- tions. METIS tries to fit the best possible configurations into GPU memory after it gets the profiler’s reduced configura- tion space. It can sometimes happen that the current GPU memory availability is too low and none of the profiler’s configurations fit in the currently available GPU. One way we handle this is by falling back to a cheaper fixed configuration and choosing to ignore the output of the pruning. As we already have access to the query complexity profile and we can pick cheaper configurations, which would meet the requirement for the current query. For instance, if the query doesn’t require joint reason- ing, we can pick a map_rerank configuration with as many chunks that fit into the current GPU memory, ignoring the remaining pruned spaces. If joint reasoning is required, we pick a stuff or map_reduce configurations with the few chunks that fit into memory. We can choose which synthesis method to use once based on the exact memory availability. This allows loose-decoupling of the RAG configurations into a smaller space and then choosing configurations based on system resource availability. This also allows SLO-based constraints on RAG queries if certain queries have strict budgets on their generation latency. 5 Refinements to METIS In spite of it all, it is possible for the profiler to (sometimes) fail and in such cases, it is important to detect if METIS’s profiler fails on a query in a fast manner to prevent it from leading to bad RAG configurations. Also it is useful to decide how to provide feedback to METIS to improve. When is the quality profile reliable? METIS uses LLM to generate the quality profile. Inspired by recent work in use 8 Above threshold - 98% good profiles Above threshold - 96% good profiles 7% below threshold - 90% bad profiles 7% below threshold - 85% bad profiles 90% Threshold Figure 9. Confidence score threshold for different profiler outputs is used to decide when not to use the profiler output. of model confidence [20, 25, 84] as a quality metric, we use confidence scores for METIS’s LLM profiler as to measure the reliability of the profile provided. We obtain the confidence scores from the LLM’s log-probs values on the output (the logarithm of the confidence score, which is directly provided with the output with no extra overhead). We then threshold the confidence score using a confidence score threshold (90% across different datasets) to predict whether the quality profile derived from the quality profiler LLM is actually good (defined as whether the profile can lead to 10% increase in F1-score or 1.5 −2× reduction in delay or both) or not. Such 90% threshold can be tuned for better performance, and we leave it to future work. From Figure 9, we draw two conclusions. First, over 93% of the quality profiles derived from LLM are of high confidence (i.e., over 90%). Further, for those high-confidence profile, over 96% of them are good profiles, meaning that they can", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "a4c4c75ae3c41f03a22c0d711c7eae462cd3dc58039ef3019f9c174db36140e9"}
{"doc_id": "arxiv:2412.10543#method:part-7", "url": "https://arxiv.org/abs/2412.10543", "anchor": "#method:part-7", "type": "paper", "title": "", "section": "Method", "text": "actually good (defined as whether the profile can lead to 10% increase in F1-score or 1.5 −2× reduction in delay or both) or not. Such 90% threshold can be tuned for better performance, and we leave it to future work. From Figure 9, we draw two conclusions. First, over 93% of the quality profiles derived from LLM are of high confidence (i.e., over 90%). Further, for those high-confidence profile, over 96% of them are good profiles, meaning that they can be used to improve quality, or reduce latency, or both. To handle those cases where the quality profile is of con- fidence score lower than 90% , METIS will fall back to the pruned configuration space of recent 10 queries. How to improve the profiler over time? METIS improves the query profiler LLM by profiling extra feedback prompt to this LLM. We generate this feedback prompt by generating the most accurate output, which is obtained by performing inference on the most resource-demanding configuration (the map_reduce configuration with a large number of input chunks (30) and a high value of intermediate length (300 tokens)) and then ask the quality profiler LLM what config- uration it should choose based on the query and the most accurate answer to that query. The key insight is that, the most accurate answer to the query provides the quality profiler LLM extra knowledge and thus can be used to further improve its decision. To control the cost of generating feedback prompts, METIS only generates the feedback prompt once every 30 queries and we only keep the last four feedback prompts. The cost of METIS’ LLM quality profiler: For the profiler LLM, we use a larger LLM as compared to the serving LLM Dataset Task Type Input Output Squad Single hop QA 0.4K - 2K 5-10 Musique Multihop QA 1K - 5K 5-20 KG RAG FinSec Doc Level QA 4K - 10K 20-40 QMSUM Summarization QA 4K - 12K 20-60 Table 1. Input and output length (# of tokens) distributions of the RAG datasets used in our evaluation. (7B parameters). Using this has minimal cost, as METIS only runs it on the query itself and in METIS as the query is at least 100× shorter than the context. Using this approach, METIS still saves cost as opposed to using a large LLM for inference (as shown in Section 7). We also show that METIS can use different closed and open-source LLMs as the profiler LLM for pruning and can still provide impressive delay reduction without hurting the accuracy in Section 7. 6 Implementation We implement METIS in about 2K lines of code in Python on top of the state-of-the-art popular LLM serving engine vLLM [41]. For the profiler used for configuration space pruning, we define a class LLMProfiler inheriting OpenAI’s Chat Completion API [61] interface (to invoke GPT-4o) and HuggingaceAPI [81] (to invoke LLama-3.1-70B) as models to profile the queries. We use Cohere-embed-v3.0 [4] as a state-of-the-art em- bedding method. We construct a FAISS [16] index using the IndexFlatL2 interface and perform L2-distance similarity search with index.search(query_embedding, top_k) on", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "54d68378c9c3a0744ece7a9ce8eefe998f03513d2303f3b8431c99e59ecc5090"}
{"doc_id": "arxiv:2412.10543#method:part-8", "url": "https://arxiv.org/abs/2412.10543", "anchor": "#method:part-8", "type": "paper", "title": "", "section": "Method", "text": "lines of code in Python on top of the state-of-the-art popular LLM serving engine vLLM [41]. For the profiler used for configuration space pruning, we define a class LLMProfiler inheriting OpenAI’s Chat Completion API [61] interface (to invoke GPT-4o) and HuggingaceAPI [81] (to invoke LLama-3.1-70B) as models to profile the queries. We use Cohere-embed-v3.0 [4] as a state-of-the-art em- bedding method. We construct a FAISS [16] index using the IndexFlatL2 interface and perform L2-distance similarity search with index.search(query_embedding, top_k) on the chunk embeddings to retrieve for RAG inference. We use the LLMChain interface from Langchain [8] in order to build efficient implementations of multiple synthesis methods. Finally, we use PyTorch’s [5] library modules support to perform query-level memory profiling and measurement to implement the best-fit scheduling logic and request batching. Particularly, we use pynvml to construct get_free_memory() with its interfaces of nvmlDeviceGetHandleByIndex and nvmlDeviceGetMemoryInfo to measure the amount of GPU memory available. We measure the current num-seqs and num-batched-tokens within vLLM to calculate which con- figuration can be fit into the current batch, based on the GPU availability and the request’s memory requirement. 7", "source": "arxiv_pdf", "published": "", "tokens": 184, "sha256": "4e33ce4c2d31ea7919623137436843b03789184715abb20c523c90f2af1be49b"}
{"doc_id": "arxiv:2412.10543#evaluation:part-1", "url": "https://arxiv.org/abs/2412.10543", "anchor": "#evaluation:part-1", "type": "paper", "title": "", "section": "Evaluation", "text": "The key takeaways from the evaluation are • Lower delay : Across 4 task representative datasets for RAG QA, METIS achieves 1.64 −2.54× lower response delay compared to fixed configurations of comparable quality. • Higher throughput : METIS achieves 1.8 −4.5× higher throughput than RAG serving systems which use fixed configurations reaching similar quality. 9 • Negligible overhead : METIS’ profiler’s delay is negligible compared to the overall delay of the LLM’s RAG inference. 7.1 Setup Models and hardware: : We evaluate METIS on a popular model for LLM inference, specifically the fine-tuned version of Mistral-7B-v3. We also use Llama3.1-70B for additional experiments. All models are fine-tuned such that they can take long contexts (up to 32K and 128K respectively). We apply AWQ-model quantization both models. We use an NVIDIA A40 GPU server with 2 GPUs to benchmark our results. The server is equipped with 384GB of memory and two Intel(R) Xeon(R) Gold 6130 CPUs with Hyper-threading and Turbo Boost enabled by default. We use 1 GPU to serve Mistral-7B-v3 and 2 GPUs to serve Llama3.1-70B. Datasets: We use multiple RAG QA datasets with various query profiles, in order to have task-representative work- loads. Table 1 summarizes their input-output statistics. • Squad [66]: Squad is a single-hop reading comprehension dataset, consisting of questions on articles, where the an- swer to every question is a segment from the correspond- ing reading passage. • Musique [78]: Musique is a multihop QA dataset with reasoning-based questions. It is designated to test LLM’s reasoning ability where one reasoning step critically relies on information from another. • KG RAG FinSec [50]: KG RAG Finsec is part of a Knowledge Graph family of RAG datasets and focuses on financial do- main questions from Fortune 500 companies. This dataset contains quarterly financial reports and queries need to read information for multiple chunks for answering. • QMSUM [93]: QMSUM is a human-annotated query-based multi-domain meeting summarization benchmark designed to test LLM’s reasoning-based summarization capabilities. This dataset contains multiple meeting transcripts and queries to summarize relevant spans of meetings. We build a retrieval database database by splitting the queries’ contexts into fixed-sized chunks using Langchain [8] for the database, with Cohere embed-v3.0 [4] embeddings and FAISS [16] L2-distance similarity search in order to re- trieve relevant chunks for RAG inference. To simulate a real RAG workload, we create a mix of queries from each dataset, and send them to METIS using arrival rates that follow a Poisson distribution. We report the results per dataset. Quality Metric: We adopt the following standard metric to measure the generation quality. • F1-score is used to evaluate the METIS’s serving model’s generated response (defined in §2) It is the most widely adopted metric for evaluating RAG QA tasks [10, 69, 72] System Metrics: We adopt the following system metrics: • Delay is used to measure the generation response delay of the model for every RAG query. We choose this system metric similar to other RAG serving papers [44, 70, 76] • Dollar Cost is used to measure the lower cost of using METIS’s profiler as", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "13da1df748fe2324a50c0df6fca43f3bcb54efc5f5b589682246a768cacdf843"}
{"doc_id": "arxiv:2412.10543#evaluation:part-2", "url": "https://arxiv.org/abs/2412.10543", "anchor": "#evaluation:part-2", "type": "paper", "title": "", "section": "Evaluation", "text": "to evaluate the METIS’s serving model’s generated response (defined in §2) It is the most widely adopted metric for evaluating RAG QA tasks [10, 69, 72] System Metrics: We adopt the following system metrics: • Delay is used to measure the generation response delay of the model for every RAG query. We choose this system metric similar to other RAG serving papers [44, 70, 76] • Dollar Cost is used to measure the lower cost of using METIS’s profiler as compared to using larger serving mod- els with fixed configurations having the closest accuracy. Baselines: We compare METIS with the following baselines. • vLLM: We serve RAG with vLLM with multiple static con- figurations across different queries. • Parrot*: We implement Parrot’s [44] configuration-based batching. Parrot* does not adapt the configuration per query. We compare with Parrot* using fixed RAG configu- rations which achieve the closest quality to us. • AdaptiveRAG*: We implement AdaptiveRAG’s [32], query complexity-based RAG-configuration selection and choose the configuration which maximizes the F1-score, without considering the system resource cost. 7.2 Overall improvement Lower delay without sacrificing generation quality: Fig- ure 10 shows METIS achieves delay reduction 1.64 −2.54× over AdaptiveRAG* with no reduction in F1-score. Over us- ing fixed configurations of similar delay, served with both Parrot* and vLLM, METIS achieves 12 −18% higher F1-score. Higher throughput at lower delay: Figure 11 shows METIS achieves higher throughput compared to fixed config- uration baselines when they choose the fixed-config which achieves the closest quality. Compared to Parrot* and vLLM, METIS achieves 1.8 −4.5× times higher throughput. Understanding METIS’ improvement: METIS’s gains come from jointly selecting the configuration based on the available resource, along with performing scheduling. METIS achieves higher quality than the fixed-config baselines as it is adapts the RAG-configuration per query. It reduces delay by resource-aware scheduling, making it better than fixed configurations which achieve closest quality. METIS achieves higher throughput due to being able to adapt configurations based on resource availability as com- pared to the baselines. Both Parrot* and vLLM schedule fixed RAG-configurations and cannot benefit from delay achieved by adapting the configuration like METIS. Parrot* can im- prove the delay over using fixed configurations with vLLM by 1.4 −1.8× but cannot improve the quality. 7.3 Analyzing the gains from METIS Delay saving: Figure 12 shows the contribution of every component of METIS. We compare with vLLM’s fixed config- uration, which achieves the highest quality (blue bar). Using the profiler’s outputs and choosing the median value every time (orange bar), we achieve 1.4 −1.68× reduction in delay. Next, we see the effect of batching (like Parrot*), by choosing the median value configuration and batching, we achieve 1.1 −1.2× reduction in delay. Finally, METIS achieves even greater delay reduction by 1.45 −1.75× by adapting the con- figuration based on available GPU memory with batching. 10 2.41X faster 16% higher 2.24X faster 12% higher 1.64X faster 15% higher 2.54X faster 18% higher Figure 10. METIS achieves 1.64−2.54× lower delay compared to both best fixed configuration baselines and quality-optimized RAG configuration without sacrificing generation quality. 0 2 4 6", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "a3341a95dece57c34b5e9fe8212acbbcbaab9c8292d87c88aad6389428db4b44"}
{"doc_id": "arxiv:2412.10543#evaluation:part-3", "url": "https://arxiv.org/abs/2412.10543", "anchor": "#evaluation:part-3", "type": "paper", "title": "", "section": "Evaluation", "text": "by choosing the median value configuration and batching, we achieve 1.1 −1.2× reduction in delay. Finally, METIS achieves even greater delay reduction by 1.45 −1.75× by adapting the con- figuration based on available GPU memory with batching. 10 2.41X faster 16% higher 2.24X faster 12% higher 1.64X faster 15% higher 2.54X faster 18% higher Figure 10. METIS achieves 1.64−2.54× lower delay compared to both best fixed configuration baselines and quality-optimized RAG configuration without sacrificing generation quality. 0 2 4 6 8 2 4 6 Average Delay (s) Dataset : KG RAG FinSec 0 2 4 6 8 2 4 6 Dataset: Musique 0 2 4 6 8 1 2 3 Dataset: Squad 0 2 4 6 8 5 10 Datset: QMSUM Average Queries per Second METIS (w/ adapted RAG config and batching) Parrot * (w/ fixed RAG config) vLLM (w/ fixed RAG config) Figure 11. METIS achieves 1.8 −4.5× higher throughput (at 1.8 seconds) than baselines which use fixed configurations of closest (not higher) quality. 1.68x 1.2x 1.75x 1.4x 1.1x 1.45x Figure 12. Understanding the delay improvement in METIS Better Better Figure 13. Even with increasing the inference model size, fixed configurations have 2.38 −6.8× higher cost and lower quality compared to METIS. Cost saving: Figure 13 shows METIS (including its pro- filer) has significant lower dollar cost and higher F1-score, compared to choosing the best fixed configuration, with increasing model complexity. The cost of using a (LLama3- 70B) inference model with vLLM and a fixed configuration 6% increase 4% increase Figure 14. Improvement for METIS using feedback from the output helps improve the F1-score by 4 −6%. 0 2 4 6 0.3 0.4 0.5 F1 Score Dataset: Musique 0 3 6 9 0.3 0.4 0.5 Dataset: QMSUM Average Delay (s) METIS Parrot* AdaptiveRAG* vLLM Figure 15. METIS achieves lower delay by 2.1 −2.4× at the same quality even with a larger inference LLM. is higher by 2.38× times while also having a lower F1-score of 6.5% times across datasets. Even more powerful inference models like GPT-4o fail to achieve the same F1-score with fixed configurations but have a much higher cost of 6.8×. Profiler feedback-based improvement: In Figure 14 we show the effect of the golden-configuration-based feedback to the profiler in order to improve its output. We use a 350 11 vLLM (fixed config) vLLM (change num_chunks) vLLM (change num_chunks + synthesis_method) vLLM (change num_chunks + synthesis_method + intermediate_length) METIS (change num_chunks + synthesis_method + intermediate_length + scheduling) Figure 16. Breakdown analysis: By tuning more knobs in METIS, we can see better quality-delay tradeoffs. 0 1 2 3 0.4 0.6 F1 Score Dataset: KG RAG FinSec 0 0.3 0.6 0.9 0.42 0.50 0.58 Dataset: Squad Average Delay (s) METIS Parrot* AdaptiveRAG* vLLM Figure 17. METIS’ performance gains remain substantial even with a smaller, open-source LLM profiler. Dataset: QMSUM Dataset: KG RAG FinSec Dataset: Musique Dataset: Squad Figure 18. METIS’ profiler delay is at most 1/10th of end-to- end response delay across queries from all datasets. query sample for the QMSUM and KG RAG FinSec dataset as the workload. We see that", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "ca33962a19de3919fb9d9cffa1f61eafc15cfa62ce7dd0cd7b083a1226578b80"}
{"doc_id": "arxiv:2412.10543#evaluation:part-4", "url": "https://arxiv.org/abs/2412.10543", "anchor": "#evaluation:part-4", "type": "paper", "title": "", "section": "Evaluation", "text": "Score Dataset: KG RAG FinSec 0 0.3 0.6 0.9 0.42 0.50 0.58 Dataset: Squad Average Delay (s) METIS Parrot* AdaptiveRAG* vLLM Figure 17. METIS’ performance gains remain substantial even with a smaller, open-source LLM profiler. Dataset: QMSUM Dataset: KG RAG FinSec Dataset: Musique Dataset: Squad Figure 18. METIS’ profiler delay is at most 1/10th of end-to- end response delay across queries from all datasets. query sample for the QMSUM and KG RAG FinSec dataset as the workload. We see that with the feedback mechanism (blue line), the F1-score improves by 4 −6% as compared to not having feedback (red line) from the outputs of the golden configuration. We ensure that the feedback mechanism can- not result in the output of very expensive configurations, as METIS’ joint scheduler will not pick increasingly expensive configurations based on the GPU resource constraint. 7.4 Sensitivity analysis Changing the inference LLM: Figure 15 shows the out- come of changing the inference LLM to a larger LLM (Llama3.1- 70B) on the Musique and QMSUM datasets. Even with a more powerful LLM, METIS achieves 2.1 −2.4× lower delay than AdaptiveRAG* at a similar F1-score. The best fixed- configuration baselines such as Parrot* and vLLM have a lower F1-score of 7 −10%. In RAG, models mainly rely on the external context to answer the question instead of the model weights and we only get a 2% improvement in F1-score compared to the smaller inference models. Incrementally tuning knobs in METIS: In Figure 16, we show the benefit we the improvement we get by incremen- tally adding more knobs to METIS. We measure this for the QMSUM dataset with the original Mistral-7B-v3 model. We first only tune the num_chunks (red point). Progressively we tune the RAG-configuration knobs of synthesis_method and intermediate_length and scheduling. We achieve 5, 4, 3% higher F1-Score compared to vLLM. Finally, by adding the scheduling, 2.8× lower delay reduction in delay. Changing the profiler LLM: Figure 17 shows the effect of changing the LLM profiler from GPT-4o to a smaller Llama3.1-70B model. METIS with the new profiler, still achieves 1.4 −2.1× over AdaptiveRAG* with a similar F1-score. Static configurations of Parrot* and vLLM which achieve similar delay, METIS achieves 10 −14% higher F1-score. Changing the embedding algorithm: METIS picks a state- of-art retrieval algorithm Cohere-embed-v3.0 [4]. Using two other popular retrieval algorithms All-mpnet-base-v2 [67] and text-embedding-3-large-256 [18], the F1-score change is within 1%. The delay has no measurable difference as the retrieval is > 100× faster than LLM synthesis [6]. Delay overhead of METIS’s per-query profiling: We show the negligible delay overhead of using an LLM profiler within METIS. Figure 18 shows the fraction of METIS’ pro- filer of the total end-to-end delay. Using the profiler at most adds 0.1 fraction and in the average case only adds 0.03−0.06 fraction to the total delay across queries from all datasets. 8", "source": "arxiv_pdf", "published": "", "tokens": 474, "sha256": "a939e2012d34d87ac309466557f752ee1be1e304b90b7147d78fb64cc0f6addc"}
{"doc_id": "arxiv:2412.10543#related-work", "url": "https://arxiv.org/abs/2412.10543", "anchor": "#related-work", "type": "paper", "title": "", "section": "Related work", "text": "Systems for serving RAG: Several systems have been proposed for RAG [2, 17, 32, 34, 37, 40, 44, 54, 76, 87, 90] which focus on improving retrieval using complex, iterative retrieval algorithms or on serving model selection. METIS can work in conjunction with such systems as METIS focuses on optimizing quality and serving latency, independent of how the retrieval algorithm identifies chunks for retrieval. KV cache storage and retrieval: Storing and reusing KV cache across different requests have been commonly studied in recent work [2, 14, 22, 29, 33, 41, 46, 48, 49, 63, 75, 86, 92]. METIS can work alongside these systems, where instead of retrieving chunks, it can retrieve the KV Caches for generat- ing the output. In RAG, some additional optimizations are needed to combine KV Caches of different chunks that don’t share a common prefix. This is important as the trivial con- catenation of KV Caches loses important cross-attention and reasoning between chunks. These optimizations are enabled by KV Cache blending-based approaches [9, 26, 30, 38, 80, 85]. However RAG workloads have a large number of related contexts across queries and storing all the KV Cache is ex- tremely expensive. We do not measure the KV Cache reuse ratio across queries and leave it for future work. 12 Prefill-Decode Optimizations: Several systems have pro- posed optimizations to speed-up prefill and decode for LLMs by leveraging unique properties of each phase [3, 11, 35, 65, 74, 82, 94, 95]. Notable techniques include chunked-prefill which allows interleaving prefill and decode requests and dis- aggregated prefill which separates compute nodes for prefill and decode. All of these optimizations enable faster genera- tion speed but don’t focus on generation quality. METIS can be applied with such LLM serving systems optimizations. 9 Limitations METIS is currently designed to work with commonly de- ployed RAG pipelines. New research directions in RAG [17, 89] have developed further complex pipelines with more agents and stages for deep chain-of-thought RAG workloads. These pipelines improve on complex workloads but achieve similar performance on all the commonly used RAG QA workloads we consider [1]. We leave METIS’ design exten- sion to chain-of-thought pipelines to future work. 10", "source": "arxiv_pdf", "published": "", "tokens": 360, "sha256": "c380f8272437143b26e7b663f2125f95b79c14c412b1af0073a83a58bddab78a"}
{"doc_id": "arxiv:2412.10543#conclusion", "url": "https://arxiv.org/abs/2412.10543", "anchor": "#conclusion", "type": "paper", "title": "", "section": "Conclusion", "text": "This paper introduces METIS, the first system that focuses on optimizing the tradeoffs between response delay and generation quality in RAG, by by jointly scheduling RAG queries and adapting key configurations on a per-query basis. Evaluation on four datasets shows that METIS outperforms the state-of-the-art, reducing generation latency by 1.64 − 2.54× without compromising response quality.", "source": "arxiv_pdf", "published": "", "tokens": 56, "sha256": "bdc2d24ec1b0123b44e3245a244cf152936dee29fdb2ccddfe2687d6adb2b70f"}
{"doc_id": "blog:news.microsoft.com#body:part-1", "url": "https://news.microsoft.com/source/features/ai/ai-agents-what-they-are-and-how-theyll-change-the-way-we-work/", "anchor": "#body:part-1", "type": "blog", "title": "", "section": "Body", "text": "--- title: AI agents — what they are, and how they'll change the way we work - Source author: Wp-Block-Co-Authors-Plus-Coauthors Is-Layout-Flow url: https://news.microsoft.com/source/features/ai/ai-agents-what-they-are-and-how-theyll-change-the-way-we-work/ hostname: microsoft.com description: AI agents take the power of generative AI a step further by working alongside you or even on your behalf, and they can be built and used by anyone. sitename: Source date: 2024-11-21 --- AI agents — what they are, and how they’ll change the way we work It’s Monday morning, the caffeine hasn’t kicked in yet, and you have a busy day ahead: Maybe you have piles of returns or new shipping invoices to review, or you need to get the latest updates out to your field technicians or help employees get more efficient IT support. Now you can get help with all of this and more by simply asking an AI agent to take care of it — while you drink a second cup of coffee and focus on your team’s long-term strategy. An agent can tackle certain tasks with you or for you, from acting as a virtual project manager to handling more complex assignments like reconciling financial statements to close the books. Microsoft 365 Copilot is already a personal assistant that helps with everything from tedious daily duties to jumpstarting creative projects. Using it to interact with various agents brings a new world of possibilities for organizations to empower their employees, drive business and accomplish even more. Agents can operate around the clock to review and approve customer returns or go over shipping invoices to help businesses avoid costly supply-chain errors. They can reason over reams of product information to give field technicians step-by-step instructions or use context and memory to open and close tickets for an IT help desk. “Think of agents as the new apps for an AI-powered world,” says Jared Spataro, Microsoft’s chief marketing officer for AI at Work. “We’re rapidly adding new capabilities to tackle individuals’ biggest pain points at work and drive real business results.” What are agents, anyway? An agent takes the power of generative AI a step further, because instead of just assisting you, agents can work alongside you or even on your behalf. Agents can do a range of things, from responding to questions to more complicated or multistep assignments. What sets them apart from a personal assistant is that they can be tailored to have a particular expertise. For example, you could create an agent to know everything about your company’s product catalog so it can draft detailed responses to customer questions or automatically compile product details for an upcoming presentation. Other agents can do even more, acting on your behalf, like one that helps fulfill sales orders — freeing you up to focus on building new customer relationships. Having agents handle some of these routine needs can boost productivity across industries, from manufacturing and research to finance and retail, helping businesses save time and money. You can use ready-made agents in Microsoft 365 and Dynamics 365, or build custom agents to help with more specific needs in Copilot Studio. Imagine you’re a", "source": "blog:news.microsoft.com", "published": "", "authors": "", "tokens": 512, "sha256": "30cab09af14e484587feb10a255be4320cd8752e60dcfc4816dc64ecdbcae086"}
{"doc_id": "blog:news.microsoft.com#body:part-2", "url": "https://news.microsoft.com/source/features/ai/ai-agents-what-they-are-and-how-theyll-change-the-way-we-work/", "anchor": "#body:part-2", "type": "blog", "title": "", "section": "Body", "text": "can do even more, acting on your behalf, like one that helps fulfill sales orders — freeing you up to focus on building new customer relationships. Having agents handle some of these routine needs can boost productivity across industries, from manufacturing and research to finance and retail, helping businesses save time and money. You can use ready-made agents in Microsoft 365 and Dynamics 365, or build custom agents to help with more specific needs in Copilot Studio. Imagine you’re a salesperson with big quarterly goals to meet. Copilot acts as your personal assistant, drafting emails, recapping a meeting you missed and helping you design a polished sales presentation. Meanwhile, an agent specialized in sales lead generation works autonomously in the background to find new prospects you can follow up with later in the week. Copilot partners on daily tasks, and your purpose-built agent uses its customized skills to help you meet your end-of-quarter goals. Agents are not new. Microsoft has done extensive research in the area and even created a multi-agent library last year for developers around the world, work that helped shape what agents can do today. They’re getting more attention now because recent advances in large language models (LLMs) help anyone — even outside the developer community — communicate with AI. That agent-LLM duo makes AI tools more tangibly useful. “People expect AI to do things for them,” not to just generate language, says Ece Kamar, the managing director of Microsoft’s AI Frontiers Lab. “If you want to have a system that can really solve real world problems and help people, that system has to have a good understanding of the world we live in, and when something happens, that system has to perceive that change and take action accordingly.” Agents are like layers on top of the language models that observe and collect information, provide input to the model and together generate an action plan and communicate that to the user — or even act on their own, if permitted. So both agents and models are equally important pieces of the puzzle, as far as generative AI tools go. Agents will become more useful and able to have more autonomy with innovations in their three necessary elements: memory, entitlements and tools. Memory helps provide continuity so that each time you ask for something, it isn’t like starting from scratch. “To be autonomous you have to carry context through a bunch of actions, but the models are very disconnected and don’t have continuity the way we do, so every prompt is in a vacuum and it might pull the wrong memory out,” says Sam Schillace, Microsoft’s deputy chief technology officer. “It’s like you’re watching a stop-motion animation, one isolated frame after another, and your mind puts it into motion. The clay model doesn’t move on its own.” To build up the memory infrastructure to address this, Schillace and his team are working on a process of chunking and chaining. That’s essentially what it sounds like: They’re experimenting with dividing up interactions in bits that can be stored and linked together", "source": "blog:news.microsoft.com", "published": "", "authors": "", "tokens": 512, "sha256": "55af94d66f2d25283176fd0d8107ba56bb6bba4854c50349f48393badd196e78"}
{"doc_id": "blog:news.microsoft.com#body:part-3", "url": "https://news.microsoft.com/source/features/ai/ai-agents-what-they-are-and-how-theyll-change-the-way-we-work/", "anchor": "#body:part-3", "type": "blog", "title": "", "section": "Body", "text": "memory out,” says Sam Schillace, Microsoft’s deputy chief technology officer. “It’s like you’re watching a stop-motion animation, one isolated frame after another, and your mind puts it into motion. The clay model doesn’t move on its own.” To build up the memory infrastructure to address this, Schillace and his team are working on a process of chunking and chaining. That’s essentially what it sounds like: They’re experimenting with dividing up interactions in bits that can be stored and linked together by relevance for faster access, akin to a memory — like grouping conversations about a certain project so an agent can recall those details when you ask for a status update and not have to search through its entire database. The work with entitlements and tools is making sure agents have secure access to, or are entitled to, information they need in order to accomplish things for you, with your permission — like who your boss is, for example — and to the computer programs they need to take action on your behalf, like Teams and PowerPoint. How to use and build agents for work You can already create and publish agents in Microsoft 365 Copilot that can help you in your daily work as easily as you’d create a spreadsheet or presentation — no coding skills required. You don’t need to be a developer to build agents using Copilot Studio, either. Anyone can connect them to relevant business data such as emails, reports and customer management systems so they can perform tasks and provide insights. And you’ll soon be able to enlist new agents in Microsoft 365 to help with common workflows and tasks. Interpreter in Teams will provide real-time speech-to-speech translation during meetings, for example, and you can opt to have it simulate your voice. The Employee Self-Service Agent will simplify human resource and IT help desk-related tasks like helping workers resolve a laptop issue or find out if they’ve maxed out certain benefits, and it can connect to company systems for further customization in Copilot Studio. Microsoft Dynamics 365 will have agents as well for a range of common business workflows across sales, supply chain, finance and customer service functions. And every SharePoint site will soon come equipped with an agent tailored to your organization’s content that allows employees to quickly tap into these vast knowledge bases and find exactly what they need in seconds, whether it’s project details buried in a workback schedule or a summary of a recent product memo. Developers have even more options. With the new Azure AI Agent Service, you’ll be able to choose from small or large language models to orchestrate, develop and scale agent-powered apps to streamline and automate complex workflows like order processing and customer data synchronization. It provides a software development kit with tools for developing agents, allowing you to efficiently integrate agent capabilities using Visual Studio Code and GitHub. One type of model, OpenAI’s recently announced o1 series, can bring more advanced reasoning capabilities to agents, allowing them to take on more complicated tasks by breaking them down into", "source": "blog:news.microsoft.com", "published": "", "authors": "", "tokens": 512, "sha256": "47cf1e24f3279396150288276637e6812a40f10d772da2d02ae0470bc033254f"}
{"doc_id": "blog:news.microsoft.com#body:part-4", "url": "https://news.microsoft.com/source/features/ai/ai-agents-what-they-are-and-how-theyll-change-the-way-we-work/", "anchor": "#body:part-4", "type": "blog", "title": "", "section": "Body", "text": "from small or large language models to orchestrate, develop and scale agent-powered apps to streamline and automate complex workflows like order processing and customer data synchronization. It provides a software development kit with tools for developing agents, allowing you to efficiently integrate agent capabilities using Visual Studio Code and GitHub. One type of model, OpenAI’s recently announced o1 series, can bring more advanced reasoning capabilities to agents, allowing them to take on more complicated tasks by breaking them down into steps — like getting the information someone on an IT help desk would need to solve a problem, factoring in solutions they’ve tried and coming up with a plan. You can also use the power of agents in LinkedIn; the platform’s first agent can help recruiters with hiring. Assessing risk for autonomous action There are extra safety considerations with agents that can act autonomously, and Microsoft is focused on making sure agents only access what you want them to, says Sarah Bird, the company’s chief product officer of Responsible AI. “Agents certainly up the stakes from a responsible AI point of view,” Bird says. “So we have to have much, much lower error rates. And there’s many more nuanced ways in which something could be an error. This is the big challenge with agents.” But the same responsible AI foundational playbook for other AI applications can be used to assess and mitigate risk with agents, she says. The new Copilot Control System helps IT departments manage Copilot and agents with data access and governance, management and security controls, as well as measurement reports and tools to track adoption and business value. Many agents, like those created for Microsoft 365 and Dynamics 365, include “human in the loop” approvals, where people are required to take the final step of reviewing and sending an email the Sales Order Agent wrote, for example. And for agents developed in Copilot Studio, authors can review the records to see which actions the agent took and why. The key is to focus on testing and moderating to ensure accuracy, Bird says, and for organizations to choose the right starting point for their needs. “We will of course make progress by building on the foundation we already have, so we’re starting the journey from a strong place,” Bird says. Looking back — and into the future Technologists have long been excited by the idea of autonomous systems working side-by-side with people to help them, says Kamar, who has been working on AI agents since 2005 and even wrote her Ph.D. thesis on the topic in 2010. The hurdle was that “we lacked that general problem-solving power” on the back end, she says. With LLMs, “we finally have this missing component,” she says. “Now we can bring back a lot of the ideas from our decades of research.” Going forward, Kamar envisions a new ecosystem or marketplace of agents, sort of like how apps empower people to do more with their smartphones. Agents already have “the basic building blocks of what it takes to complete a task,” she says. “Like", "source": "blog:news.microsoft.com", "published": "", "authors": "", "tokens": 512, "sha256": "edd8c4a310eb39ea330003ad833ea2cdb9b4e5ba1fd775a6c0a4c3d1d751e61f"}
{"doc_id": "blog:news.microsoft.com#body:part-5", "url": "https://news.microsoft.com/source/features/ai/ai-agents-what-they-are-and-how-theyll-change-the-way-we-work/", "anchor": "#body:part-5", "type": "blog", "title": "", "section": "Body", "text": "that “we lacked that general problem-solving power” on the back end, she says. With LLMs, “we finally have this missing component,” she says. “Now we can bring back a lot of the ideas from our decades of research.” Going forward, Kamar envisions a new ecosystem or marketplace of agents, sort of like how apps empower people to do more with their smartphones. Agents already have “the basic building blocks of what it takes to complete a task,” she says. “Like observing, ‘I can see your meeting is taking longer; I should delay the next meeting.’” They’re getting more helpful as they gain autonomy through the innovations in memory and entitlements. They’re relieving pain points for employees by helping with things like expense reporting, project management and meeting facilitation. And they’re driving exponential impact for businesses by taking on duties like alerting supply chain managers to low inventory and then automatically reordering to help drive sales and keep customers satisfied. Agents matter because they “open up a whole set of opportunities for working with people for getting tasks done, and that’s what we expect from AI systems,” Kamar says. “AI agents are not only a way to get more value for people but are going to be a paradigm shift in terms of how work gets done.” And this is just the beginning. Copilot is set to evolve with new capabilities like Copilot Actions, designed to handle routine tasks that can bog down employees like summarizing emails missed during time off, compiling agenda items and generating monthly reports. More capabilities like these are coming over the next year to lift the weight of work for employees and teams. “Copilot will empower every employee to do their best work in less time, and focus on more meaningful tasks,” Spataro says. “And agents created in Copilot Studio will transform every business process, helping companies streamline operations, enhance collaboration and drive innovation at scale.” Illustrations by Michał Bednarski / Makeshift Studios Story published on November 19, 2024", "source": "blog:news.microsoft.com", "published": "", "authors": "", "tokens": 333, "sha256": "2007988b18e359b4183d1751542115ddfba19712caae0a2355729eea906ad7ec"}
{"doc_id": "blog:www.databricks.com#body:part-1", "url": "https://www.databricks.com/blog/llm-inference-performance-engineering-best-practices", "anchor": "#body:part-1", "type": "blog", "title": "", "section": "Body", "text": "--- title: LLM Inference Performance Engineering: Best Practices url: https://www.databricks.com/blog/llm-inference-performance-engineering-best-practices hostname: databricks.com description: Learn best practices for optimizing LLM inference performance on Databricks, enhancing the efficiency of your machine learning models. sitename: Databricks date: 2023-12-10 --- In this blog post, the MosaicML engineering team shares best practices for how to capitalize on popular open source large language models (LLMs) for production usage. We also provide guidelines for deploying inference services built around these models to help users in their selection of models and deployment hardware. We have worked with multiple PyTorch-based backends in production; these guidelines are drawn from our experience with FasterTransformers, vLLM, NVIDIA's soon-to-be-released TensorRT-LLM, and others. Understanding LLM Text Generation Large Language Models (LLMs) generate text in a two-step process: \"prefill\", where the tokens in the input prompt are processed in parallel, and \"decoding\", where text is generated one 'token' at a time in an autoregressive manner. Each generated token is appended to the input and fed back into the model to generate the next token. Generation stops when the LLM outputs a special stop token or when a user-defined condition is met (e.g., some maximum number of tokens has been generated). If you'd like more background on how LLMs use decoder blocks, check out this blog post. Tokens can be words or sub-words; the exact rules for splitting text into tokens vary from model to model. For instance, you can compare how Llama models tokenize text to how OpenAI models tokenize text. Although LLM inference providers often talk about performance in token-based metrics (e.g., tokens/second), these numbers are not always comparable across model types given these variations. For a concrete example, the team at Anyscale found that Llama 2 tokenization is 19% longer than ChatGPT tokenization (but still has a much lower overall cost). And researchers at HuggingFace also found that Llama 2 required ~20% more tokens to train over the same amount of text as GPT-4. Important Metrics for LLM Serving So, how exactly should we think about inference speed? Our team uses four key metrics for LLM serving: - Time To First Token (TTFT): How quickly users start seeing the model's output after entering their query. Low waiting times for a response are essential in real-time interactions, but less important in offline workloads. This metric is driven by the time required to process the prompt and then generate the first output token. - Time Per Output Token (TPOT): Time to generate an output token for each user that is querying our system. This metric corresponds with how each user will perceive the \"speed\" of the model. For example, a TPOT of 100 milliseconds/tok would be 10 tokens per second per user, or ~450 words per minute, which is faster than a typical person can read. - Latency: The overall time it takes for the model to generate the full response for a user. Overall response latency can be calculated using the previous two metrics: latency = (TTFT) + (TPOT) * (the number of tokens to be generated). - Throughput: The number of output tokens per second", "source": "blog:www.databricks.com", "published": "", "authors": "", "tokens": 512, "sha256": "4d36c51acb2f7b3440e4c10381c4004c8d72f9c91f21400d6ada1f6f409d44c2"}
{"doc_id": "blog:www.databricks.com#body:part-2", "url": "https://www.databricks.com/blog/llm-inference-performance-engineering-best-practices", "anchor": "#body:part-2", "type": "blog", "title": "", "section": "Body", "text": "For example, a TPOT of 100 milliseconds/tok would be 10 tokens per second per user, or ~450 words per minute, which is faster than a typical person can read. - Latency: The overall time it takes for the model to generate the full response for a user. Overall response latency can be calculated using the previous two metrics: latency = (TTFT) + (TPOT) * (the number of tokens to be generated). - Throughput: The number of output tokens per second an inference server can generate across all users and requests. Our goal? The fastest time to first token, the highest throughput, and the quickest time per output token. In other words, we want our models to generate text as fast as possible for as many users as we can support. Notably, there is a tradeoff between throughput and time per output token: if we process 16 user queries concurrently, we'll have higher throughput compared to running the queries sequentially, but we'll take longer to generate output tokens for each user. If you have overall inference latency targets, here are some useful heuristics for evaluating models: - Output length dominates overall response latency: For average latency, you can usually just take your expected/max output token length and multiply it by an overall average time per output token for the model. - Input length is not significant for performance but important for hardware requirements: The addition of 512 input tokens increases latency less than the production of 8 additional output tokens in the MPT models. However, the need to support long inputs can make models harder to serve. For example, we recommend using the A100-80GB (or newer) to serve MPT-7B with its maximum context length of 2048 tokens. - Overall latency scales sub-linearly with model size: On the same hardware, larger models are slower, but the speed ratio won't necessarily match the parameter count ratio. MPT-30B latency is ~2.5x that of MPT-7B latency. Llama2-70B latency is ~2x that of Llama2-13B latency. We are often asked by prospective customers to provide an average inference latency. We recommend that before you anchor yourself to specific latency targets (\"we need less than 20 ms per token\"), you should spend some time characterizing your expected input and desired output lengths. Challenges in LLM Inference Optimizing LLM inference benefits from general techniques such as: - Operator Fusion: Combining different adjacent operators together often results in better latency. - Quantization: Activations and weights are compressed to use a smaller number of bits. - Compression: Sparsity or Distillation. - Parallelization: Tensor parallelism across multiple devices or pipeline parallelism for larger models. Beyond these methods, there are many important Transformer-specific optimizations. A prime example of this is KV (key-value) caching. The Attention mechanism in decoder-only Transformer-based models is computationally inefficient. Each token attends to all previously seen tokens, and thus recomputes many of the same values as each new token is generated. For example, while generating the Nth token, the (N-1)th token attends to (N-2)th, (N-3)th … 1st tokens. Similarly, while generating (N+1)th token, attention for the Nth token again needs to", "source": "blog:www.databricks.com", "published": "", "authors": "", "tokens": 512, "sha256": "29629928a931ffb3e82519c274466351c37cfe4aeaf0e0ace8b65a978657b0d3"}
{"doc_id": "blog:www.databricks.com#body:part-3", "url": "https://www.databricks.com/blog/llm-inference-performance-engineering-best-practices", "anchor": "#body:part-3", "type": "blog", "title": "", "section": "Body", "text": "Beyond these methods, there are many important Transformer-specific optimizations. A prime example of this is KV (key-value) caching. The Attention mechanism in decoder-only Transformer-based models is computationally inefficient. Each token attends to all previously seen tokens, and thus recomputes many of the same values as each new token is generated. For example, while generating the Nth token, the (N-1)th token attends to (N-2)th, (N-3)th … 1st tokens. Similarly, while generating (N+1)th token, attention for the Nth token again needs to look at the (N-1)th, (N-2)th, (N-3)th, … 1st tokens. KV caching, i.e., saving of intermediate keys/values for the attention layers, is used to preserve those results for later reuse, avoiding repeated computation. Memory Bandwidth is Key Computations in LLMs are mainly dominated by matrix-matrix multiplication operations; these operations with small dimensions are typically memory-bandwidth-bound on most hardware. When generating tokens in an autoregressive manner, one of the activation matrix dimensions (defined by batch size and number of tokens in the sequence) is small at small batch sizes. Therefore, the speed is dependent on how quickly we can load model parameters from GPU memory to local caches/registers, rather than how quickly we can compute on loaded data. Available and achieved memory bandwidth in inference hardware is a better predictor of speed of token generation than their peak compute performance. Inference hardware utilization is very important in terms of serving costs. GPUs are expensive and we need them to do as much work as possible. Shared inference services promise to keep costs low by combining workloads from many users, filling in individual gaps and batching together overlapping requests. For large models like Llama2-70B, we only achieve good cost/performance at large batch sizes. Having an inference serving system that can operate at large batch sizes is critical for cost efficiency. However, a large batch means larger KV cache size, and that in turn increases the number of GPUs required to serve the model. There's a tug-of-war here and shared service operators need to make some cost trade-offs and implement systems optimizations. Model Bandwidth Utilization (MBU) How optimized is an LLM inference server? As briefly explained earlier, inference for LLMs at smaller batch sizes—especially at decode time—is bottlenecked on how quickly we can load model parameters from the device memory to the compute units. Memory bandwidth dictates how quickly the data movement happens. To measure the underlying hardware's utilization, we introduce a new metric called Model Bandwidth Utilization (MBU). MBU is defined as (achieved memory bandwidth) / (peak memory bandwidth) where achieved memory bandwidth is ((total model parameter size + KV cache size) / TPOT). For example, if a 7B parameter running with 16-bit precision has TPOT equal to 14ms, then it's moving 14GB of parameters in 14ms translating to 1TB/sec bandwidth usage. If the peak bandwidth of the machine is 2TB/sec, we are running at an MBU of 50%. For simplicity, this example ignores KV cache size, which is small for smaller batch sizes and shorter sequence lengths. MBU values close to 100% imply that the inference system is effectively utilizing the available memory bandwidth.", "source": "blog:www.databricks.com", "published": "", "authors": "", "tokens": 512, "sha256": "3fb17a287f431181d3ab682cf1bf162131fc7c5f81f1d3f159c9582e6bdbfcd4"}
{"doc_id": "blog:www.databricks.com#body:part-4", "url": "https://www.databricks.com/blog/llm-inference-performance-engineering-best-practices", "anchor": "#body:part-4", "type": "blog", "title": "", "section": "Body", "text": "example, if a 7B parameter running with 16-bit precision has TPOT equal to 14ms, then it's moving 14GB of parameters in 14ms translating to 1TB/sec bandwidth usage. If the peak bandwidth of the machine is 2TB/sec, we are running at an MBU of 50%. For simplicity, this example ignores KV cache size, which is small for smaller batch sizes and shorter sequence lengths. MBU values close to 100% imply that the inference system is effectively utilizing the available memory bandwidth. MBU is also useful to compare different inference systems (hardware + software) in a normalized manner. MBU is complementary to the Model Flops Utilization (MFU; introduced in the PaLM paper) metric which is important in compute-bound settings. Figure 1 shows a pictorial representation of MBU in a plot similar to a roofline plot. The solid sloped line of the orange-shaded region shows the maximum possible throughput if memory bandwidth is fully saturated at 100%. However, in reality for low batch sizes (white dot), the observed performance is lower than maximum – how much lower is a measure of the MBU. For large batch sizes (yellow region), the system is compute bound, and the achieved throughput as a fraction of the peak possible throughput is measured as the Model Flops Utilization (MFU). MBU and MFU determine how much more room is available to push the inference speed further on a given hardware setup. Figure 2 shows measured MBU for different degrees of tensor parallelism with our TensorRT-LLM-based inference server. Peak memory bandwidth utilization is attained when transferring large contiguous memory chunks. When smaller models like MPT-7B are distributed across multiple GPUs, we observe lower MBU as we are moving smaller memory chunks in each GPU. Figure 3 shows empirically observed MBU for different degrees of tensor parallelism and batch sizes on the NVIDIA H100 GPUs. MBU decreases as batch size increases. However, as we scale GPUs, the relative decrease in MBU is less significant. It is also worthy to note that picking hardware with greater memory bandwidth can boost performance with fewer GPUs. At batch size 1, we can achieve a higher MBU of 60% on 2xH100-80GBs as compared to 55% on 4xA100-40GB GPUs (Figure 2). Benchmarking Results Latency We have measured time to first token (TTFT) and time per output token (TPOT) across different degrees of tensor parallelism for MPT-7B and Llama2-70B models. As input prompts lengthen, time to generate the first token starts to consume a substantial portion of total latency. Tensor parallelizing across multiple GPUs helps reduce this latency. Unlike model training, scaling to more GPUs offers significant diminishing returns for inference latency. Eg. for Llama2-70B going from 4x to 8x GPUs only decreases latency by 0.7x at small batch sizes. One reason for this is that higher parallelism has lower MBU (as discussed earlier). Another reason is that tensor parallelism introduces communication overhead across a GPU node. | Time to first token (ms) | |||| |---|---|---|---|---| | Model | 1xA100-40GB | 2xA100-40GB | 4xA100-40GB | 8xA100-40GB | | MPT-7B | 46 (1x) | 34 (0.73x) | 26 (0.56x) |", "source": "blog:www.databricks.com", "published": "", "authors": "", "tokens": 512, "sha256": "594db46a3ae310e38c22fb057df6dfc94c9902b20d6164522e94e88b148c413f"}
{"doc_id": "blog:www.databricks.com#body:part-5", "url": "https://www.databricks.com/blog/llm-inference-performance-engineering-best-practices", "anchor": "#body:part-5", "type": "blog", "title": "", "section": "Body", "text": "for inference latency. Eg. for Llama2-70B going from 4x to 8x GPUs only decreases latency by 0.7x at small batch sizes. One reason for this is that higher parallelism has lower MBU (as discussed earlier). Another reason is that tensor parallelism introduces communication overhead across a GPU node. | Time to first token (ms) | |||| |---|---|---|---|---| | Model | 1xA100-40GB | 2xA100-40GB | 4xA100-40GB | 8xA100-40GB | | MPT-7B | 46 (1x) | 34 (0.73x) | 26 (0.56x) | - | | Llama2-70B | Doesn't fit | 154 (1x) | 114 (0.74x) | Table 1: Time to first token given input requests are 512 tokens length with batch size of 1. Larger models like Llama2 70B needs at least 4xA100-40B GPUs to fit in memory At larger batch sizes, higher tensor parallelism leads to a more significant relative decrease in token latency. Figure 4 shows how time per output token varies for MPT-7B. At batch size 1, going from 2x to 4x only reduces token latency by ~12%. At batch size 16, latency with 4x is 33% lower than with 2x. This goes in line with our earlier observation that the relative decrease in MBU is smaller at higher degrees of tensor parallelism for batch size 16 as compared to batch size 1. Figure 5 shows similar results for Llama2-70B, except the relative improvement between 4x and 8x is less pronounced. We also compare GPU scaling across two different hardware. Because H100-80GB has 2.15x GPU memory bandwidth as compared to A100-40GB, we can see that latency is 36% lower at batch size 1 and 52% lower at batch size 16 for 4x systems. Throughput We can trade off throughput and time per token by batching requests together. Grouping queries during GPU evaluation increases throughput compared to processing queries sequentially, but each query will take longer to complete (ignoring queueing effects). There are a few common techniques for batching inference requests: - Static batching: Client packs multiple prompts into requests and a response is returned after all sequences in the batch have been completed. Our inference servers support this but do not require it. - Dynamic batching: Prompts are batched together on the fly inside the server. Typically, this method performs worse than static batching but can get close to optimal if responses are short or of uniform length. Does not work well when requests have different parameters. - Continuous batching: The idea of batching requests together as they arrive was introduced in this excellent paper and is currently the SOTA method. Instead of waiting for all sequences in a batch to finish, it groups sequences together at the iteration level. It can achieve 10x-20x better throughput than dynamic batching. Continuous batching is usually the best approach for shared services, but there are situations where the other two might be better. In low-QPS environments, dynamic batching can outperform continuous batching. It is sometimes easier to implement low-level GPU optimizations in a simpler batching framework. For offline batch inference workloads, static batching can avoid significant overhead and achieve better throughput. Batch Size How", "source": "blog:www.databricks.com", "published": "", "authors": "", "tokens": 512, "sha256": "e16c710e4342f47d4d10657bfbe59717d69eb20e8eed87cd39beca06abe6ab6e"}
{"doc_id": "blog:www.databricks.com#body:part-6", "url": "https://www.databricks.com/blog/llm-inference-performance-engineering-best-practices", "anchor": "#body:part-6", "type": "blog", "title": "", "section": "Body", "text": "finish, it groups sequences together at the iteration level. It can achieve 10x-20x better throughput than dynamic batching. Continuous batching is usually the best approach for shared services, but there are situations where the other two might be better. In low-QPS environments, dynamic batching can outperform continuous batching. It is sometimes easier to implement low-level GPU optimizations in a simpler batching framework. For offline batch inference workloads, static batching can avoid significant overhead and achieve better throughput. Batch Size How well batching works is highly dependent on the request stream. But we can get an upper bound on its performance by benchmarking static batching with uniform requests. | Batch size | ||||||| |---|---|---|---|---|---|---|---| | Hardware | 1 | 4 | 8 | 16 | 32 | 64 | 128 | | 1 x A10 | 0.4 (1x) | 1.4 (3.5x) | 2.3 (6x) | 3.5 (9x) | OOM (Out of Memory) error | || | 2 x A10 | 0.8 | 2.5 | 4.0 | 7.0 | 8.0 | || | 1 x A100 | 0.9 (1x) | 3.2 (3.5x) | 5.3 (6x) | 8.0 (9x) | 10.5 (12x) | 12.5 (14x) | | | 2 x A100 | 1.3 | 3.0 | 5.5 | 9.5 | 14.5 | 17.0 | 22.0 | | 4 x A100 | 1.7 | 6.2 | 11.5 | 18.0 | 25.0 | 33.0 | 36.5 | Table 2: Peak MPT-7B throughput (req/sec) with static batching and a FasterTransformers-based backend. Requests: 512 input and 64 output tokens. For larger inputs, the OOM boundary will be at smaller batch sizes. Latency Trade-Off Request latency increases with batch size. With one NVIDIA A100 GPU, for example, if we maximize throughput with a batch size of 64, latency increases by 4x while throughput increases by 14x. Shared inference services typically pick a balanced batch size. Users hosting their own models should decide the appropriate latency/throughput trade-off for their applications. In some applications, like chatbots, low latency for fast responses is the top priority. In other applications, like batched processing of unstructured PDFs, we might want to sacrifice the latency to process an individual document to process all of them fast in parallel. Figure 7 shows the throughput vs latency curve for the 7B model. Each line on this curve is obtained by increasing the batch size from 1 to 256. This is useful in determining how large we can make the batch size, subject to different latency constraints. Recalling our roofline plot above, we find that these measurements are consistent with what we would expect. After a certain batch size, i.e., when we cross to the compute bound regime, every doubling of batch size just increases the latency without increasing throughput. When using parallelism, it's important to understand low-level hardware details. For instance, not all 8xA100 instances are the same across different clouds. Some servers have high bandwidth connections between all GPUs, others pair GPUs and have lower bandwidth connections between pairs. This could introduce bottlenecks, causing real-world performance to deviate significantly from the curves above. Optimization Case Study: Quantization", "source": "blog:www.databricks.com", "published": "", "authors": "", "tokens": 512, "sha256": "dee0375f7b7419532d3a283e07390da86e87f854d6d832b6f264697768521c56"}
{"doc_id": "blog:www.databricks.com#body:part-7", "url": "https://www.databricks.com/blog/llm-inference-performance-engineering-best-practices", "anchor": "#body:part-7", "type": "blog", "title": "", "section": "Body", "text": "i.e., when we cross to the compute bound regime, every doubling of batch size just increases the latency without increasing throughput. When using parallelism, it's important to understand low-level hardware details. For instance, not all 8xA100 instances are the same across different clouds. Some servers have high bandwidth connections between all GPUs, others pair GPUs and have lower bandwidth connections between pairs. This could introduce bottlenecks, causing real-world performance to deviate significantly from the curves above. Optimization Case Study: Quantization Quantization is a common technique used to reduce the hardware requirements for LLM inference. Reducing the precision of model weights and activations during inference can dramatically reduce hardware requirements. For instance, switching from 16-bit weights to 8-bit weights can halve the number of required GPUs in memory constrained environments (eg. Llama2-70B on A100s). Dropping down to 4-bit weights makes it possible to run inference on consumer hardware (eg. Llama2-70B on Macbooks). In our experience, quantization should be implemented with caution. Naive quantization techniques can lead to a substantial degradation in model quality. The impact of quantization also varies across model architectures (eg. MPT vs Llama) and sizes. We will explore this in more detail in a future blog post. When experimenting with techniques like quantization, we recommend using an LLM quality benchmark like the Mosaic Eval Gauntlet to evaluate the quality of the inference system, not just the quality of the model in isolation. Additionally, it's important to explore deeper systems optimizations. In particular, quantization can make KV caches much more efficient. As mentioned previously, in autoregressive token generation, past Key/Values (KV) from the attention layers are cached instead of recomputing them at every step. The size of the KV cache varies based on the number of sequences processed at a time and the length of these sequences. Moreover, during each iteration of the next token generation, new KV items are added to the existing cache making it bigger as new tokens are generated. Therefore, effective KV cache memory management when adding these new values is critical for good inference performance. Llama2 models use a variant of attention called Grouped Query Attention (GQA). Please note that when the number of KV heads is 1, GQA is the same as Multi-Query-Attention (MQA). GQA helps with keeping the KV cache size down by sharing Keys/Values. The formula to calculate KV cache size is batch_size * seqlen * (d_model/n_heads) * n_layers * 2 (K and V) * 2 (bytes per Float16) * n_kv_heads Table 3 shows GQA KV cache size calculated at different batch sizes at a sequence length of 1024 tokens. The parameter size for Llama2 models, in comparison, is 140 GB (Float16) for the 70B model. Quantization of KV cache is another technique (in addition to GQA/MQA) to reduce the size of KV cache, and we are actively evaluating its impact on the generation quality. | Batch Size | GQA KV cache memory (FP16) | GQA KV cache memory (Int8) | |---|---|---| | 1 | .312 GiB | .156 GiB | | 16 | 5 GiB | 2.5 GiB | | 32 |", "source": "blog:www.databricks.com", "published": "", "authors": "", "tokens": 512, "sha256": "0e423ac1739c45f2c4799aec6224fa0e086cd28a66e9816b0b06582df22137cd"}
{"doc_id": "blog:www.databricks.com#body:part-8", "url": "https://www.databricks.com/blog/llm-inference-performance-engineering-best-practices", "anchor": "#body:part-8", "type": "blog", "title": "", "section": "Body", "text": "for Llama2 models, in comparison, is 140 GB (Float16) for the 70B model. Quantization of KV cache is another technique (in addition to GQA/MQA) to reduce the size of KV cache, and we are actively evaluating its impact on the generation quality. | Batch Size | GQA KV cache memory (FP16) | GQA KV cache memory (Int8) | |---|---|---| | 1 | .312 GiB | .156 GiB | | 16 | 5 GiB | 2.5 GiB | | 32 | 10 GiB | 5 GiB | | 64 | 20 GiB | 10 GiB | Table 3: KV cache size for Llama-2-70B at a sequence length of 1024 As mentioned previously, token generation with LLMs at low batch sizes is a GPU memory bandwidth-bound problem, i.e. the speed of generation depends on how quickly model parameters can be moved from the GPU memory to on-chip caches. Converting model weights from FP16 (2 bytes) to INT8 (1 byte) or INT4 (0.5 byte) requires moving less data and thus speeds up token generation. However, quantization may negatively impact the model generation quality. We are currently evaluating the impact on model quality using Model Gauntlet and plan to publish a followup blog post on it soon. Conclusions and Key Results Each of the factors we've outlined above influences the way we build and deploy models. We use these results to make data-driven decisions that take into consideration the hardware type, the software stack, the model architecture, and typical usage patterns. Here are some recommendations drawn from our experience. Identify your optimization target: Do you care about interactive performance? Maximizing throughput? Minimizing cost? There are predictable trade-offs here. Pay attention to the components of latency: For interactive applications time-to-first-token drives how responsive your service will feel and time-per-output-token determines how fast it will feel. Memory bandwidth is key: Generating the first token is typically compute-bound, while subsequent decoding is memory-bound operation. Because LLM inference often operates in memory-bound settings, MBU is a useful metric to optimize for and can be used to compare the efficiency of inference systems. Batching is critical: Processing multiple requests concurrently is critical for achieving high throughput and for effectively utilizing expensive GPUs. For shared online services continuous batching is indispensable, whereas offline batch inference workloads can achieve high throughput with simpler batching techniques. In depth optimizations: Standard inference optimization techniques are important (eg. operator fusion, weight quantization) for LLMs but it's important to explore deeper systems optimizations, especially those which improve memory utilization. One example is KV cache quantization. Hardware configurations: The model type and expected workload should be used to decide deployment hardware. For instance, when scaling to multiple GPUs MBU falls much more rapidly for smaller models, such as MPT-7B, than it does for larger models, such as Llama2-70B. Performance also tends to scale sub-linearly with higher degrees of tensor parallelism. That said, a high degree of tensor parallelism might still make sense for smaller models if traffic is high or if users are willing to pay a premium for extra low latency. Data Driven Decisions: Understanding the theory", "source": "blog:www.databricks.com", "published": "", "authors": "", "tokens": 512, "sha256": "980eda31f106d849fd8da6eb60da2e2bfe69456a55882de1440e1024f1e1a017"}
{"doc_id": "blog:www.databricks.com#body:part-9", "url": "https://www.databricks.com/blog/llm-inference-performance-engineering-best-practices", "anchor": "#body:part-9", "type": "blog", "title": "", "section": "Body", "text": "decide deployment hardware. For instance, when scaling to multiple GPUs MBU falls much more rapidly for smaller models, such as MPT-7B, than it does for larger models, such as Llama2-70B. Performance also tends to scale sub-linearly with higher degrees of tensor parallelism. That said, a high degree of tensor parallelism might still make sense for smaller models if traffic is high or if users are willing to pay a premium for extra low latency. Data Driven Decisions: Understanding the theory is important, but we recommend always measuring end-to-end server performance. There are many reasons an inference deployment can perform worse than expected. MBU could be unexpectedly low because of software inefficiencies. Or differences in hardware between cloud providers could lead to surprises (we have observed a 2x latency difference between 8xA100 servers from two cloud providers). To get started with LLM inference, try out Databricks Model Serving. Check out the documentation to learn more.", "source": "blog:www.databricks.com", "published": "", "authors": "", "tokens": 154, "sha256": "a458875070ded0e014d53d10a324231a08db255b72ef16428d010c6c10426683"}
{"doc_id": "blog:medium.com#body:part-1", "url": "https://medium.com/@williamwarley/understanding-ai-agents-how-they-work-types-and-practical-applications-bd261845f7c3", "anchor": "#body:part-1", "type": "blog", "title": "", "section": "Body", "text": "--- title: Understanding AI Agents: How They Work, Types, and Practical Applications author: Warley's CatOps url: https://medium.com/@williamwarley/understanding-ai-agents-how-they-work-types-and-practical-applications-bd261845f7c3 hostname: medium.com description: Introduction to AI Agents sitename: Medium date: 2024-06-11 --- Understanding AI Agents: How They Work, Types, and Practical Applications Introduction to AI Agents Definition and Importance AI Agents are autonomous entities that use artificial intelligence (AI) to perceive their environment, make decisions, and perform actions to achieve specific goals. These agents can operate independently or interact with other agents and systems to accomplish tasks. AI agents are designed to simulate human-like intelligence, enabling them to solve complex problems, adapt to changing conditions, and learn from experiences. Key Characteristics of AI Agents: - Autonomy: Operate without human intervention, making decisions and taking actions based on their programming and learned experiences. - Perception: Use sensors or input mechanisms to perceive their environment, gather data, and understand the context in which they operate. - Decision-Making: Apply reasoning and decision-making processes to choose the best course of action based on their goals and current state. - Learning: Improve their performance over time by learning from past experiences, adapting to new situations, and optimizing their strategies. Historical Background and Evolution The concept of AI agents has evolved significantly since its inception, influenced by advancements in computer science, robotics, and cognitive science. Here’s a brief overview of the historical development: 1950s-1960s: The early days of AI research focused on creating machines that could mimic human thought processes. Pioneering work by researchers like Alan Turing and John McCarthy laid the foundation for AI, introducing concepts such as the Turing Test and symbolic AI. 1970s-1980s: The development of expert systems marked a significant milestone in AI. These systems used rule-based logic to emulate the decision-making abilities of human experts in specific domains. However, their lack of learning capabilities and rigidity limited their adaptability. 1990s: The emergence of machine learning (ML) and neural networks revolutionized AI. Agents could now learn from data and experiences, improving their performance over time. Reinforcement learning (RL) also gained prominence, enabling agents to learn optimal strategies through trial and error. 2000s: The advent of big data and increased computational power further accelerated AI development. AI agents became more sophisticated, capable of handling complex tasks such as natural language processing (NLP), computer vision, and autonomous navigation. 2010s-Present: Deep learning, a subset of ML, has driven significant advancements in AI agents. Techniques such as convolutional neural networks (CNNs) and recurrent neural networks (RNNs) have enabled agents to achieve state-of-the-art performance in various domains. Additionally, the integration of AI agents with the Internet of Things (IoT) and cloud computing has expanded their capabilities and applications even further. Importance of AI Agents AI agents are crucial in the modern technological landscape due to their ability to automate complex tasks, make informed decisions, and continuously improve through learning. Their importance can be seen in several key areas: 1. Automation: AI agents can automate repetitive and time-consuming tasks, freeing up human resources for more strategic and creative work. This increases efficiency and productivity across various industries. 2. Scalability: AI agents can handle", "source": "blog:medium.com", "published": "", "authors": "", "tokens": 512, "sha256": "fc39282c59b0e70684197dc155633a75b8c5ee2084f795f95fb31a4a96d9befa"}
{"doc_id": "blog:medium.com#body:part-2", "url": "https://medium.com/@williamwarley/understanding-ai-agents-how-they-work-types-and-practical-applications-bd261845f7c3", "anchor": "#body:part-2", "type": "blog", "title": "", "section": "Body", "text": "has expanded their capabilities and applications even further. Importance of AI Agents AI agents are crucial in the modern technological landscape due to their ability to automate complex tasks, make informed decisions, and continuously improve through learning. Their importance can be seen in several key areas: 1. Automation: AI agents can automate repetitive and time-consuming tasks, freeing up human resources for more strategic and creative work. This increases efficiency and productivity across various industries. 2. Scalability: AI agents can handle large-scale operations and processes, making them ideal for applications that require processing vast amounts of data and performing tasks simultaneously. 3. Real-Time Decision Making: AI agents can process information and make decisions in real-time, which is crucial for applications like autonomous driving, financial trading, and real-time customer support. 4. Adaptability: Through machine learning and reinforcement learning, AI agents can adapt to new environments and situations, improving their performance and decision-making capabilities over time. 5. Personalization: AI agents can analyze individual user behavior and preferences to provide personalized experiences in applications such as recommendation systems, personal assistants, and targeted marketing. Evolution of AI Agents The evolution of AI agents can be traced through several key developments and milestones: 1. Early AI and Expert Systems: Initial AI research focused on rule-based systems and symbolic reasoning. Expert systems, which were designed to mimic the decision-making abilities of human experts, were among the first AI agents. However, their lack of learning capabilities and flexibility limited their effectiveness. 2. Machine Learning and Neural Networks: The introduction of machine learning algorithms allowed AI agents to learn from data rather than relying solely on predefined rules. Neural networks, inspired by the human brain, enabled agents to recognize patterns and make predictions, leading to significant improvements in tasks such as image and speech recognition. 3. Reinforcement Learning: Reinforcement learning (RL) provided a framework for AI agents to learn optimal behaviors through trial and error. Agents receive feedback in the form of rewards or penalties, allowing them to refine their strategies and actions. This approach has been particularly successful in applications like game playing and robotics. 4. Deep Learning: Deep learning, a subset of machine learning, involves training large neural networks with many layers. This has led to breakthroughs in natural language processing, computer vision, and other complex tasks. AI agents powered by deep learning can achieve state-of-the-art performance in various domains. 5. Integration with IoT and Cloud Computing: The integration of AI agents with the Internet of Things (IoT) and cloud computing has expanded their capabilities. AI agents can now leverage vast amounts of data collected from IoT devices and process it in real-time using cloud-based infrastructure. This integration has enabled applications such as smart homes, industrial automation, and predictive maintenance. Conclusion AI agents represent a significant advancement in artificial intelligence, offering the ability to automate complex tasks, make real-time decisions, and continuously improve through learning. Their evolution from early rule-based systems to sophisticated deep learning models has expanded their capabilities and applications across various industries. As we continue to innovate and integrate AI agents with emerging technologies, their impact", "source": "blog:medium.com", "published": "", "authors": "", "tokens": 512, "sha256": "18fc581f7317d47f7beaa2d7f5e99bfdf7c5019a38c2d4f9c4c38315d7ec8585"}
{"doc_id": "blog:medium.com#body:part-3", "url": "https://medium.com/@williamwarley/understanding-ai-agents-how-they-work-types-and-practical-applications-bd261845f7c3", "anchor": "#body:part-3", "type": "blog", "title": "", "section": "Body", "text": "process it in real-time using cloud-based infrastructure. This integration has enabled applications such as smart homes, industrial automation, and predictive maintenance. Conclusion AI agents represent a significant advancement in artificial intelligence, offering the ability to automate complex tasks, make real-time decisions, and continuously improve through learning. Their evolution from early rule-based systems to sophisticated deep learning models has expanded their capabilities and applications across various industries. As we continue to innovate and integrate AI agents with emerging technologies, their impact on our daily lives and the broader technological landscape will only grow. This introduction provides a comprehensive overview of AI agents, highlighting their definition, importance, historical evolution, and key advancements. How AI Agents Work To understand how AI agents work, it is essential to delve into their core concepts, components, and learning mechanisms. This chapter provides a detailed explanation of these elements to illustrate the functioning of AI agents. Core Concepts and Components 1. Perception: — AI agents use sensors or input mechanisms to perceive their environment. This can involve collecting data from various sources such as cameras, microphones, or other sensors. — Example: In autonomous vehicles, sensors like LIDAR, cameras, and radar gather information about the vehicle’s surroundings. 2. Reasoning: — After perceiving the environment, the agent processes the information to make informed decisions. This involves reasoning and applying logical rules or learned knowledge to interpret the data. — Example: A recommendation system analyzes user preferences and behaviors to suggest relevant products. 3. Action: — Based on its reasoning, the AI agent takes appropriate actions to achieve its goals. This can involve physical actions (e.g., a robot moving objects) or digital actions (e.g., sending an email). — Example: A robotic vacuum cleaner navigates a room to clean it efficiently. 4. Learning: — AI agents improve their performance over time by learning from experiences. This can involve supervised learning, unsupervised learning, or reinforcement learning, depending on the task and data available. — Example: A chatbot learns to provide better responses by analyzing previous interactions with users. Types of AI Agents 1. Simple Reflex Agents: — Operate based on a set of predefined rules and respond directly to specific stimuli from the environment. — Example: A thermostat that adjusts the temperature based on the current room temperature. 2. Model-Based Reflex Agents: — Maintain an internal model of the world to keep track of unobservable aspects of the environment, allowing for more informed decision-making. — Example: A navigation system that uses a map to plan routes and update the user’s location. 3. Goal-Based Agents: — Use goals to guide their actions, making decisions based on the desirability of outcomes and the likelihood of achieving their objectives. — Example: An AI planning system that schedules tasks to maximize efficiency and meet deadlines. 4. Utility-Based Agents: — Evaluate the utility of different actions to choose the one that maximizes overall satisfaction or performance, considering both goals and preferences. — Example: An autonomous trading system that selects trades to maximize profit while minimizing risk. 5. Learning Agents: — Continuously learn from their environment and experiences to improve", "source": "blog:medium.com", "published": "", "authors": "", "tokens": 512, "sha256": "f16b20c950fcacafa5c48a7ea7d75b2babc3cdf87709c4e4eefd2384f0e16e24"}
{"doc_id": "blog:medium.com#body:part-4", "url": "https://medium.com/@williamwarley/understanding-ai-agents-how-they-work-types-and-practical-applications-bd261845f7c3", "anchor": "#body:part-4", "type": "blog", "title": "", "section": "Body", "text": "on the desirability of outcomes and the likelihood of achieving their objectives. — Example: An AI planning system that schedules tasks to maximize efficiency and meet deadlines. 4. Utility-Based Agents: — Evaluate the utility of different actions to choose the one that maximizes overall satisfaction or performance, considering both goals and preferences. — Example: An autonomous trading system that selects trades to maximize profit while minimizing risk. 5. Learning Agents: — Continuously learn from their environment and experiences to improve their performance over time, adapting to new situations and optimizing their behavior. — Example: A recommendation engine that refines its suggestions based on user feedback and interactions. Learning Mechanisms 1. Supervised Learning: — Involves training an agent using labeled data, where the correct output is provided for each input example. The agent learns to map inputs to outputs by minimizing prediction errors. — Example: Training an image recognition model to classify images of cats and dogs using labeled datasets. 2. Unsupervised Learning: — Involves training an agent using unlabeled data, where the agent identifies patterns and structures in the data without explicit instructions. Techniques like clustering and dimensionality reduction are common. — Example: Grouping similar customer profiles for targeted marketing campaigns. 3. Reinforcement Learning (RL): — Involves training an agent to make sequences of decisions by rewarding desirable behaviors and penalizing undesirable ones. The agent learns to maximize cumulative rewards over time. — Example: Training a game-playing AI to learn optimal strategies by receiving points for winning and penalties for losing. Practical Implementation Implementing AI agents involves several practical steps, including data collection, model training, and deployment. Here’s a high-level overview: 1. Data Collection and Preprocessing: — Gather relevant data from sensors or databases, preprocess it to remove noise, and structure it for analysis. — Example: Collecting and cleaning data from sensors for an autonomous robot. 2. Model Training: — Train the agent using appropriate learning algorithms and techniques, such as neural networks, decision trees, or RL algorithms. — Example: Training a neural network to recognize objects in images. 3. Deployment: — Deploy the trained agent in the target environment, ensuring it can interact with other systems and perform its tasks effectively. — Example: Deploying a chatbot on a company’s customer service platform. 4. Monitoring and Maintenance: — Continuously monitor the agent’s performance, update it with new data, and retrain as necessary to maintain its effectiveness. — Example: Regularly updating a recommendation engine with new user data to improve suggestions. This chapter provides a detailed explanation of how AI agents work, covering their core concepts, types, learning mechanisms, and practical implementation. Types of AI Agents AI agents come in various types, each designed to handle specific tasks and environments. Understanding these types helps in selecting the right kind of agent for a particular application. This chapter explores the different types of AI agents, their characteristics, and use cases. 1. Simple Reflex Agents Overview: - Simple reflex agents act solely based on the current percept and predefined rules, without considering the history of past perceptions. - They are the simplest form of agents", "source": "blog:medium.com", "published": "", "authors": "", "tokens": 512, "sha256": "1291049a131f623687635c519aabfe7de0974ee1fc340caee5f8cfd08a6d0715"}
{"doc_id": "blog:medium.com#body:part-5", "url": "https://medium.com/@williamwarley/understanding-ai-agents-how-they-work-types-and-practical-applications-bd261845f7c3", "anchor": "#body:part-5", "type": "blog", "title": "", "section": "Body", "text": "of AI Agents AI agents come in various types, each designed to handle specific tasks and environments. Understanding these types helps in selecting the right kind of agent for a particular application. This chapter explores the different types of AI agents, their characteristics, and use cases. 1. Simple Reflex Agents Overview: - Simple reflex agents act solely based on the current percept and predefined rules, without considering the history of past perceptions. - They are the simplest form of agents and work well in environments that are fully observable and deterministic. How They Work: - These agents use condition-action rules (if-then statements) to decide on actions. - They do not maintain any internal state or model of the environment. Example: - A thermostat that turns the heating on or off based on the current temperature reading. - Use Case: Simple household appliances and basic automated systems. Advantages: - Easy to design and implement. - Efficient in predictable environments. Disadvantages: - Limited functionality in complex or partially observable environments. - Cannot learn or adapt to changes in the environment. 2. Model-Based Reflex Agents Overview: - Model-based reflex agents maintain an internal model of the environment, allowing them to handle partially observable environments better than simple reflex agents. - They can consider the history of past perceptions to make more informed decisions. How They Work: - These agents update their internal model based on incoming percepts and use this model to infer unseen aspects of the environment. - They use condition-action rules, but these rules can reference the internal model. Example: - A navigation system that uses a map to plan routes and update the user’s location. - Use Case: GPS navigation, industrial automation systems. Advantages: - Can handle partially observable environments. - More flexible and capable than simple reflex agents. Disadvantages: - More complex to design and implement. - Requires more computational resources to maintain and update the internal model. 3. Goal-Based Agents Overview: - Goal-based agents operate based on predefined goals. They make decisions by evaluating how well different actions achieve these goals. - They can plan sequences of actions to achieve their objectives. How They Work: - These agents use search and planning algorithms to determine the best course of action to reach a goal. - They consider both the current state and the desired goal state. Example: - An AI planning system that schedules tasks to maximize efficiency and meet deadlines. - Use Case: Automated scheduling, robotic path planning. Advantages: - Capable of complex decision-making and planning. - Can adapt to changes in goals and environment. Disadvantages: - Requires complex algorithms for search and planning. - May be computationally intensive, especially in dynamic environments. 4. Utility-Based Agents Overview: - Utility-based agents aim to maximize their overall utility or satisfaction by evaluating the desirability of different outcomes. - They make decisions based on a utility function that assigns a value to each possible state. How They Work: - These agents use optimization techniques to select actions that maximize their expected utility. - They consider multiple factors and trade-offs to achieve the", "source": "blog:medium.com", "published": "", "authors": "", "tokens": 512, "sha256": "c2761f0ddb77fd6880e59aae306372d001ad87e661ad327d48ac9780dfcdd821"}
{"doc_id": "blog:medium.com#body:part-6", "url": "https://medium.com/@williamwarley/understanding-ai-agents-how-they-work-types-and-practical-applications-bd261845f7c3", "anchor": "#body:part-6", "type": "blog", "title": "", "section": "Body", "text": "algorithms for search and planning. - May be computationally intensive, especially in dynamic environments. 4. Utility-Based Agents Overview: - Utility-based agents aim to maximize their overall utility or satisfaction by evaluating the desirability of different outcomes. - They make decisions based on a utility function that assigns a value to each possible state. How They Work: - These agents use optimization techniques to select actions that maximize their expected utility. - They consider multiple factors and trade-offs to achieve the best overall outcome. Example: - An autonomous trading system that selects trades to maximize profit while minimizing risk. - Use Case: Financial trading, resource management. Advantages: - Can handle complex decision-making scenarios involving trade-offs. - Capable of balancing multiple objectives and preferences. Disadvantages: - Designing an appropriate utility function can be challenging. - May require significant computational resources for optimization. 5. Learning Agents Overview: - Learning agents improve their performance over time by learning from experiences and adapting to new situations. - They can operate in dynamic and uncertain environments by continuously updating their knowledge and strategies. How They Work: - These agents use various learning algorithms, such as supervised learning, unsupervised learning, and reinforcement learning, to acquire new knowledge and skills. - They have four main components: a learning element, a performance element, a critic, and a problem generator. Example: - A recommendation engine that refines its suggestions based on user feedback and interactions. - Use Case: Personalized recommendations, autonomous systems, adaptive control. Advantages: - Capable of continuous improvement and adaptation. - Can handle complex and changing environments. Disadvantages: - Requires significant amounts of data for effective learning. - The learning process can be computationally intensive and time-consuming. This chapter explores the various types of AI agents, highlighting their characteristics, how they work, and their respective advantages and disadvantages. Applications of AI Agents AI agents are deployed across various industries to automate tasks, enhance decision-making, and improve overall efficiency. This chapter explores several practical applications of AI agents, highlighting their impact and benefits in different domains. 1. Autonomous Vehicles Overview: - AI agents play a crucial role in the development of autonomous vehicles, enabling them to perceive their environment, make driving decisions, and navigate safely. How They Work: - Autonomous vehicles use sensors like cameras, LIDAR, and radar to gather data about the surroundings. - AI agents process this data to identify objects, predict their movements, and make real-time driving decisions. - The agents use machine learning algorithms to improve their performance over time, adapting to different driving conditions. Example: - Waymo’s self-driving cars use AI agents to navigate complex urban environments and handle various driving scenarios. - Benefits: Enhanced safety, reduced traffic congestion, and increased mobility for individuals who cannot drive. 2. Robotics Overview: - AI agents are integral to robotics, enabling robots to perform tasks autonomously in various settings, from manufacturing to healthcare. How They Work: - Robots equipped with AI agents use sensors to perceive their environment and execute tasks with precision. - AI agents in robots can learn from interactions and adapt to new tasks, improving efficiency", "source": "blog:medium.com", "published": "", "authors": "", "tokens": 512, "sha256": "0edafbfdfeee96e12231dd9f7c2d4476f5eb9cb7af71cbf55d475997e7ce5136"}
{"doc_id": "blog:medium.com#body:part-7", "url": "https://medium.com/@williamwarley/understanding-ai-agents-how-they-work-types-and-practical-applications-bd261845f7c3", "anchor": "#body:part-7", "type": "blog", "title": "", "section": "Body", "text": "environments and handle various driving scenarios. - Benefits: Enhanced safety, reduced traffic congestion, and increased mobility for individuals who cannot drive. 2. Robotics Overview: - AI agents are integral to robotics, enabling robots to perform tasks autonomously in various settings, from manufacturing to healthcare. How They Work: - Robots equipped with AI agents use sensors to perceive their environment and execute tasks with precision. - AI agents in robots can learn from interactions and adapt to new tasks, improving efficiency and accuracy. - Robots use goal-based and utility-based agents to optimize their actions and achieve specific objectives. Example: - Collaborative robots (cobots) in manufacturing work alongside human workers, performing repetitive and precise tasks. - Benefits: Increased productivity, enhanced precision, and improved workplace safety. 3. Personal Assistants Overview: - AI agents power personal assistants like Siri, Alexa, and Google Assistant, enabling them to understand and respond to user queries. How They Work: - Personal assistants use natural language processing (NLP) to understand spoken or written commands. - AI agents process the input, retrieve relevant information, and generate appropriate responses. - These agents continuously learn from interactions to improve their understanding and accuracy. Example: - Amazon Alexa uses AI agents to control smart home devices, provide weather updates, and play music based on user preferences. - Benefits: Convenience, hands-free control, and personalized user experiences. 4. Game AI Overview: - AI agents are widely used in video games to create intelligent and adaptive non-player characters (NPCs) that enhance gameplay. How They Work: - Game AI agents use rule-based and learning algorithms to control NPC behavior, making them respond dynamically to player actions. - Agents can adapt their strategies based on player performance, providing a challenging and engaging experience. - Reinforcement learning is often used to train game AI agents, allowing them to optimize their behavior through trial and error. Example: - In games like “The Sims,” AI agents control the behavior of virtual characters, making decisions based on their needs and environment. - Benefits: Improved player engagement, realistic NPC behavior, and dynamic gameplay experiences. 5. Financial Trading Overview: - AI agents are employed in financial trading to analyze market data, make trading decisions, and execute trades autonomously. How They Work: - AI agents use machine learning algorithms to analyze historical and real-time market data, identifying patterns and trends. - These agents make trading decisions based on predefined strategies and continuously learn to improve their performance. - Utility-based agents optimize trading strategies to maximize profits while minimizing risks. Example: - AI-powered trading platforms like QuantConnect use AI agents to develop and execute automated trading strategies. - Benefits: Enhanced trading efficiency, reduced human error, and the ability to operate 24/7. 6. Healthcare Overview: - AI agents are transforming healthcare by assisting in diagnostics, treatment planning, and patient management. How They Work: - AI agents analyze medical data, such as patient records and imaging, to assist in diagnosing diseases and recommending treatments. - They use supervised learning to learn from labeled medical data and improve their diagnostic accuracy. - Agents can also monitor patient health in real-time, providing", "source": "blog:medium.com", "published": "", "authors": "", "tokens": 512, "sha256": "17f0967d569f50feed0d1b75fff4db4d59fe8ee3388027b2ec37e414f50229a6"}
{"doc_id": "blog:medium.com#body:part-8", "url": "https://medium.com/@williamwarley/understanding-ai-agents-how-they-work-types-and-practical-applications-bd261845f7c3", "anchor": "#body:part-8", "type": "blog", "title": "", "section": "Body", "text": "Benefits: Enhanced trading efficiency, reduced human error, and the ability to operate 24/7. 6. Healthcare Overview: - AI agents are transforming healthcare by assisting in diagnostics, treatment planning, and patient management. How They Work: - AI agents analyze medical data, such as patient records and imaging, to assist in diagnosing diseases and recommending treatments. - They use supervised learning to learn from labeled medical data and improve their diagnostic accuracy. - Agents can also monitor patient health in real-time, providing alerts and recommendations based on the collected data. Example: - IBM Watson for Oncology uses AI agents to analyze medical literature and patient data, helping oncologists develop personalized cancer treatment plans. - Benefits: Improved diagnostic accuracy, personalized treatment, and enhanced patient care. This chapter explores the diverse applications of AI agents across various industries, demonstrating their impact and benefits. Advantages of AI Agents AI agents offer numerous advantages that make them indispensable in various applications. This chapter discusses the key benefits of AI agents, highlighting how they contribute to efficiency, scalability, real-time decision-making, and adaptability. 1. Efficiency and Automation Task Automation: - AI agents excel at automating repetitive and time-consuming tasks, freeing up human resources for more complex and creative work. - Example: In customer service, AI agents can handle common inquiries, process transactions, and provide instant support, allowing human agents to focus on more complex issues. Increased Productivity: - By performing tasks continuously without fatigue, AI agents significantly increase productivity and operational efficiency. - Example: In manufacturing, robotic AI agents can work 24/7, assembling products with precision and speed. Error Reduction: - AI agents reduce the likelihood of human error by performing tasks consistently and accurately. - Example: In data entry and processing, AI agents ensure accuracy and consistency, reducing errors that can occur with manual handling. 2. Scalability Handling Large Volumes: - AI agents can process vast amounts of data and manage large-scale operations, making them ideal for applications that require scalability. - Example: In financial trading, AI agents can analyze and act on market data from multiple sources in real-time, scaling to handle increased trading volumes. Flexible Resource Allocation: - AI agents can dynamically allocate resources based on demand, ensuring optimal performance and cost-efficiency. - Example: Cloud-based AI agents can scale computing resources up or down based on application needs, optimizing performance and costs. Global Reach: - AI agents can operate across different time zones and geographies, providing services and support around the clock. - Example: AI-driven customer support agents can assist customers worldwide, ensuring continuous service availability. 3. Real-Time Decision Making Immediate Responses: - AI agents can process information and make decisions in real-time, which is critical for applications requiring instant responses. - Example: In autonomous driving, AI agents process sensor data and make split-second decisions to navigate safely. Adaptive Strategies: - AI agents can adapt their strategies based on real-time data and changing conditions, optimizing outcomes dynamically. - Example: In online advertising, AI agents adjust bidding strategies in real-time to maximize ad performance and budget efficiency. Proactive Monitoring: - AI agents continuously monitor systems and environments, detecting", "source": "blog:medium.com", "published": "", "authors": "", "tokens": 512, "sha256": "7637014467b449533fabba6363ef8c3a856ee1b2acaa88cfba0cb07b074d16fd"}
{"doc_id": "blog:medium.com#body:part-9", "url": "https://medium.com/@williamwarley/understanding-ai-agents-how-they-work-types-and-practical-applications-bd261845f7c3", "anchor": "#body:part-9", "type": "blog", "title": "", "section": "Body", "text": "information and make decisions in real-time, which is critical for applications requiring instant responses. - Example: In autonomous driving, AI agents process sensor data and make split-second decisions to navigate safely. Adaptive Strategies: - AI agents can adapt their strategies based on real-time data and changing conditions, optimizing outcomes dynamically. - Example: In online advertising, AI agents adjust bidding strategies in real-time to maximize ad performance and budget efficiency. Proactive Monitoring: - AI agents continuously monitor systems and environments, detecting anomalies and triggering appropriate actions immediately. - Example: In cybersecurity, AI agents detect and respond to threats in real-time, protecting systems from potential breaches. 4. Adaptability and Learning Continuous Improvement: - AI agents improve their performance over time by learning from experiences and feedback, adapting to new situations and tasks. - Example: Personalized recommendation systems learn from user interactions to provide increasingly relevant suggestions. Handling Uncertainty: - AI agents can operate effectively in uncertain and dynamic environments by adapting their behavior based on learned patterns and real-time data. - Example: In robotics, AI agents adapt to changes in their environment, such as obstacles or varying conditions, to complete tasks efficiently. Customization and Personalization: - AI agents can tailor their actions and responses to individual user preferences and needs, providing personalized experiences. - Example: Virtual personal assistants learn user preferences over time, offering personalized recommendations and assistance. 5. Cost Efficiency Reduced Operational Costs: - Automating tasks with AI agents reduces the need for manual labor, lowering operational costs and increasing profitability. - Example: Automated warehouses use AI agents to manage inventory and logistics, reducing labor costs and increasing efficiency. Optimized Resource Utilization: - AI agents optimize the use of resources, such as energy and materials, leading to cost savings and sustainability. - Example: Smart energy management systems use AI agents to optimize energy usage in buildings, reducing costs and environmental impact. Investment in Innovation: - The efficiency gains and cost savings from AI agents allow organizations to invest more in innovation and strategic initiatives. - Example: Companies can allocate resources saved from automation to research and development, driving future growth and competitiveness. This chapter highlights the numerous advantages of AI agents, emphasizing their role in enhancing efficiency, scalability, real-time decision-making, adaptability, and cost-efficiency. Implementing AI Agents Implementing AI agents involves a series of practical steps and considerations, from selecting the right tools and frameworks to addressing common challenges. This chapter provides a comprehensive guide to implementing AI agents effectively. Steps for Implementing AI Agents 1. Define Objectives and Requirements: — Clearly outline the goals you aim to achieve with the AI agent, including specific tasks and performance metrics. — Example: An e-commerce platform might aim to implement an AI agent for personalized product recommendations, with the objective of increasing sales and customer satisfaction. 2. Select Appropriate Tools and Frameworks: — Choose the tools and frameworks that best suit your needs based on the complexity of the task, available resources, and technical expertise. — Example: For natural language processing tasks, frameworks like Hugging Face Transformers or spaCy might be appropriate. 3. Data Collection and", "source": "blog:medium.com", "published": "", "authors": "", "tokens": 512, "sha256": "7da69a6cc7838be688bb29867a8476b46bbbffdcf007c4b8d7a8696d63c19ffb"}
{"doc_id": "blog:medium.com#body:part-10", "url": "https://medium.com/@williamwarley/understanding-ai-agents-how-they-work-types-and-practical-applications-bd261845f7c3", "anchor": "#body:part-10", "type": "blog", "title": "", "section": "Body", "text": "specific tasks and performance metrics. — Example: An e-commerce platform might aim to implement an AI agent for personalized product recommendations, with the objective of increasing sales and customer satisfaction. 2. Select Appropriate Tools and Frameworks: — Choose the tools and frameworks that best suit your needs based on the complexity of the task, available resources, and technical expertise. — Example: For natural language processing tasks, frameworks like Hugging Face Transformers or spaCy might be appropriate. 3. Data Collection and Preprocessing: — Gather and preprocess the data required for training the AI agent. Ensure that the data is clean, labeled (if necessary), and representative of the problem domain. — Example: Collect customer interaction logs and preprocess them to remove noise and irrelevant information for training a customer service chatbot. 4. Model Selection and Training: — Select the appropriate machine learning or deep learning model for your AI agent. Train the model using the preprocessed data, and fine-tune it to achieve optimal performance. — Example: Use a pre-trained transformer model like BERT for fine-tuning on a specific NLP task. Example Code for Training a Model with Hugging Face Transformers: from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments from datasets import load_dataset # Load dataset dataset = load_dataset('csv', data_files={'train': 'train.csv', 'test': 'test.csv'}) # Load tokenizer and model tokenizer = BertTokenizer.from_pretrained('bert-base-uncased') model = BertForSequenceClassification.from_pretrained('bert-base-uncased') # Tokenize data def tokenize_function(examples): return tokenizer(examples['text'], padding='max_length', truncation=True) tokenized_datasets = dataset.map(tokenize_function, batched=True) # Define training arguments training_args = TrainingArguments( output_dir='./results', evaluation_strategy='epoch', learning_rate=2e-5, per_device_train_batch_size=8, per_device_eval_batch_size=8, num_train_epochs=3, weight_decay=0.01, ) # Initialize Trainer trainer = Trainer( model=model, args=training_args, train_dataset=tokenized_datasets['train'], eval_dataset=tokenized_datasets['test'], ) # Train model trainer.train() 5. Evaluation and Testing: — Evaluate the AI agent’s performance using appropriate metrics and test it in various scenarios to ensure robustness and reliability. — Example: Evaluate a recommendation system using metrics like precision, recall, and F1-score on a validation dataset. 6. Deployment: — Deploy the AI agent in the target environment, ensuring it integrates smoothly with existing systems and can operate at scale. — Example: Deploy a trained chatbot on a cloud platform like AWS Lambda for scalable, serverless execution. Example Code for Deploying a Model on AWS Lambda: import json import boto3 from transformers import BertTokenizer, BertForSequenceClassification # Initialize AWS Lambda client client = boto3.client('lambda') # Define the Lambda function def lambda_handler(event, context): # Load tokenizer and model tokenizer = BertTokenizer.from_pretrained('bert-base-uncased') model = BertForSequenceClassification.from_pretrained('bert-base-uncased') # Parse input input_text = event['text'] inputs = tokenizer(input_text, return_tensors='pt') # Perform inference outputs = model(**inputs) predictions = outputs.logits.argmax(dim=-1).item() # Return response return { 'statusCode': 200, 'body': json.dumps({'prediction': predictions}) } # Deploy the Lambda function response = client.create_function( FunctionName='AIChatbot', Runtime='python3.8', Role='your-aws-role', Handler='lambda_function.lambda_handler', Code={'ZipFile': open('function.zip', 'rb').read()}, Timeout=15, MemorySize=128, ) 7. Monitoring and Maintenance: — Continuously monitor the AI agent’s performance and make necessary updates or retrain the model to maintain its effectiveness. — Example: Regularly update the training data and retrain a sentiment analysis model to adapt to changing trends and language usage. Tools and Frameworks 1. Machine Learning Frameworks: — TensorFlow: Widely used for building and deploying machine learning models, with extensive support for neural networks and deep learning. — PyTorch: Popular for", "source": "blog:medium.com", "published": "", "authors": "", "tokens": 512, "sha256": "80bee5fb3d1b45677f42423e475133e3e4191c2bb25f85154395fc97edea9f27"}
{"doc_id": "blog:medium.com#body:part-11", "url": "https://medium.com/@williamwarley/understanding-ai-agents-how-they-work-types-and-practical-applications-bd261845f7c3", "anchor": "#body:part-11", "type": "blog", "title": "", "section": "Body", "text": "open('function.zip', 'rb').read()}, Timeout=15, MemorySize=128, ) 7. Monitoring and Maintenance: — Continuously monitor the AI agent’s performance and make necessary updates or retrain the model to maintain its effectiveness. — Example: Regularly update the training data and retrain a sentiment analysis model to adapt to changing trends and language usage. Tools and Frameworks 1. Machine Learning Frameworks: — TensorFlow: Widely used for building and deploying machine learning models, with extensive support for neural networks and deep learning. — PyTorch: Popular for its dynamic computation graph and ease of use in research and development of deep learning models. — Scikit-Learn: Ideal for implementing traditional machine learning algorithms and preprocessing data. 2. Natural Language Processing (NLP) Frameworks: — Hugging Face Transformers: Provides pre-trained models and tools for NLP tasks such as text classification, question answering, and language translation. — spaCy: Efficient and scalable library for NLP tasks, including tokenization, named entity recognition, and dependency parsing. 3. Deployment Platforms: — AWS SageMaker: Comprehensive platform for building, training, and deploying machine learning models at scale. — Google Cloud AI Platform: Managed services for training and deploying machine learning models on Google Cloud. — Azure Machine Learning: End-to-end platform for training, deploying, and managing machine learning models on Azure. Best Practices 1. Data Quality: — Ensure high-quality data by cleaning, preprocessing, and labeling it accurately. Good data is crucial for training effective AI agents. — Example: Remove duplicates and outliers from your dataset to improve model accuracy. 2. Model Evaluation: — Use appropriate metrics to evaluate model performance and ensure it meets the desired objectives. — Example: Evaluate a classification model using metrics like accuracy, precision, recall, and F1-score. 3. Scalability and Efficiency: — Design AI agents to scale efficiently, ensuring they can handle increasing workloads and data volumes. — Example: Use distributed training and inference techniques to scale your AI agent across multiple machines. 4. Security and Privacy: — Implement security measures to protect data and ensure privacy, especially when dealing with sensitive information. — Example: Encrypt data at rest and in transit, and implement access controls to protect user data. Common Challenges and Solutions 1. Data Availability: — Challenge: Lack of sufficient labeled data for training. — Solution: Use data augmentation techniques, transfer learning, or synthetic data generation to augment your dataset. 2. Model Overfitting: — Challenge: The model performs well on training data but poorly on unseen data. — Solution: Implement regularization techniques, such as dropout and L2 regularization, and use cross-validation to assess model performance. 3. Integration Complexity: — Challenge: Integrating the AI agent with existing systems and workflows. — Solution: Use APIs and modular design principles to ensure smooth integration and compatibility with other systems. 4. Computational Resources: — Challenge: High computational requirements for training and inference. — Solution: Use cloud-based platforms and distributed computing to leverage scalable resources and reduce costs. This chapter provides a comprehensive guide to implementing AI agents, covering practical steps, tools and frameworks, best practices, and common challenges. Future Trends and Developments in AI Agents The field of AI agents is rapidly evolving, with continuous advancements that promise", "source": "blog:medium.com", "published": "", "authors": "", "tokens": 512, "sha256": "82c268770d0f29bd77ef0ae2cb6477b22177d3a8fdda5826231a3de61df5f370"}
{"doc_id": "blog:medium.com#body:part-12", "url": "https://medium.com/@williamwarley/understanding-ai-agents-how-they-work-types-and-practical-applications-bd261845f7c3", "anchor": "#body:part-12", "type": "blog", "title": "", "section": "Body", "text": "modular design principles to ensure smooth integration and compatibility with other systems. 4. Computational Resources: — Challenge: High computational requirements for training and inference. — Solution: Use cloud-based platforms and distributed computing to leverage scalable resources and reduce costs. This chapter provides a comprehensive guide to implementing AI agents, covering practical steps, tools and frameworks, best practices, and common challenges. Future Trends and Developments in AI Agents The field of AI agents is rapidly evolving, with continuous advancements that promise to enhance their capabilities and applications. This chapter explores some of the key future trends and developments in AI agents, including advances in reinforcement learning, integration with the Internet of Things (IoT), ethical considerations, and human-agent collaboration. Advances in Reinforcement Learning 1. Deep Reinforcement Learning (DRL): — Combining deep learning with reinforcement learning has led to significant breakthroughs in creating more capable and sophisticated AI agents. DRL algorithms enable agents to learn complex behaviors in high-dimensional environments. — Future Trend: Development of more efficient DRL algorithms that can learn faster and require less computational power, making them accessible for a broader range of applications. 2. Meta-Learning: — Meta-learning, or “learning to learn,” involves training AI agents to adapt quickly to new tasks with minimal data. This approach enhances the flexibility and generalization of AI agents. — Future Trend: Increased focus on meta-learning techniques to create AI agents that can efficiently transfer knowledge across different tasks and domains. 3. Multi-Agent Systems: — Multi-agent reinforcement learning (MARL) involves multiple AI agents interacting and learning within the same environment. This approach is useful for tasks requiring coordination and collaboration. — Future Trend: Advancements in MARL will enable more complex and realistic simulations, such as autonomous traffic management and collaborative robotics. Integration with IoT 1. Edge AI: — Edge AI involves deploying AI agents on edge devices, allowing for real-time data processing and decision-making closer to the source. This reduces latency and bandwidth usage. — Future Trend: Greater integration of AI agents with IoT devices to enable intelligent and autonomous operations in smart homes, industrial automation, and healthcare. 2. Distributed AI Systems: — Distributed AI systems leverage multiple connected devices to share computational loads and improve overall system performance and reliability. — Future Trend: Development of robust distributed AI frameworks that facilitate seamless collaboration between AI agents and IoT devices. 3. Predictive Maintenance: — AI agents can analyze data from IoT sensors to predict equipment failures and schedule maintenance proactively, reducing downtime and costs. — Future Trend: Enhanced predictive maintenance solutions using AI agents to improve efficiency and reliability in various industries, including manufacturing and energy. Ethical and Responsible AI 1. Bias Mitigation: — Addressing biases in AI agents is crucial to ensure fairness and equity. Researchers are developing techniques to detect and mitigate biases in training data and algorithms. — Future Trend: Implementation of more advanced bias detection and mitigation methods to promote ethical AI practices. 2. Transparency and Explainability: — As AI agents become more complex, understanding and explaining their decision-making processes is essential for building trust and accountability. — Future Trend: Development of", "source": "blog:medium.com", "published": "", "authors": "", "tokens": 512, "sha256": "14a559968231d021077028b43db24b5569a3e4fca439208f43ab4cf754cb8087"}
{"doc_id": "blog:medium.com#body:part-13", "url": "https://medium.com/@williamwarley/understanding-ai-agents-how-they-work-types-and-practical-applications-bd261845f7c3", "anchor": "#body:part-13", "type": "blog", "title": "", "section": "Body", "text": "Ethical and Responsible AI 1. Bias Mitigation: — Addressing biases in AI agents is crucial to ensure fairness and equity. Researchers are developing techniques to detect and mitigate biases in training data and algorithms. — Future Trend: Implementation of more advanced bias detection and mitigation methods to promote ethical AI practices. 2. Transparency and Explainability: — As AI agents become more complex, understanding and explaining their decision-making processes is essential for building trust and accountability. — Future Trend: Development of explainable AI (XAI) techniques that provide insights into how AI agents make decisions, enhancing transparency and user confidence. 3. Regulatory Compliance: — Ensuring that AI agents comply with regulatory standards and guidelines is vital for their safe and ethical deployment. — Future Trend: Establishment of comprehensive AI regulations and standards that guide the development and deployment of responsible AI agents. Human-Agent Collaboration 1. Human-in-the-Loop Systems: — Human-in-the-loop (HITL) systems involve human oversight and interaction with AI agents, combining human expertise with AI efficiency. — Future Trend: Increased adoption of HITL systems in critical applications such as healthcare, finance, and autonomous systems to ensure safe and effective operation. 2. Augmented Intelligence: — Augmented intelligence focuses on enhancing human capabilities with AI agents, rather than replacing humans. This approach leverages the strengths of both humans and AI. — Future Trend: Development of collaborative tools and platforms that empower humans to work alongside AI agents, improving productivity and decision-making. 3. Interactive Learning: — Interactive learning involves AI agents learning from direct interactions with humans, receiving feedback, and improving their performance. — Future Trend: Enhanced interactive learning frameworks that facilitate seamless and intuitive human-agent interactions, leading to more personalized and adaptive AI systems. Conclusion The future of AI agents is filled with exciting possibilities and challenges. Advances in reinforcement learning, integration with IoT, ethical considerations, and human-agent collaboration will drive the next wave of innovations in AI. By staying informed about these trends and developments, organizations and developers can harness the full potential of AI agents to create smarter, more efficient, and ethical solutions. This chapter explores the future trends and developments in AI agents, highlighting key advancements and their potential impact. Case Studies and Real-World Examples AI agents have made significant strides in various industries, solving complex problems and enhancing operational efficiency. This chapter presents several case studies and real-world examples to illustrate the successful application of AI agents in different domains. Case Study 1: Autonomous Vehicles Company: Waymo Challenge: Developing self-driving cars that can safely navigate complex urban environments and interact with other road users. Solution: Waymo uses AI agents to process data from sensors (LIDAR, cameras, radar) and make real-time driving decisions. Implementation: - Perception: AI agents use sensor data to detect and classify objects such as pedestrians, vehicles, and traffic signals. - Decision-Making: Agents apply machine learning algorithms to predict the behavior of other road users and plan safe driving maneuvers. - Action: The AI system controls the vehicle’s acceleration, braking, and steering to navigate through traffic. Outcome: Waymo’s self-driving cars have successfully logged millions of miles on public roads, demonstrating the", "source": "blog:medium.com", "published": "", "authors": "", "tokens": 512, "sha256": "710e7f0c045800c3fd24fbdc5e64995fad77bfcefa411fe865b25e936cc7400b"}
{"doc_id": "blog:medium.com#body:part-14", "url": "https://medium.com/@williamwarley/understanding-ai-agents-how-they-work-types-and-practical-applications-bd261845f7c3", "anchor": "#body:part-14", "type": "blog", "title": "", "section": "Body", "text": "sensors (LIDAR, cameras, radar) and make real-time driving decisions. Implementation: - Perception: AI agents use sensor data to detect and classify objects such as pedestrians, vehicles, and traffic signals. - Decision-Making: Agents apply machine learning algorithms to predict the behavior of other road users and plan safe driving maneuvers. - Action: The AI system controls the vehicle’s acceleration, braking, and steering to navigate through traffic. Outcome: Waymo’s self-driving cars have successfully logged millions of miles on public roads, demonstrating the potential of AI agents to enhance transportation safety and efficiency. Case Study 2: Healthcare Diagnostics Company: IBM Watson Health Challenge: Assisting doctors in diagnosing diseases and recommending personalized treatment plans based on vast amounts of medical data. Solution: IBM Watson for Oncology uses AI agents to analyze medical records, research papers, and clinical guidelines to provide evidence-based recommendations. Implementation: - Data Integration: AI agents aggregate and analyze data from electronic health records (EHRs), medical literature, and clinical trial results. - Natural Language Processing (NLP): Agents use NLP to interpret unstructured medical texts and extract relevant information. - Decision Support: The AI system suggests potential diagnoses and treatment options based on the latest medical evidence and patient-specific factors. Outcome: Watson for Oncology has been deployed in several hospitals worldwide, aiding oncologists in developing effective and personalized treatment plans, thus improving patient outcomes. Case Study 3: Financial Trading Company: BlackRock Challenge: Optimizing investment strategies and managing large portfolios with real-time market analysis and trading decisions. Solution: BlackRock’s Aladdin platform employs AI agents to analyze market data, assess risks, and execute trades autonomously. Implementation: - Market Analysis: AI agents continuously monitor and analyze financial news, market trends, and economic indicators. - Risk Management: Agents assess portfolio risks and suggest adjustments to optimize performance. - Automated Trading: The AI system executes trades based on predefined strategies and real-time market conditions. Outcome: Aladdin has enhanced BlackRock’s ability to manage assets efficiently, providing clients with optimized investment strategies and improved financial returns. Case Study 4: E-commerce Personalization Company: Amazon Challenge: Providing personalized shopping experiences to millions of customers by recommending relevant products. Solution: Amazon uses AI agents in its recommendation engine to analyze customer behavior and suggest products tailored to individual preferences. Implementation: - Data Collection: AI agents gather data on customer browsing history, purchase behavior, and product interactions. - Machine Learning: Agents use collaborative filtering and deep learning algorithms to identify patterns and preferences. - Personalized Recommendations: The AI system generates real-time product recommendations for each customer based on their unique profile. Outcome: Amazon’s recommendation engine significantly boosts customer engagement and sales, contributing to its status as a leading e-commerce platform. Case Study 5: Customer Service Automation Company: Bank of America Challenge: Enhancing customer service by providing instant, accurate, and personalized responses to customer inquiries. Solution: Bank of America implemented Erica, an AI-powered virtual assistant that helps customers with banking tasks and queries. Implementation: - Natural Language Understanding (NLU): Erica uses NLU to understand customer queries expressed in natural language. - Machine Learning: The virtual assistant learns from interactions to improve its responses and provide personalized assistance.", "source": "blog:medium.com", "published": "", "authors": "", "tokens": 512, "sha256": "87dfd521726c29e844071fef5a75c297aaf935faef6fdf236856f173130490ca"}
{"doc_id": "blog:medium.com#body:part-15", "url": "https://medium.com/@williamwarley/understanding-ai-agents-how-they-work-types-and-practical-applications-bd261845f7c3", "anchor": "#body:part-15", "type": "blog", "title": "", "section": "Body", "text": "a leading e-commerce platform. Case Study 5: Customer Service Automation Company: Bank of America Challenge: Enhancing customer service by providing instant, accurate, and personalized responses to customer inquiries. Solution: Bank of America implemented Erica, an AI-powered virtual assistant that helps customers with banking tasks and queries. Implementation: - Natural Language Understanding (NLU): Erica uses NLU to understand customer queries expressed in natural language. - Machine Learning: The virtual assistant learns from interactions to improve its responses and provide personalized assistance. - Integration: Erica integrates with the bank’s systems to access account information, perform transactions, and provide financial advice. Outcome: Erica handles millions of customer interactions, improving response times, reducing workload for human agents, and enhancing customer satisfaction. Case Study 6: Smart Home Management Company: Google Challenge: Creating a smart home ecosystem that automates household tasks and enhances convenience for users. Solution: Google Assistant uses AI agents to control smart home devices, manage schedules, and provide information. Implementation: - Voice Recognition: AI agents use speech recognition to understand voice commands from users. - Device Control: The assistant interacts with smart home devices (e.g., lights, thermostats, security systems) to execute commands. - Personalization: The AI system learns user preferences and routines to automate tasks and provide relevant information. Outcome: Google Assistant enhances the smart home experience, making it easier for users to manage their homes efficiently and conveniently. This chapter showcases successful applications of AI agents across various industries, demonstrating their versatility and impact. Key Insights and Final Recommendations As we conclude this comprehensive guide on AI agents, it is important to summarize the key insights and provide final recommendations for effectively leveraging AI agents in various applications and industries. Summary of Key Insights 1. Definition and Importance: — AI agents are autonomous entities that use AI to perceive their environment, make decisions, and perform actions to achieve specific goals. — They play a crucial role in automating tasks, enhancing decision-making, and improving operational efficiency across various domains. 2. How AI Agents Work: — AI agents operate based on core concepts such as perception, reasoning, action, and learning. — They can be categorized into different types, including simple reflex agents, model-based reflex agents, goal-based agents, utility-based agents, and learning agents. 3. Types of AI Agents: — Simple Reflex Agents: Operate based on predefined rules and immediate perception. — Model-Based Reflex Agents: Maintain an internal model of the environment to make informed decisions. — Goal-Based Agents: Make decisions based on predefined goals and desired outcomes. — Utility-Based Agents: Evaluate the utility of different actions to maximize overall satisfaction or performance. — Learning Agents: Continuously learn from experiences and adapt to new situations. 4. Applications of AI Agents: — AI agents are used in various industries, including autonomous vehicles, robotics, personal assistants, game AI, financial trading, and healthcare. — Their ability to automate tasks, make real-time decisions, and adapt to changing conditions makes them valuable assets in these fields. 5. Advantages of AI Agents: — Efficiency and Automation: Automate repetitive tasks, increase productivity, and reduce errors. — Scalability: Handle large-scale operations and adapt to varying workloads.", "source": "blog:medium.com", "published": "", "authors": "", "tokens": 512, "sha256": "1a01135065758864d0b7b4093740e0b41a2801a4de4dfc6955715649040054ec"}
{"doc_id": "blog:medium.com#body:part-16", "url": "https://medium.com/@williamwarley/understanding-ai-agents-how-they-work-types-and-practical-applications-bd261845f7c3", "anchor": "#body:part-16", "type": "blog", "title": "", "section": "Body", "text": "from experiences and adapt to new situations. 4. Applications of AI Agents: — AI agents are used in various industries, including autonomous vehicles, robotics, personal assistants, game AI, financial trading, and healthcare. — Their ability to automate tasks, make real-time decisions, and adapt to changing conditions makes them valuable assets in these fields. 5. Advantages of AI Agents: — Efficiency and Automation: Automate repetitive tasks, increase productivity, and reduce errors. — Scalability: Handle large-scale operations and adapt to varying workloads. — Real-Time Decision Making: Provide immediate responses and adapt strategies based on real-time data. — Adaptability and Learning: Improve performance over time and handle complex, dynamic environments. — Cost Efficiency: Reduce operational costs and optimize resource utilization. 6. Implementing AI Agents: — The implementation process involves defining objectives, selecting tools and frameworks, collecting and preprocessing data, training models, evaluating and testing, deploying, and maintaining AI agents. — Best practices include ensuring data quality, using appropriate evaluation metrics, designing for scalability and efficiency, and addressing security and privacy concerns. 7. Future Trends and Developments: — Advances in reinforcement learning, integration with IoT, ethical considerations, and human-agent collaboration are shaping the future of AI agents. — These trends promise to enhance the capabilities, applications, and ethical deployment of AI agents. 8. Case Studies and Real-World Examples: — Successful applications of AI agents in various industries highlight their practical benefits and impact. — Case studies demonstrate the versatility of AI agents in solving complex problems and improving operational efficiency. Final Recommendations 1. Stay Informed and Adaptable: — The field of AI agents is rapidly evolving. Stay informed about the latest developments, research, and best practices to leverage new opportunities and advancements. 2. Invest in Data Quality: — High-quality data is crucial for training effective AI agents. Ensure that your data is clean, representative, and accurately labeled. 3. Select the Right Tools and Frameworks: — Choose tools and frameworks that align with your specific requirements and technical expertise. Consider factors such as scalability, ease of use, and community support. 4. Focus on Ethical and Responsible AI: — Address ethical considerations, including bias mitigation, transparency, and regulatory compliance. Implement robust measures to ensure the responsible deployment of AI agents. 5. Optimize for Scalability and Efficiency: — Design AI agents to scale efficiently and handle varying workloads. Use cloud-based platforms and distributed computing to optimize performance and costs. 6. Continuous Monitoring and Improvement: — Continuously monitor the performance of AI agents and make necessary updates or retrain models to maintain their effectiveness. Stay proactive in addressing any issues that arise. 7. Leverage Human-Agent Collaboration: — Implement human-in-the-loop systems and augmented intelligence approaches to combine human expertise with AI efficiency. Facilitate seamless collaboration between humans and AI agents. 8. Explore Diverse Applications: — Explore various applications of AI agents across different industries. Identify opportunities where AI agents can provide significant value and drive innovation. By following these recommendations and leveraging the insights provided in this guide, you can effectively implement and benefit from AI agents in your projects and applications. Feel free to reach out if you have any", "source": "blog:medium.com", "published": "", "authors": "", "tokens": 512, "sha256": "2a72235b86b580bb9e7d773e070d2eb8eec239b6d684cb5f07294436c935ed52"}
{"doc_id": "blog:medium.com#body:part-17", "url": "https://medium.com/@williamwarley/understanding-ai-agents-how-they-work-types-and-practical-applications-bd261845f7c3", "anchor": "#body:part-17", "type": "blog", "title": "", "section": "Body", "text": "and augmented intelligence approaches to combine human expertise with AI efficiency. Facilitate seamless collaboration between humans and AI agents. 8. Explore Diverse Applications: — Explore various applications of AI agents across different industries. Identify opportunities where AI agents can provide significant value and drive innovation. By following these recommendations and leveraging the insights provided in this guide, you can effectively implement and benefit from AI agents in your projects and applications. Feel free to reach out if you have any questions or need further assistance with specific aspects of AI agents.", "source": "blog:medium.com", "published": "", "authors": "", "tokens": 91, "sha256": "adead498c82e56d7bf29fe8393130979553492f46dd1095812179a0bb1a85bd0"}
{"doc_id": "arxiv:2510.13149#abstract", "url": "https://arxiv.org/abs/2510.13149", "anchor": "#abstract", "type": "paper", "title": "", "section": "ABSTRACT", "text": "Enabling robots to flexibly schedule and compose learned skills for novel long- horizon manipulation under diverse perturbations remains a core challenge. Early explorations with end-to-end VLA models show limited success, as these models struggle to generalize beyond the training distribution. Hierarchical approaches, where high-level planners generate subgoals for low-level policies, bring certain improvements but still suffer under complex perturbations, revealing limited ca- pability in skill composition. However, existing benchmarks primarily empha- size task completion in long-horizon settings, offering little insight into compo- sitional generalization, robustness, and the interplay between planning and exe- cution. To systematically investigate these gaps, we propose RoboHiMan, a hi- erarchical evaluation paradigm for compositional generalization in long-horizon manipulation. RoboHiMan introduces HiMan-Bench, a benchmark of atomic and compositional tasks under diverse perturbations, supported by a multi-level train- ing dataset for analyzing progressive data scaling, and proposes three evaluation paradigms (vanilla, decoupled, coupled) that probe the necessity of skill compo- sition and reveal bottlenecks in hierarchical architectures. Experiments highlight clear capability gaps across representative models and architectures, pointing to directions for advancing models better suited to real-world long-horizon manipu- lation tasks. Videos and open-source code can be found on our project website. 1", "source": "arxiv_pdf", "published": "", "tokens": 195, "sha256": "33ea63c18268ab5ca818b266ae1229aa13b48220b107ae290cbf1641a3711898"}
{"doc_id": "arxiv:2510.13149#introduction", "url": "https://arxiv.org/abs/2510.13149", "anchor": "#introduction", "type": "paper", "title": "", "section": "INTRODUCTION", "text": "In the field of robot manipulation, a long-term goal is to enable robots to perform diverse long- horizon tasks (Zhang et al., 2024; Shi et al., 2025; Chen et al., 2024; 2025b;e;a). However, achieving this goal requires overcoming a fundamental challenge: compositional generalization. Specifically, we expect robots to, much like humans, master a set of atomic skills (e.g., opening a drawer, picking up objects) through imitation learning, and flexibly schedule and compose them to complete new long-horizon tasks (e.g., opening a drawer then placing an object inside) (Chen et al., 2025f). How- ever, in real-world applications, compositional generalization intensifies as robots must contend with various perturbations, such as changes in lighting, object appearance, or camera poses (Pumacay et al., 2024). Therefore, evaluating compositional generalization involves testing whether models can effectively compose skills under such diverse conditions. To systematically study this prob- lem, we focus on a central research question: How well can models trained solely on atomic skills generalize to long-horizon compositional tasks in the presence of various perturbations? Existing manipulation benchmarks (James et al., 2020; Liu et al., 2023; Chen et al., 2025c; Mees et al., 2022; Zhang et al., 2024; Chen et al., 2025f; Han et al., 2025) have played an important role in advancing the field by providing diverse long-horizon tasks for training and evaluation. However, they exhibit notable limitations: most benchmarks focus on evaluating models on complete long- horizon tasks without systematically examining the flexible composition of atomic skills (James et al., 2020; Liu et al., 2023; Chen et al., 2025c). DeCoBench (Chen et al., 2025f) considers skill ∗These authors contributed equally to this work. †Corresponding author: Yong-Lu Li (yonglu li@sjtu.edu.cn), Jing Huo (huojing@nju.edu.cn). 1 arXiv:2510.13149v1 [cs.RO] 15 Oct 2025 Preprint. Under review. Atomic Tasks (A) Atomic-perturbation (AP) -Bench (a) Examples of Four Types of Tasks (Atomic and Compositional Tasks under Diverse Perturbations) (b) Hierarchical Evaluation Paradigm High-Level Planner Low-Level Policy Hierarchical Evaluation Observation Instruction Plan Action Diverse Test Tasks How well can models trained solely on atomic skills generalize to long-horizon compositional tasks in the presence of multiple perturbations, and how is their performance affected by additional training data? (c) Performance Comparison Compositional-perturbation (CP) Compositional Tasks (C) Atomic Task 1 Atomic Task 2 20 A demos per task L1: L1 + 1 AP demo per factor L2: Factor 1 Factor 2 Compositional Task 1 Factor 1 Multi-Level Training Dataset L2 + 5 C demos per task L3: L3+ 1 CP demo per factor L4: Training Dataset Selection Diverse Evaluation Metrics Online / Offline", "source": "arxiv_pdf", "published": "", "tokens": 418, "sha256": "9cafe93b7a9514ab7ea3939cb011b81502a13fabc227c83ce47539dd07c488c7"}
{"doc_id": "arxiv:2510.13149#evaluation", "url": "https://arxiv.org/abs/2510.13149", "anchor": "#evaluation", "type": "paper", "title": "", "section": "Evaluation", "text": "Lack of Diverse Metrics for Robust Compositional Generalization No Progressive Multi-level Training Prior Benchmark HiMan-Benchmark Figure 1: RoboHiMan Overview. To evaluate compositional generalization, RoboHiMan introduces: (a) HiMan-Bench with four task types: atomic (A), atomic-perturbation (AP), compositional (C), and compositional-perturbation (CP); (b) a hierarchical evaluation paradigm with diverse metrics and progressive training data (L1–L4), where L1 uses minimal atomic data and L4 provides larger datasets; (c) Extensive ex- periments highlight critical performance gaps across training datasets and evaluation modes, often overlooked by prior benchmarks (notation “X →Y” denoting training on Level X and evaluation on task category Y). composition but lacks an in-depth analysis of how environmental perturbations affect composition- ality. Colosseum (Pumacay et al., 2024) only assesses the robustness of atomic skills under pertur- bations but does not evaluate multi-stage compositional tasks. More critically, existing benchmarks make it difficult to disentangle whether failures arise from insufficient planning, poor execution, or sensitivity to environmental perturbations (Mees et al., 2022; Zhang et al., 2024; Chen et al., 2025f; Han et al., 2025). The rough metric and task design of them leave open questions about which module is responsible for failures, which hinders the development of a new method in this domain. To address these limitations, we propose RoboHiMan, A a hierarchical evaluation paradigm for compositional generalization in long-horizon manipulation, which makes two core contributions. The first is HiMan-Bench, a new benchmark dedicated to compositional generalization in robot manipulation. Building on the design principles of DeCoBench and Colosseum, HiMan-Bench evaluates whether models can compose atomic skills to accomplish long-horizon tasks under diverse environmental perturbations, such as changes in object appearance, size, lighting, and distractors. Compared with prior benchmarks, HiMan-Bench explicitly measures the effect of perturbations on skill composition (advancing beyond DeCoBench) and systematically emphasizes multi-stage com- positional tasks rather than only atomic skills (extending beyond Colosseum). To enable structured evaluation, tasks are categorized into four types (Fig. 1(a)): atomic (A), atomic with perturba- tions (AP), compositional (C), and compositional with perturbations (CP). This categorization disentangles different aspects of capability, allowing separate assessment of basic skill mastery, skill composition, and the robustness of both under realistic perturbations. Nevertheless, state-of-the-art Visual-Language-Action (VLA) models (Kim et al., 2024; Black et al., 2024), even when pre-trained on large-scale demonstrations, continue to struggle with composing skills in perturbed settings. Re- cent studies (Huang et al., 2023; Chen et al., 2025f; Black et al., 2025) have attempted to mitigate this by employing hierarchical frameworks that leverage the reasoning and planning abilities of Visual- Language Models (VLMs). However, these approaches remain fragile when combining skills under perturbations, raising a key question: When a model fails to achieve compositional generalization in long-horizon tasks under perturbations, is the failure due to ineffective planning or insufficient execution capability? 2 Preprint. Under review. Diverse Perturbations Atomic and Compositional Tasks Progressive Multi-Level Training Dataset Hierarchical", "source": "arxiv_pdf", "published": "", "tokens": 470, "sha256": "389d47fac6d7c2a9b131f5eedb4b4b85172d8a7aa0eb3eef6cdb536c6b789fa3"}
{"doc_id": "arxiv:2510.13149#evaluation:part-1", "url": "https://arxiv.org/abs/2510.13149", "anchor": "#evaluation:part-1", "type": "paper", "title": "", "section": "Evaluation", "text": "Eval. of Compositional Gen. under Perturbations RLBench (2020) ✗ ✗ ✗ ✗ ✗ CALVIN (2022) ✗ ✓ ✓ ✗ ✗ Libero-Long (2023) ✗ ✓ ✓ ✗ ✗ Colosseum (2024) ✓ ✗ ✓ ✗ ✗ VLABench (2024) ✓ ✓ ✗ ✓ ✗ DeCoBench (2025f) ✗ ✓ ✗ ✓ ✗ RoboCerebra (2025) ✗ ✓ ✗ ✓ ✗ RoboHiMan (Ours) ✓ ✓ ✓ ✓ ✓ Table 1: Comparison of Long-Horizon Manipulation Benchmarks. Unlike prior benchmarks, RoboHiMan explicitly evaluates compositional generalization under perturbations and also covers robustness, composition- ality, multi-level training, and hierarchical evaluation. To this end, we introduce the second innovation of RoboHiMan, a hierarchical evaluation paradigm (see Fig. 1(b)) for systematically evaluating model capabilities. It includes a progres- sive multi-level training dataset (L1–L4), where L1 represents the most challenging setting with minimal atomic skill data, and L4 the easiest with larger, more diverse datasets. This progression allows analysis of how training complexity and exposure to perturbations affect long-horizon skill composition. RoboHiMan evaluates models using three modes: Vanilla, Decoupled, and Coupled, on a set of test tasks organized into four types shown in Fig 1(a). In Vanilla mode, the low-level policy executes tasks directly without planner guidance; Decoupled mode evaluates the planner and policy separately; and Coupled mode tests the full hierarchical system end-to-end. This setup allows us to disentangle failures arising from planning versus execution, while systematically assessing ro- bustness under diverse task conditions. Together with progressive training, these modes provide rich metrics to reveal detailed patterns of compositional generalization. Through extensive experiments, we identify: (1) Models without a planner perform poorly when composing atomic skills, exposing the limitations of low-level policies in compositionality. (2) While additional training data with compositional examples improves performance, a substantial gap persists, highlighting the inherent difficulty of compositional generalization. (3) As shown in Fig. 1(c), VLA models such as π0.5 (Black et al., 2025) perform well on LIBERO-10 (Liu et al., 2023), but fail under the diverse perturbations in HiMan-Bench, exposing limitations that prior benchmarks fail to capture. (4) Hierarchical systems remain brittle, as planning errors and imperfect execution compound over long horizons, leading to a sharp degradation in overall performance. In summary, RoboHiMan makes three contributions: (1) HiMan-Bench, a novel benchmark that evaluates how well models can compose atomic skills to complete long-horizon manipulation tasks under diverse environmental perturbations. (2) A novel hierarchical evaluation paradigm that com- bines progressive multi-level training dataset with multiple evaluation modes, allowing separate analysis of planning and execution performance while revealing robustness limitations. (3) A com- prehensive analysis of model performance, uncovering key challenges in long-horizon composi- tional generalization and providing insights beyond prior benchmark results. 2 RELATED WORKS Long-Horizon Robotic Manipulation Benchmarks. Long-horizon tasks are widely regarded as a key challenge for evaluating the planning and generalization capabilities of robotic manipula- tion. Early benchmarks, e.g., RLBench (James et al., 2020), CALVIN (Mees et al., 2022), Libero- Long (Liu et al., 2023), and more recently RoboTwin 2.0 (Chen et al., 2025c), include such tasks but mainly train and evaluate models directly on long-horizon tasks without explicitly requiring skill composition, thus providing limited", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "c2e7d216666fc307ff10401fa579f484668bc6e212e4b8d1765795b0be99a998"}
{"doc_id": "arxiv:2510.13149#evaluation:part-2", "url": "https://arxiv.org/abs/2510.13149", "anchor": "#evaluation:part-2", "type": "paper", "title": "", "section": "Evaluation", "text": "beyond prior benchmark results. 2 RELATED WORKS Long-Horizon Robotic Manipulation Benchmarks. Long-horizon tasks are widely regarded as a key challenge for evaluating the planning and generalization capabilities of robotic manipula- tion. Early benchmarks, e.g., RLBench (James et al., 2020), CALVIN (Mees et al., 2022), Libero- Long (Liu et al., 2023), and more recently RoboTwin 2.0 (Chen et al., 2025c), include such tasks but mainly train and evaluate models directly on long-horizon tasks without explicitly requiring skill composition, thus providing limited insights into task planning. Yet in practice, agents must com- pose learned skills to achieve long-horizon goals, beyond simple imitation (Belkhale et al., 2024; Gao et al., 2025). Recent benchmarks such as VLA-Bench (Zhang et al., 2024), RoboCasa (Nasiri- any et al., 2024), DeCoBench (Chen et al., 2025f), and RoboCerebra (Han et al., 2025) introduce more challenging tasks involving language-conditioned decomposition and planning. However, they still largely treat long-horizon problems as simple skill permutations and overlook real-world per- turbations (e.g., color, texture, object size) on skill composition. Colosseum (Pumacay et al., 2024) advances this line by systematically examining perturbations, revealing model vulnerabilities under environmental variations. Yet its evaluation remains mostly at the atomic-task level, where suc- cess does not guarantee robustness in compositional tasks. To this end, we propose RoboHiMan, which inherits Colosseum’s perturbation design and further emphasizes their compounded effects 3 Preprint. Under review. MO-Colors (800) RO-Colors (800) MO-Textures (120) RO-Textures (8520) MO-Size (68) RO-Size (68) Table-Colors (800) Table-Textures (8520) Distractor-Objects (8520) Background-Textures (8520) Camera-Pose (1200) Atomic Tasks (17863) Compositional Tasks (32548) Tasks Instances (50411) box_in_cupboard box_out_of_opened_drawer close_drawer put_in_opened_drawer sweep_to_dustpan box_out_of_cupboard broom_out_of_cupboard open_drawer rubbish_in_dustpan take_out_of_ opened_drawer box_exchange put_in_and_close put_in_without_close put_two_in_different put_two_in_same retrieve_and_sweep sweep_and_drop take_out_and_close take_out_without_close take_two_out_of_different take_two_out_of_same transfer_box L1 Recipe: 20 demos per A task Total Number of Trajectories: 200 L2 Recipe: L1 + 1 demo per AP task Total Number of Trajectories: 304 L3 Recipe: L2 + 5 demos per C task Total Number of Trajectories: 324 L4 Recipe: L3 + 1 demo per CP task Total Number of Trajectories: 368 (a) HiMan-Bench Task Distribution (b) Multi-Level Training Dataset Design in HiMan-Bench Figure 2: This figure illustrates the key design of HiMan-Bench, including (1) HiMan-Bench task distribu- tion, and (2) multi-level training dataset design in HiMan-Bench. in long-horizon compositional tasks. As shown in Table 1, RoboHiMan uniquely assesses composi- tional generalization under perturbations, together with robustness, skill composition, progressive training, and hierarchical evaluation. Vision-Language-Action Models for Long-Horizon Manipulation. In recent years, vi- sion–language–action (VLA) models have become a promising paradigm in robotic manipulation, processing visual and language inputs for action generation (Shao et al., 2025; Zhou et al., 2025; Ma et al., 2024). Foundation-style pretraining, as in RT-1 (Brohan et al., 2022), RT-2 (Zitkovich et al., 2023), OpenVLA (Kim et al., 2024), and π0 (Black et al., 2024), improves generalization by learning from large-scale trajectories. In contrast, methods such as PerAct (Shridhar et al., 2023), RVT (Goyal et al., 2023), RVT-2 (Goyal et al., 2024), and 3D Diffuser Actor (Ke et al., 2024) leverage 3D representations to achieve fine-grained action prediction. These approaches, however, often lack the explicit task-planning capabilities.", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "5d18b9aeb5eea3ec3de7cce7a9243d20551c4495e147b51303db3555f2c11d55"}
{"doc_id": "arxiv:2510.13149#evaluation:part-3", "url": "https://arxiv.org/abs/2510.13149", "anchor": "#evaluation:part-3", "type": "paper", "title": "", "section": "Evaluation", "text": "et al., 2024). Foundation-style pretraining, as in RT-1 (Brohan et al., 2022), RT-2 (Zitkovich et al., 2023), OpenVLA (Kim et al., 2024), and π0 (Black et al., 2024), improves generalization by learning from large-scale trajectories. In contrast, methods such as PerAct (Shridhar et al., 2023), RVT (Goyal et al., 2023), RVT-2 (Goyal et al., 2024), and 3D Diffuser Actor (Ke et al., 2024) leverage 3D representations to achieve fine-grained action prediction. These approaches, however, often lack the explicit task-planning capabilities. To tackle long-horizon manipulation tasks, many works (Wen et al., 2024; Chen et al., 2025d; Shi et al., 2025; Wen et al., 2025; Gao et al., 2025) adopt hierarchical designs, where a foundation model decomposes instructions into sub-tasks that low-level policies execute as actions. Yet these methods face two bottlenecks: (1) reliance on com- plex prompt engineering and handcrafted pipelines, limiting scalability; and (2) error accumulation when low-level policies fail to reliably follow high-level plans (Han et al., 2025). In HiMan-Bench, we use natural language as the interface and evaluate both rule-based and VLM-based high-level planners paired with low-level policies (Goyal et al., 2024; Ke et al., 2024; Black et al., 2024; 2025) to systematically analyze the challenges faced at each hierarchy in complex long-horizon tasks. 3 ROBOHIMAN In this section, we present RoboHiMan, a hierarchical evaluation paradigm for studying com- positional generalization in long-horizon manipulation under perturbations. Sec. 3.1 introduces HiMan-Bench, a benchmark comprising both atomic and compositional tasks with diverse perturba- tions, along with a progressive multi-level training dataset spanning atomic to compositional skills. Sec. 3.2 outlines the hierarchical evaluation paradigm, which includes three modes: vanilla, de- coupled, and coupled. Together, these components form a unified framework for analyzing model performance in compositional long-horizon manipulation. 3.1 HIMAN-BENCH Task and Perturbation Factors Design. We construct HiMan-Bench following the task design paradigm of RLBench (James et al., 2020), implemented with the PyRep (James et al., 2019) API atop the CoppeliaSim (Rohmer et al., 2013) simulator. Building on the 10 atomic tasks and 12 com- positional tasks provided by DeCoBench (Chen et al., 2025f), we leverage the Colosseum (Pumacay et al., 2024) API to extend the task set, ultimately constructing the HiMan-Bench distribution 4 Preprint. Under review. comprising 114 atomic tasks and 144 compositional tasks. Each atomic task consists of two sub- stages, segmented by discrete robot-state changes to capture fundamental manipulator-object inter- actions (James & Davison, 2022; Chen et al., 2025f;d), which are further composed into multi-stage tasks. Some tasks require cross-domain transfer (e.g., from drawer to cupboard manipulation). Even with atomic skills mastered, models must still correctly schedule and compose them to follow long- horizon language instructions such as “take the strawberry jello out of the drawer and put it into the cupboard”, highlighting challenges in long-horizon planning, cross-domain generalization, and robust skill composition. Specifically, to systematically investigate the role of skill composition in long-horizon tasks and its robustness under environmental perturbations, we adopt 12 perturbation factors introduced in Colos- seum (Pumacay et al., 2024): manipulation object color (MO Color), texture (MO Texture), and size (MO Size); receiver object color (RO", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "f7382cd3effa88d690a1de8a3f3f4a894b10881602bfb6a1797b69f7c1bfb641"}
{"doc_id": "arxiv:2510.13149#evaluation:part-4", "url": "https://arxiv.org/abs/2510.13149", "anchor": "#evaluation:part-4", "type": "paper", "title": "", "section": "Evaluation", "text": "to follow long- horizon language instructions such as “take the strawberry jello out of the drawer and put it into the cupboard”, highlighting challenges in long-horizon planning, cross-domain generalization, and robust skill composition. Specifically, to systematically investigate the role of skill composition in long-horizon tasks and its robustness under environmental perturbations, we adopt 12 perturbation factors introduced in Colos- seum (Pumacay et al., 2024): manipulation object color (MO Color), texture (MO Texture), and size (MO Size); receiver object color (RO Color), texture (RO Texture), and size (RO Size); light color (Light Color); table color (Table Color) and texture (Table Texture); dis- tractor objects (Distractor); background texture (Background Texture); and camera pose (Camera Pose). The perturbation space covers 20 colors, 213 textures, and 78 distractor objects sampled from the YCB Object Dataset (C¸ alli et al., 2015). Object size scaling ranges depend on the specific task (e.g., cupboard [0.75, 1.15], drawer [0.9, 1.15]). Lighting perturbations are applied by sampling RGB values from [0.0, 0.0, 0.0] to [0.5, 0.5, 0.5], while camera perturbations are applied to three viewpoints (front, left shoulder, right shoulder) with position offsets in [−0.1, −0.1, −0.1] to [0.1, 0.1, 0.1] and Euler-angle perturbations in [−0.05, −0.05, −0.05] to [0.05, 0.05, 0.05]. This de- sign aligns with existing benchmarks while extending evaluation to more challenging tasks, thereby enabling systematic assessment of robustness and generalization. Fig. 2(a) illustrates the distribu- tion of atomic and compositional task instances across different perturbation factors and variants in HiMan-Bench. For clarity, HiMan-Bench organizes tasks into four categories: atomic (A)- 10 tasks, atomic with perturbations (AP)-104 tasks, compositional (C)-12 tasks, and compo- sitional with perturbations (CP)-132 tasks. Additional implementation details are provided in Appendix B. Multi-level Training Dataset Design. HiMan-Bench proposes a hierarchical training data design to systematically investigate the impact of different data “recipes” on generalization performance. This design covers configurations ranging from the most challenging to the most comprehensive, strictly following a progressive order from difficult to easy (from difficult to easy). The construction details of each layer (data recipes and the number of expert demonstrations) are summarized in Fig. 2(b). L1: Contains demonstrations of A tasks, with 20 demonstrations for each task. This is the most challenging setting. If a model trained on this dataset performs well on compositional tasks, it indicates strong compositional generalization ability. L2: Builds upon L1 by introducing AP tasks, with 1 demonstration per AP task. The goal is to improve robustness across diverse variants, which is crucial for reducing error accumulation in long-horizon tasks. L3: Extends L2 by including demonstrations of 4 C tasks ( put in without close, sweep and drop, take out without close, and transfer box), with 5 demonstrations per task. This allows the model to directly observe part of the multi-step compositional processes. L4: Further extends L3 by introducing CP tasks for the 4 C tasks, with 1 demonstration per CP task. This exposes the model to more compositional scenarios. 3.2 HIERARCHICAL EVALUATION PARADIGM RoboHiMan employs a hierarchical evaluation paradigm for different models: a high-level planner first decomposes the instruction into subtasks relevant to the", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "7c33d2d98c306daa81289655b77970bbc31b22e08247e2990effc94af4842140"}
{"doc_id": "arxiv:2510.13149#evaluation:part-5", "url": "https://arxiv.org/abs/2510.13149", "anchor": "#evaluation:part-5", "type": "paper", "title": "", "section": "Evaluation", "text": "sweep and drop, take out without close, and transfer box), with 5 demonstrations per task. This allows the model to directly observe part of the multi-step compositional processes. L4: Further extends L3 by introducing CP tasks for the 4 C tasks, with 1 demonstration per CP task. This exposes the model to more compositional scenarios. 3.2 HIERARCHICAL EVALUATION PARADIGM RoboHiMan employs a hierarchical evaluation paradigm for different models: a high-level planner first decomposes the instruction into subtasks relevant to the current stage, while a low-level policy executes these subtasks by generating robot actions. Formally, both the planner and the policy take as input a natural language instruction l and a visual observation o, where o can be either multi-view 2D signals (e.g., RGB images) or 3D representations (e.g., point clouds). The planner then produces a subtask description s, which the low-level policy translates into a sequence of robot actions {a1:T }. For comparison, we also consider a non-hierarchical baseline, in which the planner is omitted and the low-level policy directly maps (l, o) to {a1:T }. This contrast enables explicit evaluation of the contribution of hierarchical design to generalization in long-horizon compositional tasks. Evaluation Paradigm. As illustrated in Fig. 3, the RoboHiMan evaluation paradigm is organized into three settings: 1) Vanilla (Non-hierarchical, Online). The low-level policy executes the entire task online directly from the original instruction without planner, serving as a non-hierarchical base- line. 2) Decoupled (Hierarchical, Planner and Policy Evaluated Separately). This paradigm 5 Preprint. Under review. Gripper closed: grasping the handle Open Drawer Box out of Drawer Box in Cupboard Select next subtask from human annotations Plan Rule-based Planner Atomic Skill 1 Atomic Skill 2 Atomic Skill 3 - Decoupled Evaluation: [Online] Low-level policy evaluation in Rule-based planner modes [Offline] VLM–based planner only (VQA-style subtask prediction) VLM-inferred next subtask VLM-Based Planner Policy acts autonomously w/o any planner Vanilla Mode Action VLM- Based Planner A AP C CP Offline Dataset 1. Select one trajectory and sample every N steps 2. Cast the task as a VQA problem pick up the strawberry jello in the opened bottom drawer 4. Compare with ground truth DeCo-Style Task Decomposition and Composition Take the strawberry jello out of the bottom drawer and put the strawberry jello in the cupboard Instruction Observation", "source": "arxiv_pdf", "published": "", "tokens": 377, "sha256": "941eda1dc865274b48517b87c67e2e1e62120c1a3bc3dab7189fba3ceee46888"}
{"doc_id": "arxiv:2510.13149#experiments:part-1", "url": "https://arxiv.org/abs/2510.13149", "anchor": "#experiments:part-1", "type": "paper", "title": "", "section": "EXPERIMENTS", "text": "We conduct experiments to address the questions below: Q1: Skill Composition Without Planning (Sec. 4.2). Can models without a planner reliably combine atomic skills to solve tasks? Q2: Scaling Effects of Training Data (Sec. 4.3). How does performance change as training data expands from atomic to compositional tasks with perturbations? Q3: Sensitivity to Perturbations (Sec. 4.4). Which perturbations most hinder skill composition, and how do models handle them? Q4: Gen- eralization to Unseen Compositions (Sec. 4.5). Can models generalize to new task compositions beyond those seen in training? Q5: Bottlenecks in Hierarchical Architectures (Sec. 4.6). Do fail- ures stem from flawed planning, weak execution, or both? Q6: Real-World Validation (Sec. 4.7). Do real-world tasks face similar challenges, and can hierarchical architectures help? 4.1 EXPERIMENTAL SETUP All simulation experiments are conducted on the proposed HiMan-Bench tasks introduced in Sec. 3.1, while the detailed setup of the real-world experiments is provided in the Appendix D. High-Level Planner. We adopt Qwen2.5-VL (Bai et al., 2025) as the vision-language model (VLM) backbone for the high-level planner. The training process is based on frames sampled at fixed intervals from demonstration data, using the current frame’s visual observation and the full task instruction as input, and the corresponding subtask description as output. The model is fine-tuned on the HiMan-Bench dataset, with training and inference prompts detailed in Appendix C.1. Low-Level Policy. We select four state-of-the-art VLA models (RVT-2 (Goyal et al., 2024), 3D Diffuser Actor (Ke et al., 2024), π0 (Black et al., 2024), and π0.5 (Black et al., 2025)) as low-level 6 Preprint. Under review. policies. For each baseline model, we follow the original policy design but modify the handling of language inputs. Since language plays a crucial role in distinguishing stages and guiding skill composition, we provide stage-specific instructions at different execution points. The input-output formats of each baseline model are summarized in Table 3, and the implementation details are provided in Appendix C.1. Evaluation Metric. For atomic tasks, we generate 720 episodes for evaluation, and for composi- tional tasks, 900 episodes. For each task, we include 15 episodes without perturbations (denoted as None) and 5 episodes for each perturbation factor, plus 5 episodes with all perturbations en- abled (denoted as All). During online evaluation, the environment is configured to match these test episodes. Because of variations in object placement or workspace sampling, some offline episodes are not fully reproducible online. We therefore report results only on valid episodes, which account for about 90% of the total. Performance is reported as the average success rate over atomic tasks (A), perturbed atomic tasks (AP), compositional tasks (C), and perturbed compositional tasks (CP). For offline evaluation, we measure only the high-level planner’s subtask prediction accuracy. Frames are sampled every 10 steps for atomic tasks and every 30 steps for compositional tasks, and each sampled frame is modeled as a VQA instance to evaluate planning accuracy. 4.2 SKILL COMPOSITION WITHOUT PLANNING To address Q1, we train four baseline VLA models on HiMan-Bench’s multi-level datasets and eval- uate them on diverse test sets. In Fig. 1(c), models without a planner", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "3433ca7ad8854ec08a7ccc46edb9d0460c88969f91293373302b2c4baca19091"}
{"doc_id": "arxiv:2510.13149#experiments:part-2", "url": "https://arxiv.org/abs/2510.13149", "anchor": "#experiments:part-2", "type": "paper", "title": "", "section": "EXPERIMENTS", "text": "and perturbed compositional tasks (CP). For offline evaluation, we measure only the high-level planner’s subtask prediction accuracy. Frames are sampled every 10 steps for atomic tasks and every 30 steps for compositional tasks, and each sampled frame is modeled as a VQA instance to evaluate planning accuracy. 4.2 SKILL COMPOSITION WITHOUT PLANNING To address Q1, we train four baseline VLA models on HiMan-Bench’s multi-level datasets and eval- uate them on diverse test sets. In Fig. 1(c), models without a planner (Vanilla) show marginal gains from richer data, and their performance on both compositional (C) and perturbed compositional (CP) tasks remains near zero, indicating that models without a planner cannot compose atomic skills into coherent long-horizon behaviors. In contrast, rule-based planner (RP) variants achieve substantial improvements, especially with compositional (L3) and perturbed (L4) training data. Finding: (i) Data diversity and scale offer limited benefits for Vanilla models, slightly improving robustness but failing to enable skill composition. (ii) Explicit planning is essential, as it supports robust skill composition and underscores the role of hierarchical reasoning in complex long- horizon tasks. 4.3 SCALING EFFECTS OF TRAINING DATA Figure 4: Performance scaling curves on atomic and composi- tional tasks under different scaling levels. Regarding Q2, the results in Fig. 4 il- lustrate the scaling effects of multi- level training data under a rule-based planner. For evaluations on atomic tasks (A&AP), all models show an upward trend in performance from L1 to L2. This indicates that adding ex- pert trajectories of atomic skills un- der perturbations can indeed enhance model robustness. However, after in- corporating compositional skill data, the performance on atomic tasks improves only marginally and may even degrade in some cases. In contrast, for all compositional tasks (C&CP), models trained only with atomic-level data (L1, L2) fail to generalize to compositional settings. Although π0-RP, π0.5-RP, and RVT2-RP show some improvement when trained with L3 and L4 data, the gains remain marginal. By comparison, 3D-Diffuser-Actor-RP benefits from L2 training with modest generaliza- tion gains, but its performance drops at L3 and improves again at L4. For compositional tasks, even with multi-level data including perturbations and compositional demonstrations, the generalization ability of current models remains highly constrained, revealing a clear bottleneck. Finding: (i) Scaling atomic-skill data improves atomic performance and, as robust atomic exe- cution is a prerequisite, also benefits compositional tasks. (ii) Increasing both the quantity and diversity of compositional-skill training data further enhances model performance on composi- tional tasks. However, the overall success rate remains low, even though all compositional tasks can in principle be solved by combining the atomic skills the model has learned. 7 Preprint. Under review. 4.4 SENSITIVITY OF SKILL COMPOSITION TO PERTURBATIONS. All None MO Texture MO Color MO Size RO Size Light Color Table Color Background Texture Distractor Objects Table Texture Camera Pose RO Texture RO Color 0.1 0.2 0.3 0.4 0.5 0.6 0.7 L1 L2 L3 L4 (a) Perturbation effects across training data levels. None All MO Color RO Color RO Texture Light Color MO Texture MO Size Table Texture Distractor Objects RO Size Table Color Background Texture Camera", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "c810aa1cf3cf1ead7d7d8c323e0332e3d5705c9f0c7a3a29c502d56e3ce7da38"}
{"doc_id": "arxiv:2510.13149#experiments:part-3", "url": "https://arxiv.org/abs/2510.13149", "anchor": "#experiments:part-3", "type": "paper", "title": "", "section": "EXPERIMENTS", "text": "7 Preprint. Under review. 4.4 SENSITIVITY OF SKILL COMPOSITION TO PERTURBATIONS. All None MO Texture MO Color MO Size RO Size Light Color Table Color Background Texture Distractor Objects Table Texture Camera Pose RO Texture RO Color 0.1 0.2 0.3 0.4 0.5 0.6 0.7 L1 L2 L3 L4 (a) Perturbation effects across training data levels. None All MO Color RO Color RO Texture Light Color MO Texture MO Size Table Texture Distractor Objects RO Size Table Color Background Texture Camera Pose 0.1 0.2 0.3 0.4 0.5 0.6 0.7 RVT-2_L1 RVT-2_L2 3DDA_L1 3DDA_L2 0_L1 0_L2 0.5_L1 0.5_L2 (b) Comparison of models at L1–L2. None All RO Texture MO Color RO Color MO Texture MO Size RO Size Table Texture Distractor Objects Table Color Light Color Background Texture Camera Pose 0.1 0.2 0.3 0.4 0.5 0.6 0.7 RVT-2_L3 RVT-2_L4 3DDA_L3 3DDA_L4 0_L3 0_L4 0.5_L3 0.5_L4 (c) Comparison of models at L3–L4. Figure 5: Robustness under perturbations across different settings. To investigate Q3, we conduct robustness experiments to systematically analyze the effects of pertur- bations across different tasks and model scales. As shown in Fig. 5, model robustness exhibits con- sistent trends under various perturbation factors. Fig. 5(a) shows that skill composition is particularly sensitive to perturbations such as object color (MO Color, RO Color), texture (MO Texture, RO Texture), and all factors enabled (All). Models trained only on atomic data (L1, L2) display limited adaptability, whereas introducing even a small amount of perturbed data in compositional tasks (L3, L4) leads to improvements: the model not only learns skill composition but also becomes more robust to unseen variations in appearance, geometry, and viewpoint. Fig. 5(b) and Fig. 5(c) further compare different architectures. RVT-2 and 3D Diffuser-Actor consistently outperform the baseline policies π0 and π0.5 across all training scales, while π0.5 performs better than π0. Finding: (i) For compositional tasks with various perturbations, including perturbed compositional-skill data in training, effectively improves the model’s robustness in performing compositional tasks. In comparison, adding perturbed atomic-skill data alone provides only lim- ited gains in robustness. (ii) Both data design and architectural inductive biases (e.g., keyframe selection, 3D information integration) contribute to improved generalization and robustness. 4.5 COMPOSITIONAL GENERALIZATION TO UNSEEN TASKS Figure 6: Generalization performance on seen/unseen compositional tasks. To answer Q4, we evaluate the baseline VLA models after L4 training, where testing covers 12 compositional tasks and their perturbed versions (C&CP), among which 4 tasks were already seen in training (see Sec. 3.1 and Appendix C for details). As shown in Fig. 6, RVT2-RP and 3D Diffuser-Actor-RP achieve relatively low success rates even on the seen C&CP tasks, in- dicating that the models have not sufficiently mastered the cor- responding compositional skills. On the unseen tasks, the suc- cess rates remain similarly limited, further suggesting a lack of effective compositional generalization. In contrast, while π0 and π0.5 achieve moderate performance on seen tasks, they almost completely fail on unseen tasks, which further highlights their lack of generalization. Finding: Even with partial exposure to compositional skills during training, current models still show clear limitations in learning and utilizing skill", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "835cc44cf1eed5315da27b824e880188f6581a04f7bffca216941228b31a7799"}
{"doc_id": "arxiv:2510.13149#experiments:part-4", "url": "https://arxiv.org/abs/2510.13149", "anchor": "#experiments:part-4", "type": "paper", "title": "", "section": "EXPERIMENTS", "text": "in- dicating that the models have not sufficiently mastered the cor- responding compositional skills. On the unseen tasks, the suc- cess rates remain similarly limited, further suggesting a lack of effective compositional generalization. In contrast, while π0 and π0.5 achieve moderate performance on seen tasks, they almost completely fail on unseen tasks, which further highlights their lack of generalization. Finding: Even with partial exposure to compositional skills during training, current models still show clear limitations in learning and utilizing skill compositions, making it difficult to achieve true compositional generalization on unseen tasks. 4.6 BOTTLENECKS IN HIERARCHICAL ARCHITECTURES Regarding Q5, Table 2 reveals the core bottlenecks of hierarchical architectures. In offline evalua- tion, when the planner is trained solely on atomic skill data, its generalization to unseen composi- tional skills is clearly limited; even after introducing some compositional skills during training, the success rate improves but remains relatively low. In online evaluation, this issue is further amplified, and the impact of planning errors becomes more pronounced. Specifically, in online evaluation, both RVT2-RP and RVT2-VLM maintain strong performance on atomic tasks and their perturbed variants 8 Preprint. Under review. Atomic Task VLM: place the block in the bottom drawer ✓ GT: place the block in the bottom drawer Compositional Task GT: grasp the top drawer handle VLM: place the block in the bottom drawer ✗ Compositional Task GT: sweep dirt to dustpan VLM: drop the rubbish into the dustpan ✗ L1→A&AP L1→C&CP L2→A&AP L2→C&CP L3→A&AP L3→C&CP L4→A&AP L4→C&CP Qwen2.5VL-7B (Offline) 0.466 0.153 0.676 0.182 0.673 0.181 0.610 0.305 RVT2-RP (Online) 0.590 0.281 0.678 0.287 0.653 0.357 0.603 0.395 RVT2-VLM (Online) 0.351 0.000 0.369 0.002 0.432 0.000 0.316 0.013 Performance Drop (↓) 0.239 0.281 0.309 0.285 0.221 0.357 0.287 0.382 Table 2: Comparison of offline vs. online evaluation. Blue numbers show RVT2-VLM performance drops relative to RVT2-RP. (A&AP). However, on compositional tasks and perturbed compositional tasks (C&CP), RVT2-VLM shows a significant performance drop compared to RVT2-RP. In particular, in L4→C&CP setting, its success rate decreases by 0.382, revealing marked vulnerability. Further analysis indicates that performance degradation in complex tasks stems from the compounded effects of planning failures and low-level policy execution issues. The VLM-based high-level planner fails to fully leverage the capabilities of the low-level policy when dealing with long-horizon compositional tasks. Finding: The bottlenecks of hierarchical architectures stem from three main issues: (i) the high- level planner may generate incorrect plans. (ii) the low-level policy may fail during execution. (iii) if the hierarchical system is not properly designed, failures at the high or low level are not effectively handled, leading to error accumulation and eventual task failure. 4.7 REAL-WORLD VALIDATION Diverse Perturbations --> Fail Rule-Based Planner (Success) Vanilla (Fail) Figure 7: Real World Experimental Results. To answer Q6, we conduct real-world validation experiments (Appendix D). We designed a small set of atomic skills, their long-horizon compositions, and diverse perturbations (e.g., distractors, object position changes, and human interventions). In Fig. 7, end-to-end execution without a planner (π0.5-vanilla) achieved only 17.5% success on compositional tasks, dropping further to 10.0% under perturbations. In contrast, pairing the same", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "f8d882463219f7c7883083e1c13d8c1b41795889f2a8fbfcb5c4dceec862184b"}
{"doc_id": "arxiv:2510.13149#experiments:part-5", "url": "https://arxiv.org/abs/2510.13149", "anchor": "#experiments:part-5", "type": "paper", "title": "", "section": "EXPERIMENTS", "text": "eventual task failure. 4.7 REAL-WORLD VALIDATION Diverse Perturbations --> Fail Rule-Based Planner (Success) Vanilla (Fail) Figure 7: Real World Experimental Results. To answer Q6, we conduct real-world validation experiments (Appendix D). We designed a small set of atomic skills, their long-horizon compositions, and diverse perturbations (e.g., distractors, object position changes, and human interventions). In Fig. 7, end-to-end execution without a planner (π0.5-vanilla) achieved only 17.5% success on compositional tasks, dropping further to 10.0% under perturbations. In contrast, pairing the same low-level policy with a rule-based planner(where the arm moves to the next human-annotated sub-instruction whenever it stays idle near the initial pose for a fixed duration), π0.5-RP substantially improved performance to 47.5% and 27.5%, respectively. These results confirm: (i) real-world long-horizon manipulation indeed faces compositional generalization challenges under perturbations, and (ii) hierarchical architectures demonstrate clear potential benefits, achieving higher performance when an idealized planner selects sub- instructions. 9 Preprint. Under review. 5", "source": "arxiv_pdf", "published": "", "tokens": 152, "sha256": "06b1c796af22a13950643c704d64c86cd949c93b16728653c636311d5b7242bd"}
{"doc_id": "arxiv:2510.13149#conclusion", "url": "https://arxiv.org/abs/2510.13149", "anchor": "#conclusion", "type": "paper", "title": "", "section": "CONCLUSION", "text": "In this work, we propose RoboHiMan, a hierarchical evaluation paradigm for studying composi- tional generalization under perturbations in long-horizon manipulation. RoboHiMan first intro- duces a novel benchmark, HiMan-Bench, which evaluates the ability of different VLA models to compose atomic skills into long-horizon behaviors under diverse perturbations. In addition, we design three evaluation paradigms that can effectively disentangle the sources of planning and exe- cution failures across progressively expanded training settings. Based on extensive experiments, we draw the following key conclusions: (1) Compositional skill learning is intrinsically challenging, and simply scaling up data cannot fundamentally solve this problem; (2) Model robustness to per- turbations is as critical as compositionality itself; (3) Hierarchical systems require stronger feedback mechanisms to achieve effective coordination between planning and execution. Looking forward, RoboHiMan opens up promising research directions for the robotics and VLA communities. Future work should explore robust skill composition mechanisms, feedback-rich hierarchical architectures, and perturbation-aware training recipe that improve robustness un- der distribution shifts. Equally important is the development of scalable compositional datasets and new evaluation metrics that go beyond task success to capture error recovery and skill reusabil- ity. By providing both a challenging benchmark and an analytical framework, RoboHiMan aims to accelerate progress toward building generalizable robotic agents capable of reliable long-horizon manipulation in realistic environments.", "source": "arxiv_pdf", "published": "", "tokens": 214, "sha256": "f00911033b369b3130eb62630fe26e171256d1960445dc42a01884cc806cbf71"}
{"doc_id": "arxiv:2510.13599#method:part-1", "url": "https://arxiv.org/abs/2510.13599", "anchor": "#method:part-1", "type": "paper", "title": "", "section": "Method", "text": "Per-Scan Time↓File size↓Num of Faces↓Num of Vertices↓Mean↓ Std↓ Precision↑Recall↑F-Score↑ (s) (MB) (m) (m) Christ Church 03 (∼307 m) VDBFusion 0.871 53.6 1,992,391 1,152,788 0.044 0.077 0.918 0.970 0.943 ImMesh 0.724 370.9 21,180,823 7,959,789 0.090 0.186 0.820 0.990 0.897 PlanarMesh (Ours) 0.392 10.1 398,712 411,907 0.037 0.081 0.951 0.964 0.957 OctoMap 0.432 3.4 N/A N/A 0.040 0.083 0.943 0.991 0.966 Keble College 03 (∼108 m) VDBFusion 0.968 51.3 1,821,087 1,150,250 0.033 0.113 0.962 0.940 0.951 ImMesh 0.355 163.8 9,057,110 3,838,040 0.035 0.064 0.955 0.918 0.936 PlanarMesh (Ours) 0.416 7.3 287,020 296,254 0.031 0.134 0.979 0.894 0.935 OctoMap 0.328 24.3 N/A N/A 0.040 0.159 0.964 0.966 0.965 Observatory 01 (∼324 m) VDBFusion 2.406 148.5 5,246,193 3,346,024 0.047 0.104 0.899 0.899 0.899 ImMesh 0.448 424.7 23,448,665 9,986,250 0.056 0.089 0.878 0.832 0.854 PlanarMesh (Ours) 0.213 15.3 546,415 682,725 0.042 0.114 0.929 0.847 0.886 OctoMap 0.659 67.8 N/A N/A 0.055 0.150 0.896 0.941 0.918 (0.84 -0.96), and F1 scores (0.91 -0.96) for all the sequences which is either better or comparable to other methods. Our approach is intentionally biased toward higher precision and lower recall, as we do not retain infinitesimally small faces, which are unlikely to represent meaningful structural elements in our target application of building modeling. C. File Footprint We recognize that the built environment is dominated by planes, and our approach leverages this to achieve a significant file size reduction as seen in file and mesh size columns in Tab. I. Our method produces triangular meshes with 280 -550 K faces, an order of magnitude lower than other methods (1.8 -23 M). Compared to the accumulated LiDAR point cloud (∼100 MB), our reconstruction yields a file size of approximately 10 MB, representing a roughly tenfold compression. Traditional mesh-based reconstructions often generate large files due to the storage of numerous small triangles, even in low-curvature regions. Whereas, our planar-mesh approach shares normals among faces within the same plane. By re- sampling vertices and re-triangulating on demand, we achieve significantly smaller file sizes. D. Timing Analysis This section validates the real-time capability of our approach and presents a breakdown of the computational time required by each module within our reconstruction pipeline. The processing time of each module is visualized as a stacked area plot (Fig. 7). On average, each scan is processed in around 0.4 sec, with peak processing times reaching 0.7 sec, resulting in an effective processing speed of about 2 Hz. This performance is on-par with other real- time reconstruction methods (Tab. I). If desired, a higher update frequency could be achieved by down-sampling the input LiDAR point cloud appropriately - a viable strategy for applications demanding lower latency. The FIS and RRS data structures are implemented as binary search trees, the complexity of their search operations is therefore O(log N), where N is the number of faces or boundary vertices, respectively. 0 25 50 75 100 125 150 175 200 Scan Number 0.0 0.2 0.4 0.6 0.8 1.0 Duration [s] Average Total Duration Mesh Update Overheads & Others RRS FIS Relative Position Check Fig. 7: Computation time for each module of PlanarMesh,", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "9d0430e16bd572a1d97a010114e28df35d372f7b1ea67431b18117ce726e0232"}
{"doc_id": "arxiv:2510.13599#method:part-2", "url": "https://arxiv.org/abs/2510.13599", "anchor": "#method:part-2", "type": "paper", "title": "", "section": "Method", "text": "strategy for applications demanding lower latency. The FIS and RRS data structures are implemented as binary search trees, the complexity of their search operations is therefore O(log N), where N is the number of faces or boundary vertices, respectively. 0 25 50 75 100 125 150 175 200 Scan Number 0.0 0.2 0.4 0.6 0.8 1.0 Duration [s] Average Total Duration Mesh Update Overheads & Others RRS FIS Relative Position Check Fig. 7: Computation time for each module of PlanarMesh, visualized as a stacked area plot for the ChristChurch03 sequence. E. Ablation Studies TABLE II: Ablation Study on ChristChurch03 Dataset while varying the keep seed planar-meshes duration. Num Time↓File size↓Mean↓ Std↓ Precision↑Recall↑F-Score↑ (s) (MB) (m) (m) 0 0.295 5.9 0.037 0.075 0.946 0.889 0.917 1 0.347 7.9 0.036 0.076 0.951 0.946 0.948 10 0.384 8.3 0.036 0.076 0.951 0.951 0.951 All 0.313 8.6 0.036 0.078 0.951 0.953 0.952 We present the results of an ablation study on the retention duration of seed planar-meshes. Seed planar-meshes typically contain few points or have small surface areas, often arising from outlier points. However, they can also result from limited observations of complex geometries that have not yet been sufficiently scanned. In our implementation, we keep the seed planar-meshes throughout the duration of mission and do not discard them. By increasing the duration seed planar- meshes are kept, we enhance the likelihood of identifying and developing complex geometry, thereby improving the overall recall rate. However, this comes at the cost of longer processing times and larger file sizes, as demonstrated in Tab. II. By reducing the retention duration of the seed meshes from 10 scans to 1 scan, we are able to reduce the processing time from 0.384 to 0.295 sec and reduce the file size 8.6 to 5.9 MB. Notably, keeping all scans results in lower processing times by eliminating the overhead caused by the removal process, providing an extra benefit if file size is not prioritized. This trade-off can be achieved without any degradation in the precision. V. CONCLUSIONS In this work we introduced PlanarMesh, a novel real-time adaptive resolution mesh reconstruction system for 3D LiDAR data. By leveraging free-space information and introducing the Reverse Radius Search (RRS) for local curvature estimation, our method dynamically adjusts mesh resolution to capture both large-scale structures and fine detail efficiently. The system operates incrementally using multi-threaded BVH- based search operations, achieving real-time performance at approximately 2 Hz while significantly reducing memory footprint—producing mesh files up to 10 times smaller than raw input data. Our experimental evaluation demonstrates that PlanarMesh achieves accuracy on par with, or exceeding state-of-the-art methods, with output file size more than 5 times smaller, making it ideal for scalable 3D mapping in robotics and mobile scanning applications. At present, the reconstruction system uses continually more memory until it exceeds available memory at about 300 scans (about 300 m at 1 m/sec walking). In future work we aim to introduce a submapping mechanism to offload older mesh components and to achieve bounded memory usage. Additionally, we aim to incorporate loop closure for improved consistency in", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "21a861238dffbaf7851939d372bf8ebd19bbaf31dc6c72ab16197426112206a4"}
{"doc_id": "arxiv:2510.13599#method:part-3", "url": "https://arxiv.org/abs/2510.13599", "anchor": "#method:part-3", "type": "paper", "title": "", "section": "Method", "text": "methods, with output file size more than 5 times smaller, making it ideal for scalable 3D mapping in robotics and mobile scanning applications. At present, the reconstruction system uses continually more memory until it exceeds available memory at about 300 scans (about 300 m at 1 m/sec walking). In future work we aim to introduce a submapping mechanism to offload older mesh components and to achieve bounded memory usage. Additionally, we aim to incorporate loop closure for improved consistency in large-scale mapping and further explore mesh hole filling and edge smoothing algorithms.", "source": "arxiv_pdf", "published": "", "tokens": 92, "sha256": "baed8dd7f96c4cdb6600e83a38a0b0e889caf34c60ac3b5843020fa95b7a5365"}
{"doc_id": "arxiv:2510.13358#abstract", "url": "https://arxiv.org/abs/2510.13358", "anchor": "#abstract", "type": "paper", "title": "", "section": "ABSTRACT", "text": "Offline reinforcement learning enables sample-efficient policy acquisition without risky online interaction, yet policies trained on static datasets remain brittle under action-space perturbations such as actuator faults. This study introduces an offline-to-online framework that trains policies on clean data and then performs adversarial fine-tuning, where perturbations are injected into executed actions to induce compensatory behavior and improve resilience. A performance-aware curriculum further adjusts the perturbation probability during training via an exponential-moving-average signal, balancing robustness and stability throughout the learning process. Experiments on continuous- control locomotion tasks demonstrate that the proposed method consistently improves robustness over offline-only baselines and converges faster than training from scratch. Matching the fine-tuning and evaluation conditions yields the strongest robustness to action-space perturbations, while the adaptive curriculum strategy mitigates the degradation of nominal performance observed with the linear curriculum strategy. Overall, the results show that adversarial fine-tuning enables adaptive and robust control under uncertain environments, bridging the gap between offline efficiency and online adaptability. Keywords Offline reinforcement learning · Offline-to-online reinforcement learning · Robot control · Robust control 1", "source": "arxiv_pdf", "published": "", "tokens": 172, "sha256": "509b7f10db36cb16b3089d249f744f126663279a00065c72f54d7dd4f6879171"}
{"doc_id": "arxiv:2510.13358#introduction:part-1", "url": "https://arxiv.org/abs/2510.13358", "anchor": "#introduction:part-1", "type": "paper", "title": "", "section": "Introduction", "text": "Offline reinforcement learning (offline RL) [1] trains policies from fixed datasets without direct interaction with the environment. By avoiding online sampling, offline RL reduces data collection costs and potential safety risks. This benefit is particularly important in safety-critical domains such as healthcare [2], energy management [3], and robot control [4, 5]. However, standard RL algorithms often fail in offline settings due to overestimation and error accumulation in Q-values for out-of-distribution state-action pairs [6]. Conservative methods address this issue by constraining the learned policy to align with the behavior policy in the dataset [6–9], which improves stability [5] but limits adaptability. Thus, robustness to unexpected disturbances not covered in fixed datasets remains a key requirement for real-world deployment. Our key insight is that conservative offline RL and action-space robustness are fundamentally incompatible. Conservative methods constrain policies to dataset actions to prevent extrapolation errors, yet robustness to action perturbations requires learning from the out-of-distribution samples that these constraints prohibit. This incompatibility is illustrated in Figure 1. In the left panel, a conservatively trained policy produces an action distribution (blue) that aligns with the offline dataset (dashed) under normal conditions. However, when action perturbations occur, the executed actions shift into out-of-distribution regions (red) where the policy fails. The right panel illustrates that if the policy could learn to produce compensatory actions that keep the executed action distribution consistent across both normal and perturbed conditions, robustness would be achieved (blue and red overlap). Building on this insight, we formalize adversarial fine-tuning within an offline-to-online framework [10–13]. We first pretrain a policy on clean data for sample efficiency. Then, during online fine-tuning, we deliberately inject adversarial arXiv:2510.13358v1 [cs.RO] 15 Oct 2025 S. Ayabe et al. Figure 1: Incompatibility between conservative offline RL and action-space robustness. Left: A conservatively trained policy matches the dataset (dashed line) under normal conditions (blue) but fails under perturbations (red). Right: Robustness requires compensatory actions that maintain consistent distributions across both conditions (blue and red overlap). perturbations, enabling the policy to learn compensatory behaviors for perturbed conditions. Furthermore, we introduce a curriculum learning mechanism that gradually increases the perturbation probability based on training progress or policy performance. This curriculum balances learning difficulty and stability, preventing catastrophic performance drops while guiding the policy toward robustness in both normal and perturbed scenarios. However, existing work on robustness in offline RL has focused primarily on distributional shifts in the state-space, leaving this fundamental incompatibility largely unaddressed. Most prior work addresses state perturbations (e.g., sensor noise) through smoothness penalties and output stabilization [14–16], which remain compatible with conservative constraints. In contrast, action perturbations (e.g., actuator faults) have been studied far less. These disturbances directly modify executed actions, and recent work shows that offline RL is particularly vulnerable to such perturbations [17], as datasets typically lack perturbation-aware transitions while conservative constraints prevent necessary exploration. To validate our approach, we conduct experiments in legged robot environments from OpenAI Gym [18], where either random or adversarial perturbations are added to joint torque commands to simulate actuator faults. The proposed method is compared with two baselines: offline-only training and fully online", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "ce2f232d6a253281340dffff8af8606783aaf2becb8e221feb5cccae20496d80"}
{"doc_id": "arxiv:2510.13358#introduction:part-2", "url": "https://arxiv.org/abs/2510.13358", "anchor": "#introduction:part-2", "type": "paper", "title": "", "section": "Introduction", "text": "been studied far less. These disturbances directly modify executed actions, and recent work shows that offline RL is particularly vulnerable to such perturbations [17], as datasets typically lack perturbation-aware transitions while conservative constraints prevent necessary exploration. To validate our approach, we conduct experiments in legged robot environments from OpenAI Gym [18], where either random or adversarial perturbations are added to joint torque commands to simulate actuator faults. The proposed method is compared with two baselines: offline-only training and fully online training, and it consistently achieves superior robustness across all environments. In addition, a comparison of linear and adaptive schedules shows that the adaptive approach maintains robustness while preserving training stability. The contributions of this work are summarized as follows: • We propose an adversarial fine-tuning method that injects perturbations during online training. This enables targeted adaptation to action perturbations while preserving the sample efficiency of offline pretraining. • We demonstrate that adversarial fine-tuning consistently outperforms offline-only and fully online baselines in terms of robustness. • We propose an adaptive curriculum that adjusts perturbation probability based on policy performance. This prevents overfitting to adversarial conditions while maintaining training stability, addressing a key limitation of fixed-schedule approaches. 2", "source": "arxiv_pdf", "published": "", "tokens": 196, "sha256": "f95e34d4288c3a2ef8eb97baf0c89a835f065eaf112e45a10782f9c4476c48fa"}
{"doc_id": "arxiv:2510.13358#related-work:part-1", "url": "https://arxiv.org/abs/2510.13358", "anchor": "#related-work:part-1", "type": "paper", "title": "", "section": "Related Work", "text": "2.1 Offline RL Offline RL is a framework that trains policies using pre-collected datasets without interaction with the environment. In the lack of exploration, directly applying conventional online RL algorithms in this setting often leads to overestimation of Q-values for out-of-distribution state-action pairs, which may cause unstable learning [5]. To address this issue, conservative approaches have been developed to keep the learned policy close to the data distribution [6–9,19]. For instance, TD3+BC [8] enhances offline training efficiency by incorporating a simple behavior cloning loss into the actor update of TD3 [20]. These algorithms provide a foundation for offline policy training but remain limited by their reliance on fixed datasets. 2 S. Ayabe et al. 2.2 Robust RL In reinforcement learning for robot control, robustness is crucial for handling uncertainties such as sensor noise, matched physical parameter, unexpected terrain variations, or mechanical failures [21–25]. A common approach is to introduce perturbations during training to improve policy generalization across conditions. Domain randomization [26–28] improves adaptability by randomizing environment parameters such as friction, while adversarial training [23,29–32] improves robustness by exposing policies to adversarial perturbations. In offline RL, robustness has also been studied, mainly against state-space perturbations. RORL [14] improves robustness by smoothing the value distribution and applying conservative estimation, and Nguyen et al. [16] propose a defense mechanism based on adversarial perturbations with KL divergence. These methods demonstrate progress in improving robustness against state perturbations. In contrast, robustness against action-space perturbations remains less explored. Recent studies have applied an adversarial action-space perturbation generation method based on differential evolution [24] to offline-trained policies [17]. The results demonstrate that offline-trained policies are more vulnerable to such perturbations than online-trained policies. However, effective methods to improve robustness against such perturbations have not yet been established. The difficulty arises because conservative constraints in offline RL suppress out-of-distribution actions. As a result, adaptation is impossible unless perturbation-adapted actions and outcomes are already present in the dataset. Moreover, the lack of online interaction makes it impossible to acquire such data during training. These characteristics make robustness to action-space perturbations a particularly challenging problem, motivating the present study. 2.3 Offline-to-Online RL Offline-to-online RL has emerged as a promising framework to overcome the limitations of offline RL, which is highly dependent on dataset quality and lacks adaptability to unseen disturbances [10–13,33,34]. This framework first pretrains a policy on fixed datasets and then fine-tunes it online through direct interaction with the environment. By combining the efficiency of offline pretraining with the adaptability of online training, offline-to-online RL provides a way to address action-space perturbations that cannot be handled in purely offline settings. A major challenge in offline-to-online RL is performance degradation in the early phase of fine-tuning, caused by distributional shifts between offline data and newly collected online experiences [10,11]. Various approaches have been proposed to mitigate this issue. Representative examples include adaptive or relaxed policy constraints [10,12,35], data selection methods that prioritize near-on-policy samples [11], dynamic switching among multiple policies depending on the state [13,34], and exploration strategies combined with meta-adaptation [33]. This study leverages offline-to-online RL to improve robustness against action-space", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "26a8d1aa64dbd745c3903e740b353fccc0eb2fe9a2405daa161fde6215309d08"}
{"doc_id": "arxiv:2510.13358#related-work:part-2", "url": "https://arxiv.org/abs/2510.13358", "anchor": "#related-work:part-2", "type": "paper", "title": "", "section": "Related Work", "text": "major challenge in offline-to-online RL is performance degradation in the early phase of fine-tuning, caused by distributional shifts between offline data and newly collected online experiences [10,11]. Various approaches have been proposed to mitigate this issue. Representative examples include adaptive or relaxed policy constraints [10,12,35], data selection methods that prioritize near-on-policy samples [11], dynamic switching among multiple policies depending on the state [13,34], and exploration strategies combined with meta-adaptation [33]. This study leverages offline-to-online RL to improve robustness against action-space perturbations. Such perturbations represent discrepancies between training and deployment environments. To address this challenge, the policy is first pretrained offline and then fine-tuned in an online environment where perturbations are introduced. This approach enables policies to acquire robustness to action-space perturbations adaptively, which is difficult to achieve in purely offline settings. 3 Preliminaries This section provides background on reinforcement learning methods used in this study. We first introduce the formulation of offline reinforcement learning (RL) and discuss its limitations under action-space perturbations. We then describe the offline-to-online RL framework to address these limitations. 3.1 Offline RL Reinforcement learning (RL) is formulated as a Markov decision process (MDP), represented by the tuple (S, A, P, P0, r, γ). Here, S denotes the state space, A the action space, P(s′|s, a) the state transition probabil- ity, P0 the initial state distribution, r(s, a) the reward function, and γ the discount factor. The objective is to find an optimal policy π∗that maximizes the expected return. Offline RL trains policies using a fixed dataset D = {(st, at, st+1, rt)i} without further interaction with the envi- ronment. The dataset is usually collected from expert demonstrations, past policy rollouts, or random actions, and is assumed to follow a behavior policy πβ. A common difficulty in this setting is overestimation of Q-values for out-of- distribution state-action pairs. Without exploration, these errors remain uncorrected and lead to poor generalization. Conservative learning mitigates this problem by constraining the learned policy to remain close to the behavior policy. 3 S. Ayabe et al. The objective can be written as: π = arg max π Est∼D [Qπ(st, π(st))] s.t. D(π(·|st), πβ(·|st)) < ϵ, (1) where D denotes a divergence metric such as the KL divergence [5]. Such conservative designs, however, create vulnerabilities when perturbations affect the executed actions. In real systems, the executed action may deviate from the intended policy output due to disturbances or noise, which we denote as the perturbed action ˜at. Consequently, both the reward ˜rt = r(st, ˜at) and the next state ˜st+1 ∼P(· | st, ˜at) depend on ˜at. Offline datasets typically contain unperturbed actions a, making it impossible to assess whether policies remain effective once perturbations are applied. Collecting transitions with perturbation-adapted actions and outcomes is practically infeasible without online interaction. In addition, the conservative nature of offline RL may inadvertently reinforce actions that are fragile under perturbations. These limitations expose a structural vulnerability of offline RL to action-space perturbations, stemming from both the lack of robust data and the restrictive training paradigm. Therefore, new frameworks that can actively adapt to perturbations are required. 3.2 Offline-to-Online", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "864a65b30fac5f9ec2b52674a65ac46aebae53ec0f45dfcd3b36fe912d384c9b"}
{"doc_id": "arxiv:2510.13358#related-work:part-3", "url": "https://arxiv.org/abs/2510.13358", "anchor": "#related-work:part-3", "type": "paper", "title": "", "section": "Related Work", "text": "it impossible to assess whether policies remain effective once perturbations are applied. Collecting transitions with perturbation-adapted actions and outcomes is practically infeasible without online interaction. In addition, the conservative nature of offline RL may inadvertently reinforce actions that are fragile under perturbations. These limitations expose a structural vulnerability of offline RL to action-space perturbations, stemming from both the lack of robust data and the restrictive training paradigm. Therefore, new frameworks that can actively adapt to perturbations are required. 3.2 Offline-to-Online RL Offline-to-online RL [10,11] has emerged as a practical framework to address the limitations of offline RL. The process consists of two stages: (1) offline pretraining, where a policy is initialized using fixed datasets, and (2) online fine-tuning, where the policy continues to learn through interactions with the target environment. In this study, offline pretraining is performed with TD3+BC [8], a conservative actor–critic algorithm. The policy update rule is given by π = arg max π E(st,at)∼D h Qπ(st, π(st)) −∥π(st) −at∥2i , (2) where the second term enforces conservatism by penalizing deviation from the behavior policy. During online fine-tuning, the constraint is removed and the policy is updated using the standard TD3 rule [20]: π = arg max π Est∼R [Qπ(st, π(st))] , (3) where R denotes the replay buffer containing online experience. Given a transition (st, at, st+1, rt) from R, the target for the critics is yt = rt + γ min i∈{1,2} Qθ− i \u0010 st+1, πϕ−(st+1) + ε \u0011 , (4) where γ is the discount factor, Qθ− i and πϕ−denote the target critic and target policy, respectively, and ε ∼ clip(N(0, σ2), −c, c) is the target policy smoothing noise. The operator clip(x, a, b) restricts the value x to the interval [a, b]. The critics minimize the Bellman error: L(θi) = E \u0002 (Qθi(st, at) −yt)2\u0003 , i = 1, 2. (5) Some prior work [12,35] retains the TD3+BC penalty during fine-tuning. However, when perturbations are introduced, the effectiveness of stored actions under perturbed dynamics is not guaranteed. In such cases, maintaining the constraint may hinder adaptation. Therefore, we discard the penalty during online fine-tuning to allow more flexible policy updates. 4 Proposed Method This study addresses the problem of improving policy robustness against action-space perturbations in the offline RL setting. As discussed in Section 3.1, offline RL constrains policies to stay close to actions in the dataset, as shown in Equation (1). Thus, when the dataset lacks transitions with perturbed actions, it is fundamentally difficult to obtain a policy that adapts to perturbed environments. To solve this limitation, we adopt an offline-to-online approach, pretraining a policy on a clean offline dataset and fine-tuning it in online adversarial environments. 4.1 Problem Formulation We formulate the problem of robust policy training under action-space perturbations. The objective is to find an optimal policy π∗that maximizes the expected cumulative reward under adversarial perturbations: π∗= arg max π min ˜a∈U E \" ∞ X t=0 γtr(st, ˜a) # , (6) 4 S. Ayabe et al. Figure 2: Overview of the proposed framework with three stages: (1) Offline Pretraining, where", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "2179c42b14a4e80d29f38f32bf19542a4853dcf6a334b447d9110372770414a7"}
{"doc_id": "arxiv:2510.13358#related-work:part-4", "url": "https://arxiv.org/abs/2510.13358", "anchor": "#related-work:part-4", "type": "paper", "title": "", "section": "Related Work", "text": "a policy on a clean offline dataset and fine-tuning it in online adversarial environments. 4.1 Problem Formulation We formulate the problem of robust policy training under action-space perturbations. The objective is to find an optimal policy π∗that maximizes the expected cumulative reward under adversarial perturbations: π∗= arg max π min ˜a∈U E \" ∞ X t=0 γtr(st, ˜a) # , (6) 4 S. Ayabe et al. Figure 2: Overview of the proposed framework with three stages: (1) Offline Pretraining, where a policy πoff is trained on a clean dataset; (2) Adversarial Perturbation Generation, where perturbations {δa} are precomputed with πoff before fine-tuning; and (3) Adversarial Fine-Tuning, where πoff is refined in perturbed environments using these perturbations and replay buffer updates. where ˜a is an adversarially perturbed action chosen from a predefined set U. In our approach, the initial policy for this optimization is provided by offline training on a clean dataset, which improves training efficiency during online fine-tuning. We then approximate the intractable min–max objective within the TD3- based actor–critic framework, described in Section 3.1. In this framework, the collected transitions (st, at, ˜rt, ˜st+1) are stored in the replay buffer and used to compute the critic target: yt = ˜rt + γ min i∈{1,2} Qθ− i \u0010 ˜st+1, πϕ−(˜st+1) + ε \u0011 , (7) with ˜st+1 ∼P(· | st, ˜at) and ˜rt = r(st, ˜at). Through repeated actor–critic updates, the policy gradually acquires robustness to perturbations during online fine-tuning. Building on this setting, the next section introduces our adversarial fine-tuning method. 4.2 Adversarial Fine-tuning Adversarial fine-tuning adapts offline-pretrained policies to perturbed environments by injecting adversarial perturba- tions with a probability q per episode. An overview of the overall framework is illustrated in Figure 2. The policy πft and critic Qft are initialized from offline-pretrained models πoff and Qoff. To stabilize fine-tuning, a portion of the offline dataset Doff is inserted into the replay buffer R at the beginning of training, following prior works [11,12,35]. A predefined ratio of offline samples is used for initialization so that training starts from a balanced mixture of offline and online experiences, reducing instability during early fine-tuning. In each episode, the agent samples actions from the current policy and perturbs them with probability q: a′ t = at + δa ⊙at with probability q, (8) where ⊙denotes the element-wise (Hadamard) product and δa is an adversarial perturbation. Before fine-tuning, a set of perturbations {δa} is pre-generated using the differential evolution (DE) algorithm [17,24] with respect to the 5 S. Ayabe et al. Algorithm 1 Adversarial Fine-Tuning Require: πoff, Qoff: offline-pretrained policy and critic Require: Doff: offline dataset Require: q ∈[0, 1]: fixed perturbation probability 1: Initialize πft, Qft ←πoff, Qoff 2: Initialize replay buffer R with a fraction of offline samples 3: while within training budget do 4: Generate perturbation δ using Eq. (12) 5: πft ←EPISODELOOP(πft, Qft, δ, q) 6: end while Algorithm 2 EPISODELOOP (πft, Qft, δ, q) 1: Sample initial state s0 ∼P0(·) 2: while episode not terminated do 3: Sample action a ∼πft(· | s) 4: a ←a + δ ⊙a with", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "1b65c8bd3618e550c8bf7fd34ee46210cd001cfd4b94547f164abf03a355b790"}
{"doc_id": "arxiv:2510.13358#related-work:part-5", "url": "https://arxiv.org/abs/2510.13358", "anchor": "#related-work:part-5", "type": "paper", "title": "", "section": "Related Work", "text": "dataset Require: q ∈[0, 1]: fixed perturbation probability 1: Initialize πft, Qft ←πoff, Qoff 2: Initialize replay buffer R with a fraction of offline samples 3: while within training budget do 4: Generate perturbation δ using Eq. (12) 5: πft ←EPISODELOOP(πft, Qft, δ, q) 6: end while Algorithm 2 EPISODELOOP (πft, Qft, δ, q) 1: Sample initial state s0 ∼P0(·) 2: while episode not terminated do 3: Sample action a ∼πft(· | s) 4: a ←a + δ ⊙a with probability q 5: Optimize πft and Qft 6: end while 7: return πft offline-pretrained policy. This precomputation avoids the costly inner-loop minimization required by conventional adversarial training during fine-tuning. Algorithms 1 and 2 summarize the overall procedure for the fixed-q case. In Section 4.3, we extend this algorithm to introduce curriculum schedules that adaptively update q over time. 4.3 Curriculum-based Fine-tuning While fixed-q fine-tuning (Section 4.2) provides a stable baseline, it cannot account for changes in training dynamics or difficulty across episodes. We therefore introduce two curriculum-based extensions that gradually adjust the perturbation probability q: a linear curriculum and an adaptive curriculum. Both use the same general update rule: q ←clip(q + ∆q, 0, 1) , (9) where ∆q represents the increment per update, and clip(·) ensures that q remains within [0, 1]. 4.3.1 Linear curriculum The linear schedule increases q at a constant rate throughout fine-tuning. Starting from qinit, the perturbation probability is linearly raised toward qmax as training progresses. This gradual increase encourages the agent to first adapt to relatively mild perturbations and later to stronger ones, improving robustness without destabilizing early learning. The update rule simply uses a fixed increment, ∆q = c, (10) where c is a constant step size. Although straightforward, overly large c may lead to excessive exposure to perturbations too early, which can degrade nominal performance under normal (unperturbed) conditions (see Appendix A). 4.3.2 Adaptive curriculum The adaptive curriculum dynamically adjusts the perturbation probability q according to the policy’s performance. When the policy shows improvement, the curriculum increases q to enhance robustness. When the performance declines, the curriculum decreases q to reduce task difficulty and stabilize training. Through this adaptive mechanism, the agent maintains a balance between robustness and stability during fine-tuning. The overall fine-tuning process using the adaptive curriculum is summarized in Algorithm 3. This algorithm extends Algorithm 1 by introducing performance-based updates of the perturbation probability q through periodic evaluation. During training, the current policy is periodically evaluated across multiple episodes, and the average normalized score Rn is used as a measure of policy performance. The evaluation process is described in detail in Algorithm 4. 6 S. Ayabe et al. Algorithm 3 Adversarial Fine-Tuning with Adaptive Curriculum Require: πoff, Qoff: offline-pretrained policy and critic Require: Doff: offline dataset Require: qinit ∈[0, 1]: initial perturbation probability 1: Initialize πft, Qft ←πoff, Qoff 2: Initialize replay buffer R with a fraction of offline samples 3: q ←qinit, n ←0 4: while within training budget do 5: Generate perturbation δ using Eq. (12) 6: πft ←EPISODELOOP(πft, Qft, δ, q) 7: if at regular evaluation intervals then", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "a19d8f0b187a2eadebd1d0072a315e26c0a985fb73b9daf81026d5ef77c3f599"}
{"doc_id": "arxiv:2510.13358#related-work:part-6", "url": "https://arxiv.org/abs/2510.13358", "anchor": "#related-work:part-6", "type": "paper", "title": "", "section": "Related Work", "text": "Algorithm 4. 6 S. Ayabe et al. Algorithm 3 Adversarial Fine-Tuning with Adaptive Curriculum Require: πoff, Qoff: offline-pretrained policy and critic Require: Doff: offline dataset Require: qinit ∈[0, 1]: initial perturbation probability 1: Initialize πft, Qft ←πoff, Qoff 2: Initialize replay buffer R with a fraction of offline samples 3: q ←qinit, n ←0 4: while within training budget do 5: Generate perturbation δ using Eq. (12) 6: πft ←EPISODELOOP(πft, Qft, δ, q) 7: if at regular evaluation intervals then 8: Rn ←POLICYEVALUATION(πft, q, n) 9: Update q using Eqs. (9) and (11) 10: n ←n + 1 11: end if 12: end while Algorithm 4 POLICYEVALUATION (πft, q, n) Require: M: total number of evaluation episodes 1: for each episode m = 1, 2, . . . , M do 2: Generate perturbation δ using Eq. (12) 3: Sample initial state s0 ∼P0(·) 4: R(m) ←0 5: while episode not terminated do 6: Sample action a ∼πft(· | s) 7: a ←a + δ ⊙a with probability q 8: Execute a, observe reward r, and update R(m) ←R(m) + r 9: end while 10: end for 11: Compute Rn ← 1 M PM m=1 R(m) 12: return Rn The update rule modifies the increment ∆q based on the change in the exponentially smoothed evaluation score: ∆q = η ( ¯Rn −¯Rn−1), where ¯Rn = βRn + (1 −β) ¯Rn−1. (11) The exponential moving average ¯Rn provides a stable estimate of performance trends by filtering out short-term fluctuations. The parameters η and β control the adaptation dynamics: a smaller β captures long-term stability, while a larger β responds more quickly to recent changes. A larger η leads to stronger adaptation, whereas a smaller η results in smoother updates. By adjusting these parameters, the adaptive curriculum gradually modifies q to improve robustness without causing instability. The details of these parameter settings are provided in Appendix A. 5", "source": "arxiv_pdf", "published": "", "tokens": 316, "sha256": "aae7e552e6c2259a67e0f1e64a89edbc6401ae0800b0ff14ce3a58ba75cdc87f"}
{"doc_id": "arxiv:2510.13358#experiments:part-1", "url": "https://arxiv.org/abs/2510.13358", "anchor": "#experiments:part-1", "type": "paper", "title": "", "section": "Experiments", "text": "This section presents experimental evaluations of the proposed adversarial fine-tuning framework. First, we describe the experimental setup, including pretrained models, fine-tuning conditions, perturbation configurations, and evaluation protocols. Next, we compare fine-tuned policies with offline and fully online baselines to evaluate robustness under different perturbation conditions. Finally, we examine curriculum fine-tuning strategies that vary the perturbation probability q during training and analyze their impact on robustness against adversarial action-space perturbations. 5.1 Experimental Setup Pretrained Model The policy is pretrained for 5 million steps using TD3+BC [8], an offline reinforcement learn- ing algorithm, implemented with the d3rlpy library [36]. The expert dataset from Datasets for Deep Data-Driven Reinforcement Learning (D4RL) [37] is used for training. 7 S. Ayabe et al. Figure 3: Legged robot environments: Hopper-v2 (left), HalfCheetah-v2 (center), Ant-v2 (right). Fine-Tuning Settings Fine-tuning is conducted for 1 million steps using the TD3 algorithm. Each configuration is trained with 5 independent runs. The perturbation probability q and the offline data ratio roff are set to q = 0.5, roff = 0 in the Hopper environment. In the HalfCheetah and Ant environments, q is set to 0.1 and roff to 0.1. In the schedule-q setting, training is extended to 3 million steps to ensure policy convergence. The smoothing factor β in Equation (11) is fixed to 0.9 for all environments. The step-size parameter η is set to 1.0 in Hopper, 0.7 in HalfCheetah, and 0.3 in Ant. Perturbation Settings In addition to adversarial perturbations (see Section 4.2), fine-tuning is also conducted under two other conditions: normal and random perturbations. Formally, the perturbation vector is defined as: δ =    0 normal, δr ∼U([−ϵ, ϵ]Na) random perturbation, δa ∼DE(ϵ) adversarial perturbation. (12) In the normal condition, no perturbation is applied (δ = 0). For the random perturbation condition, δr is sampled from a uniform distribution over [−ϵ, ϵ]Na, where ϵ controls the perturbation strength and Na denotes the action dimension. In the Hopper-v2 and HalfCheetah-v2 environments, the magnitudes of δr and δa are fixed at ϵ = 0.3 during both training and evaluation. In the Ant-v2 environment, the perturbation magnitude is fixed at ϵ = 0.5. Legged Robot Environments We use forward walking tasks in the Hopper-v2, HalfCheetah-v2, and Ant-v2 environ- ments of OpenAI Gym [18]. These tasks are simulated within the MuJoCo physics engine, as illustrated in Figure 3. The reward functions for each robot are defined as follows: r(s, a) =      vfwd −0.001 ∥a∥2 + 1, Hopper-v2 vfwd −0.1 ∥a∥2 + 1, HalfCheetah-v2 vfwd −0.5∥a∥2 −0.5 · 10−3 ∥f∥2 + 1, Ant-v2 (13) where vfwd is the forward walking speed and f denotes the contact force. Robustness Evaluation We evaluate the robustness of fine-tuned policies on a forward walking task in a legged robot environment. Each policy is evaluated under three distinct conditions: normal (i.e., no perturbation), with random perturbations, and with adversarial perturbations. In the random and the adversarial perturbation condition, the perturbation probability is set to q = 1. The evaluation metric is the D4RL-normalized episodic reward over 100 episodes in each condition. All experiment", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "17a12f28b58860e257758ec8ea13093a7f80b3d768c6ba88f9fea2efd06de716"}
{"doc_id": "arxiv:2510.13358#experiments:part-2", "url": "https://arxiv.org/abs/2510.13358", "anchor": "#experiments:part-2", "type": "paper", "title": "", "section": "Experiments", "text": "forward walking speed and f denotes the contact force. Robustness Evaluation We evaluate the robustness of fine-tuned policies on a forward walking task in a legged robot environment. Each policy is evaluated under three distinct conditions: normal (i.e., no perturbation), with random perturbations, and with adversarial perturbations. In the random and the adversarial perturbation condition, the perturbation probability is set to q = 1. The evaluation metric is the D4RL-normalized episodic reward over 100 episodes in each condition. All experiment results are averaged across the 5 independent runs. 5.2 Robustness Evaluation of Fine-tuned Models against Baselines Table 1 compares adversarial fine-tuning against two baselines: offline-only training and fully online training from scratch. Offline-only policies fail under action perturbations due to their lack of exposure to distribution shifts during training. For example, in Ant-v2 under adversarial conditions, adversarial fine-tuning achieves a score of 91.6, compared to -21.0 for offline training and 24.0 for fully online training. A similar trend is observed in Hopper-v2, where the offline score drops to 13.7 while the fine-tuned score reaches 83.5. These results highlight that adversarially fine-tuned policies are far more robust to action-space perturbations than policies trained solely offline. The results also show that 8 S. Ayabe et al. Table 1: Fine-tuning improves robustness and achieves the highest scores, in contrast to conservative offline methods that drop to sub-zero rewards under adversarial perturbations. Values are reported as D4RL-normalized episodic reward (mean ± std). Offline results are from [17]. Fully Online denotes policies trained from scratch under adversarial perturbations. Fine-tuned denotes pretrained offline policies further trained under the specified perturbation condition. Bold numbers indicate the best performance for each evaluation condition. Environments", "source": "arxiv_pdf", "published": "", "tokens": 276, "sha256": "f46786e57854a546bf8bc8ccb34e05412838bfd6dc6808e5b0e5c2a289bc2262"}
{"doc_id": "arxiv:2510.13358#evaluation:part-1", "url": "https://arxiv.org/abs/2510.13358", "anchor": "#evaluation:part-1", "type": "paper", "title": "", "section": "Evaluation", "text": "Conditions Offline Fully Online (Adversarial) Fine-tuned Normal Random Adversarial Hopper-v2 Normal 111.1 ± 2.9 80.9 ± 39.4 88.5 ± 40.3 102.5 ± 20.1 84.3 ± 30.0 Random 62.9 ± 42.8 77.2 ± 40.2 78.5 ± 41.1 95.8 ± 24.4 89.3 ± 24.6 Adversarial 13.7 ± 2.4 57.0 ± 33.1 16.7 ± 1.6 57.4 ± 27.1 83.5 ± 17.9 HalfCheetah-v2 Normal 96.6 ± 3.4 34.2 ± 10.8 92.7 ± 4.2 93.7 ± 6.2 89.7 ± 7.4 Random 75.0 ± 18.8 30.5 ± 10.2 73.4 ± 14.4 82.7 ± 10.8 73.8 ± 13.7 Adversarial 12.1 ± 8.0 27.6 ± 8.0 41.0 ± 14.6 51.7 ± 18.0 55.3 ± 7.1 Ant-v2 Normal 119.9 ± 39.8 34.4 ± 12.3 127.7 ± 31.5 134.2 ± 25.4 130.0 ± 18.5 Random 86.6 ± 43.3 31.8 ± 12.8 103.4 ± 36.3 104.6 ± 33.8 99.6 ± 32.8 Adversarial −21.0 ± 25.9 24.0 ± 16.2 32.9 ± 32.2 56.2 ± 25.4 91.6 ± 11.9 matching the fine-tuning condition to the evaluation condition (e.g., Fine-tuned (Adversarial) for adversarial evaluation) yields the best performance. Figure 4 shows the learning dynamics under different perturbation scenarios. Fine-tuned policies typically exhibit an initial performance drop but recover quickly, converging faster than fully online training. This recovery occurs because adversarial perturbations promote exploration of previously unseen action regions, enabling the policy to learn compensatory behaviors more effectively. When the perturbation condition during fine-tuning matches the evaluation condition, convergence becomes faster and more stable, further supporting the advantage of targeted fine-tuning. These results demonstrate that adversarial fine-tuning effectively combines the robustness advantages of online training with the sample efficiency of offline pretraining. This approach provides a practical solution for deploying RL policies in real-world robotic systems where actuator perturbations are inevitable. 5.3 Improving Adversarial Robustness via Curriculum Fine-tuning We compare three strategies: fixed perturbation probability (qfix), linearly increasing curriculum (qlin), and adaptive curriculum (qada) that adjusts q based on policy performance. To isolate the effect of adaptive scheduling, we calibrate qlin’s maximum value qmax so that both curricula provide equal average perturbation exposure over training. Table 2 compares the three strategies across training durations and evaluation conditions. At 1M steps under adversarial conditions, qlin shows inconsistent performance: lower than qfix in Hopper-v2 (30.7 vs 83.5) and Ant-v2 (84.6 vs 91.6), but comparable in HalfCheetah-v2 (58.7 vs 55.3). In contrast, qada consistently outperforms qfix across all three environments (Hopper-v2: 87.9 vs 83.5; HalfCheetah-v2: 63.4 vs 55.3; Ant-v2: 98.2 vs 91.6), demonstrating more reliable early-stage robustness. By 3M steps, the two curricula exhibit distinct trade-offs. qlin improves adversarial performance in some environments (Ant-v2: 84.6 at 1M to 117.7 at 3M) but degrades normal performance (Hopper-v2: 95.1 to 76.5; HalfCheetah-v2: 91.7 to 88.7; Ant-v2: 125.0 to 123.7). Figure 5 confirms this overfitting phenomenon, showing q increasing continuously throughout training. In contrast, qada maintains or improves normal performance across environments (Hopper-v2: 100.2 to 97.5; HalfCheetah-v2: 89.0 to 93.0; Ant-v2: 124.8 to 130.8) while achieving comparable adversarial robustness (Ant-v2: 118.5 vs 117.7). This occurs because the adaptive schedule reduces perturbation pressure when performance drops, preventing unnecessary difficulty escalation. This demonstrates that performance-based", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "a0ba6ce5300262ee960c7f92903df2392f07f70aed18d8123b13c629055cc02e"}
{"doc_id": "arxiv:2510.13358#evaluation:part-2", "url": "https://arxiv.org/abs/2510.13358", "anchor": "#evaluation:part-2", "type": "paper", "title": "", "section": "Evaluation", "text": "3M) but degrades normal performance (Hopper-v2: 95.1 to 76.5; HalfCheetah-v2: 91.7 to 88.7; Ant-v2: 125.0 to 123.7). Figure 5 confirms this overfitting phenomenon, showing q increasing continuously throughout training. In contrast, qada maintains or improves normal performance across environments (Hopper-v2: 100.2 to 97.5; HalfCheetah-v2: 89.0 to 93.0; Ant-v2: 124.8 to 130.8) while achieving comparable adversarial robustness (Ant-v2: 118.5 vs 117.7). This occurs because the adaptive schedule reduces perturbation pressure when performance drops, preventing unnecessary difficulty escalation. This demonstrates that performance-based adaptation successfully avoids the overfitting-robustness trade-off inherent in fixed scheduling. Moreover, it suggests that dynamic curriculum mechanisms can offer a principled way to balance robustness and stability in broader reinforcement learning settings. 6", "source": "arxiv_pdf", "published": "", "tokens": 113, "sha256": "8f5e1dbbe5195cc401341156501770775bbaf84d6b29486c64e5271bf857c501"}
{"doc_id": "arxiv:2510.13358#conclusion", "url": "https://arxiv.org/abs/2510.13358", "anchor": "#conclusion", "type": "paper", "title": "", "section": "Conclusion", "text": "Conservative offline RL methods ensure stability but fundamentally conflict with action-space robustness because they prohibit learning from the out-of-distribution samples necessary for adaptation to perturbations. This study addressed 9 S. Ayabe et al. 0.0 0.2 0.4 0.6 0.8 1.0 Training steps (×106) 0 25 50 75 100 125 Normalized episodic reward 0.0 0.2 0.4 0.6 0.8 1.0 Training steps (×106) 0 25 50 75 100 125 Normalized episodic reward 0.0 0.2 0.4 0.6 0.8 1.0 Training steps (×106) 0 25 50 75 100 125 Normalized episodic reward Offline Fully Online (A) FT (N) FT (R) FT (A) (a) Hopper-v2 0.0 0.2 0.4 0.6 0.8 1.0 Training steps (×106) 0 20 40 60 80 100 Normalized episodic reward 0.0 0.2 0.4 0.6 0.8 1.0 Training steps (×106) 0 20 40 60 80 100 Normalized episodic reward 0.0 0.2 0.4 0.6 0.8 1.0 Training steps (×106) 0 20 40 60 80 100 Normalized episodic reward (b) HalfCheetah-v2 0.0 0.2 0.4 0.6 0.8 1.0 Training steps (×106) 30 0 30 60 90 120 Normalized episodic reward 0.0 0.2 0.4 0.6 0.8 1.0 Training steps (×106) 30 0 30 60 90 120 Normalized episodic reward 0.0 0.2 0.4 0.6 0.8 1.0 Training steps (×106) 30 0 30 60 90 120 Normalized episodic reward (c) Ant-v2 Figure 4: Performance curves show that fine-tuning accelerates convergence and improves robustness. Matching training and evaluation conditions (e.g., FT-A for adversarial evaluation, red curves in right column) yields fastest convergence. Offline baselines (dashed) excel in normal conditions but fail under perturbations. Rows: Hopper-v2, HalfCheetah-v2, Ant-v2. Columns: evaluation under normal, random, and adversarial perturbations. Shaded regions show standard deviation. this incompatibility through an offline-to-online framework with adversarial fine-tuning, where policies pretrained offline are subsequently fine-tuned in perturbed environments. Additionally, we introduced an adaptive curriculum that adjusts the perturbation probability based on policy performance to prevent overfitting while maintaining training stability. Experiments using legged robots demonstrated that our method outperformed both offline-only and fully online baselines and converged faster than training from scratch. In particular, the adaptive curriculum alleviated the overfitting observed in simpler linear schedules, which often sacrificed normal performance, thereby achieving an effective trade-off between robustness and stability. Future work should investigate theoretical guarantees for curriculum-based adaptation. Another promising direction is to explore the interplay between state-space and action-space perturbations in more complex environments. Acknowledgments This work was supported by JSPS KAKENHI Grant Number JP23K24914. 10 S. Ayabe et al. Table 2: The adaptive curriculum (qada), which adjusts based on performance, suppresses the degradation in nominal performance caused by overfitting in the linear curriculum (qlin) and maintains high performance. Results are reported for fixed-q setting (qfix), linearly increasing strategy (qlin), and adaptively adjusted strategy (qada) after 1M, 2M, and 3M training steps. Values indicate D4RL-normalized episodic rewards under normal, random, and adversarial perturbation conditions. Environments", "source": "arxiv_pdf", "published": "", "tokens": 466, "sha256": "10ddc6de0bcd2c8681ec93f927c44bad55082ab30c1dae8d2b5fe7932fb67bd1"}
{"doc_id": "arxiv:2510.13358#evaluation", "url": "https://arxiv.org/abs/2510.13358", "anchor": "#evaluation", "type": "paper", "title": "", "section": "Evaluation", "text": "Conditions qfix qlin qada 1M 2M 3M 1M 2M 3M Hopper-v2 Normal 84.3 95.1 86.8 76.5 100.2 75.0 97.5 Random 89.3 71.4 76.2 85.6 92.5 74.2 93.8 Adversarial 83.5 30.7 85.2 79.1 87.9 88.2 81.5 HalfCheetah-v2 Normal 89.7 91.7 91.9 88.7 89.0 93.8 93.0 Random 73.8 74.8 78.0 74.0 70.9 79.0 76.6 Adversarial 55.3 58.7 72.4 76.4 63.4 69.2 75.1 Ant-v2 Normal 130.0 125.0 130.5 123.7 124.8 131.0 130.8 Random 99.6 91.9 90.3 98.8 95.5 97.3 110.1 Adversarial 91.6 84.6 104.2 117.7 98.2 115.7 118.5 0.0 0.5 1.0 1.5 2.0 2.5 3.0 Training steps (×106) 0.0 0.2 0.4 0.6 0.8 1.0 q_value 0.0 0.5 1.0 1.5 2.0 2.5 3.0 Training steps (×106) 0.0 0.2 0.4 0.6 0.8 1.0 q_value 0.0 0.5 1.0 1.5 2.0 2.5 3.0 Training steps (×106) 0.0 0.2 0.4 0.6 0.8 1.0 q_value q_linear q_adaptive Figure 5: The trajectories of the perturbation probability q illustrate the key behavioral difference between the two curricula. The adaptive strategy (qada, red) adjusts the growth of q in response to policy performance, preventing the relentless increase seen in the linear strategy (qlin, blue).", "source": "arxiv_pdf", "published": "", "tokens": 182, "sha256": "2be8c4c5cea574bbd628ca0fcd02ea086963d09b5687f469c404dceabc8bbfd6"}
{"doc_id": "arxiv:2510.13367#abstract", "url": "https://arxiv.org/abs/2510.13367", "anchor": "#abstract", "type": "paper", "title": "", "section": "ABSTRACT", "text": "Despite their effectiveness and popularity in offline or model-based reinforcement learning (RL), transformers remain underexplored in online model-free RL due to their sensitivity to training setups and model design decisions such as how to structure the policy and value networks, share components, or handle temporal information. In this paper, we show that transformers can be strong baselines for continuous control in online model-free RL. We investigate key de- sign questions: how to condition inputs, share components between actor and critic, and slice sequential data for training. Our experi- ments reveal stable architectural and training strategies enabling competitive performance across fully and partially observable tasks, and in both vector- and image-based settings. These findings offer practical guidance for applying transformers in online RL. 1", "source": "arxiv_pdf", "published": "", "tokens": 123, "sha256": "5b93de329a2c86d9de7abc4a62baa78bfd87f12e42bd5ef818178f5510b0fa78"}
{"doc_id": "arxiv:2510.13367#introduction:part-1", "url": "https://arxiv.org/abs/2510.13367", "anchor": "#introduction:part-1", "type": "paper", "title": "", "section": "INTRODUCTION", "text": "The transformer architecture [38] has become a cornerstone of modern deep learning, revolutionizing a wide array of domains such as natural language processing [7], computer vision [8], and robotics [20]. Its strengths include modeling long-range depen- dencies [5], flexibly handling multimodal inputs [39], and scaling effectively with model size and data [21]. Transformers have also gained traction in reinforcement learn- ing (RL) [14, 22, 26, 28, 33, 40], especially in offline [1–3, 19] and model-based RL [12], where the learning problem is reframed as a sequence modeling over pre-collected datasets. These settings allow transformers to exploit their autoregressive capabilities to perform well without the challenges of online exploration. How- ever, purely offline approaches, such as behavior cloning (BC), are constrained by limited expressivity and often fail to generalize to out-of-distribution (OOD) states [31]. To mitigate this, hybrid pipelines that combine offline pretraining with online fine-tuning have been proposed [25], improving adaptability while still depend- ing on curated expert data. Yet, both offline and hybrid RL are inherently limited by the cost, scarcity, and domain-specific nature of expert demonstrations. Online RL, in contrast, learns directly through environment interac- tion, which enables broader exploration and better generalization across tasks and domains. Although constrained in real-world sce- narios by training time or safety concerns, online RL is especially promising in simulation-driven workflows, where large-scale data ∗Equal contribution. Figure 1: Transformer baselines enhanced by our findings achieve competitive performance across selected tasks. Aver- age reward is normalized over the specified number of tasks. For MuJoCo-POMDP, we use PPO-GRU from [24]. collection is feasible. These properties make it particularly well- suited for sim-to-real transfer [36], which is crucial for deploying RL systems in robotics and real-world control. Transformers are known for capturing long-term dependencies and enabling multitask generalization [1, 12, 19, 27], but remain underused in fully online RL. In model-free settings, adoption is limited by training instability and sensitivity to architectural and optimization design choices. Prior work [9, 27, 41] focuses mostly on discrete action spaces. We instead target continuous control – a crucial yet underexplored domain, where high-dimensional and precise actions pose unique challenges. In this work, we study how to effectively apply transformers in model-free online RL, with a focus on understanding key ar- chitectural and optimization choices. Our goal is not to propose a radically new architecture, but to surface practical insights that make transformers usable in online continuous control. We pro- pose a unified training pipeline and systematically evaluate several critical design decisions – such as how to condition the model, how to share parameters between actor and critic, and how to construct input sequences for temporal modeling. We show that transformers, though rarely used in this setting, can serve as strong baselines for both Markov Decision Processes (MDPs) and Partially Observable MDPs (POMDPs), as well as for vector- and image-based tasks (Figure 1). Our evaluation on MuJoCo [37], its POMDP variants [16], and ManiSkill3 [35] highlights the broad applicability of our approach in continuous control. We also in- clude the code for our experiments in the supplementary materials, enabling reproduction of", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "34e32efd58949e78f0c35729783bc4d32fdc724c99767d1938a401c006bd66db"}
{"doc_id": "arxiv:2510.13367#introduction:part-2", "url": "https://arxiv.org/abs/2510.13367", "anchor": "#introduction:part-2", "type": "paper", "title": "", "section": "INTRODUCTION", "text": "construct input sequences for temporal modeling. We show that transformers, though rarely used in this setting, can serve as strong baselines for both Markov Decision Processes (MDPs) and Partially Observable MDPs (POMDPs), as well as for vector- and image-based tasks (Figure 1). Our evaluation on MuJoCo [37], its POMDP variants [16], and ManiSkill3 [35] highlights the broad applicability of our approach in continuous control. We also in- clude the code for our experiments in the supplementary materials, enabling reproduction of the results for the proposed approaches. Our contributions are as follows: • Transformer Viability in Online RL: We demonstrate that with the right training setup, transformers are competitive arXiv:2510.13367v1 [cs.LG] 15 Oct 2025 across MDP and POMDP locomotion tasks and both vector- and image-based robotic control. • Insights and Recommendations: We distill actionable guidelines from extensive experiments to support stable and efficient transformer training in online RL. 2 TRANSFORMERS IN RL Offline RL. Chen et al. [1] reformulates RL as sequence mod- eling problem, introducing the Decision Transformer (DT), which generates actions autoregressively from past states, actions, and returns-to-go. This approach replaces value estimation with su- pervised learning on offline data, achieving strong performance in offline RL. Janner et al. [19] extends this idea by introducing the Trajectory Transformer (TT), which incorporates learned dynamics for planning and bridges imitation and model-based RL. Lee et al. [23] further adapts DT to train a single agent across multiple en- vironments. Despite strong offline results, these methods face key limitations – most notably, their reliance on expert data, which is costly, hard to collect, or unsafe in real-world settings. Even with expert data, offline RL models often struggle to generalize due to limited exploration and shifts in the environment distribution. Hybrid Methods. To mitigate the reliance on expert demonstra- tions and improve generalization beyond static datasets, training must incorporate online interaction. Online Decision Transformer (ODT) [42] moves in this direction by combining offline likelihood maximization with online updates. SMART [34] successfully applies transformers in online RL by using self-supervised offline pretrain- ing, which enables more stable online learning. Hybrid methods aid exploration and adaptation but still rely on offline data, limiting their general applicability. Online RL. Fully online training removes the need for expert data but remains less explored and more challenging for trans- formers than offline and hybrid approaches. Esslinger et al. [10] trains a transformer from scratch in the online setting using a Deep Q-Network (DQN)-like algorithm, achieving better results than Deep Recurrent Q-Network (DRQN) [17] on POMDP tasks. While based on DQN, Deep Transformer Q-Network (DTQN) is limited to discrete action spaces, and thus cannot directly handle contin- uous control. Parisotto et al. [27] introduced Gated Transformer- XL (GTrXL), a RL-focused variant of Transformer-XL [5], adding gating and identity map reordering. GTrXL proved effective in memory-intensive tasks [28]. The gating mechanism stabilize large multi-layered transformers by allowing them to bypass the atten- tion and feed-forward components within each block. This enables the model to dynamically control how much information is trans- formed versus passed through unchanged, effectively regulating the flow of content", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "4a36f20996b7abd2e3d51f37e495d04fab439b31bb1a55cc5c7324a17468fd22"}
{"doc_id": "arxiv:2510.13367#introduction:part-3", "url": "https://arxiv.org/abs/2510.13367", "anchor": "#introduction:part-3", "type": "paper", "title": "", "section": "INTRODUCTION", "text": "and thus cannot directly handle contin- uous control. Parisotto et al. [27] introduced Gated Transformer- XL (GTrXL), a RL-focused variant of Transformer-XL [5], adding gating and identity map reordering. GTrXL proved effective in memory-intensive tasks [28]. The gating mechanism stabilize large multi-layered transformers by allowing them to bypass the atten- tion and feed-forward components within each block. This enables the model to dynamically control how much information is trans- formed versus passed through unchanged, effectively regulating the flow of content around attention processing. Another effort to apply transformers in online RL involves the Recurrent Linear Transformer (ReLiT) and its gated variant, AGaLiTe [29], which reduce quadratic complexity via context-independent inference, but may underperform on tasks requiring richer temporal modeling. While these works offer promising directions, transformer-based online RL remains underexplored. In this paper, we take a systematic Figure 2: Scheme of the GPT-2-like block used as a backbone during all the experiments in this work. approach to investigating how transformers can be made compet- itive in fully online model-free RL, identifying key training and architectural principles for their effective use. 3 METHODOLOGY To show that transformers can be a strong baseline for continuous control tasks, we conduct a series of experiments guided by key research questions, each addressing a core design challenge. For each, we distill practical insights, then combine the best strategies into a unified setup to demonstrate competitive performance. The research questions (RQs) we investigate are: (1) How does input conditioning affect transformer performance? (2) What is the impact of sharing the backbone between actor and critic? (3) How does data slicing influence training? We use MLPs as a standard baseline for MDP tasks and CNNs for image-based settings. RNNs offer a lightweight sequential alterna- tive to transformers, making them a useful point of comparison to assess whether the added complexity of transformers is warranted. Environments. We validate our best transformer configura- tion by comparing it to MLP and RNN baselines in both MDP and POMDP settings, and demonstrate that it generalizes from vector-based to more challenging image-based tasks. Our evaluation spans three environment suites: MuJoCo [37] for standard continu- ous control tasks, using environments such as HalfCheetah, Ant, Hopper, Humanoid, Walker, Pusher, and Reacher; ManiSkill3 [35] for robotic manipulation, with vector-based tasks like PushT, Pick Cube, TriFingerRotateCube, as well as image-based tasks such as PushCube, PickCube, and PokeCube; and MuJoCo-POMDP [16], which introduces partial observability by masking velocity or posi- tion to highlight the importance of temporal modeling in sequential decision-making. This setup enables a comprehensive assessment of transformer performance across diverse observation modalities and control challenges. Transformer Baselines. To train transformer-based models, we use both on-policy and off-policy algorithms: Proximal Policy Optimization (PPO) [32], Twin Delayed Deep Deterministic Policy Gradient (TD3) [11], and Soft Actor-Critic (SAC) [15]. Our goal is to identify training patterns that that remain consistent across algorithms. For ManiSkill3 experiments, we use the official SAC and PPO implementations. For MuJoCo MDP and POMDP tasks, we use Figure 3: Performance of transformer conditioning methods on MuJoCo-POMDP tasks, where velocity is masked to induce partial observability. The", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "9061577b76a0585e8c99a803e004c2cb6ff8e10b1f3b6a00648f3f16733c5910"}
{"doc_id": "arxiv:2510.13367#introduction:part-4", "url": "https://arxiv.org/abs/2510.13367", "anchor": "#introduction:part-4", "type": "paper", "title": "", "section": "INTRODUCTION", "text": "To train transformer-based models, we use both on-policy and off-policy algorithms: Proximal Policy Optimization (PPO) [32], Twin Delayed Deep Deterministic Policy Gradient (TD3) [11], and Soft Actor-Critic (SAC) [15]. Our goal is to identify training patterns that that remain consistent across algorithms. For ManiSkill3 experiments, we use the official SAC and PPO implementations. For MuJoCo MDP and POMDP tasks, we use Figure 3: Performance of transformer conditioning methods on MuJoCo-POMDP tasks, where velocity is masked to induce partial observability. The two CrossAttn variants differ in sequence input order. CleanRL PPO and TD3 implementations [18] using default parame- ters. We use a transformer decoder (Figure 2) inspired by GPT-2 [30], with key modifications: pre-layer normalization from [27], GELU activation instead of ReLU in the feed-forward layers, and integra- tion with FlashAttention [6] for efficient training. This backbone is used across all experiments and referred to as “GPT”, with TD3- GPT, SAC-GPT, and PPO-GPT denoting the training algorithm used. Full model and training configurations are listed in Appendix A in Tables 4, 5, and 6. Experimental protocol. For each experiment, we conducted three runs per agent with different random initializations and per- formed evaluation during training using 100 random seeds. The results are presented as the mean episodic reward or success rate ± the standard error of the mean. 4 EXPERIMENTS AND RESULTS 4.1 RQ1: How does transformer conditioning affect performance? A key challenge in applying transformers to online RL is deter- mining how to condition the model on relevant inputs. In this section, we investigate how different conditioning strategies af- fect performance in partially observable settings. Specifically, we use the MuJoCo-POMDP benchmark [16], where velocity informa- tion is masked to induce partial observability and emphasize the importance of sequential processing. We evaluate four common conditioning methods with TD3-GPT, where the transformer receives a context window and predicts the action 𝑎𝑡from the final hidden state ℎ𝑡. These strategies (Table 1) reflect standard choices in transformer-based RL models and vary in the type and structure of information given to the model: • ObsOnly feeds a sequence of past observations into the trans- former; 𝑎𝑡is predicted from the final embedding. • Interleaved extends this by including previous actions in the sequence, keeping prediction from the last observation. • EmbedConcat encodes observations, actions, and rewards separately, then concatenates them into a single vector before feeding the sequence into the transformer. The action is predicted from the last combined token. • CrossAttn uses a two-layer transformer: the first applies self-attention to actions, the second cross-attends to obser- vations. The final token is used for 𝑎𝑡prediction. These methods represent growing contextual richness and com- plexity, allowing us to study how various forms of temporal and multimodal information affect performance in POMDPs. Interpretation. Figure 3 highlights two key findings. First, adding more types of information (actions and rewards) consistently im- proves performance in partially observable settings. Second, the EmbedConcat method proves most effective – not only for incor- porating multimodal inputs, but also for its training stability in online RL. Unlike CrossAttn or Interleaved, which rely on com- plex attention", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "2169698d55fbf31894f038863c74a41e082e254fe1fa8796492d6cd296fcaa90"}
{"doc_id": "arxiv:2510.13367#introduction:part-5", "url": "https://arxiv.org/abs/2510.13367", "anchor": "#introduction:part-5", "type": "paper", "title": "", "section": "INTRODUCTION", "text": "growing contextual richness and com- plexity, allowing us to study how various forms of temporal and multimodal information affect performance in POMDPs. Interpretation. Figure 3 highlights two key findings. First, adding more types of information (actions and rewards) consistently im- proves performance in partially observable settings. Second, the EmbedConcat method proves most effective – not only for incor- porating multimodal inputs, but also for its training stability in online RL. Unlike CrossAttn or Interleaved, which rely on com- plex attention or position-specific queries, EmbedConcat simplifies processing by flattening and fusing all modalities upfront, mak- ing optimization easier under online training instability. Its strong performance stems from two factors: (1) it includes rewards and actions – critical in POMDPs alongside observations, and (2) it merges each triplet (observation, action, reward) into a single em- bedding, yielding a homogeneous sequence. This allows the at- tention mechanism to focus purely on temporal dependencies. In contrast, Interleaved mixes heterogeneous inputs, complicating both temporal alignment and modality separation, which may hin- der learning stability. Similar experiments with the MDP versions of the MuJoCo envi- ronments (Figure 4) show that for tasks with Markovian properties, there is no need to use EmbedConcat, and ObsOnly is sufficient for successful training, but similar to the POMDP versions, the EmbedConcat method also shows strong performance on MDP.", "source": "arxiv_pdf", "published": "", "tokens": 218, "sha256": "15c2822da7ffb92ed4b919e1e446ab52746473199d3946f7cb2ace37178afb84"}
{"doc_id": "arxiv:2510.13367#method:part-1", "url": "https://arxiv.org/abs/2510.13367", "anchor": "#method:part-1", "type": "paper", "title": "", "section": "Method", "text": "Equations ObsOnly seq = [ot−M+1, . . . , ot] ht = SelfAttention(seq)[−1] Interleaved seq = [ot−M+1, at−M+1, . . . , at−1, ot] ht = SelfAttention(seq)[−1] EmbedConcat en = concat(emb(on), emb(an−1), emb(rn)) seq = [et−M+1, . . . , et] ht = SelfAttention(seq)[−1] CrossAttn seqx = [at−M+1, . . . , at−1] seqy = [ot−M+1, . . . , ot] zt = SelfAttention(seqx) ht = CrossAttention(zt, seqy)[−1] Table 1: Transformer conditioning strategies. We explored these strategies on both MDP and POMDP versions of Mu- JoCo environments. Figure 4: Conditioning experiments for MDP version of MuJoCo tasks. Using observations only is enough for tasks with Markovian properties. To better understand model behavior, we visualized the trans- former’s final hidden states ℎ𝑡using t-SNE, along with the cor- responding predicted actions (Figure 5). Well-performing models showed clearly separated clusters in latent space, each linked to dis- tinct learned behaviors. The EmbedConcat method, which achieved the best results (Figure 3), also exhibited the most structured and in- terpretable clustering. We present visualizations for EmbedConcat and Interleaved as representative examples. Practical Takeaway: For POMDPs, transformer performance improves when embeddings of observations, actions, and re- wards are combined into a single input vector. In MDPs, obser- vations alone are sufficient. 4.2 RQ2: How does actor–critic backbone sharing affect training stability and efficiency? We use TD3-GPT, where a transformer encodes observation se- quences as in the ObsOnly setup. To reduce parameters without sacrificing performance, we investigate whether the actor and the critic can share the transformer backbone. To isolate architectural effects, we use non-terminating MDPs where observations are suf- ficient. Figure 6 shows the tested sharing strategies: (1) Separate: the actor and the critic use independent trans- former backbones. Figure 5: t-SNE visualization of action representations from the transformer on HalfCheetah. Clusters with EmbedConcat appear more structured. (2) Shared without freezing: shared transformer is updated only by the actor; the critic gradients are blocked. (3) Shared with freezing: the actor and the critic share the same transformer, updated by both gradients. Interpretation. Figure 7 shows that using separate backbones for the actor and the critic ensures stable training, while sharing a backbone degrades performance unless the transformer is frozen during critic updates. This indicates conflicting gradient signals between actor and critic: the actor maximizes rewards, while the critic minimizes Temporal Difference error – objectives that may conflict. These opposing gradients can destabilize learning or sup- press updates due to gradient explosion. To test this, we logged and plotted gradient norms (Figure 8). In the “shared without freezing” setup, gradient norms grow continuously to extreme values. In contrast, the other two setups maintain stable updates by avoiding actor–critic interference. Similar issues of training instability be- tween the actor and the critic are discussed by Garcin et al. [13] and Cobbe et al. [4]. Practical Takeaway: Sharing a transformer between actor and critic in off-policy RL causes gradient interference and instability. Using separate backbones improves stability but increases cost. Freezing the shared transformer during critic updates provides a stable and efficient compromise. Figure 6: Visual illustration of transformer-based agent with", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "332c7f1150ca3f88c07e5204c7e00c3f2b383633e88844891fb2a23c0d11bb70"}
{"doc_id": "arxiv:2510.13367#method:part-2", "url": "https://arxiv.org/abs/2510.13367", "anchor": "#method:part-2", "type": "paper", "title": "", "section": "Method", "text": "other two setups maintain stable updates by avoiding actor–critic interference. Similar issues of training instability be- tween the actor and the critic are discussed by Garcin et al. [13] and Cobbe et al. [4]. Practical Takeaway: Sharing a transformer between actor and critic in off-policy RL causes gradient interference and instability. Using separate backbones improves stability but increases cost. Freezing the shared transformer during critic updates provides a stable and efficient compromise. Figure 6: Visual illustration of transformer-based agent with separated and shared architectures. In the frozen variant, gradients from the critic are not propagated through the backbone and observation encoder (red dashed line). Figure 7: Comparison of TD3-GPT backbone sharing methods. “Separate” and “shared with freezing” setups perform similarly, while removing freezing causes collapse due to gradient conflict. 4.3 RQ3: How does input data slicing affect the training of transformers? Due to the transformer’s sequential nature, its use in online RL requires careful handling of its output sequence. Broadly, there are two main ways to process it: Method 1: predict the action or Q-value only from the last hid- den state processed by the transformer. Method 2: predict actions or Q-values from every hidden state in the sequence. The transformer output (batch, context, hidden_dim) in Method 1 is sliced via [:,-1,:] to extract the current observa- tion’s embedding. The actor and the critic thus learn from the full input sequence but predict only from the last token, requiring that the final position contains the full episode state. Method 2 trains on all hidden states, potentially yielding better gradients and faster learning. Method 1 is more sensitive to how replay buffer data is sliced, since predictions rely solely on the current state’s embedding – the last input token. With fixed context length during training, the model implicitly assumes all sequences match this size. Otherwise, it may act suboptimally due to out-of-context evaluation, performing well only once the episode reaches the training context length. An effective data handling strategy should ideally preserve the advantages of both methods while mitigating their limitations. We argue that Method 1, when combined with cross-episode slicing, offers such a compromise: it supports stable and efficient training Figure 8: Gradient norms for different backbone sharing methods on HalfCheetah. Freezing the shared backbone dur- ing critic updates stabilizes training; removing it leads to gradient explosions. Blue curve lies beneath the green. with convergence behavior comparable to Method 2. To better un- derstand this process, consider Figure 11. Using Method 1, we can slice state sequences either within or across episodes. At the start of an episode, the agent interacts with the environment and accumu- lates observations until the context length 𝐶is reached (e.g., 𝐶=3 in the figure). As shown in the left part of Figure 11, the within-episode approach collects data from the episode start but skips training on the first 𝐶−1 steps. This gap can cause poor early-episode behavior, which is critical in tasks such as Humanoid, Walker, Hopper, and even ManiSkill3 (see Figure 14). The issue is that early observations (e.g., 𝑜1, 𝑜2) never appear at the final", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "d4a0e1e777c8f609c3ced30443e36e529743c1653d6c55f2694c28d38e07c4fe"}
{"doc_id": "arxiv:2510.13367#method:part-3", "url": "https://arxiv.org/abs/2510.13367", "anchor": "#method:part-3", "type": "paper", "title": "", "section": "Method", "text": "the environment and accumu- lates observations until the context length 𝐶is reached (e.g., 𝐶=3 in the figure). As shown in the left part of Figure 11, the within-episode approach collects data from the episode start but skips training on the first 𝐶−1 steps. This gap can cause poor early-episode behavior, which is critical in tasks such as Humanoid, Walker, Hopper, and even ManiSkill3 (see Figure 14). The issue is that early observations (e.g., 𝑜1, 𝑜2) never appear at the final token position, leaving the agent untrained to act at those steps. The longer the context, the more pronounced the problem. Cross-episode slicing (Figure 11, right) solves this by allowing input sequences to span across episode boundaries. The model still predicts from the last token, but now includes early-episode data in context, enabling better learning from the first 𝐶−1 steps. We evaluate this in three training strategies: • “Every token”: make predictions at every token without applying cross-episode slicing, as the agent is trained to operate over all timesteps. • “Cross-episode”: make predictions only at the last token while applying cross-episode slicing to ensure balanced learn- ing across all states within an episode. • “Within-episode”: make predictions only at the last token, while starting data collection only after the required context length has been reached. Interpretation. Figure 9 shows that both “Every token” and “Cross-episode” enable effective learning, while “Within-episode” impairs performance despite Method 1’s efficiency. To test if “Cross- episode” slicing improves early behavior, we measure rewards over the first 12 steps with context length 10. As shown in Figure 10, “Within-episode” underperforms early, while “Cross-episode” re- solves this without the overhead of training on every token. The application of the cross-episode slicing approach demon- strated promising results, stabilizing and improving training with- out the need to train the agent through predictions at every token in the transformer output sequence. Nevertheless, this question warrants further investigation, particularly through exploring al- ternative masking strategies. We considered three strategies of how to work with cross-episode slices: Figure 9: Evaluation performance of three ways of slicing data. Last token methods use last token to predict action during training, while Every token method trains transformer to make action prediction from every token. (1) Ignore masking and let the model learn the boundaries that separate the end of the previous episode and the beginning of the new one. (2) Mask irrelevant observations with zero vector. (3) Mask irrelevant observations with the first available obser- vation from the current episode. In the additional experiment, we have tested four approaches of data-slicing, depicted on the Figure 12: 1) slicing within the episode (left); 2) cross-episode slicing without additional masking (the sec- ond from the left); 3) cross-episode slicing with zero mask (the third from the left); 4) cross-episode slicing with the first available observation mask (the right one). Figure 14 (left) supports the findings presented in the main text. Cross-episode slicing enables the transformer to successfully learn even in the ManiSkill3 PickCube environment. Among the three cross-episode slicing variants, the most effective are zero padding and the first available", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "08e47748bb800d6ce34843433ee3b0f6fd91a61763d28e1958a4a4130a9b50c5"}
{"doc_id": "arxiv:2510.13367#method:part-4", "url": "https://arxiv.org/abs/2510.13367", "anchor": "#method:part-4", "type": "paper", "title": "", "section": "Method", "text": "within the episode (left); 2) cross-episode slicing without additional masking (the sec- ond from the left); 3) cross-episode slicing with zero mask (the third from the left); 4) cross-episode slicing with the first available observation mask (the right one). Figure 14 (left) supports the findings presented in the main text. Cross-episode slicing enables the transformer to successfully learn even in the ManiSkill3 PickCube environment. Among the three cross-episode slicing variants, the most effective are zero padding and the first available observation padding. With these results in hand, we can analyze the agent’s behavior over the first 14 steps in the environment (given a training context length of 10). Figure 14 (right) illustrates the agent’s average reward over the first 14 steps of an episode, averaged across 100 seeds in PickCube. The figure supports the conclusion that the cross-episode slicing improves action quality during the initial 9 steps of the environ- ment, compared to within-episode slicing, which lacks this mecha- nism. Notably, both the first observation masking and cross-episode masking approaches enhance the agent’s reward at the beginning of the episode, thereby contributing to improved overall episode performance. Zero-padding, in turn, yields the weakest performance among the three cross-episode slicing methods (which is consistent with Figure 10: Reward received during first 12 steps with different ways of training. “Cross-episode” slicing reaches similar / better results compared with “Every token” approach. Figure 11: Within-episode slicing (left) vs. cross-episode slic- ing (right). The former stores data only after reaching the full context, while the latter slices sequences across episodes. the results shown in Figure 14 (left)). However, it still enables re- ward improvement in later steps. In contrast, the within-episode slicing approach fails to meet the challenge, and the agent contin- ues to perform inefficiently even when sufficient context becomes available. Practical Takeaway: When predicting from the last token, cross-episode slicing is essential for early-episode learning. Predicting from every token avoids this but adds cost without improving performance or stability. 5 COMPARISON WITH BASELINES To assess the strength of our transformer setup, we implement all practical takeaways in complete TD3, PPO, and SAC agents and compare them with strong baselines: LSTM, MLP, GTrXL, ODT, and CNN variants. RL parameters are kept fixed across runs (Appen- dix A, Tables 4 and 5). TD3 and PPO are from CleanRL [18] using Figure 12: Four approaches of data slicing. Cross-episode slicing is represented by three ways of masking. Figure 13: Performance on selected MuJoCo tasks. We train GPT and GTrXL using both PPO and TD3 to ensure fair evaluation. default hyperparameters, while SAC follows the original ManiSkill3 benchmark settings. 5.1 MuJoCo MDP Environments For MDPs, we apply the ObsOnly conditioning from RQ1, cross- episode slicing from RQ3, and separate backbones from RQ2 to avoid gradient interference. Figure 13 shows that TD3-GPT performs on par with TD3-MLP and TD3-LSTM across all tasks, achieving stable and strong learning. This confirms that transformers can effectively process sequential input even in fully observable settings. In contrast, PPO fails to train both GPT and MLP agents, suggesting it is suboptimal in this setting.", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "d71fd7a19077d4c6aa14dc11fb2a3d81ed1e7da600fea489c654b56bcb940124"}
{"doc_id": "arxiv:2510.13367#method:part-5", "url": "https://arxiv.org/abs/2510.13367", "anchor": "#method:part-5", "type": "paper", "title": "", "section": "Method", "text": "settings. 5.1 MuJoCo MDP Environments For MDPs, we apply the ObsOnly conditioning from RQ1, cross- episode slicing from RQ3, and separate backbones from RQ2 to avoid gradient interference. Figure 13 shows that TD3-GPT performs on par with TD3-MLP and TD3-LSTM across all tasks, achieving stable and strong learning. This confirms that transformers can effectively process sequential input even in fully observable settings. In contrast, PPO fails to train both GPT and MLP agents, suggesting it is suboptimal in this setting. Transformer-based baselines like GTrXL and ODT also perform poorly. Although ODT is not fully online, our approach outperforms it while relying solely on online data. For fair comparison, we evaluate GTrXL using both TD3 and PPO. 5.2 MuJoCo POMDP Environments With partial observability, we switch from ObsOnly to EmbedConcat conditioning, retain separate actor and critic backbones, and use TD3 for training. As shown in Figure 15, TD3-GPT outperforms MLPs, RNNs, and vanilla SAC-Transformer baselines. GTrXL is the strongest alternative, but still lags behind our model in most tasks. 5.3 Vector-based ManiSkill3 Environments For the vector-based ManiSkill3 environments, we trained a GPT- based agent with SAC using a context size of 10. Due to the Markov- ian nature of ManiSkill3 tasks, we use the ObsOnly conditioning approach with cross-episode slicing and first observation mask- ing (see Appendix C for more experiments with masking). Table 2 shows that the transformer-based model achieves performance com- parable to the MLP baseline and in some cases even surpasses it. In Figure 14: The return comparison of the slicing strategies on the first 14 steps in the PickCube environment (right) and their associated average success rate (left). turn, the LSTM baseline, trained under the same setup, achieved similar overall performance, except on the PushT task, where it underperformed. These results suggest that the transformer is a competitive baseline for this class of continuous control tasks. Table 2: GPT performance on vector-based ManiSkill3 tasks. PushT PickCube TriFingerRotateCube SAC-GPT 0.65 ± 0.02 0.99 ± 0.0 0.94 ± 0.0 SAC-MLP 0.59 ± 0.01 0.99 ± 0.0 0.85 ± 0.0 SAC-LSTM 0.22 ± 0.01 0.94 ± 0.0 0.90 ± 0.0 5.4 Image-based ManiSkill3 Environments An important component of our research is assessing the effective- ness of the proposed takeaways in image-based environments, as real-world manipulation tasks typically rely on camera-equipped robots. In this experiment, we evaluated a transformer-based agent on image-based versions of several ManiSkill environments (Push Cube, PickCube, and PokeCube). Transformer training parameters were kept identical to those used in the vector-based setting, in- cluding ObsOnly conditioning with the first observation masking. To process visual input, we used a lightweight convolutional encoder composed of convolutional layers with ReLU activations, interleaved with max-pooling operations, progressively reducing spatial resolution while increasing channel depth up to 64. All other transformer training parameters remained identical to those used in the vector-based setting. Figure 15: POMDP MuJoCo tasks with masked positions (left) and masked velocities (right). * – results from [24]. Table 3: GPT performance on image-based ManiSkill tasks. PushCube PickCube PokeCube SAC-GPT 0.99 ± 0.0 0.97 ± 0.02 0.86 ± 0.05 SAC-CNN 0.99 ± 0.0", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "7fd87d98419288a174e581e059c37c77efdffdc5e4f6dfb504b1fd0e9e08af5d"}
{"doc_id": "arxiv:2510.13367#method:part-6", "url": "https://arxiv.org/abs/2510.13367", "anchor": "#method:part-6", "type": "paper", "title": "", "section": "Method", "text": "encoder composed of convolutional layers with ReLU activations, interleaved with max-pooling operations, progressively reducing spatial resolution while increasing channel depth up to 64. All other transformer training parameters remained identical to those used in the vector-based setting. Figure 15: POMDP MuJoCo tasks with masked positions (left) and masked velocities (right). * – results from [24]. Table 3: GPT performance on image-based ManiSkill tasks. PushCube PickCube PokeCube SAC-GPT 0.99 ± 0.0 0.97 ± 0.02 0.86 ± 0.05 SAC-CNN 0.99 ± 0.0 0.98 ± 0.01 0.61 ± 0.02 Table 3 demonstrates the transformer’s ability to learn effectively even in image-based environments, demonstrating that our practi- cal takeaways extend to realistic robotic settings where agents rely on visual observations. 6", "source": "arxiv_pdf", "published": "", "tokens": 117, "sha256": "791004f7a276dd2b058eb64464e6c2c83639aacd8a143ceb04efd7534edb4bbd"}
{"doc_id": "arxiv:2510.13367#conclusion", "url": "https://arxiv.org/abs/2510.13367", "anchor": "#conclusion", "type": "paper", "title": "", "section": "CONCLUSION", "text": "In this work, we re-evaluate the use of transformers in online rein- forcement learning and position them as strong baselines capable of matching or surpassing MLPs, LSTMs, CNNs, and the GTrXL variant across continuous control tasks. Through extensive experi- ments guided by targeted research questions, we analyzed the inner workings of transformer-based agents and distilled practical take- aways to inform future applications in online RL. By consolidating these insights into a unified training setup, we demonstrated strong performance and stable learning across diverse environments. This reframes the transformer from a notoriously unstable model into a competitive and easy-to-train alternative to common baselines. We believe our work offers valuable insights into the training dynamics of transformers in online RL and lays a foundation for future advancements in the field.", "source": "arxiv_pdf", "published": "", "tokens": 128, "sha256": "5103f7e5e744e188971611a581e09a17e5487e3304344328b06c15319995c879"}
{"doc_id": "arxiv:2510.13704#abstract", "url": "https://arxiv.org/abs/2510.13704", "anchor": "#abstract", "type": "paper", "title": "", "section": "ABSTRACT", "text": "Recent works have proposed accelerating the wall-clock training time of actor- critic methods via the use of large-scale environment parallelization; unfortu- nately, these can sometimes still require large number of environment interactions to achieve a desired level of performance. Noting that well-structured representa- tions can improve the generalization and sample efficiency of deep reinforcement learning (RL) agents, we propose the use of simplicial embeddings: lightweight representation layers that constrain embeddings to simplicial structures. This ge- ometric inductive bias results in sparse and discrete features that stabilize critic bootstrapping and strengthen policy gradients. When applied to FastTD3, Fast- SAC, and PPO, simplicial embeddings consistently improve sample efficiency and final performance across a variety of continuous- and discrete-control envi- ronments, without any loss in runtime speed. “Order is not imposed from the outside, but emerges from within1.” — Ilya Prigogine 1", "source": "arxiv_pdf", "published": "", "tokens": 139, "sha256": "12ed143dd109bbfc75bd1eab6baab195df8a38e88cf08385d98837d6e3815d48"}
{"doc_id": "arxiv:2510.13704#introduction:part-1", "url": "https://arxiv.org/abs/2510.13704", "anchor": "#introduction:part-1", "type": "paper", "title": "", "section": "INTRODUCTION", "text": "Deep reinforcement learning (deep RL) has delivered impressive progress in continuous control, enabling agile locomotion (Smith et al., 2022; Zhuang et al., 2023; Margolis et al., 2024) and dex- terous manipulation (Popov et al., 2017; Akkaya et al., 2019; Luo et al., 2025). Yet a persistent tension remains between training speed (wall-clock efficiency) and sample efficiency (the number of environment interactions). Some modern agents such as TD-MPC2 (Hansen et al., 2023) and SR-SPR/BBF (D’Oro et al., 2022; Schwarzer et al., 2023) achieve strong returns with relatively few interactions, but demand substantial compute and engineering complexity. In contrast, recent fast actor–critic variants have scaled throughput with massive parallelization (Li et al., 2023; Singla et al., 2024; Gallici et al., 2025; Seo et al., 2025). While methods such as FastTD3 (Seo et al., 2025) rapidly solve humanoid benchmarks, they require far more interactions to reach compara- ble performance. Similar limitations have been observed in Parallel Q-Learning (Li et al., 2023) and large-scale actor–critic frameworks such as IMPALA and SEED RL (Espeholt et al., 2018; 2020). This trade-off limits applicability in domains where interactions are expensive and time is constrained, such as robotics. A natural objection is that, in modern simulators, environment steps are cheap and can be generated in massive parallel batches, so sample efficiency is less important. However, this view overlooks 1This perspective resonates with deep RL: stability cannot be forced solely through more compute, heavier regularizers, or larger critics. Instead, inductive biases that shape the geometry of representations can allow order to emerge from within, leading to more stable critics and more efficient policies under non-stationarity. 1 arXiv:2510.13704v1 [cs.LG] 15 Oct 2025 Preprint. several practical and scientific concerns. First, algorithms that are data-hungry in simulation rarely transfer well to real-world scenarios (Tobin et al., 2017; Akkaya et al., 2019). Second, large-scale parallelization requires substantial compute and energy resources, raising both efficiency and sus- tainability concerns (Schwartz et al., 2020; Henderson et al., 2020). Third, sample efficiency is closely tied to generalization: agents that exploit structure from fewer trajectories tend to be more ro- bust under distributional shifts (Zhang et al., 2018; Yao et al., 2025). Moreover, in high-dimensional simulators such as IsaacGym, each step can be significantly more expensive, compounding ineffi- ciency as tasks grow harder (Makoviychuk et al., 2021; Rudin et al., 2021). These issues highlight why sample efficiency remains central even in the era of massively parallel deep RL. Shaping representations with auxiliary losses (Anand et al., 2019; Laskin et al., 2020; Schwarzer et al., 2021; Castro et al., 2021; Fujimoto et al., 2023) has been shown to improve sample efficiency in deep RL. However, such methods increase algorithmic complexity and add computational over- head through extra forward and backward passes (Fujimoto et al., 2023). Alternatively, architectural components, such as convolutions (Fukushima, 1980; LeCun et al., 1989) and attention (Bahdanau et al., 2016), can be used to induce structure leading to desirable downstream properties. Discrete and sparse representations have several desirable properties in comparison to their dense and continuous counterparts. Notably, sparse and discrete representations increase robustness to noise (Donoho", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "570e6a2d504a22e350414dd6d79dbd4be0aef411583f5ad598fb032bc858b8d1"}
{"doc_id": "arxiv:2510.13704#introduction:part-2", "url": "https://arxiv.org/abs/2510.13704", "anchor": "#introduction:part-2", "type": "paper", "title": "", "section": "INTRODUCTION", "text": "efficiency in deep RL. However, such methods increase algorithmic complexity and add computational over- head through extra forward and backward passes (Fujimoto et al., 2023). Alternatively, architectural components, such as convolutions (Fukushima, 1980; LeCun et al., 1989) and attention (Bahdanau et al., 2016), can be used to induce structure leading to desirable downstream properties. Discrete and sparse representations have several desirable properties in comparison to their dense and continuous counterparts. Notably, sparse and discrete representations increase robustness to noise (Donoho et al., 2006), training stability by reducing catastrophic interference (Liu et al., 2019), sample efficiency (Fumero et al., 2023), interpretability (Murphy et al., 2012; Lavoie et al., 2023; Wabartha & Pineau, 2024) and improved generative modeling (Lavoie et al., 2025). In this work, we posit that several of those properties are beneficial in the context of reinforcement learning. Within RL, several successful agents like TD-MPC2, Dreamer V2/V3, IQRL, and MAD-TD (Hansen et al., 2023; Alonso et al., 2024; Hafner et al., 2023; Scannell et al., 2024; Voelcker et al., 2025) use discrete latents; yet they typically rely on auxiliary model-based losses to shape the representation. While several methods exist for learning discrete representations explicitly (Jang et al., 2017; Mad- dison et al., 2017; van den Oord et al., 2018), these methods use straight-through estimation (Bengio et al., 2013) which is a biased gradient estimator. Fortunately, discretization may be implicitly induced via Simplicial Embeddings (SEM) (Lavoie et al., 2023), an architectural component that partitions a latent representation into a sequence of L simplices. SEM is fully differentiable, thus avoiding the negative effect of explicit discretization while enacting some of the desirable properties of discrete and sparse representations. Concretely, we show that SEM improves both data efficiency and asymptotic performance across diverse environments such as IsaacGym (Makoviychuk et al., 2021), HumanoidBench (Sferrazza et al., 2024), and the Arcade Learning Environment (Bellemare et al., 2013), while preserving (and often improving) wall-clock speed. 2 PRELIMINARIES 2.1 ACTOR–CRITIC REINFORCEMENT LEARNING We consider a standard Markov decision process (MDP) defined by the tuple M = (S, A, P, r, γ), with state space S, action space A, transition distribution P(s′|s, a), reward function r : S×A →R, and discount factor γ ∈[0, 1). The objective is to maximize the expected discounted return J(π) = Eπ h ∞ X t=0 γtr(st, at) i , (1) where the agent follows a policy π(a|s). Actor–critic methods maintain both a parameterized pol- icy πθ(a|s) (the actor) and an action-value function Qϕ(s, a) (the critic). The critic is trained to minimize the Bellman error LQ(ϕ) = E(s,a,r,s′)∼D h\u0000Qϕ(s, a) −y \u00012i , y = r + γ Ea′∼πθ(·|s′) \u0002 Qϕ−(s′, a′) \u0003 , (2) where ϕ−denotes target network parameters and D is a replay buffer. The actor is updated via the policy gradient defined as ∇θJ(πθ) = Es∼D,a∼πθ \u0002 ∇θ log πθ(a|s) Qϕ(s, a) \u0003 . While this can be effective, bootstrapped training is notoriously fragile. Errors in Qϕ propagate recursively through the target y, and when the representation used to compute Qϕ is poorly conditioned, these errors amplify and cause divergence or", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "51f6f24b028d61b2c90bbafefb8528d9a7296fa1d5af74078d97e6badc623e16"}
{"doc_id": "arxiv:2510.13704#introduction:part-3", "url": "https://arxiv.org/abs/2510.13704", "anchor": "#introduction:part-3", "type": "paper", "title": "", "section": "INTRODUCTION", "text": "= r + γ Ea′∼πθ(·|s′) \u0002 Qϕ−(s′, a′) \u0003 , (2) where ϕ−denotes target network parameters and D is a replay buffer. The actor is updated via the policy gradient defined as ∇θJ(πθ) = Es∼D,a∼πθ \u0002 ∇θ log πθ(a|s) Qϕ(s, a) \u0003 . While this can be effective, bootstrapped training is notoriously fragile. Errors in Qϕ propagate recursively through the target y, and when the representation used to compute Qϕ is poorly conditioned, these errors amplify and cause divergence or collapse (Fujimoto et al., 2018). 2 Preprint. A recent line of work has sought to reduce the wall-clock cost of actor–critic training. FastTD3 (Seo et al., 2025) builds on TD3 (Fujimoto et al., 2018) by leveraging (i) parallel simulation across many environment instances, (ii) large-batch critic updates, and (iii) algorithm design choices like distri- butional critics (C51) (Bellemare et al., 2017), noise scaling and clipped double Q-learning (CDQ) (Fujimoto et al., 2018). Together, these design choices enable high-throughput training while re- taining stable convergence, although FastTD3 still remains sample-inefficient (see App. C for more details). Policies and critics often rely on latent representations extracted from raw states (Lesort et al., 2018). Formally, an encoder fψ : S →Rd maps observations s into embeddings z = fψ(s), which are then consumed by either the critic, the actor, or both depending on the architecture. Some methods share a common encoder across actor and critic (e.g., SAC (Haarnoja et al., 2018), DrQ (Yarats et al., 2021), DrQ-v2 (Yarats et al., 2022)), while others (e.g., DDPG (Lillicrap et al., 2015), FastTD3 (Seo et al., 2025)) maintain separate encoders. Regardless of parameter sharing, these representations play a central role in learning (Garcin et al., 2025). The critic estimates values Qϕ(s, a) ≡Qϕ(fψ(s), a), and the actor conditions its policy πθ(a|s) ≡πθ(a|fψ(s)) on the chosen embedding. Ideally, z should preserve the Markov property and expose predictive features of the reward r and dynamics P. Yet the choice and stability of such embeddings is far from guaranteed. When unconstrained, learned representations can introduce severe pathologies that destabilize value learning. For example, if ∥fψ(s)∥→∞, critics may extrapolate to arbitrarily large Q-values outside the support of the re- play buffer, inflating the Bellman error. Formally, if Qϕ(z, a) = w⊤z + b with linear heads, then ∥Qϕ∥→∞as ∥z∥→∞, leading to exploding targets y and divergent gradients. Similarly, if z exhibits strong correlations or degenerate directions, the critic’s regression problem becomes ill- conditioned: the covariance matrix Σ = E[zz⊤] may approach singularity, amplifying variance in temporal-difference updates. These phenomena are empirically linked to representation collapse, where value estimates drift irrecoverably and policy updates follow unstable gradients (Moalla et al., 2024; Castanyer et al., 2025). 2.2 SIMPLICIAL EMBEDDINGS Simplicial embeddings (SEM; Lavoie et al., 2023) provide a lightweight inductive bias on repre- sentation geometry by constraining latent codes to lie on a product of simplices. Concretely, given encoder outputs fψ(s) ∈RL×V , the latent vector is partitioned into L groups of size V , and a softmax is applied within each group: ˜zℓ,v = exp(zℓ,v/τ) PV v′=1 exp(zℓ,v′/τ) , ∀ℓ∈{1, . . .", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "50af5e2f26229fc865bdda35b029c7439851cb845032f6ac945d5969c91d4a15"}
{"doc_id": "arxiv:2510.13704#introduction:part-4", "url": "https://arxiv.org/abs/2510.13704", "anchor": "#introduction:part-4", "type": "paper", "title": "", "section": "INTRODUCTION", "text": "updates follow unstable gradients (Moalla et al., 2024; Castanyer et al., 2025). 2.2 SIMPLICIAL EMBEDDINGS Simplicial embeddings (SEM; Lavoie et al., 2023) provide a lightweight inductive bias on repre- sentation geometry by constraining latent codes to lie on a product of simplices. Concretely, given encoder outputs fψ(s) ∈RL×V , the latent vector is partitioned into L groups of size V , and a softmax is applied within each group: ˜zℓ,v = exp(zℓ,v/τ) PV v′=1 exp(zℓ,v′/τ) , ∀ℓ∈{1, . . . , L}, v ∈{1, . . . , V }, (3) where τ > 0 is a temperature parameter controlling the degree of sparsity. The resulting embedding ˜z lies in the product space ∆V −1×· · ·×∆V −1, i.e., L categorical distributions of dimension V . This transformation ensures boundedness through group-wise normalization, induces sparsity as softmax competition (sharpened at low τ) drives near one-hot encodings, and promotes group structure by partitioning features into modular subspaces akin to mixtures-of-experts (Shazeer et al., 2017; Ceron et al., 2024c; Willi et al., 2024). In self-supervised learning and downstream classification, SEM has been shown to stabilize training and improve generalization, particularly in low-label and transfer settings (Lavoie et al., 2023). SEM does not rely on auxiliary losses or reconstruction terms; akin to an activation function, it only modifies the embedding geometry with the group-wise softmax, limiting computational overhead. 3 NON-STATIONARITY AMPLIFIES REPRESENTATION COLLAPSE Several works have shown that non-stationarity can lead to severe degradation of learned represen- tations across different domains (Lyle et al., 2022; Kumar et al., 2021a; Lyle et al., 2025; Castanyer et al., 2025). In supervised learning, label noise and distribution shifts can induce representation collapse, where features lose diversity and neurons become inactive (Li et al., 2022; Sokar et al., 2023; Dohare et al., 2024). Similar observations have been made in deep RL: the constantly chang- ing data distribution, induced by an evolving policy, exacerbates this phenomenon, often resulting in unstable critics and poor generalization (Nauman et al., 2024a; Kumar et al., 2021a). These studies 3 Preprint. 0 20 40 60 80 100 Epochs 0.5 1.0 1.5 2.0 Performance (loss ) 0 20 40 60 80 100 Epochs 20 25 30 35 Effective Rank ( ) 0 20 40 60 80 100 Epochs 0 10 20 30 40 50 Dormant neurons (%) ( ) Stationary Non-stationary Non-stationary + SEM Fig. 1: Training dynamics on CIFAR-10 with stationary vs. non-stationary targets. In the stationary regime (fixed targets), losses decrease smoothly, neuron dormancy and effective rank remains controlled, suggesting stable representation learning. In the non-stationary regime (targets shuffled every 20 epochs), the model exhibits higher variance in losses, increased dormant neuron rates, and reduced effective rank. The addition of SEM mitigates this instability. suggest that collapse is not an isolated pathology of specific architectures, but a general failure mode that emerges when training signals are non-stationary. In App. B we provide a formal analysis that demonstrates the relationship between non-stationarity and neuron dormancy. A demonstration on CIFAR-10. We illustrate this phenomenon with a toy experiment on CIFAR- 10 (Krizhevsky, 2009). We compare two", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "e26af210ab52e8249388d455db6b48515ef8489e8b9dd7311473f8a153e46244"}
{"doc_id": "arxiv:2510.13704#introduction:part-5", "url": "https://arxiv.org/abs/2510.13704", "anchor": "#introduction:part-5", "type": "paper", "title": "", "section": "INTRODUCTION", "text": "exhibits higher variance in losses, increased dormant neuron rates, and reduced effective rank. The addition of SEM mitigates this instability. suggest that collapse is not an isolated pathology of specific architectures, but a general failure mode that emerges when training signals are non-stationary. In App. B we provide a formal analysis that demonstrates the relationship between non-stationarity and neuron dormancy. A demonstration on CIFAR-10. We illustrate this phenomenon with a toy experiment on CIFAR- 10 (Krizhevsky, 2009). We compare two training regimes: (i) a stationary setting with fixed labels, and (ii) a non-stationary setting where labels are periodically shuffled to mimic the bootstrap dynam- ics of RL. Let (x, y) be training samples with y ∈{1, . . . , K}. In the stationary regime, targets are fixed, so the conditional distribution p(y|x) is constant and the empirical risk minimizer θ⋆ t remains stable up to stochastic fluctuations. In the non-stationary regime, labels are periodically shuffled so that y 7→πt(y), where πt is a permutation applied every T steps. This induces inflection points in the minimizer, shifting whenever πt changes. Fig. 1 shows that in the stationary regime, training is stable: losses decrease smoothly, dormant neuron rates remain low, and effective rank increases, indicating robust representation learning (Dohare et al., 2024; Sokar et al., 2023; Liu et al., 2025c). In contrast, in the non-stationary regime, we observe instability: oscillating losses, rising neuron dormancy, and collapsing feature rank. Even in this simple supervised setting, instability in the target distribution alone is sufficient to undermine representational integrity. Stabilizing Representations under Non-Stationarity with SEM Simplicial Embeddings (SEM) can mitigate this effect by projecting features onto a structured space that prevents collapse. The transformation enforces energy preservation; since each block has unit mass, representations can- not vanish and tr(Σt) remains bounded away from zero. It also promotes diversity, as intra-block competition spreads information across coordinates, while multiple blocks (L) increase effective rank, counteracting covariance deflation. As shown in Fig. 1, critics trained with SEM retain higher effective rank, larger gradient energy, and lower neuron dormancy even when targets drift. Takeaways: • Non-stationarity exacerbates representation collapse, as evidenced by increased neuron dormancy and reduced effective rank. • Simplicial Embeddings (SEM) introduce a simplex-based geometric prior that sustains feature diversity and prevent feature collapse. 4 UNDERSTANDING THE IMPACT OF SEM ON DEEP RL NETWORKS In actor–critic methods such as FastTD3, the critic is trained against bootstrapped targets yt(s, a) = r(s, a) + γ Qϕ−(s′, πθ(s′)). Both the target distribution Dt (samples (s, a, r, s′) from the replay buffer) and the target value yt evolve as the policy πθ is updated. This continual drift produces a persistent bias term in bt = ∇Lt+1(θ⋆ t ) = E(s,a)∼Dt+1 h\u0000Qϕ(s, a) −yt+1(s, a) \u0001 ∇θQϕ(s, a) i , which is nonzero whenever πθ or Dt changes. Thus, the critic is never optimizing a fixed objective but is instead forced to chase a moving target. 4 Preprint. ReLu Softmax Tanh Linear SEM module SEM Module ReLu Linear SEM module Baseline SEM Critic Actor Fig. 2: Actor–critic network architecture with SEM.", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "3d0c1bb268ef048d15b4add892c22c901a19fedb1ebd5890305976970bb47334"}
{"doc_id": "arxiv:2510.13704#introduction:part-6", "url": "https://arxiv.org/abs/2510.13704", "anchor": "#introduction:part-6", "type": "paper", "title": "", "section": "INTRODUCTION", "text": "policy πθ is updated. This continual drift produces a persistent bias term in bt = ∇Lt+1(θ⋆ t ) = E(s,a)∼Dt+1 h\u0000Qϕ(s, a) −yt+1(s, a) \u0001 ∇θQϕ(s, a) i , which is nonzero whenever πθ or Dt changes. Thus, the critic is never optimizing a fixed objective but is instead forced to chase a moving target. 4 Preprint. ReLu Softmax Tanh Linear SEM module SEM Module ReLu Linear SEM module Baseline SEM Critic Actor Fig. 2: Actor–critic network architecture with SEM. The actor (left) and critic (middle) architec- tures are modified with a SEM module, which partitions features into groups and applies group-wise softmax (right panel), constraining them to a product of simplices. Representation collapse under such non-stationarity poses a fundamental barrier to stable and ef- ficient deep RL (see App. A for additional contex). Standard actor–critic methods are particularly vulnerable. The critic’s representations are trained against drifting targets, and the actor in turn depends on those representations to update its policy. This tight coupling amplifies instability, lead- ing to poor sample efficiency in continuous control tasks. To address this challenge, we evaluate Simplicial Embeddings (SEM) as a representation-level regularizer. SEM aims to encourage the hid- den features of both actor and critic networks to maintain a well-structured geometric organization, preventing collapse and preserving diversity. By stabilizing the embedding space, SEM provides a principled mechanism for variance reduction and improved sample efficiency. Setup. Because this section involves a large number of ablations and is computationally expen- sive, we restrict experiments to five benchmarks from the Humanoid suite (Sferrazza et al., 2024), evaluated on (Seo et al., 2025). We report aggregate performance across the five tasks and six seeds, with full details provided in the App. F. Integrating SEM on Actor-Critic Algorithms. We choose FastTD3 (Seo et al., 2025), as our primary testbed. FastTD3 is specifically designed to be a simple and compute-efficient baseline for continuous-control and humanoid benchmarks. Its streamlined architecture yields strong perfor- mance while significantly reducing wall-clock training time. At the same time, FastTD3 inherits the critic-driven weaknesses of TD3; its bootstrapped value targets are generated online by the actor, making the critic susceptible to non-stationarity. This coupling amplifies representation collapse, as instabilities in the critic propagate to both value estimates and policy updates. We conduct most of our ablations on FastTD3, while later sections demonstrate that the benefits of SEM also extend to other actor–critic algorithms such SAC (Haarnoja et al., 2018) and PPO (Schulman et al., 2017). SEM can be applied to the actor, the critic, or both network streams. We build on prior work showing that the penultimate layer plays a critical role in representation quality (Moalla et al., 2024; Ceron et al., 2024c; Sokar & Castro, 2025), and that regularizing this layer can yield substantial perfor- mance gains. Fig. 2 illustrates how SEM is integrated into the actor–critic networks of FastTD3. For the critic, SEM replaces the baseline linear head with a structured projection, regularizing value estimates in the distributional C51 setting. For the actor, SEM is applied at the penultimate layer before the final linear+tanh, ensuring", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "89fd1d8e35300b6e896864d73ea7661702fc476363101de49ef463b7f88efc0c"}
{"doc_id": "arxiv:2510.13704#introduction:part-7", "url": "https://arxiv.org/abs/2510.13704", "anchor": "#introduction:part-7", "type": "paper", "title": "", "section": "INTRODUCTION", "text": "layer plays a critical role in representation quality (Moalla et al., 2024; Ceron et al., 2024c; Sokar & Castro, 2025), and that regularizing this layer can yield substantial perfor- mance gains. Fig. 2 illustrates how SEM is integrated into the actor–critic networks of FastTD3. For the critic, SEM replaces the baseline linear head with a structured projection, regularizing value estimates in the distributional C51 setting. For the actor, SEM is applied at the penultimate layer before the final linear+tanh, ensuring that the policy is conditioned on bounded and sparse features. Across the paper, dashed blue (blue, - -) curves indicate the baseline, while solid green, (green, —) curves represent the interventions added to the baseline. Fig. 3 shows clear gains when applying SEM to the actor or to both actor and critic, and more moderate gains when applied only to the critic. Although different SEM dimensions (V ) improve sample efficiency and asymptotic performance, V = 64 appears most effective. We further explore the relationship between L and V (see sec 4), as this tradeoff was a central focus of the original SEM study (Lavoie et al., 2023). These results echo the non-stationary CIFAR-10 experiment, where SEM prevented feature collapse and stabilized learning (see Fig. 1). 5 Preprint. 0 2 4 6 8 Environment Steps ×104 0.0 0.5 1.0 Average Normalized Return Baseline vs (+ SEM Actor) 0 2 4 6 8 Environment Steps ×104 Baseline vs (+ SEM Critic) Baseline + SEM (V=4) + SEM (V=16) + SEM (V=64) 0 2 4 6 8 Environment Steps ×104 Baseline vs (+ SEM Actor/Critic) Fig. 3: Average normalized return on 5 HumanoidBench tasks over 6 seeds. Baseline agent (blue, - -) vs. SEM variants applied to actor, critic, or both. Each curve corresponds to an embed- ding dimension; dim= 64 (green, —) is highlighted. SEM accelerates early learning and improves asymptotic performance, with dim= 64 giving the most stable gains. 2 4 6 8 ×104 0.0 2.5 5.0 7.5 h1hand-walk-v0 ×102 Episode Return FastTD3 + SEM (Actor) 2 4 6 8 ×104 2 4 6 ×101 Actor · Net e-rank 2 4 6 8 ×104 2 4 6 ×101 Actor · Net h 2 2 4 6 8 ×104 1.0 1.5 2.0 2.5 ×101 Critic · Net e-rank 2 4 6 8 ×104 1.5 2.0 2.5 3.0 ×101 Critic · Net h 2 2 4 6 8 Environment Steps ×104 0.5 1.0 h1hand-stand-v0 ×103 2 4 6 8 Environment Steps ×104 2 4 6 ×101 2 4 6 8 Environment Steps ×104 2 4 6 ×101 2 4 6 8 Environment Steps ×104 1.0 1.5 2.0 ×101 2 4 6 8 Environment Steps ×104 2 3 ×101 Fig. 4: Learning and representation diagnostics on 2 HumanoidBench tasks over 6 seeds. SEM reaches high return earlier, raises actor/critic effective rank, and keeps actor features compact. The Effect of SEM on Learning Dynamics in Deep RL. We empirically evaluate the impact of SEM on the stability and efficiency of actor–critic algorithms. Our analysis combines both learning performance (returns, losses, TD error, critic disagreement) and representation", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "b0b8ec48ca8a82d9af6df4e41d2a52f793a11861dac7b2a4a2edc5f4b3213fd3"}
{"doc_id": "arxiv:2510.13704#introduction:part-8", "url": "https://arxiv.org/abs/2510.13704", "anchor": "#introduction:part-8", "type": "paper", "title": "", "section": "INTRODUCTION", "text": "1.0 1.5 2.0 ×101 2 4 6 8 Environment Steps ×104 2 3 ×101 Fig. 4: Learning and representation diagnostics on 2 HumanoidBench tasks over 6 seeds. SEM reaches high return earlier, raises actor/critic effective rank, and keeps actor features compact. The Effect of SEM on Learning Dynamics in Deep RL. We empirically evaluate the impact of SEM on the stability and efficiency of actor–critic algorithms. Our analysis combines both learning performance (returns, losses, TD error, critic disagreement) and representation quality (effective rank, feature norms), allowing us to connect sample-efficiency gains to underlying representational dynamics. This dual perspective highlights not only whether SEM improves performance, but also why it stabilizes training. A detailed explanation of each metric is provided in App. G. To understand why SEM improves performance, we turn to representation-level diagnostics. Fig. 4 shows that SEM increases the effective rank of actor features, and bounds actor feature norms. Late in training, SEM also lifts the critic effective rank, a signs of more expressive and robust value learn- ing. High effective rank is a proxy for avoiding representational collapse (Moalla et al., 2024; Mayor et al., 2025). In the deep RL literature, representation collapse under drift has been empirically as- sociated with capacity loss (Lyle et al., 2021), deterioration of feature rank (Kumar et al., 2021b), and implicit under-parameterization (Kumar et al., 2021a). In supervised and self-supervised set- tings, techniques like orthogonality regularization and rank-preserving weight regularizers are used to prevent feature collapse (He et al., 2024). These representational patterns align with our for- mal analysis, showing that SEM prevents covariance deflation and sustains gradient energy, thereby preventing feature collapse and boosting performance. As shown in Fig. 5, SEM improves optimization stability over the baseline. Agents with SEM achieve higher returns earlier and maintain smaller, more stable TD errors, reduced critic disagree- ment, and lower critic-distribution discrepancy. Such effects are crucial, as instability in boot- strapped critics is a primary failure mode of actor–critic methods (Fujimoto et al., 2019; Kumar et al., 2021a). By constraining representation geometry, SEM produces better-conditioned features that yield more calibrated value estimates, echoing similar findings in representation regularization for deep RL (Anand et al., 2019; Laskin et al., 2020; Schwarzer et al., 2021). These results indicate that SEM not only accelerates learning but also yields more calibrated value estimates, mitigating instability in bootstrapped critics. In Fig. 6, we focus our lens on the SEM module itself and examine how it shapes representations and action behavior. As training proceeds, the SEM layer’s activations become markedly sparser (higher Gini (Hurley & Rickard, 2009; Zonoobi et al., 2011)) and more sharply peaked (lower simplex entropy), while the overall action variance from the policy also declines. This trend is consistent with SEM’s design, where the block-wise softmax promotes competition and selective activation. 6 Preprint. 2 4 6 8 ×104 0.0 2.5 5.0 7.5 h1hand-walk-v0 ×102 Episode Return FastTD3 + SEM (Actor) 2 4 6 8 ×104 7.5 5.0 2.5 ×101 Actor Loss 2 4 6 8 ×104 2 3 4 Critic Loss 2 4 6 8 ×104 0.25 0.50 0.75", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "1ddf1036bad5a0410c3a3bf1a46e51a796d004805733906d72f69815ce4b62f2"}
{"doc_id": "arxiv:2510.13704#introduction:part-9", "url": "https://arxiv.org/abs/2510.13704", "anchor": "#introduction:part-9", "type": "paper", "title": "", "section": "INTRODUCTION", "text": "more sharply peaked (lower simplex entropy), while the overall action variance from the policy also declines. This trend is consistent with SEM’s design, where the block-wise softmax promotes competition and selective activation. 6 Preprint. 2 4 6 8 ×104 0.0 2.5 5.0 7.5 h1hand-walk-v0 ×102 Episode Return FastTD3 + SEM (Actor) 2 4 6 8 ×104 7.5 5.0 2.5 ×101 Actor Loss 2 4 6 8 ×104 2 3 4 Critic Loss 2 4 6 8 ×104 0.25 0.50 0.75 1.00 TD | | (mean) 2 4 6 8 ×104 0.5 1.0 Q-gap |Q1 Q2| (mean) 2 4 6 8 ×104 2 4 6 ×10 2 Cramér (Critics) 2 4 6 8 Environment Steps ×104 0.5 1.0 h1hand-stand-v0 ×103 2 4 6 8 Environment Steps ×104 7.5 5.0 2.5 ×101 2 4 6 8 Environment Steps ×104 2 3 4 2 4 6 8 Environment Steps ×104 0.5 1.0 2 4 6 8 Environment Steps ×104 0.5 1.0 2 4 6 8 Environment Steps ×104 0 2 4 ×10 2 Fig. 5: Learning dynamics on 2 HumanoidBench tasks. SEM reaches high return faster, with lower losses, smaller TD error, reduced critic disagreement, and better-calibrated value estimates. 2 4 6 8 ×104 0.0 2.5 5.0 7.5 h1hand-walk-v0 ×102 Episode Return FastTD3 + SEM (Actor) 2 4 6 8 ×104 6 8 ×10 1 Pre-rep · Gini 2 4 6 8 ×104 6 8 ×10 1 Post-rep · Gini 2 4 6 8 ×104 2 3 4 SimNorm · Entropy H(p) 2 4 6 8 ×104 0.8 1.0 Actor · Action 2 4 6 8 Environment Steps ×104 0.5 1.0 h1hand-stand-v0 ×103 2 4 6 8 Environment Steps ×104 4 6 8 ×10 1 2 4 6 8 Environment Steps ×104 4 6 8 ×10 1 2 4 6 8 Environment Steps ×104 3 4 2 4 6 8 Environment Steps ×104 0.6 0.8 1.0 Fig. 6: Sparsity, entropy, and action std on 2 HumanoidBench tasks. SEM agents achieve higher returns with sparser features, lower entropy, and more stable action scales. 0 1 2 3 4 5 6 7 8 Environment Steps ×104 0.0 0.2 0.4 0.6 0.8 1.0 1.2 Average Normalized Return CRELU Gumbel + ST SEM V.Q. 0 2 4 6 8 Environment Steps ×104 V = 4 0 2 4 6 8 Environment Steps ×104 V = 64 L = 1 L = 2 L = 4 L = 16 L = 64 0 2 4 6 8 Environment Steps ×104 L = 1 0 2 4 6 8 Environment Steps ×104 L = 64 V = 4 V = 8 V = 16 V = 64 Fig. 7: Aggregated average return on 5 HumanoidBench tasks. We constrain the encoder’s out- put of the actor. (left) SEM outperforms alternative methods to impart structure on the encoder’s output. (middle) Effect of varying L. Small L generally leads to better return given enough repre- sentation capacity. (right) Effect of varying V . Large V generally leads to better returns. As a result, the module imposes structured, energy-preserving constraints on its layer, encouraging more", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "c7f502620c7cf0af2bd7591d1e45582dd5799ed21e63631156d88133c1089163"}
{"doc_id": "arxiv:2510.13704#introduction:part-10", "url": "https://arxiv.org/abs/2510.13704", "anchor": "#introduction:part-10", "type": "paper", "title": "", "section": "INTRODUCTION", "text": "V = 16 V = 64 Fig. 7: Aggregated average return on 5 HumanoidBench tasks. We constrain the encoder’s out- put of the actor. (left) SEM outperforms alternative methods to impart structure on the encoder’s output. (middle) Effect of varying L. Small L generally leads to better return given enough repre- sentation capacity. (right) Effect of varying V . Large V generally leads to better returns. As a result, the module imposes structured, energy-preserving constraints on its layer, encouraging more decisive feature usage and reducing noise in the downstream policy mapping. Interestingly, this pattern also resonates with prior work in deep RL and representation learning. Hernandez- Garcia & Sutton (2019) show that enforcing sparsity in representations can improve robustness and mitigate interference in Q-learning settings. Moreover, recent studies on sparse architectures in deep RL such find that appropriately structured sparsity can enhance training stability and efficiency (Graesser et al., 2022; Ceron et al., 2024c;b; Ma et al., 2025). Comparing SEM to other Regularization Methods To contextualize the benefits of simplicial embeddings, we compare SEM to alternative methods to induce structure on the encoder’s output. We compare SEM to commonly used methods for learning discrete explicit representations: Gumbel + straight-through (Jang et al., 2017; Maddison et al., 2017) and Vector Quantization (van den Oord et al., 2018). We also compare SEM to C-RELU (Abbas et al., 2023) which have been shown to improve the representation’s stability. We present the results in Fig. 7 (left) and find SEM to be more efficient and to lead to higher return than alternative methods. We conjecture that such improvement over Gumbel + ST and Vector quantization can be attributed to the fact that SEM does not necessitate the use of the straight-through estimator. Analyzing SEM Parameters in Deep RL Lavoie et al. (2023) highlighted the effect of the sim- plex dimensionality V and number of simplices L, which jointly control sparsity and capacity of the representation. Investigating these parameters in deep RL is essential to understand how SEM balances representation capacity and stability under non-stationary training, and whether the same 7 Preprint. 0 2 4 6 8 Environment Steps ×104 0.0 0.5 1.0 Average Normalized Return Baseline +SEM (Actor) # Envs = 32 # Envs = 64 # Envs = 128 0 2 4 6 8 Environment Steps ×104 0.0 0.5 1.0 Baseline +SEM (Actor) RB Size = 1024 RB Size = 51200 0 2 4 6 8 Environment Steps ×104 0.0 0.5 1.0 Baseline + SEM (Actor) Batch = 2k Batch = 32k Batch = 131k 0 2 4 6 8 Environment Steps ×104 0.0 0.5 1.0 Average Normalized Return Baseline +SEM (Actor) No CDQ CDQ 0 2 4 6 8 Environment Steps ×104 0.0 0.5 1.0 Baseline +SEM (Actor) No Distributional (C51) Distributional (C51) 0 2 4 6 8 Environment Steps ×104 0.0 0.5 1.0 Baseline +SEM (Actor) Noise scale = 0.001 Noise scale = 0.4 Fig. 8: Effect of core design choices on FastTD3 with and without SEM on 5 HumanoidBench tasks. SEM solid green, (green, —) consistently improves sample efficiency and asymptotic return", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "8106527ce335b9e45ebe3a7f04e38166ae2dc2a0544823367e5f25759bb7eca9"}
{"doc_id": "arxiv:2510.13704#introduction:part-11", "url": "https://arxiv.org/abs/2510.13704", "anchor": "#introduction:part-11", "type": "paper", "title": "", "section": "INTRODUCTION", "text": "1.0 Average Normalized Return Baseline +SEM (Actor) No CDQ CDQ 0 2 4 6 8 Environment Steps ×104 0.0 0.5 1.0 Baseline +SEM (Actor) No Distributional (C51) Distributional (C51) 0 2 4 6 8 Environment Steps ×104 0.0 0.5 1.0 Baseline +SEM (Actor) Noise scale = 0.001 Noise scale = 0.4 Fig. 8: Effect of core design choices on FastTD3 with and without SEM on 5 HumanoidBench tasks. SEM solid green, (green, —) consistently improves sample efficiency and asymptotic return across all settings, showing robustness to both hyperparameter variation and architectural design choices. tradeoffs observed in self-supervised representation learning extend to RL. We study the effect of varying V and L in Fig. 7 (middle and right, respectively). We find that increasing V generally improves performance, but only up to a certain point. On the other hand, we find that providing too much free capacity by increasing L deteriorates the returns, suggesting that restricting the represen- tation capacity is crucial. FastTD3 Design Choices and Simplicial Embeddings. FastTD3 extends TD3 with several de- sign choices that improve throughput and stability, including parallel simulation, large-batch train- ing, and distributional critics (Seo et al., 2025). These modifications enable actor–critic learning to scale efficiently in wall-clock time, but they do not address the geometry of the learned representa- tions. In this section, we analyze how SEM complements FastTD3 by regularizing representation space and evaluate its effectiveness across the algorithmic design choices. In Fig. 8, we observe that SEM outperforms the baseline even when the agent is trained with reduced data availability (fewer environments, smaller replay buffers, or smaller batch sizes). Comparable gains also appear when algorithmic design choices such as CDQ and C51 are removed. These results demonstrate the robustness of SEM across both data-limited and simplified agent settings. 5 EMPIRICAL EVALUATION We further evaluate the effectiveness and generality of SEM across a diverse set of deep RL al- gorithms and environments. Our study spans both off-policy and on-policy methods, including FastTD3, FastTD3-SimBaV2, FastSAC (Seo et al., 2025), and PPO (Schulman et al., 2017). Ex- periments are conducted on challenging humanoid benchmarks (28-h1hand tasks), (Sferrazza et al., 2024), IsaacLab (Mittal et al., 2023), IsaacGym suite (Makoviychuk et al., 2021), MTBench (Joshi et al., 2025), and the Atari-10 suite (Aitchison et al., 2023), covering both continuous-control and pixel-based settings. Following prior work (Seo et al., 2025; Castanyer et al., 2025), we evalu- ate continuous-control tasks with six seeds and Atari results with three seeds, and aggregate perfor- mance across environments is reported. Full environment details and hyperparameter configurations are provided in App. H. Fast Actor–Critic Algorithms. We first evaluate SEM on the HumanoidBench benchmark us- ing three recent fast actor–critic baselines: FastTD3, FastTD3–SimBaV2, and FastSAC (Seo et al., 2025). These algorithms represent compute–efficient variants of TD3 and SAC, designed to scale with parallel simulation while maintaining strong performance on high–dimensional humanoid con- trol. FastTD3–SimBaV2 incorporates hyperspherical normalization and reward scaling to acceler- ate critic training and stabilize optimization (Lee et al., 2025b); and FastSAC adapts the entropy– regularized SAC framework with similar throughput–oriented design choices, achieving high", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "c899345617e4cccdcceaff38cfce0510f73806d577dc330b02e1655820d1d9ea"}
{"doc_id": "arxiv:2510.13704#introduction:part-12", "url": "https://arxiv.org/abs/2510.13704", "anchor": "#introduction:part-12", "type": "paper", "title": "", "section": "INTRODUCTION", "text": "first evaluate SEM on the HumanoidBench benchmark us- ing three recent fast actor–critic baselines: FastTD3, FastTD3–SimBaV2, and FastSAC (Seo et al., 2025). These algorithms represent compute–efficient variants of TD3 and SAC, designed to scale with parallel simulation while maintaining strong performance on high–dimensional humanoid con- trol. FastTD3–SimBaV2 incorporates hyperspherical normalization and reward scaling to acceler- ate critic training and stabilize optimization (Lee et al., 2025b); and FastSAC adapts the entropy– regularized SAC framework with similar throughput–oriented design choices, achieving high paral- lel efficiency while preserving training stability. 8 Preprint. 0 2 4 6 8 Environment Steps ×104 0.00 0.25 0.50 0.75 1.00 Average Normalized Return Baseline + SEM (Actor) 0 2 4 6 8 Environment Steps ×104 0 2 4 6 8 ×10 1 Baseline + SEM (Actor) 0 2 4 6 8 Environment Steps ×104 0 2 4 6 ×10 1 Baseline + SEM (Actor) Fig. 9: SEM on fast actor–critic algorithms. Average normalized return on HumanoidBench with FastTD3 (left), FastTD3–SimBa (middle), and FastSAC (right). SEM, solid green, (green, —), consistently improves sample efficiency and yields higher final performance across all algorithms. 0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0 Total Timesteps (in millions) ×101 0 2 4 6 8 IQM Human Normalized Score ×10 1 PPO + SEM (Actor) 0 20 40 60 80 100 Environment Steps (in millions) 0.0 0.1 0.2 0.3 0.4 0.5 0.6 Normalized Score 0 1 2 3 4 5 6 Environment Steps ×104 2 4 6 8 Episode Return ×101 Booster T1 FastTD3 + SEM (Actor) + SEM (Actor+Critic) Fig. 10: Performance of PPO with and without SEM across tasks. Left: PPO on the Atari-10 suite (pixel-based). Center: PPO in IsaacGym. Right: Booster T1 (humanoid robot with fixed arms) comparing FastTD3. Applied SEM accelerates learning and improves return over the baseliens. Across all three baselines, integrating SEM into the actor consistently accelerates early learning and improves asymptotic return. As shown in Fig. 9, SEM agents not only converge faster than their respective baselines, but also maintain lower variance across seeds. These results demonstrate that SEM provides complementary benefits to fast actor–critic methods, enhancing both stability and sample efficiency without modifying their underlying optimization procedures (see App. I for per-task learning curves). Fig. 10 (right) further shows that SEM improves policy learning on the Booster T1 humanoid robot (Seo et al., 2025), a real-robot benchmark used to validate transfer from large-scale MuJoCo training (Zakka et al., 2025) (see D.0.5 for more details). We also evaluate FastTD3 on 12-h1, 12-g1 tasks and 9-IsaacGym tasks, where a similar pattern is observed, as shown in App. J. Proximal Policy Optimization Algorithm. To evaluate the generality of SEM beyond off-policy methods, we integrate it into PPO (Schulman et al., 2017), a popular on-policy method, using the CleanRL implementation (Huang et al., 2022). We evaluate SEM on two distinct benchmarks, Isaac- Gym for continuous control and the ALE (Bellemare et al., 2013) for pixel-based discrete control in Atari games. In both domains, SEM improves PPO by accelerating convergence and increas- ing final returns. The per-environment learning curves are shown in", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "e0b8368f0ad950059afb57d00ce2045563cc69a5c3f0692dbc67ba5e7ab05477"}
{"doc_id": "arxiv:2510.13704#introduction:part-13", "url": "https://arxiv.org/abs/2510.13704", "anchor": "#introduction:part-13", "type": "paper", "title": "", "section": "INTRODUCTION", "text": "Policy Optimization Algorithm. To evaluate the generality of SEM beyond off-policy methods, we integrate it into PPO (Schulman et al., 2017), a popular on-policy method, using the CleanRL implementation (Huang et al., 2022). We evaluate SEM on two distinct benchmarks, Isaac- Gym for continuous control and the ALE (Bellemare et al., 2013) for pixel-based discrete control in Atari games. In both domains, SEM improves PPO by accelerating convergence and increas- ing final returns. The per-environment learning curves are shown in Fig. 19. Aggregate results are summarized in Fig. 10, with the left panel showing performance gains on the Atari-10 suite and the middle panel showing improvements on the IsaacGym tasks. These results demonstrate that SEM’s benefits are not limited to TD3-style critics but extend to policy-gradient methods and vision-based RL, underscoring its broad applicability. Multitask and Offline-to-online Deep RL. Recent work by Joshi et al. (2025) introduced a large- scale benchmark for multi-task reinforcement learning (MTRL) in robotics. Implemented in Isaac- Gym, this benchmark comprises over seventy robotic control problems spanning both manipulation and locomotion, with subsets such as MT50 focused on manipulation. We compare FastTD3 (Seo et al., 2025) to its SEM-augmented variants (+SEM). As shown in Fig. 11 (right), +SEM improves sample efficiency, achieving faster learning and higher returns within the same training budget. We also evaluate Flow Q-Learning (FQL; Park et al., 2025b), an actor–critic–style method that couples a value function with a flow-based policy generator trained to transport actions toward high-value regions. Despite differing from standard policy-gradient updates, FQL preserves the critic-guided policy improvement structure central to actor–critic algorithms. Applying SEM to the flow-based actor improves sample efficiency and stability during the offline-to-online transi- tion (see Fig. 11), where representations must adapt from static replay data to evolving on-policy 9 Preprint. 0 1M 2M Steps 0.0 0.2 0.4 0.6 0.8 1.0 Performance puzzle-4x4 0 1M 2M Steps cube-double 0 1M 2M Steps scene FQL + SEM (Actor) 1 2 3 4 5 Environment Steps ×104 2 4 6 8 Episode Return ×10 1 MTBench- Mt50 FastTD3 + SEM (Actor) + SEM (Critic) + SEM (Actor+Critic) Fig. 11: Left: Offline-to-online RL results on 3 OGBench tasks (5 seeds) (Park et al., 2025a). Online fine-tuning starts at 1M steps. Right: MTBench MT50 (robotics tasks) comparing FastTD3. Applied SEM accelerates learning and improves return over the baseliens. distributions. We view these observations as opening opportunities to investigate whether simplex- constrained embeddings can help mitigate representation drift and facilitate adaptation; establishing this more broadly will require further study and is left to future work. 6 DISCUSSION Our results demonstrate that geometric priors on representation space can substantially improve the efficiency of deep RL agents. By constraining features to a product of simplices, SEM yields bounded and sparse embeddings that avoid feature collapse and neuron dormancy under non- stationarity. This lightweight inductive bias requires no auxiliary losses, adds effectively zero com- putational cost (see Table 2), and consistently improves sample efficiency and asymptotic return across various actor–critic methods and a diverse set of benchmarks. Unlike existing model-based approaches in RL which use discrete", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "1736e378803b88497dfbdb4c57994d876c5e6c3fdaff40496bfe9c3b939e4351"}
{"doc_id": "arxiv:2510.13704#introduction:part-14", "url": "https://arxiv.org/abs/2510.13704", "anchor": "#introduction:part-14", "type": "paper", "title": "", "section": "INTRODUCTION", "text": "on representation space can substantially improve the efficiency of deep RL agents. By constraining features to a product of simplices, SEM yields bounded and sparse embeddings that avoid feature collapse and neuron dormancy under non- stationarity. This lightweight inductive bias requires no auxiliary losses, adds effectively zero com- putational cost (see Table 2), and consistently improves sample efficiency and asymptotic return across various actor–critic methods and a diverse set of benchmarks. Unlike existing model-based approaches in RL which use discrete state-embeddings (Hansen et al., 2023; Hafner et al., 2020; 2023; Scannell et al., 2025), SEM does not require auxiliary objectives or additional networks. Surprisingly, we find that the benefits of SEM are most pronounced when ap- plied to the actor’s penultimate layer, where feature geometry most directly shapes policy gradients. Our analyses indicate that SEM alleviates several optimization difficulties in deep RL (Moalla et al., 2024; Juliani & Ash, 2024). By preserving effective rank, bounding feature norms, and reducing critic disagreement, SEM provides more reliable gradients and stabilizes the bootstrapped targets that often undermine critic training. These effects highlight representation geometry as a simple but powerful lever for stabilizing learning under non-stationarity. Limitations and Future Work. SEM is not a universal remedy. In tasks with extreme distribution shift or very sparse rewards, feature collapse and critic drift may still occur, and SEM introduces hyperparameters (L, V , τ) that require light tuning to balance sparsity and capacity. For consistency and computational efficiency, we adopted the baseline models’ default hyperparameters across all experiments. Nonetheless, RL agents are notably sensitive to these choices (Ceron et al., 2024a; Patterson et al., 2024), and ideally each experimental setting would undergo a dedicated hyperpa- rameter search, though this is often computationally prohibitive. Moreover, our experiments focus on continuous control and Atari; its impact on large-scale vision or language-conditioned RL re- mains untested. Future work should investigate adaptive schedules for (L, V, τ), and integration in more general-purpose algorithms such as MR.Q (Fujimoto et al., 2025), which combine multiple objectives and scale across domains. Another direction is to examine whether SEM benefits value- based algorithms, and to explore both its potential for scaling network architectures (Ceron et al., 2024b; Sokar et al., 2025) and its interaction with architectural priors (e.g., MoEs, Residual Nets) (Ceron et al., 2024c; Castanyer et al., 2025; Kooi et al., 2025). ACKNOWLEDGMENTS The authors would like to thank Ali Saheb Pasand and Lu Li for valuable discussions during the preparation of this work. We thank Younggyo Seo for his kindness in answering questions about the FastTD3 repository. Gopeshh Subbaraj deserves a special mention for providing valuable feedback on an early draft of the paper. 10 Preprint. The research was enabled in part by computational resources provided by the Digital Research Al- liance of Canada (https://alliancecan.ca) and Mila (https://mila.quebec). We want to acknowledge funding support from Google and CIFAR AI. We would also like to thank the Python community (Van Rossum & Drake Jr, 1995; Oliphant, 2007) for developing tools that en- abled this work, including NumPy (Harris et al., 2020), Matplotlib (Hunter, 2007), Jupyter", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "df884c2ae5434d890ccb031fa991546553f745639135781a6f637ab5aa3fb799"}
{"doc_id": "arxiv:2510.13704#introduction:part-15", "url": "https://arxiv.org/abs/2510.13704", "anchor": "#introduction:part-15", "type": "paper", "title": "", "section": "INTRODUCTION", "text": "providing valuable feedback on an early draft of the paper. 10 Preprint. The research was enabled in part by computational resources provided by the Digital Research Al- liance of Canada (https://alliancecan.ca) and Mila (https://mila.quebec). We want to acknowledge funding support from Google and CIFAR AI. We would also like to thank the Python community (Van Rossum & Drake Jr, 1995; Oliphant, 2007) for developing tools that en- abled this work, including NumPy (Harris et al., 2020), Matplotlib (Hunter, 2007), Jupyter (Kluyver et al., 2016), and Pandas (McKinney, 2013). ETHICS STATEMENT This paper presents work whose goal is to advance the field of Machine Learning, and reinforcement learning in particular. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here. REPRODUCIBILITY STATEMENT We provide all the details to reproduce our results in the Appendix (see section App. E and App. H). LLM USE LLMs were used to assist paper editing and to write the code for plotting experiments.", "source": "arxiv_pdf", "published": "", "tokens": 165, "sha256": "c22cec0dc98a8b050afcd63c4b9bfb1f8de0dd9958d9a68067f4f484c4fe4a18"}
{"doc_id": "arxiv:2510.13048#introduction:part-1", "url": "https://arxiv.org/abs/2510.13048", "anchor": "#introduction:part-1", "type": "paper", "title": "", "section": "Introduction", "text": "Kitbashing is a term coined by hobbyists to describe the reuse of parts from multiple toy-kit models in novel and different ways Authors’ Contact Information: Minghao Guo, MIT, CSAIL, USA; Victor Zordan, Roblox, Clemson University, USA; Sheldon Andrews, École de technologie supérieure, Canada; Wojciech Matusik, MIT, CSAIL, USA; Maneesh Agrawala, Stanford University, USA; Hsueh-Ti Derek Liu, Roblox, Canada. from their original intent.1 In this way, a real-world hobbyist can make a novel robot, for example, from pieces sourced from various models/kits. Digital creators have adopted this approach to develop kitbash datasets that can be assembled in various ways to create unique environments or 3D models. We take inspiration from this concept in the development of articulated 3D objects, namely by taking advantage of existing assemblies to build new ones. Specifically, articulation refers to the connection of subparts from an assembly of parts that move relative to one another. For example, a child part might slid or turn/rotate relative to its parent. Beyond articulation, such objects must also be functional, meaning that they satisfy task-specific requirements for given actions. Functional goals vary based on the object, such as clearance with collision-free paths as one subpart moves without penetrating neighboring subparts, or reachability as end-effectors are able to make contact with target locations in an environment. Functional, articulated objects mimic real-world counterparts from simple mechanisms to complicated machines. And in a virtual setting, they facilitate the creation of richer environments with increased interactivity and immersion. To this end, we aspire to build 3D articulated, functional objects through the re-use of object articulations from existing 3D models and simple descriptions of functionality. 1https://en.wikipedia.org/wiki/Kitbashing arXiv:2510.13048v1 [cs.RO] 14 Oct 2025 39:2 • Guo et al. To date, the production of articulated objects remains largely a manual process. We propose to automate their construction through an optimization that places 3D subparts based on knowledge from existing assemblies and functional requirements. We observe that existing articulations expose functional features in the geometry, specifically in the region that is spatially surrounded by the kine- matic part of interest. This geometric region encodes cues that should be preserved when attaching the part to a new parent. We note that these features can provide a strong geometric guide for reattachment; for example, a wheel should snap into a recess with similar geometric affordances. With this insight, we capture these cues with a vector distance field (VDF) that measures offsets from the part to its surrounding parent. Because an articulation spans a con- tinuum of poses, we accumulate the VDF-matching error across the entire pose dimension. The resulting energy is therefore kinematics- aware: it favors placements whose surrounding geometry matches the original socket throughout the entire articulation cycle. Ag- gregating over poses effectively adds a time dimension on top of the spatial dimension. We address this added complexity with an alternating optimization scheme that keeps runtime practical. We also want the optimizer to account for the functional goals desired of the new object. Functional tasks may range from reaching tasks to accommodate a specific scene, to packing a reconfigurable table into a truck", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "ca7bfda0702ea263659460cbac0f772b1c1f3300d704f82547df3c48653635ee"}
{"doc_id": "arxiv:2510.13048#introduction:part-2", "url": "https://arxiv.org/abs/2510.13048", "anchor": "#introduction:part-2", "type": "paper", "title": "", "section": "Introduction", "text": "it favors placements whose surrounding geometry matches the original socket throughout the entire articulation cycle. Ag- gregating over poses effectively adds a time dimension on top of the spatial dimension. We address this added complexity with an alternating optimization scheme that keeps runtime practical. We also want the optimizer to account for the functional goals desired of the new object. Functional tasks may range from reaching tasks to accommodate a specific scene, to packing a reconfigurable table into a truck (Fig. 1). Such functionality objectives are heteroge- neous and typically evaluated through procedures such as inverse kinematics or contact detection on swept volumes, for which inte- grating gradient-based optimization would require case-specific en- gineering. We address this with annealed Langevin dynamics, which requires only evaluating their objective values without the need for their gradient; thus, it accommodates a broad range of functional objectives without redesigning the solver for each specific case. These functional terms are co-optimized with our kinematics-aware attachment energy within the same Langevin sampling framework. In sum, starting from a design plan that outlines the desired articulation in a kinematic tree and a set of parts taken from known articulated objects, our technique solves for a novel assembly that meets the plan using the given parts while also upholding additional functional requirements (see Fig. 1). In summary, our optimization approach makes the following contributions: (1) introduction of VDF part snapping; (2) kinematics-aware energy formulation drawn from existing assemblies; (3) satisfaction of general functionality requirements through Langevin dynamics. 2", "source": "arxiv_pdf", "published": "", "tokens": 250, "sha256": "4b5272b822042f6a0c8af7d69399a538100972b1fb5c3dbde318bac05e1f7586"}
{"doc_id": "arxiv:2510.13048#related-work:part-1", "url": "https://arxiv.org/abs/2510.13048", "anchor": "#related-work:part-1", "type": "paper", "title": "", "section": "Related Work", "text": "Given a functionality objective and a kinematic graph, our method places all parts such that the assembled object fulfills the function- ality. Our problem formulation is related to part-based 3D object synthesis. We focus our discussion on synthesizing functional, artic- ulated objects, and refer readers to recent surveys [Chaudhuri et al. 2020; Mitra et al. 2013; Ritchie et al. 2023] for static object synthesis. 2.1 Synthesis of Functional Objects Topology optimization [Deaton and Grandhi 2014] is a broad term for work that considers functionality in the context of 3D shape modeling. Work in this area typically treats structural properties (e.g., material distributions) of the object as a variable that is opti- mized to satisfy functional goals (e.g., minimizing compliance given a target load). However, these methods mostly consider optimiz- ing the interior of single objects, deviating from our focus on the functionality and relationship between a collection of articulated components. Another relevant subfield is functional shape analy- sis [Hu et al. 2018], which covers a range of topics from visualizing to characterizing functional objects. Closer to our approach, Zheng et al. [2013] characterize a shape’s functionality by its components’ arrangement. By preserving simple geometric relationships (e.g., symmetries and contacts), the method can mix and match substruc- tures from different shapes and synthesize static objects with similar functionality. Garg et al. [2016] propose an interactive interface to allow users to monitor and resolve infeasible configurations. Yao et al. [2017] present an interactive tool for creating interlocking woodworking-style joints for furniture and toys, along with tools for verifying the object’s overall ease of assembly and stability. Koo et al. [2014] propose an optimization tool to compute the arrangement between geometric proxies (specifically, a collection of cuboids), given user-specified functional relationships between them, includ- ing coverage, fit inside, support, and flush. Our work can be viewed as a generalization of the method by [Koo et al. 2014] to handle triangle meshes and accommodates “global” functional goals (e.g., reaching distinct targets across multiple articulation poses) that depend on the configuration of the entire object. 2.2 Modeling Articulated Objects Another line of research focuses on modeling articulated objects, i.e., a collection of rigid body components connected by mechanical joints, with the aim of producing visually plausible articulations [Liu et al. 2024b]. Several works are designed to infer articulation pa- rameters for an input object made up of parts. These works assume that the input parts are correctly arranged with respect to each other. One approach is to distill the articulations between the parts using images/videos [Jiang et al. 2022; Liu et al. 2023; Nie et al. 2023; Pekelny and Gotsman 2008; Song et al. 2024; Wu et al. 2025] or from pre-trained language/vision models [Liu et al. 2024a; Mandi et al. 2024; Qiu et al. 2025; Vora et al. 2025]. When having access to ground truth articulation datasets (e.g., PartNetMobility [Xiang et al. 2020] or procedural generation [Joshi et al. 2025]), several works have demonstrated successful inference of articulation parameters from a static object [Fu et al. 2024; Goyal et al. 2025; Hu et al. 2017; Luo", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "0ee927a5cb4e397ce6c289cf4ee78e18e81f357ccb887b88682259fde6ff6f14"}
{"doc_id": "arxiv:2510.13048#related-work:part-2", "url": "https://arxiv.org/abs/2510.13048", "anchor": "#related-work:part-2", "type": "paper", "title": "", "section": "Related Work", "text": "Pekelny and Gotsman 2008; Song et al. 2024; Wu et al. 2025] or from pre-trained language/vision models [Liu et al. 2024a; Mandi et al. 2024; Qiu et al. 2025; Vora et al. 2025]. When having access to ground truth articulation datasets (e.g., PartNetMobility [Xiang et al. 2020] or procedural generation [Joshi et al. 2025]), several works have demonstrated successful inference of articulation parameters from a static object [Fu et al. 2024; Goyal et al. 2025; Hu et al. 2017; Luo et al. 2024; Wang et al. 2019; Yan et al. 2019]. However, most of these meth- ods assume that the part arrangement is given, which is inapplicable to our problem with unknown arrangements of input geometries. The works closest to ours are the methods of Liu et al. [2024c] and Lei et al. [2023]. From the given kinematic graph connectivity, they use graph denoising diffusion to generate geometry and articulation parameters jointly. However, they both depend on training neural networks on the available datasets, which are limited in size and restrict their ability to handle configurations beyond the training distribution. Our approach is training-free and instead optimizes each assembly directly to meet the specified functionalities. Kinematic Kitbashing for Modeling Functional Articulated Objects • 39:3 initial placement Ek km = 10.2 optimized placement Ek km = 0.04 source parts Fig. 2. Kinematics-aware attachment for a novel part pair. A wheel mounted on a trash can is reattached to a car body. Our vector distance field energy guides it to the fender recess, matching its original socket. 2.3 Articulated Mechanical Assemblies The modeling and design of articulated mechanical assemblies— structures with movable parts and mechanical joints—has become a topic of significant interest for the computer graphics and com- putational fabrication communities. Prior work has focused chiefly on semi-automatic and optimization-based workflows for creating such assemblies, with applications in robotics, animation, and com- putational fabrication. Methods have been proposed for articulated mechanisms that can be physically fabricated, including complex linkages [Coros et al. 2013; Thomaszewski et al. 2014], compliant joints [Megaro et al. 2017b; Zhang et al. 2021], and cable-driven systems [Li et al. 2017; Megaro et al. 2017a]. Other approaches focus on robotic and animated characters with expressive or task-specific motions [Ceylan et al. 2013; Geilinger et al. 2018; Maloisel et al. 2023; Megaro et al. 2015a,b], and dynamic objects [Bächer et al. 2014]. A common strategy across many of these methods is to begin with a high-level specification of motion, such as a reference trajec- tory [Coros et al. 2013; Thomaszewski et al. 2014] or an animation sequence [Ceylan et al. 2013], and then solve a non-linear opti- mization problem to determine the parameters of the mechanical assembly that best realizes these motions. While powerful, these methods require a carefully designed differentiated forward model and are usually tailored to a particular mechanism class; adapting them to new objectives often requires substantial re-engineering. Our approach, by contrast, accepts black-box functional objectives. This offers a practical and scalable alternative to existing methods, particularly in cases where rapid exploration of designs is essential. 3", "source": "arxiv_pdf", "published": "", "tokens": 510, "sha256": "c916ac594533dbf6a4e4c0dcd26e019c448fff326d9b9d5f1335c25acd03a198"}
{"doc_id": "arxiv:2510.13048#approach:part-1", "url": "https://arxiv.org/abs/2510.13048", "anchor": "#approach:part-1", "type": "paper", "title": "", "section": "Approach", "text": "We frame kinematic kitbashing as an optimization that places a given set of kinematic parts into a shared world frame, where the parts are sourced from a database of articulated objects with known kinematic structures and joint parameters. Specifically, the input consists of (1) a kinematic tree whose nodes are associated with the kinematic parts, and edges represent their parent-child adjacency in the articu- lation, and (2) a functionality objective that evaluates the assembled mechanism on a task such as reachability or trajectory tracking. The optimization searches for rigid transformations that place the parts while achieving the desired functionality in accordance with the kinematic tree. Each child part carries its joint position and axis in the local frame, information that is readily available from source assets. We do not assume access to the corresponding joints on the parent geometry. This setup reflects realistic scenarios for articulated object modeling: Parts are straightforward to collect, but manually annotating precise attachment positions and child part poses on every potential parent mesh for desired functional goals is both tedious and error-prone. 3.1 Problem Formulation & Notations An articulated object consists of a set of rigid parts linked by me- chanical joints that constrain their relative motion [Featherstone 2014]. The joint connectivity forms a directed graph. In this work, we focus on a common case where this graph is a rooted tree, re- ferred to as the kinematic tree: each part has a single parent (except the root, which has no parent), and no cycles are allowed. We represent each rigid part together with the joint that connects it to its parent as a kinematic part. Let A𝑘= (V𝑘, F𝑘) be the mesh of the 𝑘-th kinematic part in the kinematic tree, where V𝑘and F𝑘denote its vertices and faces. Each joint is parameterized by a generalized coordinate 𝜃∈[𝜃lo,𝜃hi], whose dimension equals the DoFs of the articulated motion. The joint parameters of all the parts are collected into 𝜽𝑖, which specifies an articulation pose 𝑖 of the articulated object. Forward kinematics function T(V𝑘;𝜽𝑖) : R3×|V𝑘| ×R𝑑→R3×|V𝑘| with 𝑑the total number of DoFs of all joints, propagates these joint parameters 𝜽𝑖along the kinematic tree and outputs a set of transformed vertex locations from the rest-pose vertices V𝑘, with joint parameters 𝜽𝑖controlling the transformations. We write the 𝑘-th part A (𝑖) 𝑘 at an articulated pose indexed by 𝑖as A (𝑖) 𝑘 = \u0000 T(V𝑘;𝜽𝑖), F𝑘 \u0001 . (1) An articulated object S in reference coordinates consists of a set of kinematic parts at different articulation poses: S = {A (𝑖) 𝑘} . (2) Given a collection of kinematic parts sourced from exemplar articulated objects, our kinematic kitbashing aims to compute a set of rigid transformations P = \b P𝑘 𝐾 𝑘=1 = \b (R𝑘, d𝑘) 𝐾 𝑘=1 (3) to place each part A (𝑖) 𝑘 in world coordinates. We use R𝑘∈SO(3) and d𝑘∈R3 to denote rotation and transla- tion, respectively, for part 𝑘. P𝑘denotes the homo- geneous rigid transforma- tion matrix by concatenat- ing R𝑘, d𝑘. We use e A (𝑖) 𝑘(P) to denote the assembled instance of A", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "053beabf9b3a6d272cf5612b4f04400ac4058c3d4255effeb2282955c1a93c66"}
{"doc_id": "arxiv:2510.13048#approach:part-2", "url": "https://arxiv.org/abs/2510.13048", "anchor": "#approach:part-2", "type": "paper", "title": "", "section": "Approach", "text": "sourced from exemplar articulated objects, our kinematic kitbashing aims to compute a set of rigid transformations P = \b P𝑘 𝐾 𝑘=1 = \b (R𝑘, d𝑘) 𝐾 𝑘=1 (3) to place each part A (𝑖) 𝑘 in world coordinates. We use R𝑘∈SO(3) and d𝑘∈R3 to denote rotation and transla- tion, respectively, for part 𝑘. P𝑘denotes the homo- geneous rigid transforma- tion matrix by concatenat- ing R𝑘, d𝑘. We use e A (𝑖) 𝑘(P) to denote the assembled instance of A (𝑖) 𝑘 once it has been positioned in world coordinates: e A (𝑖) 𝑘(P) = \u0000 T(R𝑘V𝑘+ d𝑘;𝜽𝑖), F𝑘 \u0001. (4) 39:4 • Guo et al. w/o kinematics x2 w/ kinematics source parts Fig. 3. Effect of kinematic attachment. A drawer slide and a hinged door taken from the table are attached to a cabinet. Without kinematic-aware attachment (top), i.e., the placement is optimized for a single pose and ignores the full articulation, the doors are misoriented. With the kinematic dimension (bottom), our method positions both parts plausibly. Similar to the usage of S for the articulated shape in reference coordinates, we use e S(P) = { e A (𝑖) 𝑘(P) } (5) to denote the collection of all articulated components across all poses in world coordinates, determined by the set of transformations P. We formally define our optimization problem as: min P 𝐸km( e S(P)) + 𝜆𝐸func( e S(P)) . (6) The objective is split into two terms. The kinematic attachment term, 𝐸km, measures geometric attachment quality, ensuring that the place- ment of adjacent parts in the kinematic tree is both geometrically compatible and physically meaningful. A detailed formulation of this term is discussed in Sec. 3.2. The functionality term, 𝐸func, mea- sures how well the articulated object performs a user-specific task. Examples of 𝐸func include trajectory-tracking error, swept-volume overlap, and reachability of specified target positions. Since these energies are evaluated via collision queries and inverse-kinematics solves, we treat 𝐸func as a black-box energy within our Langevin sampler, detailed in Sec. 3.3. 3.2 Kinematics-Aware Geometric Attachment The kinematic attachment term 𝐸km is designed to support assem- blies with complex kinematic trees (Fig. 1) and cross-category parts (Fig. 2). Our key insight is that articulation exposes functional fea- tures in the geometry of the parent part, specifically in the region that is spatially surrounding the kinematic part. This geometric region encodes cues that should be preserved when reattaching the part to a new parent. For example, the wheel on a trash can rotates relative to the container, and the surrounding geometry on the con- tainer resembles a hub slot that accommodates this rotation. These features provide a strong geometric guide for attaching parts: when snapping the wheel to a new parent, such as a car body, it should be placed at a location that exhibits similar geometric affordances. 3.2.1 Vector Distance Field Attachment Energy. We encode these surrounding region cues using a vector distance field (VDF) [Faugeras and Gomes 2000; Ma et al. 2020; Marschner et al. 2023]. Given a source mesh M𝑠and a reference mesh M𝑡, the VDF assigns each articulated", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "cc56eb97513bba36b5041003200b47fa7365ad5de3be37c4f8b06bf5d3b0112f"}
{"doc_id": "arxiv:2510.13048#approach:part-3", "url": "https://arxiv.org/abs/2510.13048", "anchor": "#approach:part-3", "type": "paper", "title": "", "section": "Approach", "text": "rotation. These features provide a strong geometric guide for attaching parts: when snapping the wheel to a new parent, such as a car body, it should be placed at a location that exhibits similar geometric affordances. 3.2.1 Vector Distance Field Attachment Energy. We encode these surrounding region cues using a vector distance field (VDF) [Faugeras and Gomes 2000; Ma et al. 2020; Marschner et al. 2023]. Given a source mesh M𝑠and a reference mesh M𝑡, the VDF assigns each articulated rest-pose VDF ICP articulated rest-pose Fig. 4. ICP vs. VDF attachment. ICP ignores the clearance required be- tween parts and snaps the arm directly into the torso surface. When the arm articulates, this oversight becomes critical, producing collisions. VDF, in contrast, keeps the necessary gap (red arrows) for collision-free articulation. vertex x ∈M𝑠a vector from x to its closest point on M𝑡, such that u(M𝑠; M𝑡) = {projM𝑡(x) −x|x ∈M𝑠} , (7) where projM𝑡(x) is the projection operator. Throughout this paper, we adopt the sign convention that the vectors point toward the surface of the reference mesh. source part output Let M𝑘be the parent mesh of A (𝑖) 𝑘 in a source articulated object. For every part-parent pair u(A (𝑖) 𝑘, M𝑘), we com- pute the VDF by project- ing the surface vertices of the part A (𝑖) 𝑘 onto its parent mesh M𝑘over a set of articulation parameters {𝜽𝑖} uni- formly sampled over the joint range. At attachment time, given a new parent part M new 𝑘 , we optimize placement transformations {R𝑘, d𝑘} so that the VDF between the transformed part e A (𝑖) 𝑘 and its new parent M new 𝑘 matches the VDF observed in the source pair (A (𝑖) 𝑘, M𝑘). The VDF matching energy for a kinematic part 𝑘is defined as: 𝐸km 𝑘= 𝑁 ∑︁ 𝑖=1 𝐸km 𝑘,𝑖= 𝑁 ∑︁ 𝑖=1 R𝑘u \u0010 A (𝑖) 𝑘; M𝑘 \u0011 −u \u0010 e A (𝑖) 𝑘(P); M new 𝑘 \u0011 2 . (8) Here 𝐸km 𝑘 measures each child-parent pair in the kinematic tree and then sums over the 𝑁sampled articulated poses indexed by 𝑖, where each pose is specified by the joint parameters 𝜽𝑖sampled over the joint parameter range. Aggregating the VDF error across the entire pose range makes the energy kinematics-aware: the placement must align the functional neighborhood of the part not just at a single pose but consistently throughout its articulation. The total 𝐸km =Í 𝑘𝐸km 𝑘 is minimized with respect to the set of rigid placement P = {(R𝑘, d𝑘)} across all part pairs and poses. We minimize 𝐸km 𝑘 iteratively: At each iteration, we first recom- pute VDF vectors between the articulated shape and its new parent u( e A (𝑖) 𝑘, M new 𝑘 ), and then update the rigid transform (R𝑘, d𝑘) to re- duce the residuals. To enhance robustness, we incorporate two tech- niques inspired by the iterative closest point (ICP) algorithm: (1) We compute point-to-plane alignment to allow tangential sliding [Chen and Medioni 1992]. (2) We also adopt Welsch’s weighting function on the residuals to suppress outliers [Holland and", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "ef5da5a5cef3a5c4a658e8a15242bf45b9c72e39fe67b3e09d4e3edc9e637434"}
{"doc_id": "arxiv:2510.13048#approach:part-4", "url": "https://arxiv.org/abs/2510.13048", "anchor": "#approach:part-4", "type": "paper", "title": "", "section": "Approach", "text": "we first recom- pute VDF vectors between the articulated shape and its new parent u( e A (𝑖) 𝑘, M new 𝑘 ), and then update the rigid transform (R𝑘, d𝑘) to re- duce the residuals. To enhance robustness, we incorporate two tech- niques inspired by the iterative closest point (ICP) algorithm: (1) We compute point-to-plane alignment to allow tangential sliding [Chen and Medioni 1992]. (2) We also adopt Welsch’s weighting function on the residuals to suppress outliers [Holland and Welsch 1977; Kinematic Kitbashing for Modeling Functional Articulated Objects • 39:5 source parts w/o any prior w/ data-driven prior w/ user-specified position Fig. 5. Kinematic attachment under different priors. Our kinematics- aware attachment energy alone may settle in different local minima depend- ing on the initialization. The data-driven prior guides the wheels into the canonical hub slots, providing placement with less dependence on initial- ization. A user may pin the wheel to any desired location, even one absent from the training set, such as the Airbus wing, forcing the wheel to follow this out-of-distribution location and enabling customized assemblies. Sorkine-Hornung and Rabinovich 2017]. Our complete formulation is provided in the Appendix. Compared to ICP, VDFs inherently respect the clearance between a part and its parent, whereas ICP’s nearest-point matches drive the surfaces into direct contact and ignore the gap needed for articulation, as shown in Fig. 4. 3.2.2 Alternating Local-Global Optimization. Although the attach- ment energy 𝐸km can, in principle, be optimized using standard second-order methods, we observe that these approaches become computationally expensive due to the integration over the joint parameters 𝜽𝑖at all articulation poses. Evaluating gradients across many articulation snapshots increases runtime significantly, espe- cially when fine temporal resolution is required. To improve per- formance, we adopt an alternating optimization approach inspired by Bouaziz et al. [2014], instead of a Newton-type solver. Rather than optimizing one transform that must fit all articulation poses simul- taneously, we assign each pose 𝑖an auxiliary transformation Q𝑖and bind these transforms to a shared rigid transformation P𝑘=(R𝑘, d𝑘) for part 𝑘. The subproblems for computing Q𝑖are now independent and can be solved in parallel, and P𝑘can be updated in closed form as the average of all Q𝑖. This decoupling greatly reduces the runtime. Specifically, we optimize the following modified objective: Local Step : min Q𝑖 𝑁 ∑︁ 𝑖=1 \u0010 𝐸km 𝑘,𝑖+ 𝜌 2 Log\u0000Q−1 𝑖P𝑘 \u0001 2\u0011 , (9) Global Step : P𝑘= Exp \u0010 1 𝑁 𝑁 ∑︁ 𝑖=1 Log Q𝑖 \u0011 , (10) where Log(·) : SE(3) →R6 is the vectorized logarithm map in Lie algebra and Exp(·) is its inverse map [Chirikjian 2011]. In the local step, to account for the non-Euclidean nature of SE(3), we replace the standard Euclidean penalty with a geodesic distance between P𝑘and Q𝑖, defined via the logarithmic map. The global step similarly computes the average of Q𝑖in the Lie algebra and maps it back to the group using Exp, ensuring that P𝑘remains on the SE(3) manifold. We set 𝜌= 104 to weight the penalty term. Optimization proceeds by alternating between the local and the global steps. We", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "9957345eb30f4829c03eec33fc43efed13b7f1cc59a2ae9c8e563605c2f1efbb"}
{"doc_id": "arxiv:2510.13048#approach:part-5", "url": "https://arxiv.org/abs/2510.13048", "anchor": "#approach:part-5", "type": "paper", "title": "", "section": "Approach", "text": "2011]. In the local step, to account for the non-Euclidean nature of SE(3), we replace the standard Euclidean penalty with a geodesic distance between P𝑘and Q𝑖, defined via the logarithmic map. The global step similarly computes the average of Q𝑖in the Lie algebra and maps it back to the group using Exp, ensuring that P𝑘remains on the SE(3) manifold. We set 𝜌= 104 to weight the penalty term. Optimization proceeds by alternating between the local and the global steps. We execute up to 20 iterations. Compared to Newton- type optimization methods, this approach significantly improves performance by enabling parallel optimization of all Q𝑖in the local step. For 𝑇=20 in the cabinet door example (Fig. 3), our approach reduces the wall-clock optimization time from 163 s to 19 s. 3.2.3 User Controls and Attachment Priors. The kinematic attach- ment energy 𝐸km can be augmented with attachment priors that bias a child part towards likely locations on its parent. Certain kine- matic parts, especially those with clear functional semantics, tend to appear in consistent relative locations with respect to their par- ent parts. For example, car wheels typically attach near the lower sides of the chassis, and drawers are inserted into centrally aligned cavities within furniture. Our framework can optionally exploit this regularity to improve controllability during attachment, which can be performed in two ways: (1) The user can pin a target location for the part’s center of mass; the optimization then determines the rotation that minimizes the attachment energy while satisfying this positional anchor. (2) When semantically labeled exemplars are available, we learn a probability prior over relative transforms for each part category from the PartNet Mobility dataset [Xiang et al. 2020] and add it to the attachment energy. Fig. 5 shows the effect of no prior, a user pin, and a learned prior, demonstrating how each reduces the dependence on initialization. Since this prior is optional, we present its detailed formulation in Appendix. 3.3 Optimization via Annealed Langevin Dynamics When the assembly must also hit a precise functional target, such as steering a desk lamp’s beam onto a reading spot, stricter guidance is required. Since 𝐸km only assesses the local kinematic fit, it admits an entire family of equally good placements. For instance, the lamp arm shown in the inset can be placed at many different angles that minimize 𝐸km, yet most of these orientations fail to direct light toward the target. VDF matching may also settle in local minima when the initial pose is a severe outlier. To resolve this ambiguity source parts kinematic attachment only we conduct a global opti- mization that minimizes 𝐸km together with the functional- ity term 𝐸func, restricting the solution space to configura- tions that achieve the goal. The task objective 𝐸func encompasses a diverse range of evaluations, such as in- verse kinematics and swept- volume contact checks, so using gradient-based solvers would re- quire bespoke derivations for each new target. Meanwhile, the op- timization variable P spans 6𝐾continuous DoFs, already larger than 30 for a six-part assembly, where evolutionary or simulated- annealing schemes lose efficiency", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "8af2f20b95613386e1901c29676846916145764914e6cf09703ac9a209dd2b1b"}
{"doc_id": "arxiv:2510.13048#approach:part-6", "url": "https://arxiv.org/abs/2510.13048", "anchor": "#approach:part-6", "type": "paper", "title": "", "section": "Approach", "text": "mization that minimizes 𝐸km together with the functional- ity term 𝐸func, restricting the solution space to configura- tions that achieve the goal. The task objective 𝐸func encompasses a diverse range of evaluations, such as in- verse kinematics and swept- volume contact checks, so using gradient-based solvers would re- quire bespoke derivations for each new target. Meanwhile, the op- timization variable P spans 6𝐾continuous DoFs, already larger than 30 for a six-part assembly, where evolutionary or simulated- annealing schemes lose efficiency [Knight and Lunacek 2007; Zhou et al. 2024]. We address both challenges with annealed Langevin dynamics. Treating the combined objective of the kinematic at- tachment term 𝐸km and the functionality term 𝐸func in Eq. 6 as an 39:6 • Guo et al. ... initialized parts Langevin dynamics w/o functionality Langevin dynamics w/ functionality 0 0 3 p ets 0 8 2 p ets 0 5 p ets 0 0 1 p ets ... functionality objective output Fig. 6. Intermediate results of annealed Langevin sampling. Langevin dynamics run with both the kinematic attachment term and the functionality term gradually assemble the parts and end with a mechanism whose end-effectors reach the targets (top). Without the functionality term, the sampler still finds a geometrically valid assembly, but the final configuration misses the targets, illustrating the ambiguity when functionality is ignored (bottom). unnormalized probability density allows us to replace determinis- tic minimization with stochastic sampling. The sampling process converges towards high-probability modes, i.e., configurations with concentrated low objective values, while naturally discarding im- probable outliers, so optimal placements can be found without the need for analytic gradients. A detailed analysis of Langevin dynam- ics as a robust mode finder, and its relation to mean-shift and voting schemes, is discussed in [Je et al. 2024]. Specifically, Langevin dynamics samples a sequence of rigid trans- form sets {P𝑠|𝑠= 𝑆, ..., 0}. It starts with P𝑆drawn from Gaussian noises and iteratively steers towards the target distribution 𝑝0(P0) ∝exp\u0000−𝐸km( e S(P0)) 𝜆 \u0001 exp(−𝐸func( e S(P0))). (11) The update at each step follows P𝑠−1 ←P𝑠+ 𝛼𝑠 2 ∇P𝑠log𝑝𝑠(P𝑠) + √𝛼𝑠𝜖, 𝑠= 𝑆, ..., 1, (12) where 𝛼𝑠is the step size, 𝜖∼N (0, I) is the noise. The score function ∇P𝑠log𝑝𝑠(P𝑠) is estimated via Monte Carlo sampling: ∇P𝑠log𝑝𝑠(P𝑠) = Í P0∈P0 𝑝0(P0)∇P𝑠log𝑝𝑠|0(P𝑠|P0) Í P0∈P0 𝑝0(P0) , where P0 is a batch of samples drawn from the proposal function 𝑝𝑠|0(P𝑠|P0). It gives an unbiased approximation [Pan et al. 2024; Song and Ermon 2019]. A full derivation is provided in the Appendix. We run Langevin dynamics for 𝑆= 300 iterations. To evaluate 𝑝0(·), we first apply the optimization in Sec. 3.2 and then compute 𝐸km and 𝐸func on the resulting configuration. The score estimation uses automatic differentiation with 30 samples. A list of functionality objectives used in our examples is provided in the Appendix. 4", "source": "arxiv_pdf", "published": "", "tokens": 465, "sha256": "527ea38420da45d42710a0dbc67bc88c46913aed2eedc8530318b0e8f80bfa1a"}
{"doc_id": "arxiv:2510.13048#results", "url": "https://arxiv.org/abs/2510.13048", "anchor": "#results", "type": "paper", "title": "", "section": "Results", "text": "We evaluate our framework on the articulated object benchmark and a suite of custom functional tasks. The experiments compare against state-of-the-art baselines and showcase diverse assemblies. 4.1 Comparison with Baselines We benchmark our method against publicly released baselines that share the same inputs as ours to assemble new objects. This includes 4-PCS [Aiger et al. 2008; Chaudhuri and Koltun 2010] and Part Slot Machine [Wang et al. 2022] for synthesizing static objects, and NAP [Lei et al. 2023] and CAGE [Liu et al. 2024c] for synthesizing articulated objects conditioned on the kinematic graph. All baseline results are produced with the publicly available code. Experiments are run on the Table and Storage Furniture test split of PartNet- Mobility [Xiang et al. 2020], used by CAGE and Part Slot Machine. Following the baseline papers, we report five metrics: 1) Rooted: percentage of assemblies whose parts form a single connected com- ponent. 2) Stable: percentage of assemblies that remain upright under gravity. 3) COV: coverage, i.e., the fraction of the reference test set that is reproduced by the generated shapes. 4) MMD: mini- mum matching distance between generated and reference test set. 5) AOR: average overlapping ratio, the mean volume overlap be- tween any two sibling parts. These metrics capture geometric (Root, Stable), kinematic (COV, MMD), and functional qualities (AOR). We present quantitative results in Table 1 and qualitative ones in Fig. 7 and 11. Baselines for static shapes (4-PCS and Part Slot Machine) achieve high Rooted score, yet their lack of articulation awareness leads to poor kinematic coverage and large functional errors. Kinematics-aware baselines (NAP and CAGE) reduce this gap, but still exhibit noticeable error and inter-part collision. Our ablation without the kinematic prior (Ours w/o km.) already outperforms the baselines on the functional metrics, confirming that VDF-based attachment provides the clearance required for articulated parts. Incorporating the kinematic prior (Ours w/ km.) lifts every metric further and yields the best overall performance. 4.2 Functional Examples In addition to the articulated examples presented above, we demon- strate our pipeline on a diverse suite of examples with functional Kinematic Kitbashing for Modeling Functional Articulated Objects • 39:7 Table 1. Quantitative comparison with four baselines. Our approach with kinematic-aware attachment delivers the highest scores across all geometric, kinematic, and functional metrics. Geometric Kinematic Functional", "source": "arxiv_pdf", "published": "", "tokens": 380, "sha256": "0f09f31fbba528ad3ee2bd8969ec1114d4537e9609e286ed4961b9ce47f7cd05"}
{"doc_id": "arxiv:2510.13048#method:part-1", "url": "https://arxiv.org/abs/2510.13048", "anchor": "#method:part-1", "type": "paper", "title": "", "section": "Method", "text": "Rooted↑ COV↑MMD↓Stable↑AOR↓ 4-PCS 95.4% 56.9% 0.092 81.5% 16.4% Part Slot Machine 98.5% 63.1% 0.083 84.6% 10.7% NAP 96.9% 69.2% 0.072 92.3% 3.5% CAGE 100% 73.8% 0.051 90.7% 1.1% Ours (w/o km.) 100% 80.0% 0.052 95.4% 0.2% Ours (w/ km.) 100% 83.1% 0.038 96.9% 0.1% 4-PCS Part Slot Machine NAP CAGE Ours Fig. 7. Qualitative comparison (1/2). Geometry-only methods (4-PCS, Part Slot Machine) frequently misorient parts; Kinematics-aware baselines (NAP and CAGE) improve orientation but still yield floating or colliding parts. Our method aligns parts correctly and achieves fully functional articulation. objectives that vary in scale, kinematic structure (chains and trees), and part semantics (intra- and cross-category). Multi-head Lamp. This example assembles a weighted base, three straight support bars, three revolute elbows, and three lamp heads into a branched kinematic tree that terminates at the lamp heads. The functionality is that each spotlight must reach a prescribed target point on the table surface while remaining collision–free and within joint limits. Our optimization finds a set of part placements that brings all beams within 2◦of their targets and eliminates the self-collision failure, as shown in Fig. 1(a). Reconfigurable Table. This example combines a central tabletop with drawers, a fold-down screen, and cabinet doors into a star- shaped kinematic tree rooted at the tabletop. The functionality requires that the parts can be reconfigured to fit within a unit cube, allowing the whole table to be packed into a cargo bay without self-collision. Our method produces a foldable object that reduces the deployed volume by 62% while satisfying clearance constraints at intermediate articulation poses, as shown in Fig. 1(b). source parts functionality source parts functionality Fig. 8. More examples on kinematic kitbashing. Functionality objectives, such as following prescribed trajectories, can be achieved for a gear-driven paddle (left) and a humanoid assembled from mechanical modules (right). Gear–Driven Paddler. The input consists of a branch of interlock- ing gears (one driver and several idlers of varying sizes) coupled to paddles through rigid bars. The functionality requires all four paddles to trace prescribed circular trajectories: each diagonal pair must counter-rotate in phase, and the articulation must remain en- tirely free of paddle–paddle collisions. Our optimization meets these requirements, as shown in Fig. 8(a). Mechanical Humanoid. A set of mechanical links and joint mod- ules is assembled into a human-like kinematic chain. The functional- ity requires the arms and feet to follow prescribed trajectories while respecting joint limits. Fig. 8(b) shows the result. Our optimization meets the motion target by placing the parts in suitable orientations and positions. Mix-and-Match Assemblies. Our method also supports mix-and- match designs. Using kinematic attachment only, we snap oversized carriage wheels and a dumpster onto a vintage truck and graft robotic legs and wings onto a fantasy torso, as shown in Fig. 9. 5 Discussion and conclusion We have introduced an automatic pipeline for kitbashing functional articulated objects from existing part libraries. A kinematics-aware attachment energy, built upon vector distance fields, preserves each part’s original socket across its full range of articulation. Combined with functional objectives in an annealed Langevin sampler, this energy places all parts", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "341c1e5558b8ecdaa8c6a724867955150289745acd0dc4a688e374444e1ee82b"}
{"doc_id": "arxiv:2510.13048#method:part-2", "url": "https://arxiv.org/abs/2510.13048", "anchor": "#method:part-2", "type": "paper", "title": "", "section": "Method", "text": "we snap oversized carriage wheels and a dumpster onto a vintage truck and graft robotic legs and wings onto a fantasy torso, as shown in Fig. 9. 5 Discussion and conclusion We have introduced an automatic pipeline for kitbashing functional articulated objects from existing part libraries. A kinematics-aware attachment energy, built upon vector distance fields, preserves each part’s original socket across its full range of articulation. Combined with functional objectives in an annealed Langevin sampler, this energy places all parts so that attachments remain geometrically sound while satisfying user-specified goals, such as collision-free ac- tuation, reachability, trajectory tracking, and more. Our method con- sistently outperformed state-of-the-art baselines across geometric, kinematic, and functional metrics on standard benchmarks and gen- erated a wide variety of assemblies whose behaviors differ markedly from the source models. These results demonstrate that uniting kinematics-aware attachment with gradient-free functional opti- mization enables rapid, versatile synthesis of articulated assets. Currently, our runtime is limited by the many iterations required by the Langevin sampling. A practical way to accelerate convergence is to run several Langevin chains in parallel at different temperatures and periodically exchange their states [Shih et al. 2023]. Higher- temperature chains explore the energy landscape more broadly, while lower-temperature chains concentrate on fine-tuning promis- ing solutions, so the overall procedure typically converges in fewer iterations. Also, our method cannot strictly guarantee collision-free 39:8 • Guo et al. articulation. Approaches for articulated collision checking and reso- lution [Garg et al. 2016] could provide hard guarantees, or serve as a post-processing filter, at the cost of additional computation. More broadly, our kinematic kitbashing complements the recent large-scale generative models [Poole et al. 2022; Qiu et al. 2025] for shapes and articulations by adding a controllable, kinematics-aware placement stage that can enforce functional objectives, making the workflow more artist-centric. At present, we assume a predefined kinematic tree. Coupling our method with graph-generative models, perhaps derived from an LLM, could automatically propose candi- date kinematic structures, which our optimization would then refine, enabling a fully automatic pipeline from abstract kinematic graph to the articulated shape. Additionally, the data-driven attachment prior used in our attachment optimization is intentionally simple; replacing or augmenting it with a learned prior model trained on large corpora of successful assemblies, could improve generalization to novel geometries and reduce feature-engineering effort.", "source": "arxiv_pdf", "published": "", "tokens": 383, "sha256": "232f770b1530ff880edaeae425b71bba3a0ac3c29cb71919dd9ff4224418d2de"}
{"doc_id": "arxiv:2510.13461#abstract:part-1", "url": "https://arxiv.org/abs/2510.13461", "anchor": "#abstract:part-1", "type": "paper", "title": "", "section": "ABSTRACT", "text": "Accurate prediction of vehicle collision dynamics is crucial for advanced safety systems and post-impact control applications, yet existing methods face inherent trade-offs among computational efficiency, prediction accuracy, and data require- ments. This paper proposes a dual Physics-Informed Neural Network framework ad- dressing these challenges through two complementary networks. The first network integrates Gaussian Mixture Models with PINN architecture to learn impact force distributions from finite element analysis data while enforcing momentum conserva- tion and energy consistency constraints. The second network employs an adaptive PINN with dynamic constraint weighting to predict post-collision vehicle dynam- ics, featuring an adaptive physics guard layer that prevents unrealistic predictions whil e preserving data-driven learning capabilities. The framework incorporates un- certainty quantification through time-varying parameters and enables rapid adap- tation via fine-tuning strategies. Validation demonstrates significant improvements: the impact force model achieves relative errors below 15.0% for force prediction on finite element analysis (FEA) datasets, while the vehicle dynamics model reduces average trajectory prediction error by 63.6% compared to traditional four-degree- of-freedom models in scaled vehicle experiments. The integrated system maintains millisecond-level computational efficiency suitable for real-time applications while providing probabilistic confidence bounds essential for safety-critical control. Com- prehensive validation through FEA simulation, dynamic modeling, and scaled vehi- cle experiments confirms the framework’s effectiveness for Precision Immobilization Technique scenarios and general collision dynamics prediction. KEYWORDS Precision immobilization technique; Post impact control; Vehicle collision dynamics; Physics-informed neural networks; Gaussian mixture model; Uncertainty quantification 1. Introduction 1.1. Background and Motivation Road traffic accidents are still a significant threat to public safety. Vehicle collisions in traffic accidents often go through three dynamic stages: pre-collision, during-collision, and post-collision [1]. Although its time duration with direct vehicle collision is very short, the second stage of ‘during colision’ is crucial in the entire vehicle behavior CONTACT Daofei Li Email: dfli@zju.edu.cn arXiv:2510.13461v1 [eess.SY] 15 Oct 2025 progression after accidents. Therefore, accurately predicting the vehicle motion dy- namics in collision is not only of great significance for accident reconstruction analysis and vehicle safety system design, but also a key technological foundation for devel- oping next-generation active safety technologies, e.g. post-impact control strategies [2; 3; 4]. Vehicle rear-quarter collisions, where impacts occur at the vehicle’s rear cor- ner area as in Precision Immobilization Technique (PIT) maneuvers, represent one of the most complex and dynamically challenging impact scenarios in traffic safety research [5; 6; 7]. The research of PIT technique and post-impact control systems have created new demands for accurate, real-time collision prediction capabilities that tra- ditional methods struggle to meet. With advancing research in vehicle active safety control, there is a growing urgent need for a modeling approach that balances accu- racy and computational efficiency to support model-based control systems, enhance post-impact reliability, and promote practical applications. 1.2. Literature Review 1.2.1. Impact Force Modeling Methods To better model vehicle post-impact dynamics, it is necessary to understand the vehicle behavior changes during the collision process, which are intuitively reflected in the impact force curve. For lite collisions such as PIT maneuvers, the force curve can be estimated from accelerometer data [8]. Huibers et al. [8] found that for", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "4ff82e022a74d58f4f8b5de0746925e48acb98c96786728ce64b021946bee67f"}
{"doc_id": "arxiv:2510.13461#abstract:part-2", "url": "https://arxiv.org/abs/2510.13461", "anchor": "#abstract:part-2", "type": "paper", "title": "", "section": "ABSTRACT", "text": "approach that balances accu- racy and computational efficiency to support model-based control systems, enhance post-impact reliability, and promote practical applications. 1.2. Literature Review 1.2.1. Impact Force Modeling Methods To better model vehicle post-impact dynamics, it is necessary to understand the vehicle behavior changes during the collision process, which are intuitively reflected in the impact force curve. For lite collisions such as PIT maneuvers, the force curve can be estimated from accelerometer data [8]. Huibers et al. [8] found that for collisions with the same barrier type, the trend of force curve F(x) remains consistent across different vehicle impacts. However, the magnitude of impact force varies depending on specific operational condition and the structural characteristic of the vehicle involved. Various modeling methods for impact force have been developed. Lumped parame- ter models based on spring-damper systems [9; 10] and pulse models [11; 12] represent common approaches. Pulse approximation methods, including haversine, half-sine, and triangular waveforms, have been widely adopted in the vehicle safety domain [13; 14]. These simplified mathematical formulations provide computationally efficient repre- sentations of impact forces, enabling rapid analysis and preliminary design evaluations while maintaining acceptable accuracy for certain crash scenarios. For more detailed analysis, multibody dynamics models in the discrete time domain [15] and finite el- ement models [16; 17] can be employed. These approaches enable comprehensive in- vestigation of vehicle behavior in terms of deformation, displacements, velocity, and accelerations throughout the entire impact duration. However, the substantial compu- tational demand limits their application to analyzing vehicle dynamics and modeling transient collision behaviors, rendering them unsuitable for direct use in developing vehicle active safety algorithms. Nevertheless, the data generated by these methods can still, to some extent, substitute for real-world collision data that are prohibitively expensive to obtain. Chen et al. [16] developed a deep neural network-based approach for solving inverse problems in vehicle collisions using finite element data, achieving prediction accuracy with mean errors below 3%. Yildiz and Kiran [17] applied the finite element method to particle swarm-based optimization for vehicle crash safety. 1.2.2. Post-impact Vehicle Dynamics Modeling Vehicle post impact motion prediction faces multiple challenges. Firstly, the collision process involves complex nonlinear dynamic phenomena, including the coupled effects 2 of various factors such as structural deformation, changes in tire mechanical proper- ties, and suspension system responses [18]. Secondly, post-impact vehicle motion often exhibits large sideslip angles and yaw rates that far exceed the dynamic response range under normal driving conditions [19]. Furthermore, in special collision scenarios such as PIT maneuvers, the vehicle dynamic response becomes even more complex, making it difficult for traditional vehicle dynamics models to accurately describe its motion patterns. Research by the Michigan State Police has revealed that performing PIT maneuvers on vehicles equipped with Electronic Stability Control (ESC) systems can yield unpredictable outcomes at both low and high speeds, whereas vehicles without ESC demonstrate significantly more consistent and predictable responses [20]. Existing collision modeling methods face inherent limitations. Momentum conser- vation approaches, such as the Kudlich-Slibar method, offer high computational ef- ficiency but assume instantaneous collision and neglect critical physical phenomena during impact, such", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "2825de61edf5e5305b2da851800f21aec3835b76db89da49fd0c97f173574385"}
{"doc_id": "arxiv:2510.13461#abstract:part-3", "url": "https://arxiv.org/abs/2510.13461", "anchor": "#abstract:part-3", "type": "paper", "title": "", "section": "ABSTRACT", "text": "describe its motion patterns. Research by the Michigan State Police has revealed that performing PIT maneuvers on vehicles equipped with Electronic Stability Control (ESC) systems can yield unpredictable outcomes at both low and high speeds, whereas vehicles without ESC demonstrate significantly more consistent and predictable responses [20]. Existing collision modeling methods face inherent limitations. Momentum conser- vation approaches, such as the Kudlich-Slibar method, offer high computational ef- ficiency but assume instantaneous collision and neglect critical physical phenomena during impact, such as tire forces [21; 22; 23; 24; 25]. To address these issues, Zhou et al. [5; 6] developed a multi-stage vehicle dynamics model for PIT, quantitatively analyzing the influence of operational parameters on impact outcomes. Elmarakbi et al. [26] simulated vehicle oblique frontal collisions using a 6-degree-of-freedom mathe- matical model, while Gidlewski et al. [27] modeled the dynamics and energy balance of vehicle frontal-to-side collisions. However, these studies rely on oversimplified linear tire slip angle models, which fail to accurately capture actual vehicle responses. Kim et al. [12] introduced an effective factor to approximate the nonlinear region of tire behavior, enabling a 4DOF model to describe vehicle responses under collision forces and validate against CarSim simulations. Nevertheless, this method still requires ex- tensive precise model parameters and calibration based on tire force data, which is impractical in emergency situations characterized by high uncertainty. 1.2.3. Vehicle Dynamic Modeling Based on Data-driven Methods In recent years, with the rapid development of artificial intelligence technology, espe- cially the successful application of deep learning in various fields, machine learning methods have demonstrated great potential in vehicle dynamics modeling [28; 29]. Spielberg et al. [30] developed a high-performance autonomous driving vehicle model based on neural networks, using a feedforward-feedback control architecture and his- torical state information to handle vehicle dynamics under different road friction con- ditions. Williams et al. [31] combined the model predictive path integral (MPPI) al- gorithm and a neural network-learned dynamics model to achieve real-time control of complex nonlinear systems. Pan et al. [32] integrated complex multibody dynam- ics simulation with deep learning to achieve real-time vehicle dynamics prediction. These methods utilize large amounts of data for neural network-based vehicle dynam- ics modeling, particularly the nonlinear tire mechanics in vehicles [30; 33]. However, these methods rely on extensive training data, and lack physical model transparency and intuitiveness, while the trained models can only be used for specific vehicle types and operating conditions. Although purely data-driven neural network models excel at learning complex nonlinear patterns, they often fail to guarantee consistency with physical constraints, potentially generating physically unrealizable state prediction for the vehicle. To overcome the limitations of purely data-driven methods, Physics-Informed Neu- ral Networks (PINN) have emerged as an effective solution. PINN represents a class of universal function approximators that embed physical laws, formulated as partial differential equations, into the neural network learning process [34; 35]. Unlike meth- 3 ods that rely on data generated from physical models [30], PINN incorporates physical prior knowledge as regularization terms to constrain the solution space of acceptable functions, thereby enhancing the generalization capability of function approximation even", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "41f5896480ce3495ce1511bc1ee3e85f3c655157a9adf1a1b7b4732c129670f1"}
{"doc_id": "arxiv:2510.13461#abstract:part-4", "url": "https://arxiv.org/abs/2510.13461", "anchor": "#abstract:part-4", "type": "paper", "title": "", "section": "ABSTRACT", "text": "the limitations of purely data-driven methods, Physics-Informed Neu- ral Networks (PINN) have emerged as an effective solution. PINN represents a class of universal function approximators that embed physical laws, formulated as partial differential equations, into the neural network learning process [34; 35]. Unlike meth- 3 ods that rely on data generated from physical models [30], PINN incorporates physical prior knowledge as regularization terms to constrain the solution space of acceptable functions, thereby enhancing the generalization capability of function approximation even under data-scarce conditions. This advantage is particularly evident for simple vehicle models [36; 37; 38; 39]. Such approach is especially suitable for engineering scenarios where data availability is limited. For example, Tan et al. [40] utilize PINN to model the dynamics of experimental vehicles. To address the challenge of neural network convergence with limited data, Fang et al. [41] proposed a Fine-Tuning Hy- brid Dynamics (FTHD) method, which fine-tunes a pre-trained Deep Dynamics Model (DDM) using a small training dataset. Similarly, the PINN-based approach proposed by Long et al. [42] enhances con- ventional neural networks by incorporating physical prior knowledge such as vehicle kinematics and boundary conditions. These methods demonstrate excellent extrapo- lation capability across dynamic scenarios, enabling the reconstruction of high-speed trajectories using only low-speed training data. However, their limitations primarily lie in excessive reliance on physical models, restricting their applicability to relatively simple scenarios and making them inadequate for characterizing uncertainties in com- plex dynamic environments. 1.2.4. Research Gaps Despite significant advances in both collision modeling and physics-informed learning, several critical gaps remain: (1) Existing neural network architectures struggle to effectively handle the strong discontinuities of contact and force during collisions. (2) Collisions involve multiple time scales, from millisecond impact duration to seconds-long post-collision motion. Most existing studies can only simplify the impact process based on experience [1; 6]. (3) Traditional neural networks require extensive training data, which is costly and dangerous to obtain for collision scenarios. (4) To address security issues, existing models struggle to balance computational complexity and accuracy. Based on the current research status and existing challenges, this paper proposes two PINN-based neural networks. The first network integrates finite element analysis (FEA) data with baseline physical models to model millisecond-level impact forces, while the second network incorporates fine-tuning techniques to achieve accurate mod- eling of post-collision vehicle attitude changes. Both models integrate gaussian mixture models (GMM) to handle uncertain state distributions. The first model is validated using a self-constructed FEA database, demonstrating its capability for accurate im- pact force modeling. The second model is validated through a combined FEA-CarSim simulation environment and scaled vehicle experiments. 1.3. Contributions This paper addresses these gaps through the following contributions: (1) We propose a dual physics-informed neural network framework where the first network models time-varying collision force distributions and the second predicts post-collision vehicle dynamics, achieving complete collision process modeling 4 across temporal scales. (2) We design physics guard layer that balances physical consistency with data learning through dynamic constraint weight adjustment and soft boundary con- straints, effectively addressing the issue of physical constraints suppressing neural network learning. (3) We establish", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "a96ea928e5bb2019f7484744a58e461537e375afffb0bbdac0362ed458618294"}
{"doc_id": "arxiv:2510.13461#abstract:part-5", "url": "https://arxiv.org/abs/2510.13461", "anchor": "#abstract:part-5", "type": "paper", "title": "", "section": "ABSTRACT", "text": "This paper addresses these gaps through the following contributions: (1) We propose a dual physics-informed neural network framework where the first network models time-varying collision force distributions and the second predicts post-collision vehicle dynamics, achieving complete collision process modeling 4 across temporal scales. (2) We design physics guard layer that balances physical consistency with data learning through dynamic constraint weight adjustment and soft boundary con- straints, effectively addressing the issue of physical constraints suppressing neural network learning. (3) We establish a multi-fidelity validation framework from FEA to CarSim sim- ulation and scaled vehicle experiments, reducing trajectory prediction error by 63.6% compared to traditional 4DOF models while maintaining millisecond-level computational efficiency. 2. Baseline Vehicle Collision Dynamics Modeling 2.1. Vehicle Dynamics Model Considering Impact Forces Since PINN training requires incorporating physical constraints, a continuous base- line model must first be established to provide the necessary physical foundations for the vehicle dynamics modeling framework. As shown in Figure 1, to capture essen- tial collision dynamics while maintaining computational tractability, we adopt a four- degree-of-freedom (4DOF) vehicle model that extends traditional planar approaches by including roll motion, which is critical for accurate representation of vehicle behav- ior during high-acceleration maneuvers and collision events [12]. The state vector is defined as: x = [vx, vy, ψ, ˙ψ, ϕ, ˙ϕ, X, Y ]T ∈R8 (1) where vx and vy are longitudinal and lateral velocities, ˙ψ is yaw rate, ˙ϕ is roll rate, ϕ is roll angle, (X, Y ) is global position, and ψ is heading angle. Figure 1. Schematic diagrams of the 4DOF vehicle model with impact forces applied The governing equations follow the lateral-yaw-roll model with impact force aug- 5 mentation, incorporating the coupling effects between lateral, yaw, and roll dynamics: m˙vx = mvy ˙ψ + Fx,tire + Fx,collision m˙vy = −mvx ˙ψ + Fy,tire + Fy,collision Izz ¨ψ = Mz,tire + Mz,collision Ixx,s ¨ϕ + Ixz ¨ψ = msghrc sin ϕ −Ksϕ −Bs ˙ϕ + Mx,collision (2) where m is vehicle mass, Izz and Ixx,s are yaw and roll moments of inertia, Ixz is product of inertia representing the coupling between yaw and roll motions, Ks and Bs are roll stiffness and damping coefficients, ms is sprung mass, and hrc is roll center height. The resultant tire forces and moments are computed as the sum of contributions from all four wheels: Fx,tire = Fx,fl + Fx,fr + Fx,rl + Fx,rr Fy,tire = Fy,fl + Fy,fr + Fy,rl + Fy,rr Mz,tire = lf(Fy,fl + Fy,fr) −lr(Fy,rl + Fy,rr) + tw 2 (Fx,fr −Fx,fl + Fx,rr −Fx,rl) (3) where lf and lr are distances from the center of gravity to the front and rear axles, respectively, and tw is track width. The individual tire forces are modeled using a simplified magic formula tire model [1], which provides a good balance between computational efficiency and physical accuracy. The lateral forces are expressed as: Fy,i = −sin \u0000tan−1(Ciαi) \u0001 q (µsFzi)2 −F 2 xi, with i = {fl, fr, rl, rr} (4) where i ∈fl, fr, rl, rr are tire position (front-left, front-right, rear-left, rear-right), µs is tire-road friction", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "c05805b880dcb0cf59d9a1b2ee71714b931168300f06e4445012a0a6aa1c83dd"}
{"doc_id": "arxiv:2510.13461#abstract:part-6", "url": "https://arxiv.org/abs/2510.13461", "anchor": "#abstract:part-6", "type": "paper", "title": "", "section": "ABSTRACT", "text": "of gravity to the front and rear axles, respectively, and tw is track width. The individual tire forces are modeled using a simplified magic formula tire model [1], which provides a good balance between computational efficiency and physical accuracy. The lateral forces are expressed as: Fy,i = −sin \u0000tan−1(Ciαi) \u0001 q (µsFzi)2 −F 2 xi, with i = {fl, fr, rl, rr} (4) where i ∈fl, fr, rl, rr are tire position (front-left, front-right, rear-left, rear-right), µs is tire-road friction coefficient and slip angles αi incorporate the effects of vehicle kinematics and steering input: αfl = δ −arctan vy + lf ˙ψ vx −w 2 ˙ψ ! αfr = δ −arctan vy + lf ˙ψ vx + w 2 ˙ψ ! αrl = −arctan vy −lr ˙ψ vx −w 2 ˙ψ ! αrr = −arctan vy −lr ˙ψ vx + w 2 ˙ψ ! (5) where δ is the front wheel steering angle. While longitudinal forces can generally be mapped from driver pedal inputs, for the specific PIT scenarios examined in this paper where drivers do not apply acceleration or deceleration inputs post-collision, 6 the longitudinal force is simplified to rolling resistance only: Fx,i = (fr0 + fr1 · v + fr2 · v2) · Fz,i (6) where fr is the rolling friction coefficient, fr0 denotes the basic rolling friction coeffi- cient, fr1 and fr2 are velocity-dependent coefficients, and v is the vehicle’s longitudinal speed. The vertical load distribution is crucial for accurate tire force prediction and accounts for both longitudinal and lateral load transfer effects caused by vehicle ac- celerations and body roll: Fz,fl = m 2 \u0012 glr lf + lr −hcogax lf + lr \u0013 − Kf Kf + Kr \u0012mhcogay w + msghrc sin ϕ w \u0013 Fz,fr = m 2 \u0012 glr lf + lr −hcogax lf + lr \u0013 + Kf Kf + Kr \u0012mhcogay w + msghrc sin ϕ w \u0013 Fz,rl = m 2 \u0012 glf lf + lr + hcogax lf + lr \u0013 − Kr Kf + Kr \u0012mhcogay w + msghrc sin ϕ w \u0013 Fz,rr = m 2 \u0012 glf lf + lr + hcogax lf + lr \u0013 + Kr Kf + Kr \u0012mhcogay w + msghrc sin ϕ w \u0013 (7) where hcog is the height of the center of gravity; ax and ay are the longitudinal and lateral accelerations; Kf and Kr are the front and rear cornering stiffnesses; w is the vehicle track width; ms is the sprung mass; hrc is the height of the roll center; and ϕ is the roll angle. 2.2. Momentum-conservation-based Collision Model Vehicle collision processes are fundamentally momentum transfer phenomena, with their physical foundation rooted in the momentum conservation principle from New- ton’s laws of motion. In collision systems where external forces are negligible compared to impact forces, the total system momentum remains conserved before and after col- lision. This principle provides a theoretical foundation for deterministic impulse calcu- lations, offering higher certainty compared to directly modeling the complex nonlinear processes of impact forces. The momentum conservation-based collision modeling approach was", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "2e9e3f45fdd39a627c7fa8d688c2975ae8508988ac9880c134702e2e5fcfa5c4"}
{"doc_id": "arxiv:2510.13461#abstract:part-7", "url": "https://arxiv.org/abs/2510.13461", "anchor": "#abstract:part-7", "type": "paper", "title": "", "section": "ABSTRACT", "text": "Model Vehicle collision processes are fundamentally momentum transfer phenomena, with their physical foundation rooted in the momentum conservation principle from New- ton’s laws of motion. In collision systems where external forces are negligible compared to impact forces, the total system momentum remains conserved before and after col- lision. This principle provides a theoretical foundation for deterministic impulse calcu- lations, offering higher certainty compared to directly modeling the complex nonlinear processes of impact forces. The momentum conservation-based collision modeling approach was first system- atically developed by Brach et al. [21], treating colliding vehicles as rigid bodies and determining collision impulse through analysis of momentum changes before and after impact. Compared to direct impact force modeling, this approach offers advantages: impulse calculations primarily depend on relatively deterministic physical quantities such as collision geometry parameters, vehicle masses, and relative velocities, while complex factors like material nonlinearity and structural deformation have relatively minimal impact on impulse calculations. Consider a two-dimensional planar problem of two-vehicle collision. A ground-fixed coordinate system XOY is established, where the X-axis is along the tangential direc- tion of the road. As shown in Figure 2 Let the target vehicle be target vehicle and the striking vehicle be bullet vehicle. The pre-collision vehicle motion states are described by six parameters: vtx, vty, ˙ψt, vbx, vby, ˙ψb, and the corresponding post-collision motion states are: Vtx, Vty, ˙Ψt, Vbx, Vby, ˙Ψb. According to the law of momentum conservation, the system’s linear momentum is 7 Bullet Vehicle Target Vehicle Figure 2. A view of two-vehicle collision model conserved in both X and Y directions as mt · (Vtx −vtx) = −mb · (Vbx −vbx) = Px mt · (Vty −vty) = −mb · (Vby −vby) = Py (8) where Px and Py are the collision impulses in the X and Y directions, respectively. For angular momentum conservation, with each vehicle’s center of mass as the moment center: Izzt( ˙Ψt −˙ψt) = Pxdt sin(θt + ξt) −Pydt cos(θt + ξt) Izzb( ˙Ψb −˙ψb) = Pxdb sin(θb + ξb) −Pydb cos(θb + ξb) (9) To solve for the impulses Px and Py, two additional physical constraint conditions are introduced.The restitution coefficient e is defined as the negative ratio of the rela- tive separation velocity after collision to the relative approach velocity before collision: e = −Vbn −Vtn vbn −vtn (10) where the subscript n denotes normal component of the collision contact surface. The tangential friction coefficient µ is defined as the ratio of tangential impulse to normal impulse: µ = Pt Pn (11) Decomposing the impulse into the normal-tangential coordinate system yields: µ · (Px cos Γ + Py sin Γ) = Py cos Γ −Px sin Γ (12) By solving coupled equations 8-12, the collision impulses Px and Py can be determined. A key characteristic of this solution process is that the impulse calculation primar- ily depends on the following relatively deterministic physical parameters: geometric parameters (collision point location, vehicle dimensions, collision angle, etc.) can be accurately determined through collision geometry analysis; inertial parameters (vehi- cle mass, moment of inertia, etc.) possess high certainty; kinematic parameters (pre-", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "0c743a8624984c111fedd599800929a22e21609c6454a40ed2276ad62d536c28"}
{"doc_id": "arxiv:2510.13461#abstract:part-8", "url": "https://arxiv.org/abs/2510.13461", "anchor": "#abstract:part-8", "type": "paper", "title": "", "section": "ABSTRACT", "text": "Py sin Γ) = Py cos Γ −Px sin Γ (12) By solving coupled equations 8-12, the collision impulses Px and Py can be determined. A key characteristic of this solution process is that the impulse calculation primar- ily depends on the following relatively deterministic physical parameters: geometric parameters (collision point location, vehicle dimensions, collision angle, etc.) can be accurately determined through collision geometry analysis; inertial parameters (vehi- cle mass, moment of inertia, etc.) possess high certainty; kinematic parameters (pre- 8 collision velocity states) can be obtained through trajectory analysis or sensor data; and material parameters (coefficient of restitution and friction coefficient), while ex- hibiting some variability, have relatively limited impact and can be determined through empirical data. In contrast, directly modeling the impact force process requires consid- ering numerous uncertain factors such as material nonlinearity, structural deformation, and contact stiffness variations, making impulse calculation inherently more determin- istic. The uncertainties in practical applications will be implicitly incorporated in the GMM impact forces training discussed below.According to the impulse-momentum theorem, impulse equals the integral of force over time Px = Z ∆t 0 Fx(t)dt Py = Z ∆t 0 Fy(t)dt (13) Based on the determined impulses Px and Py, simplified force-time models can be employed to represent the temporal distribution of impact forces. Here we used a GMM model. 3. Impact Force Modelling Considering Uncertainties 3.1. FEA Environment Setting To establish a reliable foundation for the stochastic impact force model, this paper conducted systematic simulation research using LS-DYNA R12.0 FEA software. LS- DYNA is one of the most advanced explicit nonlinear FEA software packages in- ternationally, with extensive applications and validation in automotive crash simula- tion. The software employs explicit time integration algorithms that effectively handle complex problems including large deformation, material nonlinearity, and contact col- lisions, making it particularly suitable for high-speed impact and collision analysis. The FEA scenarios designed in this paper are shown in the Figure 3, (a) is the grid diagram and (b) is the diagram of the FEA simulation collision, where a ve- hicle approaches from the rear-quarter and collides with the target vehicle at the rear axle body location. The target vehicle is set to 2000kg, while the bullet vehicle considers three different mass conditions: 1500kg/2000kg/2500kg. High-fidelity vehi- cle models are employed, with two different vehicle models selected. Each simulation adopts rigorous numerical settings to ensure computational accuracy and stability. Time integration uses the central difference scheme with a time step of 1 × 10−6 sec- onds, which can capture high-frequency phenomena during collision while maintaining numerical stability. Spatial discretization employs mixed hexahedral and tetrahedral meshes, with mesh sizes in critical regions controlled at 2-5mm to accurately capture stress gradients and deformation details. As shown in Figure 4, 6 cases with different parameter distributions are sampled from the 100 cases. Figure 4 (a) presents the different parameter distributions, reveal- ing that the values of Px/Py under different conditions can be essentially classified by the collision angle. As shown in Figure 4(b), since the collision scenarios are primarily side-impact collisions, the actual Py is generally larger than", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "81f0ed7e4702b05cfad65a52e04735a5bcb7e948cf6e1d6bd7b254d0ce97fb64"}
{"doc_id": "arxiv:2510.13461#abstract:part-9", "url": "https://arxiv.org/abs/2510.13461", "anchor": "#abstract:part-9", "type": "paper", "title": "", "section": "ABSTRACT", "text": "with mesh sizes in critical regions controlled at 2-5mm to accurately capture stress gradients and deformation details. As shown in Figure 4, 6 cases with different parameter distributions are sampled from the 100 cases. Figure 4 (a) presents the different parameter distributions, reveal- ing that the values of Px/Py under different conditions can be essentially classified by the collision angle. As shown in Figure 4(b), since the collision scenarios are primarily side-impact collisions, the actual Py is generally larger than Px. 9 Bullet Vehicle Target Vehicle Collision plane (a) (b) Figure 3. The PIT scenario setting in FEA simulation: (a) before and (b) after collision （a) Cluster of Different Approaching Angle （b) LS-DYNA Results Figure 4. Twelve sample results of FEA force calculated via ax and ay 10 3.2. PINN Based Impact force Modeling Considering Uncertainties Traditional approaches to impact force modeling typically rely on preset geometric profiles (e.g., triangular or rectangular pulses). While computationally efficient, such methods exhibit notable limitations in handling complex collision scenarios: (1) fixed geometric configurations fail to accommodate the inherent variations in force time- histories under differing collision conditions; (2) these methods cannot accurately pre- dict vehicle state transitions under collision when serving as inputs for subsequent collision dynamics analyses; (3) quantification of uncertainty in prediction outcomes remains challenging. To address these challenges, this paper proposes an innovative modeling frame- work that abandons traditional basis function methods, adopting a hybrid strategy integrating Physics-Informed Neural Networks (PINN) and Gaussian Mixture Models (GMMs). The core idea is to enable the neural network to autonomously learn the temporal distribution patterns of impact forces under strict physical constraints. The complete PINN-GMM modeling framework can be expressed as Fcollision(t; θ, W) = NN PINN-GMM(θ, t; W) (14) where Fcollision(t) = [Fx(t), Fy(t)]T denotes the impact force vector at time t, θ repre- sents the collision scenario feature vector, θ = [mt, mb, vty, vby, vtx, vbx, θb], W stands for the set of neural network weight parameters, and NN PINN-GMM indicates the neu- ral network mapping function integrated with physical constraints. The key innovation of this framework resides in embedding physical laws as hard constraints into the network training process to ensure the physical rationality of prediction results, while quantifying prediction uncertainty through the GMM com- ponent. Compared with traditional methods, this framework exhibits three distinct features: (1) integration of physical laws as hard constraints; (2) probabilistic model- ing of time-varying uncertainty; (3) adaptive pulse shape learning. 3.2.1. Physics Constraints Design To ensure model outputs comply with physical laws, this paper designs a multi-level physical constraint system. The first level is the momentum conservation constraint, where impulse during collision processes must satisfy the law of momentum conserva- tion. In the x and y directions respectively: Z tend tstart Fx(t) dt = Px = meff∆vx Z tend tstart Fy(t) dt = Py = meff∆vy (15) where meff = m1m2 m1+m2 is the effective mass, and ∆vx and ∆vy are the relative veloc- ity changes before and after collision. The corresponding impulse conservation loss function is: Limpulse = λimp \"\u0012Z tend tstart Fx(t)dt −Px", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "0688ad893e93d36f1afaa3f1d3589d2d33539ac47fd8aa00fd9dbb3202d7fe68"}
{"doc_id": "arxiv:2510.13461#abstract:part-10", "url": "https://arxiv.org/abs/2510.13461", "anchor": "#abstract:part-10", "type": "paper", "title": "", "section": "ABSTRACT", "text": "momentum conservation constraint, where impulse during collision processes must satisfy the law of momentum conserva- tion. In the x and y directions respectively: Z tend tstart Fx(t) dt = Px = meff∆vx Z tend tstart Fy(t) dt = Py = meff∆vy (15) where meff = m1m2 m1+m2 is the effective mass, and ∆vx and ∆vy are the relative veloc- ity changes before and after collision. The corresponding impulse conservation loss function is: Limpulse = λimp \"\u0012Z tend tstart Fx(t)dt −Px \u00132 + \u0012Z tend tstart Fy(t)dt −Py \u00132# (16) 11 The second level is the energy consistency constraint, where energy constraints are introduced to ensure energy conversion during collision processes complies with physical laws: Lenergy = λeng Z tend tstart F(t) · vrel(t) dt −Edissipated 2 (17) where the dissipated energy Edissipated = 1 2meffV 2 rel(1 −e2), e is the coefficient of resti- tution which is set 0.5 as normal semi plastic deformation, and Vrel is the magnitude of relative velocity before collision. 3.2.2. Neural Network Architecture and Time-Varying Probabilistic Modeling As shown in the Figure 5, the network adopts an encoder-decoder architecture, pri- marily comprising four core components with deep integration. The multi-scale feature encoder processes the input collision scenario features θ, employing a multi-branch structure to extract feature representations at different scales: hfeat = Swish(W3 · Swish(W2·Swish(W1·θ))). The sinusoidal positional time encoder employs sinusoidal positional encoding to process temporal information, enhancing the network’s under- standing of temporal patterns: PE(t, 2i) = sin \u0000t 100002i/d \u0001 , PE(t, 2i+1) = cos \u0000t 100002i/d \u0001 . This encoding approach enables the network to better capture the periodicity and temporal dependencies of impact forces. The temporal attention mechanism in- troduces self-attention to enhance the network’s focus on critical temporal nodes: Attention(Q, K, V ) = softmax \u0010 QKT √dk \u0011 V . The physical constraint integration layer serves as the output layer design, ensuring final outputs satisfy physical laws. Input Collision Feature Collision Feature Encoder Attention Encoder Decoder Output Force Prediction Sinusoidal Positional Time Encoder Physical Constraint Layers Loss K Q V Input Sinusoidal Positional Temporal Attention Physical Loss Output Figure 5. Scheme of PINN-GMM neural network framework The network employs the Swish activation function: Swish(x) = x · σ(x) = x 1+e−x . The Swish function exhibits excellent numerical stability and gradient propagation characteristics, particularly suitable for training deep physical networks. Compared to traditional ReLU or Tanh functions, Swish demonstrates superior convergence per- formance in handling physics-constrained optimization. The network output layer is designed to include three main components: instantaneous impact force prediction 12 F(t) = [Fx(t), Fy(t)]T , adaptive collision duration ∆t = σ(z∆t) × ∆tmax, and dynamic GMM parameters Φ = [π, µ, Σ]. Finally, this paper designs a time-varying GMM where parameters are modeled as continuous functions of time and scenario features: p(F(t)|θ) = K X k=1 πk(t, θ)N(F(t)|µk(t, θ), Σk(t, θ)) (18) The time-varying weight function πk(t, θ) = exp(wk(t,θ)) PK j=1 exp(wj(t,θ)); the time-varying mean function µk(t, θ) = Fbase(t, θ) + δk(t, θ) and the time-varying covariance function Σk(t, θ) = diag(σ2 min + exp(sk(t,", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "999f36a8c1e4d0a510b383a8b79bc59721af015e0806b3f8288665cf8e2625b5"}
{"doc_id": "arxiv:2510.13461#abstract:part-11", "url": "https://arxiv.org/abs/2510.13461", "anchor": "#abstract:part-11", "type": "paper", "title": "", "section": "ABSTRACT", "text": "duration ∆t = σ(z∆t) × ∆tmax, and dynamic GMM parameters Φ = [π, µ, Σ]. Finally, this paper designs a time-varying GMM where parameters are modeled as continuous functions of time and scenario features: p(F(t)|θ) = K X k=1 πk(t, θ)N(F(t)|µk(t, θ), Σk(t, θ)) (18) The time-varying weight function πk(t, θ) = exp(wk(t,θ)) PK j=1 exp(wj(t,θ)); the time-varying mean function µk(t, θ) = Fbase(t, θ) + δk(t, θ) and the time-varying covariance function Σk(t, θ) = diag(σ2 min + exp(sk(t, θ))). This design enables the model to provide dy- namically adjusted uncertainty estimates at different time instants and under different collision scenarios, offering significant advantages over traditional static uncertainty modeling approaches. 3.3. Model Analysis Based on FEA Dataset Numerical simulation data from 100 unique collision scenarios are utilized, with the training set and test set split at a 3:1 ratio. The network architecture adopts an 8- layer fully connected network, featuring hidden layer neuron counts of [256, 512, 512, 256, 128, 64, 32, 16]. Training parameters are set as follows: learning rate of 1e-3, batch size of 32, and 500 training epochs. The PINN-GMM-based prediction model is trained on a computing platform equipped with an NVIDIA GeForce RTX 4070 graphics card. As illustrated in the Figure 6, the loss function of the trained model converges and stabilizes at approximately 300 epochs, demonstrating favorable con- vergence characteristics. Both training loss and validation loss decrease synchronously, with no significant overfitting observed. As shown in Figure 6, all cases in the test set exhibit favorable overall predictions and uncertainty distributions, particularly regarding the trend and magnitude of force peaks. Thus, the proposed model can be deemed to achieve satisfactory performance. The RMSE values of Fx and Fy across all cases are 2.1e+03 and 3.9e+03, respectively, indicating that the model attains a relatively high level of prediction accuracy. 4. Adaptive PINN Vehicle Dynamics Response Modeling Framework Traditional PINN approaches face a fundamental dilemma: rigid physical constraints can completely suppress data-driven learning, effectively reducing neural networks to complex implementations of existing physical models. This paper addresses this critical challenge through a Adaptive Physics Integration mechanism that maintains physical consistency while preserving the network’s learning capability. 4.1. PINN Network Architecture Design Following the impact force prediction from the first network, this section designs a second Adaptive physics-informed neural network to model vehicle dynamic responses under uncertain impact forces. Unlike traditional methods that apply physical con- straints as post-processing corrections, this framework directly embeds adaptive phys- 13 Figure 6. PINN-GMM losses and six samples of force prediction results ical constraints into the network architecture, effectively addressing the core issue of constraint-induced learning inhibition. The Adaptive PINN can be expressed as: ˆx(t + 1) = NN Adaptive-PINN(Fcollision(t), Θvehicle, t; Wadaptive) (19) where ˆx(t + 1) = [vx(t + 1), vy(t + 1), ˙ψ(t + 1), ˙ϕ(t + 1)]T is the predicted vehicle state vector at time t + 1, Fcollision(t) = [Fx(t), Fy(t)]T is the impact force vector from the first PINN network, Θvehicle contains vehicle parameters (mass m, moment of iner- tia Izz, Ixx,s, Ixz, geometric parameters lf, lr,", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "4cb4fcb19ca71a1742164a215f5eaf955ece0b4cb884189e6b264ea35adfc6cc"}
{"doc_id": "arxiv:2510.13461#abstract:part-12", "url": "https://arxiv.org/abs/2510.13461", "anchor": "#abstract:part-12", "type": "paper", "title": "", "section": "ABSTRACT", "text": "of constraint-induced learning inhibition. The Adaptive PINN can be expressed as: ˆx(t + 1) = NN Adaptive-PINN(Fcollision(t), Θvehicle, t; Wadaptive) (19) where ˆx(t + 1) = [vx(t + 1), vy(t + 1), ˙ψ(t + 1), ˙ϕ(t + 1)]T is the predicted vehicle state vector at time t + 1, Fcollision(t) = [Fx(t), Fy(t)]T is the impact force vector from the first PINN network, Θvehicle contains vehicle parameters (mass m, moment of iner- tia Izz, Ixx,s, Ixz, geometric parameters lf, lr, tw, and tire parameters), and Wadaptive represents the adaptive neural network parameters, whose constraint sensitivity is dynamically adjusted based on training progress and data quality. The vehicle dynamics PINN employs a specialized encoder-decoder architecture optimized for multi-physics constraint integration. As shown in Figure 7 First, the input force Fcollision(t) is encoded by the Multi-Scale Feature Encoder module, where the vehicle parameter vector Θvehicle is densely embedded to create a feature representation of the encoded physical properties: hvehicle = Swish(W3 · Swish(W2 · Swish(W1 · Θvehicle))) (20) where W1, W2, W3 are learnable weight matrices that progressively transform vehicle 14 Input Physics Guard 4DOF Baseline Output Hidden Layers … … … Freezing Layers 60% 40% soft constraints Fine-tuning Figure 7. Scheme of A-PINN architecture for post-impact vehicle dynamics prediction parameters into high-dimensional feature representations. The decoder then integrates temporal and vehicle features via a cross-attention mechanism, followed by an output layer of physical constraints: hfused = CrossAttention(hcombined, hvehicle) (21) Traditional PINN methods use fixed constraint weights, which can easily lead to excessive dominance of physical constraints or insufficient data learning. This paper proposes adaptive constraint weights based on training progress: λphysics(t, epoch) = λmin + (λmax −λmin) · σ \u0012epoch −t0 τ \u0013 · DataQuality(t) (22) where λmin = 0.1 and λmax = 10.0 are the minimum and maximum constraint weights, σ(·) is the sigmoid activation function, t0 = 300 indicates the transition round at which the constraint strength begins to decay, τ = 100 controls the smoothness of the transition, and DataQuality(t) measures the local data reliability at time t. Data quality assessment is calculated using nearest neighbor distance: DataQuality(t) = exp \u0012 −dnearest(t) hbandwidth \u0013 (23) where dnearest(t) is the euclidean distance to the nearest training sample in the temporal feature space at time t, and hbandwidth = 0.1 is the bandwidth parameter that controls the locality of data quality assessment. The CrossAttention mechanism allows the network to selectively focus on relevant temporal features based on vehicle characteristics, while PhysicsGuard ensures outputs remain within physically meaningful bounds. 15 4.2. Adaptive Physics Guard To address the fundamental challenge of ensuring physical consistency while enabling the network to learn systematic model discrepancies from limited real data, an Adap- tive Physical Guard layer is introduced as the final processing stage. This layer serves as a critical safeguard that prevents physically impossible predictions while preserving the network’s ability to capture unmodeled dynamics. To prevent the vanishing gradients of the model, the physical protection layer is enforced through soft constraints: Lsoft-boundary = λboundary X i SoftPlus \u0012|ˆxi| −bi si \u00132 (24) where λboundary = 1.0 is", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "261ad9054c5b55249254f537ed7ebf3dc9994018282653949a5bb607f9fbf058"}
{"doc_id": "arxiv:2510.13461#abstract:part-13", "url": "https://arxiv.org/abs/2510.13461", "anchor": "#abstract:part-13", "type": "paper", "title": "", "section": "ABSTRACT", "text": "while enabling the network to learn systematic model discrepancies from limited real data, an Adap- tive Physical Guard layer is introduced as the final processing stage. This layer serves as a critical safeguard that prevents physically impossible predictions while preserving the network’s ability to capture unmodeled dynamics. To prevent the vanishing gradients of the model, the physical protection layer is enforced through soft constraints: Lsoft-boundary = λboundary X i SoftPlus \u0012|ˆxi| −bi si \u00132 (24) where λboundary = 1.0 is the boundary constraint weight, ˆxi represents the i-th pre- dicted state variable, bi is the physical boundary of the state variable ˆxi, si is the learnable smooth parameter, and SoftPlus(·) = ln(1 + exp(·)) ensures the differentia- bility of the entire constraint region. To prevent physical constraints from completely suppressing data learning, imple- ment a dynamic balance monitoring system: Balance Ratio(t) = Ldata(t) Ldata(t) + Lphysics(t) (25) where Ldata(t) is the data fit loss at time t,Lphysics(t) is the physical constraint loss at time t, Balance Ratio quantifies the relative dominance of data versus physics in the current loss landscape, with values close to 0 indicating physics dominance and close to 1 indicating data dominance. When physical constraints are overly dominant, the system automatically adjusts the constraint strength: λphysics = λphysics · max(0.1, Balance Ratio) (26) where λphysicsis the current constraint weight, and a factor of 0.1 ensures that a mini- mum constraint strength is maintained even if data dominance is detected to maintain basic physical consistency. Physical residual learning can be expressed as: Lphysics = λphysics(t) ∂ˆx ∂t −f4DOF(ˆx, Fcollision) 2 (27) where ˆx is the predicted vehicle state vector, f4DOF(·) represents the baseline 4DOF vehicle dynamics function from the equation 2, Fcollision is the crash force input, and λphysics(t) is the time-varying physical constraint weight. 4.3. Vehicle State GMM Output and Uncertainty Modeling Uncertainty in the second PINN model is primarily due to: (1) impact force uncertainty from the first network Fcollision(t);(2) vehicle parameter measurement error Θvehicle;(3) unmodeled dynamics of the baseline 4DOF model;(4) model uncertainty due to lim- 16 ited training data; and (5) uncertainty due to adaptive physical constraint weight adjustments. The network output layer predicts the GMM parameters of the vehicle state vector ˆx(t + 1) = [vx(t + 1), vy(t + 1), ˙ψ(t + 1), ˙ϕ(t + 1)]T , where vx(t + 1) and vy(t + 1) are the longitudinal and lateral speeds, respectively, ˙ψ(t + 1) is the yaw rate, and ˙ϕ(t + 1) is the roll rate: p(ˆx(t + 1)|Fcollision(t), Θvehicle) = J X j=1 ωj(t)N(ˆx(t + 1)|µx,j(t + 1), Σx,j(t + 1)) (28) where J is the number of mixed components, ωj(t)is the time-varying weight of the jth component, µx,j(t+1) ∈R4 is the mean vector of the jth component, and Σx,j(t+1) ∈ R4×4 is the covariance matrix of the jth component. The original network outputs zω,j, zµ,j, zσ,j are transformed as follows to ensure a valid probability distribution: ωj(t) = exp(zω,j) PJ k=1 exp(zω,k) µx,j(t + 1) = f4DOF(ˆx(t), Fcollision(t)) + tanh(zµ,j) ⊙σbound Σx,j(t + 1) = diag(σ2 min + exp(zσ,j)) (29) where", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "da19d59fbb8940a0261463b3fd6820e02b283a3ea1d0eb7f5a343053dfcbbbd5"}
{"doc_id": "arxiv:2510.13461#abstract:part-14", "url": "https://arxiv.org/abs/2510.13461", "anchor": "#abstract:part-14", "type": "paper", "title": "", "section": "ABSTRACT", "text": "where J is the number of mixed components, ωj(t)is the time-varying weight of the jth component, µx,j(t+1) ∈R4 is the mean vector of the jth component, and Σx,j(t+1) ∈ R4×4 is the covariance matrix of the jth component. The original network outputs zω,j, zµ,j, zσ,j are transformed as follows to ensure a valid probability distribution: ωj(t) = exp(zω,j) PJ k=1 exp(zω,k) µx,j(t + 1) = f4DOF(ˆx(t), Fcollision(t)) + tanh(zµ,j) ⊙σbound Σx,j(t + 1) = diag(σ2 min + exp(zσ,j)) (29) where the first equation ensures that the sum of weights is 1 and non-negative through the softmax function; The second equation centers on the 4DOF physics model prediction and limits the learning bias to [−1, 1] via the tanh(·) function, where σbound = [5.0, 3.0, 1.0, 0.8]T is the maximum allowable deviation of [vx, vy, ˙ψ, ˙ϕ]T , re- spectively; the third equation ensures positive definite covariance matrix by diagonal- ization and exponential function, σmin = 0.01 prevents numerical instability. Training objectives combine data fitting and physical consistency, ensuring that each GMM component satisfies the physical laws: LGMM-physics = λphys-gmmLphysics-gmm + λconsistencyLconsistency (30) Physical consistency loss ensures that the mean of each GMM component follows the vehicle dynamics equation: Lphysics-gmm = J X j=1 ωj(t) ∂µx,j ∂t −f4DOF(µx,j, Fcollision) −δlearned,j 2 (31) where ∂µx,j ∂t is the time derivative of the mean of the j component, f4DOF(·) is the 4DOF vehicle dynamics function (from the equation 2), δ learned,j is the learnable physics correction for the j component to capture unmodeled dynamics. Preventing GMM component over-clustering due to diversity consistency loss: Lconsistency = J−1 X j=1 J X k=j+1 exp −∥µx,j −µx,k∥2 2τ 2 ! (32) 17 where τ = 2.0 is the bandwidth parameter that controls the degree of separation between components. Losses increase when two components are too close together, encouraging diversity. 4.4. Uncertainty Propagation and Trajectory Distribution Modeling Vehicle state uncertainty is propagated to the global trajectory coordinate system by traceless transformation. For each GMM component j, generate 2n + 1 sigma points from its mean µx,j(t+1)and covariance Σx,j(t+1)(where n = 4is the state dimension): χj,0 = µx,j(t + 1) χj,i = µx,j(t + 1) + \u0012q (n + λ)Σx,j(t + 1) \u0013 i , i = 1, . . . , n χj,i = µx,j(t + 1) − \u0012q (n + λ)Σx,j(t + 1) \u0013 i−n , i = n + 1, . . . , 2n (33) where λ = α2(n + κ) −n is the scaling parameter, α = 0.001 controls the sigma point distribution, κ = 0 is the secondary scaling parameter, \u0000p (n + λ)Σx,j(t + 1) \u0001 i represents the i column of the square root of the matrix. Each sigma point is transformed into the global coordinate system through kine- matic equations: Yj,i = \u0014 X(t) + vx,i cos ψ(t)∆t −vy,i sin ψ(t)∆t Y (t) + vx,i sin ψ(t)∆t + vy,i cos ψ(t)∆t \u0015 (34) where X(t), Y (t) is the current global position, ψ(t) is the current heading angle, vx,i, vy,i is the velocity component of the ith sigma point, and ∆t", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "9b3f648025ce668faa5d1fed03d5cda5d71e4e086b76b0ca6414082166179540"}
{"doc_id": "arxiv:2510.13461#abstract:part-15", "url": "https://arxiv.org/abs/2510.13461", "anchor": "#abstract:part-15", "type": "paper", "title": "", "section": "ABSTRACT", "text": "+ 1) \u0001 i represents the i column of the square root of the matrix. Each sigma point is transformed into the global coordinate system through kine- matic equations: Yj,i = \u0014 X(t) + vx,i cos ψ(t)∆t −vy,i sin ψ(t)∆t Y (t) + vx,i sin ψ(t)∆t + vy,i cos ψ(t)∆t \u0015 (34) where X(t), Y (t) is the current global position, ψ(t) is the current heading angle, vx,i, vy,i is the velocity component of the ith sigma point, and ∆t is the time step. The parameter distribution of the transformed trajectory is calculated through weighted statistics: µtraj,j(t + 1) = 2n X i=0 W (m) i Yj,i Σtraj,j(t + 1) = 2n X i=0 W (c) i (Yj,i −µtraj,j(t + 1))(Yj,i −µtraj,j(t + 1))T (35) where the weights are defined as: W (m) 0 = λ/(n + λ), W ((m) i = 1/(2(n + λ))) (i = 1, . . . , 2n), W (c) 0 = λ/(n + λ) + (1 −α2 + β), W (c) i = 1/(2(n + λ)) (i = 1, . . . , 2n),β = 2 is the higher-order moment correction parameter. The final global trajectory GMM distribution combines all transformation compo- nents: p(X(t+1), Y (t+1)|Fcollision(t)) = J X j=1 ωj(t)N([X(t+1), Y (t+1)]T ; µtraj,j(t+1), Σtraj,j(t+1)) (36) 18 4.5. Training Strategy The complete training loss integrates multiple physical and statistical objectives to ensure both accuracy and physical consistency: Ltotal = Ldata + λphysLphysics-GMM + λtrajLtrajectory + λsmoothLsmoothness + λboundLsoft-boundary Ldata = ||ˆx(t + 1) −xtrue(t + 1)||2 Lsmoothness = ∂2ˆx ∂t2 2 (37) where the weight coefficients λ• control the relative importance of each loss component during training. xtrue is the true data. x0 is the known initial vehicle state at collision onset time t0. The PINN model proposed in this paper adopts a pre-training and fine-tune approach, and the physical prior knowledge or data knowledge based on true values used by all the compared Models are relatively fair. The initial pre-training phase focuses on embedding basic physical constraints, with training more considering physics loss to enable the network to learn vehicle dynamic physical laws. 100 force data obtained from FEA are input into a 4DOF model for basic simulation, and the output of the 4DOF model is used as the basis for PINN pre-training, where the trajectory sampling frequency is 100Hz and consistent with the data frequency used for fine-tuning with Carsim data. The fine-tune leverages the physically consistent foundation established during pre- training, achieving optimization performance with less training data through selective layer freezing and mixed loss optimization. In this paper, 5 and 20 real Carsim simu- lation trajectories which contain 2000 and 8000 samples are used for training during the fine-tune. To prevent overfitting and preserve learned physics while adapting to limited real data, a layer freezing approach is employed: Wfine-tune = {Wfrozen, Wactive} (38) where Wfrozen represents the frozen parameters from pre-training that encode funda- mental physics relationships, and Wactive represents the active parameters that adapt to specific collision scenarios. The number of frozen layers Nfrozen is determined as Nfrozen = ⌊0.6", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "db41bfa1b53420069938bb9410924b05b48cc3fb62f64370c830ca441ae5a3e9"}
{"doc_id": "arxiv:2510.13461#abstract:part-16", "url": "https://arxiv.org/abs/2510.13461", "anchor": "#abstract:part-16", "type": "paper", "title": "", "section": "ABSTRACT", "text": "real Carsim simu- lation trajectories which contain 2000 and 8000 samples are used for training during the fine-tune. To prevent overfitting and preserve learned physics while adapting to limited real data, a layer freezing approach is employed: Wfine-tune = {Wfrozen, Wactive} (38) where Wfrozen represents the frozen parameters from pre-training that encode funda- mental physics relationships, and Wactive represents the active parameters that adapt to specific collision scenarios. The number of frozen layers Nfrozen is determined as Nfrozen = ⌊0.6 × Ntotal⌋. Ensuring that 60% of layers remain frozen to preserve physics knowledge while allowing 40% to adapt to trajectory-specific patterns. 4.6. Baseline Models To validate the proposed PINN method, a comprehensive comparison was conducted with two baseline methods. Notably, the comparison between different models is based on the premise that the prior physical knowledge and true data are completely consis- tent, where 20% of the data obtained from the dataset was selected as known data. A conventional numerical integration approach using the same 4DOF vehicle dynamics equations without neural network approximation serves as the 4DOF baseline model: x(t + ∆t) = x(t) + f(x(t), u(t), Fcollision(t)) · ∆t (39) 19 where f(·) represents the system dynamics function derived from the 4DOF equations, and u(t) represents control inputs (steering, throttle, braking). For basic and easily obtainable vehicle parameters, they are directly exported from the Carsim model, while for tire parameters Ki, Ci that have a significant dynamic impact on the vehicle under extreme conditions, particle swarm optimization (PSO) is used to obtain the best matching parameters from 8000 prior sampling data. Another baseline model is a standard feedforward neural network without physi- cal constraints, trained purely based on input-output mapping, removing the Physics Guard layer in PINN and only considering data loss during training, representing a purely empirical method for fitting the data: ˆxdata(t + 1) = NN data(Fcollision(t), Θvehicle, t; Wdata) (40) 4.7. Carsim-FEA Force Dataset Combined Simulation Analysis By inputting transient impact forces obtained from FEA software into the Carsim model, the corresponding vehicle trajectory is obtained. First, the vehicle’s dynamic characteristics under different forces are analyzed based on the global X-Y trajectory. A clustering method for vehicle dynamics simulation data based on force integrals (Px, Py) is employed. This method, grounded in physical principles, uses Px, Py as the core classification criteria. First, four basic force integral features are extracted for each simulation case: X- and Y-direction force integrals (Ifx, Ify), total impulse (Itotal = R T 0 |F(t)|dt), and force statistics (maximum value, mean, and standard deviation). The clustering process utilizes an improved K-means algorithm, adaptively determining the optimal number of clusters using the silhouette coefficient. 20 random initializations and a maximum of 500 iterations are used to ensure result stability. Finally, post- processing merges small clusters (≤5%) to ensure the physical plausibility of the clustering results. As shown in Figure 8, cluster analysis divides the 100 Carsim simulation cases into 4 categories based on impact characteristic patterns, and different clusters exhibit significant differences in X-Y trajectory features. Among them, the vehicles in C0-C4 experience gradually increasing impact,", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "3d9c757e6454c06602568c68367050e9ae2b5cbe4eb3f9f83ae78f4db7089602"}
{"doc_id": "arxiv:2510.13461#abstract:part-17", "url": "https://arxiv.org/abs/2510.13461", "anchor": "#abstract:part-17", "type": "paper", "title": "", "section": "ABSTRACT", "text": "optimal number of clusters using the silhouette coefficient. 20 random initializations and a maximum of 500 iterations are used to ensure result stability. Finally, post- processing merges small clusters (≤5%) to ensure the physical plausibility of the clustering results. As shown in Figure 8, cluster analysis divides the 100 Carsim simulation cases into 4 categories based on impact characteristic patterns, and different clusters exhibit significant differences in X-Y trajectory features. Among them, the vehicles in C0-C4 experience gradually increasing impact, where most trajectories in C0 and C1 show that the vehicles do not spin significantly under impact force, whereas all vehicles in C2 and C3 exhibit clear spinning, leading to loss of control in the latter half of the vehicle trajectories. Since the tire characteristics of the vehicle in these two scenarios are quite different, and tire characteristics are precisely the hard-to-model parts in vehicle dynamics, the following section will present the specific performance of different models in these two types of cases. Based on the performance comparison analysis in the Table 1, different models exhibit distinct characteristics and trade-offs in terms of tracking accuracy, computa- tional efficiency, and physical consistency. The NN-WITHOUT-PINN model ranks first overall with the lowest average error (8.25m) and demonstrates superior performance in most RMSE metrics including X, Y positions and vx velocity, while maintaining the highest computational efficiency (7.45±1.08 ms/step). However, this purely data- driven model lacks physical constraints, potentially leading to predictions that violate fundamental vehicle dynamics principles and pose safety risks in complex scenarios due to unrealistic outputs. The traditional 4DOF model, despite its fast computa- tion speed (3.21±2.03 ms/step), suffers from poor adaptability due to fixed parameter settings and exhibits the worst performance across all accuracy metrics, particularly 20 Figure 8. Comparing PINN-based vehicle state prediction with baselines under impact force with uncertain- ties with X-direction position errors reaching 37.75±29.47m, failing to meet high-precision tracking requirements. In contrast, the PINN-GMM-20TRAC model demonstrates the best comprehensive performance and practical value. While its average error (10.47m) is slightly higher than the pure neural network approach, it excels in critical parameters for vehicle stability control: vy velocity prediction error of only 1.54±1.87 km/h and yaw rate prediction error of 2.62±2.68 deg/s, significantly outperforming other models. More importantly, PINN-GMM-20TRAC incorporates physics-informed neural network con- straints to ensure predictions comply with vehicle dynamics principles, maintaining physical reasonableness while achieving high accuracy. The model achieves an optimal balance between computational efficiency and prediction precision, offering better sta- bility and robustness compared to PINN-GMM-5TRAC. This makes it the optimal choice for practical engineering applications that require the integration of accuracy, efficiency, and safety considerations. Table 1. Model Performance Comparison Analysis Overall Performance Ranking on Tracking Rank", "source": "arxiv_pdf", "published": "", "tokens": 448, "sha256": "4b4b2d3b1af49c79fc4bd9b1c9f13b2295650b34f09035fb4ebc0c3b1221efe7"}
{"doc_id": "arxiv:2510.13461#model:part-1", "url": "https://arxiv.org/abs/2510.13461", "anchor": "#model:part-1", "type": "paper", "title": "", "section": "Model", "text": "Avg Error (m) Score Time (ms)/step 1 NN-ONLY 8.25 0.121 7.45±1.08 2 PINN-20 10.47 0.096 10.78±1.32 3 PINN-5 14.13 0.071 10.75±1.34 4 4DOF 35.20 0.028 3.21±2.03 RMSE Performance (Mean±Std) Variable 4DOF PINN-5 PINN-20 NN-ONLY X(m) 37.75±29.47 10.77±25.23 7.13±12.20 9.55±9.22 Y (m) 16.40±13.66 13.76±46.34 9.02±6.08 4.88±5.29 vx(km/h) 42.96±57.71 7.78±13.84 7.85±13.83 5.94±6.30 vy(km/h) 12.59±8.32 1.36±1.82 1.54±1.87 1.58±1.89 Yaw Rate(deg/s) 28.93±99.78 15.03±100.75 2.62±2.68 6.26±6.35 A typical case for C0 is shown in Figure 9. The 4DOF model in this case exhibits the best accuracy in trajectory tracking, as evidenced by its closest approach to the reference trajectory over the entire path, especially in the challenging curved portion, and achieves the lowest RMSE values on several metrics including position accuracy and yaw rate prediction, since the 4dof model with tires in the linear region theo- retically also has excellent performance. However, the PINN-GMM-20 TRAC model has several significant advantages. Most importantly, PINN-based methods provide uncertainty quantification, as shown in (b) uncertainty distribution, where the proba- 21 bility density distribution is centered around the actual trajectory with a peak value of about 0.0015, providing valuable confidence estimates for safety-critical decisions. From this point of view, PINN also has good performance in this case. It is worth noting that training with 5 tracks outperforms training with 20 tracks, which proves that this case is a special case. Although the PINN model showed slightly higher tra- jectory bias in the latter half of the path, it maintained competitive performance in longitudinal velocity tracking and demonstrated robust handling of complex vehicle dynamics transitions, especially during the initial high yaw rate phase (approximately 15 degrees/sec). (a) (c) (d) (e) (b) Figure 9. A typical Case for C0 As shown in Figure 10, the main reason for this is that the tire model of the vehicle has not been estimated well. Throughout the sliding process of the vehicle, the state of the tire keeps changing, and the current state of the vehicle affects the state of the tire. Therefore, the PINN method can better capture the changes in the tire, thus obtaining better vehicle state prediction results, while not deviating from the laws of physics to produce unrealistic scenarios.Pure neural network models completely violate the laws of physics, with trajectories exhibiting situations impossible in real-world scenarios, particularly evident in the increase in longitudinal velocity without force input. 22 (a) (c) (d) (e) (b) Figure 10. A typical Case for C2 5. Scaled Vehicle Experiments Results and Analysis 5.1. Experiment Setup To validate the model’s performance on real vehicles, a scaled vehicle experiment is conducted using two chassis platforms that accurately represent vehicle dynamic characteristics. The parameters of the Bullet Vehicle and Target Vehicle are detailed in Table 2 and Table 3, respectively. The scale ratio between the experimental vehicles and actual vehicles is approximately 1:4. Table 2. Bullet Vehicle Parameters Parameter Value External Dimensions 2100mm×1120mm×455mm (L×W×H) Ground Clearance 120mm Weight & Payload Weight: 290KG Max Acceleration 0.8g (No Load) Rated Power 12KW Peak Power 24KW Max Speed 100 km/h Steering Type Front-wheel steering Drive Mode Independent four-wheel drive Communication CAN Bus As", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "184fb0b26aa863679ff46055d97a24bafa62638c6fadd5a39630a0690962fb94"}
{"doc_id": "arxiv:2510.13461#model:part-2", "url": "https://arxiv.org/abs/2510.13461", "anchor": "#model:part-2", "type": "paper", "title": "", "section": "Model", "text": "dynamic characteristics. The parameters of the Bullet Vehicle and Target Vehicle are detailed in Table 2 and Table 3, respectively. The scale ratio between the experimental vehicles and actual vehicles is approximately 1:4. Table 2. Bullet Vehicle Parameters Parameter Value External Dimensions 2100mm×1120mm×455mm (L×W×H) Ground Clearance 120mm Weight & Payload Weight: 290KG Max Acceleration 0.8g (No Load) Rated Power 12KW Peak Power 24KW Max Speed 100 km/h Steering Type Front-wheel steering Drive Mode Independent four-wheel drive Communication CAN Bus As shown in Figure 11, PIT experiments are conducted to validate the feasibility and accuracy of the proposed modeling method. Both vehicles used in the experiment are equipped with steer-by-wire and drive-by-wire capabilities, and each is integrated with an Industrial Personal Computer (IPC) for executing planning and control algorithms. A GNSS-IMU system and a camera are installed for data acquisition. Communication between the two vehicles is achieved via TCP/IP protocol within a local area network for real-time data transmission. 23 Table 3. Target Vehicle Parameters Parameter Value External Dimensions 1900mm×900mm×350mm (L×W×H) Weight & Payload Weight: 100KG Max Speed 30 km/h Steering Type Front-wheel steering Drive Mode rear-wheel drive Communication Serial RS232 The bullet vehicle operates an autonomous driving algorithm designed to execute the PIT maneuver. Based on the real-time position of the target vehicle, it performs prediction, planning, and control to achieve the desired PIT state before initiating the PIT maneuver. Control commands are directly sent via the CAN bus to assign torque signals to each of the four wheels. The entire PIT process can be divided into two phases. The first phase is the chasing process, where the primary control objective of the bullet vehicle is to achieve a parallel body motion and matching velocity with the target vehicle, while maintaining a lateral distance that ensures the required collision angle and speed for PIT execution. The second phase involves performing the PIT maneuver, whose main goal is to induce a significant yaw change in the target vehicle, thereby disabling its ability to continue moving forward. The model proposed in this paper primarily predicts the vehicle’s posture changes during post-impact under the assumption that the steering wheel and throttle are released, and that no electronic stability control (ESC) or similar systems are active. Similar to simulation, first use the 8000 sampling data obtained from simulating a 4DOF model containing force inputs as pre-training data to allow the network to fully learn the physical laws, then use the 100 sampling data of scaled experiment for Fine-tuning the model. The test results are presented in Table 4. The proposed Physics-Informed Neural Network demonstrates superior trajectory tracking performance, achieving a 63.6% reduction in average trajectory error (0.73±0.58 m vs. 2.01±1.77 m) compared to the conventional 4DOF model, indicating its enhanced capability in capturing complex vehicle dynamics. Notably, the proposed model excels in longitudinal positioning (R2 = 0.984) and velocity prediction (R2= 0.708), demonstrating successful integration of physical constraints with data-driven learning. However, this improvement in accuracy comes with a modest computational overhead, as the proposed method requires slightly longer inference times (13.76 ms vs. 4.42 ms)", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "13c1d48059dc85407f659509901753d8bb10a99a813ff77181428f65ce9f7e49"}
{"doc_id": "arxiv:2510.13461#model:part-3", "url": "https://arxiv.org/abs/2510.13461", "anchor": "#model:part-3", "type": "paper", "title": "", "section": "Model", "text": "tracking performance, achieving a 63.6% reduction in average trajectory error (0.73±0.58 m vs. 2.01±1.77 m) compared to the conventional 4DOF model, indicating its enhanced capability in capturing complex vehicle dynamics. Notably, the proposed model excels in longitudinal positioning (R2 = 0.984) and velocity prediction (R2= 0.708), demonstrating successful integration of physical constraints with data-driven learning. However, this improvement in accuracy comes with a modest computational overhead, as the proposed method requires slightly longer inference times (13.76 ms vs. 4.42 ms) than the 4DOF model. The inference time remains stable across various operational conditions, indicating good computational stability. This trade-off between accuracy and computational cost is reasonable, as more complex models inherently require greater computational resources. The increased inference time of the proposed model is primarily attributed to its neural network architecture, which is predictable and expected. Meanwhile, the model’s prediction accuracy can be further improved with additional training data, as validated in previ- ous Carsim simulation experiments. Although the proposed approach maintains high accuracy in short-to-medium term predictions, its physical constraints may become in- sufficient for extended prediction horizons, particularly when limited data availability hinders neural network convergence. Therefore, the proposed model is particularly suitable for applications requiring 24 Target Vehicle Bullet Vehicle Chasing PIT GNSS/IMU IPC GNSS/IMU IPC TCP/IP t =20s t =17s t =21s Figure 11. Scaled Vehicles and Experiment Scenario high-precision short-term predictions (such as the PIT post-collision trajectory opti- mization discussed in this paper), while the traditional 4DOF model retains advan- tages in real-time control systems where computational efficiency is critical. However, when control systems require calibration through data-driven approaches, the proposed framework offers a valuable solution for model improvement through continuous data integration. As shown in Figure 12, case 3 presents even with this earlier collision time, the proposed model maintains excellent trajectory tracking performance prior to collision. The 4DOF model exhibits more severe deviations in this case, particularly showing substantial errors in Y position predictions. The velocity time series observation indi- cates that the proposed model more accurately captures the vehicle’s dynamic char- acteristics during steering maneuvers, while the 4DOF model demonstrates obvious deficiencies in lateral dynamics modeling. 25 Table 4. Vehicle Dynamics Model Performance Analysis of Scaled Vehicle Experiments Overall Performance Ranking on Trajectory Tracking", "source": "arxiv_pdf", "published": "", "tokens": 372, "sha256": "140ad0bd6474e72d18131c55ecde7ba3da1473333d42f48da9b0d09070f59952"}
{"doc_id": "arxiv:2510.13686#abstract", "url": "https://arxiv.org/abs/2510.13686", "anchor": "#abstract", "type": "paper", "title": "", "section": "Abstract", "text": "Although digital fabrication processes at the desktop scale have become proficient and prolific, systems aimed at producing larger- scale structures are still typically complex, expensive, and unreliable. In this work, we present an approach for the fabrication of scal- able macroscale structures using simple robots and interlocking lattice building blocks. A target structure is first voxelized so that it can be populated with an architected lattice. These voxels are then grouped into larger interconnected blocks, which are pro- duced using standard digital fabrication processes, leveraging their capability to produce highly complex geometries at a small scale. These blocks, on the size scale of tens of centimeters, are then fed to mobile relative robots that are able to traverse over the structure and place new blocks to form structures on the meter scale. To facilitate the assembly of large structures, we introduce a live digi- tal twin simulation tool for controlling and coordinating assembly robots that enables both global planning for a target structure and live user design, interaction, or intervention. To improve assembly throughput, we introduce a new modular assembly robot, designed ∗Both authors contributed equally to this research. This work is licensed under a Creative Commons Attribution 4.0 International License. SCF ’25, Cambridge, MA, USA © 2025 Copyright held by the owner/author(s). ACM ISBN 979-8-4007-2034-5/2025/11 https://doi.org/10.1145/3745778.3766665 for hierarchical voxel handling. We validate this system by demon- strating the voxelization, hierarchical blocking, path planning, and robotic fabrication of a set of meter-scale objects. CCS Concepts • Computer systems organization →External interfaces for robotics; Robotic components; • Human-centered computing →User interface toolkits. Keywords Large-Scale Fabrication, Digital Fabrication, Modular Assembly, Robotics, Cellular Materials ACM Reference Format: Miana Smith, Paul Arthur Richard, Alexander Htet Kyaw, and Neil Gershen- feld. 2025. Hierarchical Discrete Lattice Assembly: An Approach for the Digital Fabrication of Scalable Macroscale Structures. In ACM Symposium on Computational Fabrication (SCF ’25), November 20–21, 2025, Cambridge, MA, USA. ACM, New York, NY, USA, 15 pages. https://doi.org/10.1145/3745778. 3766665 1", "source": "arxiv_pdf", "published": "", "tokens": 325, "sha256": "a184031436221fb9aef382a1f15564e3e1355e968a3c8a9fd1428eec305aa691"}
{"doc_id": "arxiv:2510.13686#introduction", "url": "https://arxiv.org/abs/2510.13686", "anchor": "#introduction", "type": "paper", "title": "", "section": "Introduction", "text": "Machines for digital fabrication typically cannot make objects or structures larger than themselves. Although this is not necessar- ily limiting on the desktop scale, as target structures scale toward human or architectural scales, the prospect of ever-growing ma- chine footprints eventually becomes untenable [Meisel et al. 2022]. Current digital fabrication approaches for larger-scale structures often section the target structure into parts that can be produced arXiv:2510.13686v1 [cs.RO] 15 Oct 2025 SCF ’25, November 20–21, 2025, Cambridge, MA, USA Smith et al. separately and then assembled, usually manually [Formlabs 2023] [Kovacs et al. 2017], [Baudisch et al. 2019]. Approaches that consider assembly automation often rely on large and expensive industrial robotic arms with limited overall mobility [Menges et al. 2017]. In either case, there is no clear strategy toward scaling to unlimited build footprints or autonomous environments. In contrast, collective robotic construction, in which a group of robots collaborate to assemble a structure larger than any individual robot, offers significant promise towards scale-agnostic, efficient, and autonomous fabrication [Petersen et al. 2019]. However, hard- ware demonstration of these systems has typically been limited in scale [Jenett et al. 2019], load capacity [Petersen et al. 2011], or geo- metric freedom [Melenbrink et al. 2021]. This is because automated large-scale fabrication with mobile robots is inherently challeng- ing, with many competing priorities, such as structural stability versus mass efficiency, geometric complexity versus ease of robotic fabrication, or robotic functionality versus swarm simplicity. By designing a material system together with its robotic assem- bly system, we can address some of these issues. The mechanical interplay between the assembly robots and the underlying material system can enable local error correction, allowing relatively simple robots to build large and precise structures [Jenett et al. 2019]. In this work, we present an approach to the assembly of large struc- tures using small relative robots that manipulate hierarchical blocks of lattice material. Our approach uses a new type of discrete lattice block and robotic system aimed at improving assembly throughput and mechanical stability. We use standard digital fabrication pro- cesses to pre-produce compounded blocks of architected lattices– a high efficiency material system– at the size scale of tens of cen- timeters, and then use mobile robots to take over beyond that, to the meter scale. To ensure structural stability, we developed a tool similar to a 3D printing slicer that imports a 3D mesh, voxelizes it, and finds interleaved patterns for robots to assemble. It then simulates the assembly live, streaming instructions to the hardware system. Our contributions are: • An interactive tool for voxelizing 3D shapes and finding connected patterns for different shapes of blocks. • A digital twin path planning and simulation environment integrated with the hardware system, for both assembly control and live design and re-design of structures. • The development of a compounded set of self-aligning, in- terlocking, and load bearing octet lattice blocks designed for robotic assembly. • The development of a new modular inchworm style assembly robot, designed for hierarchical assembly and low system cost and complexity. 2", "source": "arxiv_pdf", "published": "", "tokens": 505, "sha256": "98211dd5f7e5c03846504b6489ae2526707dd2d8101d4e2a966885ad306d11dc"}
{"doc_id": "arxiv:2510.13686#related-work:part-1", "url": "https://arxiv.org/abs/2510.13686", "anchor": "#related-work:part-1", "type": "paper", "title": "", "section": "Related Work", "text": "Our approach builds on existing research in scalable building sys- tems, robotic construction and assembly systems, and material- robot and modular robotic systems. In this section, we overview these fields and identify trends and limitations that guided our approach in this work. 2.1 Scalable Building Systems Although this work is focused on the robotic assembly of structures, scalable prototyping systems designed for human use offer useful insights into reliable mechanical design for building at larger scales. Specifically, we draw from systems that are not significantly limited by the build volume of the fabrication systems used. Broadly, these systems tend to fall into three categories: continuous, sectioned, and discrete fabrication processes. 2.1.1 Continuously Fabricated Systems. An example of a continu- ous fabrication system capable of producing meter-scale structures is Protopiper [Agrawal et al. 2015], which extrudes tape-based pipes to draw out human- to architecture- scale structures. How- ever, this system has limited load bearing capacity. Wire benders, such as [PensaLabs 2025], are similarly able to extrude much larger structures than the machine itself, and with potentially good load- bearing capacity, such as in [Bhundiya and Cordero 2023]. However, these continuous extrusion based processes have no mechanisms for preventing error accumulation in the built structure, and instead rely on the precision of the machine, or human intervention, to cor- rect problems as they arise— a potentially significant issue, given that a small error in bend angle over a long distance can result in substantial deflection from the target shape. 2.1.2 Sectioned Fabrication and Assembly. Splitting up a target structure into subcomponents can help ameliorate some of the is- sues with error accumulation, while additionally opening up a wider array of fabrication systems. These approaches are typically geared toward breaking a larger model down into either 2D components that can be cut and assembled together, or smaller 3D models that can be separately fabricated and assembled. LuBan3D [Luban3D 2025] is a software package aimed at doing exactly this: facilitating users to fabricate structures much bigger than the machines they have, such as the house in [Sass and Botha 2006]. More special- ized examples aimed at non-expert users include Kyub [Baudisch et al. 2019], which enables the design and automatic creation of load-bearing structures made from laser cut plywood, or HingCore [Abdullah et al. 2022] and PopCore [Abdullah et al. 2024], which offer joinery-free methods for processing foamcore or equivalent sandwich panel material. The primary issue with these methods is that they produce highly specific parts for specific designs, often requiring dexterous methods for assembly. This makes automa- tion much harder: handling extremely diverse geometries in many degrees of freedom is a hard problem in robotics. Additionally, this typically prevents re-use of parts, as components are likely not portable to new designs, requiring full re-fabrication and re- assembly to make design iterations. 2.1.3 Discrete Assembly. Continuing the trend toward breaking up a big structure into smaller parts is the class of systems that use discrete assembly, where a single part type (or a small library of component types) is used to iteratively build many different target", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "2f583e200cee0bfb9ce450b5dbec10bc42053c8c668a99ca2b4144bb0e4d78bc"}
{"doc_id": "arxiv:2510.13686#related-work:part-2", "url": "https://arxiv.org/abs/2510.13686", "anchor": "#related-work:part-2", "type": "paper", "title": "", "section": "Related Work", "text": "many degrees of freedom is a hard problem in robotics. Additionally, this typically prevents re-use of parts, as components are likely not portable to new designs, requiring full re-fabrication and re- assembly to make design iterations. 2.1.3 Discrete Assembly. Continuing the trend toward breaking up a big structure into smaller parts is the class of systems that use discrete assembly, where a single part type (or a small library of component types) is used to iteratively build many different target structures. A classic example of this is LEGO®, which enables an imprecise assembler (e.g. a child) to build precise and relatively robust structures. This is because the material system is designed to self-align and error correct during the assembly process, so that the accuracy of the final structure is primarily determined by the tolerances in the part manufacturing, and not by the tolerances Hierarchical Discrete Lattice Assembly SCF ’25, November 20–21, 2025, Cambridge, MA, USA of the assembler, in contrast to continuous extrusion fabrication processes. LEGO® uses a volumetric brick-based decomposition, but other forms exist, including strut-and-node systems [Kovacs et al. 2017], facet-based assemblies [Jenett et al. 2018], and voxel- based assemblies [Gregg et al. 2018]. This style of system lends itself well to robotic fabrication, as it features a limited and similar set of parts, often designed to accommodate imprecise assemblers. Among discrete assembly approaches for large-scale structures, we focus on using discrete architected lattices, as these systems offer competitive mechanical properties at a lightweight [Schaedler and Carter 2016]. This is desirable both to improve the performance of the resultant structure and to lower the payload demands on an assembly robot, which reduces some engineering challenges associ- ated with heavy building materials [Goessens et al. 2018]. Discrete versions of architected lattices have demonstrated extreme perfor- mance results [Cheung and Gershenfeld 2013], designed material anisotropies [Jenett et al. 2020], and integrated electronic function- ality [Smith et al. 2025]. Manually, these have been assembled into a wide array of meter-scale structures, such as morphing aero- and hydro- structures [Jenett et al. 2017] [Parra Rubio et al. 2023], ve- hicles [Jenett 2020], or load-bearing static structures [Smith et al. 2025], demonstrating the broad applicability that is desirable for a generalist building system. 2.2 Robotic Fabrication Systems We now consider the question of how to automate the building of large structures. Approaches for this, again, roughly fall into three categories: large machines to build large structures, large robots to assemble large structures, or small mobile robots to assemble large structures. 2.2.1 Static Gantries. In the first approach, a typically gantry-based machine is used to assemble a structure, as in [Apolinarska et al. 2016], or, more commonly, to 3D print a structure, as shown in [Batikha et al. 2022] often using systems in the style of [COBOD 2025], for example. Though these approaches have been demon- strated at commercial architectural scales, they are still fundamen- tally limited by the need for very large machines, which introduces both an upper limit on the size of the structure that can be made, as well as significant logistics challenges,", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "d702376e9e11990301f9eb8495634dcb847a8520ec51054f38ac7b66ee8a716c"}
{"doc_id": "arxiv:2510.13686#related-work:part-3", "url": "https://arxiv.org/abs/2510.13686", "anchor": "#related-work:part-3", "type": "paper", "title": "", "section": "Related Work", "text": "a structure, as in [Apolinarska et al. 2016], or, more commonly, to 3D print a structure, as shown in [Batikha et al. 2022] often using systems in the style of [COBOD 2025], for example. Though these approaches have been demon- strated at commercial architectural scales, they are still fundamen- tally limited by the need for very large machines, which introduces both an upper limit on the size of the structure that can be made, as well as significant logistics challenges, either in terms of transport- ing parts, in the case of pre-fabrication, or building the machine in place, in the case of in-situ fabrication. With the extrusion based approaches, there are additional open challenges in material for- mulation and performance [Marchment and Sanjayan 2020] [Roux et al. 2023]. For these reasons, we are interested in further exploring assembly based approaches. 2.2.2 Industrial Arms. At the large, complex, and expensive side of the assembly based approaches are projects that have focused on us- ing industrial robotic arms for assembly. These have demonstrated a range of material types, such as steel rebar [Ma et al. 2020], timber [Apolinarska et al. 2021], or bricks [Gharbia et al. 2020], including commercial systems such as [HadrianX 2025] or [Construction- Robotics 2025]. However, these systems have limited accessible footprints, based on the reach of the robotic arm (or the linear axis it is mounted on), or feature significant localization challenges, if on a mobile base [Bodea et al. 2022]. These projects often rely on the high performance of their robotic and sensing systems to com- pensate for the simplicity in their building materials— by shifting some of the complexity onto the material system, we can reduce some of the demands (and size and cost) on the robotic system. 2.2.3 Material-Robot Systems. Building systems that rely on the co-design and interaction of the robots and material systems are often referred to as material-robot systems [Jenett et al. 2019]. The degree of material-robot integration varies across the literature. At one extreme end, self-reconfiguring modular robots, as in [Neubert and Lipson 2016] or [Hauser et al. 2020], could be considered a material-robot system where there is no distinction between the building system and the robot, resulting in a very complex material system. At the other end, systems such as [Goessens et al. 2018] or [Leder et al. 2022] only shift a minimal amount of complexity onto the material system. For discrete lattices, systems for voxel-based assembly, such as in [Jenett et al. 2019], [Smith et al. 2024], or [Gregg et al. 2024], as well as strut-based assembly, such as in [Hsu et al. 2016] or [Yoon and Rus 2007] have been explored. [Jenett et al. 2019] demonstrated that an inchworm style robot could accurately traverse and assemble a voxel structure with no global feedback, based on the error correc- tion between the voxels and the robot. However, this system relies on magnets and an underlying steel table to form connections and provide mechanical stability, and has low load bearing capacity on its own. This system is evolved in [Park et al.", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "18693cfa491c7ad763899a179b7c6fea995d27c6463baeef38f2f8c6cae8add5"}
{"doc_id": "arxiv:2510.13686#related-work:part-4", "url": "https://arxiv.org/abs/2510.13686", "anchor": "#related-work:part-4", "type": "paper", "title": "", "section": "Related Work", "text": "et al. 2016] or [Yoon and Rus 2007] have been explored. [Jenett et al. 2019] demonstrated that an inchworm style robot could accurately traverse and assemble a voxel structure with no global feedback, based on the error correc- tion between the voxels and the robot. However, this system relies on magnets and an underlying steel table to form connections and provide mechanical stability, and has low load bearing capacity on its own. This system is evolved in [Park et al. 2023] to demonstrate the assembly of 100s of voxels [Gregg et al. 2024]; however, the com- plexity of the robotic system is substantially increased, requiring three separate high degree of freedom robotic systems to perform assembly (a voxel carrying robot, a voxel installing robot, and a voxel fastening robot internal to the lattice), while the size scale of the lattice limits shape fidelity (300mm pitch). In both of these voxel assembly systems, as the structure grows, the assembly throughput decreases, as the robot must traverse the built structure back and forth to pick up and install new material. [Abdel-Rahman et al. 2022] proposes that recursive hierarchical systems can help improve this issue— if assembly robots can as- semble more assembly robots, then the swarm can increase its own parallelization, and if assembly robots can manipulate larger quanti- ties of material at once, then they can build more efficiently. [Smith et al. 2024] demonstrates a first version of the hardware for a load- bearing modular voxel assembler, but is limited to desktop-scale objects. 2.2.4 Synthesis. Based on the prior art, we can distill some guide- lines for an ideal robotic assembly-based digital fabrication system. Collective robotic approaches that have demonstrated the most promise towards scaling to practical applications typically strike a careful balance between the complexity of the material system and robotic system. The material system needs to be kept simple enough to be manufactured, while sophisticated enough to account for limited robotic functionality. The robot, in turn, needs enough features and degrees of freedom to reliably traverse and manip- ulate the lattice, but not so many that it becomes untenable to increase the amount of robots used. An ideal connection system only requires access from a single direction, without adding sig- nificant installation time. To maintain effective assembly times for larger structures, as well as to improve reliability, the system, as SCF ’25, November 20–21, 2025, Cambridge, MA, USA Smith et al. much as possible, should be able to parallelize its function and offer hierarchical assembly. 3 System Overview We present a system for robotically assembling hierarchical discrete lattices. While prior voxel assembly systems have focused on a cuboctahedron lattice (equivalent to a node-connected octet lattice), we instead use an edge connected octet lattice because it inherently creates stable alignment features within each unit cell of the lattice (see Fig.2 for reference). By pre-connecting these unit cells laterally, we create compounded lattice/voxel blocks that can then tile to form stable 1D, 2D, or 3D structures (see Fig.3 for example tilings). These compounded blocks are thus all built from the same 1×1×1 unit", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "8a3a06b1f23db876d29bf4b554adee0707e55c1f69af1a461aa13b010c7b06c7"}
{"doc_id": "arxiv:2510.13686#related-work:part-5", "url": "https://arxiv.org/abs/2510.13686", "anchor": "#related-work:part-5", "type": "paper", "title": "", "section": "Related Work", "text": "assembly systems have focused on a cuboctahedron lattice (equivalent to a node-connected octet lattice), we instead use an edge connected octet lattice because it inherently creates stable alignment features within each unit cell of the lattice (see Fig.2 for reference). By pre-connecting these unit cells laterally, we create compounded lattice/voxel blocks that can then tile to form stable 1D, 2D, or 3D structures (see Fig.3 for example tilings). These compounded blocks are thus all built from the same 1×1×1 unit voxel, but may vary in size and orientation (e.g., 2×2, 2×3, or 2×4), allowing a structure to mix different voxel types to optimize for geometry, performance, or assembly speed. The basic geometry of the lattice block is designed to self-align and constrain all but one degree of freedom when placed. In this work, the final degree of freedom is constrained with a releasable snap-fit (see Fig.6 for further detail). The self-aligning compounded voxel block offers a few key ad- vantages for robotic fabrication: 1) the alignment features permit placement error on the order of 1/2 the lattice pitch; 2) the installa- tion of new voxels is purely vertical; and 3) the snap fit connection does not require physical access to the connection plane to engage, but is still reversible. Together, these features substantially reduce the requirements on the robotic system. To successfully build up a structure, the robots only need to traverse over the existing lattice, carry new voxels, and coarsely place them. To this end, we devel- oped inchworm-style robotic arms that can crawl over the structure, extending on the paradigm used in e.g. [Jenett and Cheung 2017], [Park et al. 2023], or [Abdel-Rahman et al. 2022]. However, the use of compounded blocks introduces new chal- lenges to the design and path planning of these structures. To this end, we developed a new pipeline for intaking standard 3D meshes, voxelizing them, grouping the voxels into compounded blocks, and then simulating their robotic assembly. The simulation space is inte- grated with the hardware, so that it additionally acts as the central control/coordination for assembly. This workflow is shown in Fig.1. The software system is further discussed in Section 4, while the hardware system is discussed in Section 5, with assembly examples and system evaluation in Section 6. 4 Software Implementation A custom Web-based software environment was developed in JavaScript, enabling full control and simulation of the robotic assem- bly pipeline on any internet-connected device, without requiring local installation. Leveraging the Three.js visualization library, the system offers real-time 3D interaction through a synchronized digi- tal twin of the robot. The pipeline begins with voxelization of a 3D mesh and proceeds through build sequencing, path planning, and real-time visualization with robot feedback. 4.1 Voxelization The first step in the pipeline is the voxelization of the input geom- etry, any 3D mesh in STL format. The mesh is discretized using Figure 2: An overview of the lattice type and building blocks used in this project. The basic lattice type is an edge- connected octet lattice, which is decomposed into an ex- tended cuboctahedron-octet, which is", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "a4ca7f4d846549a2b2970cabc2a56658687c1279da16d14076dfd1bd833204cf"}
{"doc_id": "arxiv:2510.13686#related-work:part-6", "url": "https://arxiv.org/abs/2510.13686", "anchor": "#related-work:part-6", "type": "paper", "title": "", "section": "Related Work", "text": "voxelization of a 3D mesh and proceeds through build sequencing, path planning, and real-time visualization with robot feedback. 4.1 Voxelization The first step in the pipeline is the voxelization of the input geom- etry, any 3D mesh in STL format. The mesh is discretized using Figure 2: An overview of the lattice type and building blocks used in this project. The basic lattice type is an edge- connected octet lattice, which is decomposed into an ex- tended cuboctahedron-octet, which is then compounded into different arrangements for robotic assembly. the size of the unit voxel, resulting in a resolution of 65mm. Once voxelized, we analyze the grid to identify repeating patterns com- posed of multiple adjacent voxels. These patterns can vary in size and orientation and may be composed of simple or stacked config- urations. Some example patterns are detailed in Figure 2. To ensure connectivity and constructibility, the search prioritizes larger pat- terns first (e.g. 4x2x2 blocks) and proceeds hierarchically down to smaller units, filling in gaps as needed. Each detected pattern must be locally connected to at least one neighbor, either directly or via a base layer. 4.2 Building Sequence Given the location and type of each voxel in the structure, the next step is to define a build order that ensures feasibility while min- imizing future path planning complexity. The building sequence determines the order in which each voxel of the structure is assem- bled and is influenced primarily by two parameters: the number of robots and the location of their associated voxel Feed. We assume each robot is assigned a unique voxel Feeder from which it collects and places voxels. The structure is first partitioned by assigning each voxel to the closest source based on the Manhattan distance. In case of equidistant voxels (a tie), a simple alternating policy ensures balanced distribution among Feeds. Once assigned, the construction process is parallelized between robots. For each robot, a building sequence is computed indepen- dently. The structure is decomposed into horizontal layers, starting from the base and progressing upward. Within each layer, voxels are placed outward from the feed point, ensuring that the built region grows in a connected and stable manner. This strategy guarantees that every newly placed voxel rests on an existing one from the layer below, thereby creating a new accessible surface on the current layer. As a result, the robot can step onto this newly created platform to continue placing subsequent Hierarchical Discrete Lattice Assembly SCF ’25, November 20–21, 2025, Cambridge, MA, USA Figure 3: (Top) An example of tiling 2x2 offset voxel blocks to establish a first layer or overhang, with red arrows indicating the axes of potential extension. (Bottom) Beyond the first layer, layers can achieve interconnection through staggering layers. voxels. The building sequence algorithm enforces this outward growth, ensuring that at every stage, the next voxel to be placed is both structurally supported and physically reachable by the robot. 4.3 Path Planning Once the build sequence is established, each robot must au- tonomously navigate from its associated Feed location to its as- signed placement", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "66fc94f33d55c0bd25dd0a6579db7fcfca86950b790c24cfc8552cc4848e40ed"}
{"doc_id": "arxiv:2510.13686#related-work:part-7", "url": "https://arxiv.org/abs/2510.13686", "anchor": "#related-work:part-7", "type": "paper", "title": "", "section": "Related Work", "text": "first layer or overhang, with red arrows indicating the axes of potential extension. (Bottom) Beyond the first layer, layers can achieve interconnection through staggering layers. voxels. The building sequence algorithm enforces this outward growth, ensuring that at every stage, the next voxel to be placed is both structurally supported and physically reachable by the robot. 4.3 Path Planning Once the build sequence is established, each robot must au- tonomously navigate from its associated Feed location to its as- signed placement position, taking into account the current state of the partially build structure. To generate feasible and collision-free paths, we employ an A* algorithm adapted to the discrete 3D voxel space, which minimizes the function: ℎ(𝑛) = 𝑓(𝑛) + 𝑔(𝑛) (1) Where g(n) is the cost of moving from the start to voxel n and h(n) is a heuristic estimating the cost to reach the goal from n. We use the Manhattan distance as the heuristic, which performs efficiently in the voxel grid while preserving path optimality in our constrained setup. The A* algorithm is particularly powerful in a voxelized environ- ment, as the search space is inherently discrete, the connectivity is regular, and the heuristic directly reflects the grid geometry. By expanding nodes along the most promising directions first, it efficiently balances exploration and optimality. As a result, the al- gorithm consistently returns near-optimal paths with limited com- putation, even as the structure grows. Its effectiveness has already been validated in earlier iterations of this project [Abdel-Rahman et al. 2022; Smith et al. 2024]. 4.4 Robotic Assembly Simulation 4.4.1 Digital Twin for Live Simulation: To monitor and validate the robotic assembly process in real time, we developed a digital twin environment that mirrors the simulated robot’s actions on the physical hardware. Path planning outputs are executed in the simulation and simultaneously relayed to the robot via a Python middleware layer, using WebSocket communication over Wi-Fi. This architecture offers several key benefits: (1) Allows real-time updates during construction, enabling small-scale corrections in position or trajectory. (2) Provides live mapping of the robot’s state and the evolving structure. (3) Offloads computation entirely to the simulation side, reduc- ing the computational resources needed on the robot side. This tool is used within the pipeline to align the simulation with the hardware using the data flow presented in 4.4.2, with all ele- ments generated directly from the target structure. Nevertheless, it can also be used as a standalone application, allowing the user to manually place every element of the simulation (voxelized envi- ronment, robots, feeds, voxels to build) and subsequently link the configuration to the hardware. An example is presented in Figure 5. Figure 4: Simulation overview illustrating the four main ele- ments: MILAbot, support stairs, voxel feed, and target struc- ture. 4.4.2 Data Flow and Feedback: The system operates using a feed- forward architecture: voxelization, build sequencing, and path plan- ning are executed sequentially within the simulation environment. The resulting movement instructions are transmitted from the dig- ital twin to the physcial robot— the Modular Inchworm Lattice Assembler robot (MILAbot)— via a Python bridge, which", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "7559792b01c6bdc2c67dcafefc4b8d5d1f7701e0dde83a5234b4ad9ce7a9bd7e"}
{"doc_id": "arxiv:2510.13686#related-work:part-8", "url": "https://arxiv.org/abs/2510.13686", "anchor": "#related-work:part-8", "type": "paper", "title": "", "section": "Related Work", "text": "An example is presented in Figure 5. Figure 4: Simulation overview illustrating the four main ele- ments: MILAbot, support stairs, voxel feed, and target struc- ture. 4.4.2 Data Flow and Feedback: The system operates using a feed- forward architecture: voxelization, build sequencing, and path plan- ning are executed sequentially within the simulation environment. The resulting movement instructions are transmitted from the dig- ital twin to the physcial robot— the Modular Inchworm Lattice Assembler robot (MILAbot)— via a Python bridge, which converts high-level actions into low-level joint target values using Web- Socket communication. Simultaneously, the robot streams back its current joint states (see Figure 5). If a deviation from the ex- pected pose is detected, a realignment sequence is triggered. This involves returning to the last valid position and re-approaching the target slowly to ensure accurate alignment without requiring full re-planning. 5 Hardware Implementation In this section, we discuss the hardware implementation of the voxel assembly system, covering the voxel design, manufacturing, and performance, as well as the robotic system design and operation. SCF ’25, November 20–21, 2025, Cambridge, MA, USA Smith et al. Figure 5: Top: Data flow from web-based simulation to MI- LAbot through middleware. Bottom: Digital twin synchro- nized with physical execution. 5.1 Material System Our material system uses self-aligning compounded octet lattice blocks to create interlocking structures. In this work, we produce these lattice blocks in polylactic acid (PLA) through FFF 3D print- ing. We use FFF 3D printing because of its ability to easily handle complex geometries at a low cost and a reasonable speed. However, we envision that for future versions of this system, the compounded blocks might be further broken down into separate voxels, such as in [Gregg et al. 2018], or individual faces, such as in [Jenett et al. 2020], [Smith et al. 2024], or [Jenett et al. 2018], so that they can be mass-manufactured from higher performance materials, such as GFRP in [Jenett et al. 2020], CFRP in [Gregg et al. 2024] or aluminum in [Smith et al. 2025]. To improve the printability of the voxels, we add flanges to sup- port the large overhanging regions during printing, as done in [Leamon 2025]. The flanges enable clean print quality without the use of supports, enabling more efficient material usage as well as improved absolute mechanical properties of the voxel block. How- ever, the flanges are not intrinsically necessary, and it possible to print the lattice in its unmodified form. We printed the compounded voxels in different overall sizes depending on printer availabilty and print bed-size, and if necessary, later laterally pre-assemble them with printed snap clips. A representative 4x2x2 compounded block is shown in Fig.6. Voxel blocks are vertically connected using a screw-released snap fit connector. The snap fit consists of three parts: a set of pincers that by default sit too wide to engage with a voxel below it, a screw (in this case an M4 8 mm long socket head screw) to fasten it directly into the base of a voxel, and a screw hat, which acts as a spacer to push", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "d207abc340763d88dfe9af654fcf5d4cb87109f5712d2c0b0133744acecd8948"}
{"doc_id": "arxiv:2510.13686#related-work:part-9", "url": "https://arxiv.org/abs/2510.13686", "anchor": "#related-work:part-9", "type": "paper", "title": "", "section": "Related Work", "text": "snap clips. A representative 4x2x2 compounded block is shown in Fig.6. Voxel blocks are vertically connected using a screw-released snap fit connector. The snap fit consists of three parts: a set of pincers that by default sit too wide to engage with a voxel below it, a screw (in this case an M4 8 mm long socket head screw) to fasten it directly into the base of a voxel, and a screw hat, which acts as a spacer to push the legs of snap pincers out, so that they can engage with the voxels below (these are shown in Fig.6). The snap fit connectors are installed into the lowest octets of the printed voxels. The act of fastening the M4 screw then forces the pincers into the correct state, such that the voxel block can snap into one below it. To uninstall the connector, the M4 screw can be removed, which released the snap fit, allowing the block to removed. Though this is currently done manually, future versions of the robotic system could include an end effector for unscrewing the connectors. Figure 6: (Top) A 4x2x2 block of FFF printed PLA octet lattice with printability features added. (Bottom) Screw-release snap fit used for vertical connections. So that the voxels sit on a flat surface, we additionally produced base-layer voxel blocks that consist of the upper half of the standard block. These are added onto the protruding octets to create a flat base layer without adding extra height to the system. 5.2 Assembly Robots We introduce the Modular Inchworm Lattice Assembler robot, or MILAbot. As in prior voxel assembly robot systems, such as [Smith et al. 2024], [Park et al. 2023], [Jenett et al. 2019], the primary objective of the MILAbot design is to enable scalable voxel assembly with a system that can locomote over and assemble load-bearing discrete lattice structures. Looking toward scaling to the assembly of larger structures, the MILAbot is designed to increase assembly throughput while minimizing overall system cost and complexity. Additionally, the MILAbot is designed with eventual self-assembly in mind, that is, a starter MILAbot should be able to assemble more MILAbots, to autonomously improve efficiency as described in [Abdel-Rahman et al. 2022] and [Smith 2023]. The MILAbot is made from four primary module types: actuated joints, passive spacers, gripper feet, and voxel carriers as showm in Fig.7. These modules are arranged in a configuration similar to a five degree of freedom robot arm, with grippers on both ends, and three voxel carriers on one side. Toward eventual robotic-self as- sembly, the modules are capped by incoming- and outgoing- PCBs on either side that use a heating circuit to reflow low-melt solder between modules, forming a simultaneous eletrical and mechanical connection, such as in [Smith et al. 2024], though this is not a focus of this work. The robot modules each have their own microcon- troller, which are all networked over an I2C bus [NXP 2021], with a Hierarchical Discrete Lattice Assembly SCF ’25, November 20–21, 2025, Cambridge, MA, USA Figure 7: The Modular Inchworm Lattice Assembler robot", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "564995d7b220a35198b183cbb478d53722a8ae7276631216fccde51bdd459d5b"}
{"doc_id": "arxiv:2510.13686#related-work:part-10", "url": "https://arxiv.org/abs/2510.13686", "anchor": "#related-work:part-10", "type": "paper", "title": "", "section": "Related Work", "text": "outgoing- PCBs on either side that use a heating circuit to reflow low-melt solder between modules, forming a simultaneous eletrical and mechanical connection, such as in [Smith et al. 2024], though this is not a focus of this work. The robot modules each have their own microcon- troller, which are all networked over an I2C bus [NXP 2021], with a Hierarchical Discrete Lattice Assembly SCF ’25, November 20–21, 2025, Cambridge, MA, USA Figure 7: The Modular Inchworm Lattice Assembler robot (MILAbot) consists of four primary module types: actuated joints, passive spacers, gripper feet, and voxel carriers. WiFi-enabled microcontroller acting as the primary microcontroller on the bus. We typically run the robot at 14V, but the electronics are designed to handle up to 24V. Figure 8: (Top) A cutaway view of the motor module design, which is based on a BLDC motor with two stages of reduction, feedback on the output shaft, and a voxel frame to enable module manipulation by another robot. (Bottom) A complete motor module, with incoming and outgoing PCB attachment plates. The actuator modules are built around a GM3506 gimbal motor, a small brushless DC motor. We use a two stage 3D printed reduc- tion consisting of a 1:36 split ring planetary followed by a 1:4 spur gear reduction, which has the added benefit of moving the output shaft off from the motor shaft, allowing us to more easily track the position of the output shaft. The total reduction is then 1:144. The module is fabricated from a mix of 3D printed polycarbonate, for its temperature resistance, and aluminum plates. The module controller board uses an Adafruit M4 Express (SAMD51) as the micronctroller, a DRV8316 for motor control, and two AS5407 mag- netic encoders for motor and output shaft feedback. Motor control is done using SimpleFOC [Skuric et al. 2022]. Each module weighs approximately 400g and can output approximately 10-15 Nm before failure. Figure 9: The operation of both gripper types in the MILAbot. (Top) The structure gripper, (bottom) the voxel payload grip- per(s). The MILAbot uses two voxel gripper types. The first type adheres the robot to the structure, and the second type carries a voxel payload. The operation of both types is shown in Fig.9. The gripper feet are designed to step in the middle of a 2x2x1 grid of voxels, which introduces some constraints on where the robot can locomote. Its relatively larger size affords the robot greater stability and helps to distribute the load of the robot’s locomotion over a larger area on the voxels. Each gripper foot is actuated using a FeeTech FS117 hobby servo, which retracts and extends a set of aluminum gripping features underneath the upper corner nodes of some voxels. Because the voxel carrier grippers do not need to support a signif- icant load, they are designed to be smaller and lighter, using Miuzei MG90s micro servos to open and close a petal feature, modeled after the grippers used in [Jenett et al. 2019] and [Abdel-Rahman et al. 2022]. To maximize the carrying capacity of the robot without significantly impacting its", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "4e5469f591b6113269299314f037a86d0468e87615ee82f60ce6200a0d95a787"}
{"doc_id": "arxiv:2510.13686#related-work:part-11", "url": "https://arxiv.org/abs/2510.13686", "anchor": "#related-work:part-11", "type": "paper", "title": "", "section": "Related Work", "text": "servo, which retracts and extends a set of aluminum gripping features underneath the upper corner nodes of some voxels. Because the voxel carrier grippers do not need to support a signif- icant load, they are designed to be smaller and lighter, using Miuzei MG90s micro servos to open and close a petal feature, modeled after the grippers used in [Jenett et al. 2019] and [Abdel-Rahman et al. 2022]. To maximize the carrying capacity of the robot without significantly impacting its maneuverability, we use three per robot installed on one leg (though the robot could physically support installing an additional three voxel carriers on the other leg, this would cause significant collision issues for any non-flat structure). Both grippers are designed with large alignment features. In the case of the gripper feet, these match the alignment features of the voxel blocks themselves, while the voxel carriers use a smaller petal shape. The combination of these mechanical alignment features in the robot and in the underlying structure means that we are able to achieve relatively precise robot and voxel placement without SCF ’25, November 20–21, 2025, Cambridge, MA, USA Smith et al. Figure 10: Picking up and passing a voxel block from one robot to another. high-performance robots (i.e. the robots have minimal feedback and a lot of compliance). An example of this is shown in Fig.10, where two robots pass a voxel block to each other. The two robots approach the target voxel (gray), and one steps onto it and lifts to remove it (the voxel is not constrained with snaps in this case). The other robot then steps forward, re-indexing its position on the lattice. Because the voxel carriers are placed on the robot at a multiple of the lattice pitch, this places the second robot at a \"known\" location for the robot carrying the other voxel, which only needs to then move into that position, with the alignment petals proving fine alignment. The second robot then grips the voxel while the first lets go and retracts. The robot relies on the ability of the voxels to correct large placement errors to install new blocks simply by dropping them into place and then stepping on them to engage the snap fits. This process is shown in Fig. 11, where a robot carrying two voxels steps into position, swings the back leg forward and rotates it to place the correct voxel in the appropriate pre-placement position, drops the voxel, returns the back leg backwards, steps forward once, and then stomps on the voxel to engage it, forming a sturdy connection. 6", "source": "arxiv_pdf", "published": "", "tokens": 430, "sha256": "d5db522186ba7ffe9d23e1c6c3fb1befa178ca8fb3ae70fd04c7343c8bfcba71"}
{"doc_id": "arxiv:2510.13686#evaluation:part-1", "url": "https://arxiv.org/abs/2510.13686", "anchor": "#evaluation:part-1", "type": "paper", "title": "", "section": "Evaluation", "text": "In this section, we evaluate the performance of the presented system through a set of assembly demonstrations, the throughput and fidelity of the mechanical system, the mechanical efficiency of the lattice type, and general robotic performance metrics. 6.1 Assembly Demonstrations We use the complete workflow to demonstrate robotic assembly of basic structures using one and two robots. 6.1.1 Single Robot Assembly. First, we use a single robot to assem- ble a 4x4x4 voxel block. We voxelize the structure into four 4x2x2 Figure 11: An example voxel installation sequence. 1) The robot is in pre-placement position. 2) The robot steps into the placement position and 3) drops the voxel roughly into place, before 4) retracting, and then 5) taking a step forward. 6) The robot stomps the new voxel to engage the snap fit connection. blocks, the largest compounded size our system currently accom- modates. This way, the robot only needs make four placements, in contrast to prior approaches to voxel assembly, which would have required 64 voxel placements for the equivalent structure. A condensed version of the assembly sequence is shown in Fig. 12. The robot carries two blocks at once, as the minimum amount of trips it can make for this structure is two (either two voxels twice, or three voxels and one voxel once). The first two voxels are placed oriented toward the camera plane, while the second two are placed parallel to it, resulting in a fully connected structure. Next, each robot assembles its support staircase for the bench example, as previously shown in Fig.1 and Fig.4. Freeze frames from the build sequence are shown in Fig.13. The build process for the stair case is similar to that of the cube, but at the end, we Hierarchical Discrete Lattice Assembly SCF ’25, November 20–21, 2025, Cambridge, MA, USA Figure 12: Assembly of a 4x4x4 voxel block using a set of four compounded blocks, carried two at a time. demonstrate a small overhang with this structure that is able to support the weight of the assembler robot on it. Overhangs, or first layers, both require staggered voxel blocks to achieve. This can be done either by layer shifting a 2-voxel tall stack, or by attaching base-plate voxels at a stagger, as done here. The offset provides alignment features for the next block placement and braces the newly installed voxel against the existing structure, especially during the drop-to-place procedure. 6.1.2 Double Robot Assembly. Because each robot has its own voxel feed in our workflow, most multi-robot structure assembly is identical to single robot assembly, up until the point at which the structures meet. The joining of separate build fronts, or unlinked structures, is not something considered by any of the prior voxel assembly systems, but is likely a critical feature for scaling these systems up. In this example, the two robots finish the assembly of the bench by filling in the base of the seat (see Fig.14 for reference). One robot drops a voxel block into the empty space, and then steps on it to align it and the structure. The", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "eeb1139ff61d6edc80f51e5502086ec2ccaa751d1655e5c8dc2902299dc697dc"}
{"doc_id": "arxiv:2510.13686#evaluation:part-2", "url": "https://arxiv.org/abs/2510.13686", "anchor": "#evaluation:part-2", "type": "paper", "title": "", "section": "Evaluation", "text": "structures meet. The joining of separate build fronts, or unlinked structures, is not something considered by any of the prior voxel assembly systems, but is likely a critical feature for scaling these systems up. In this example, the two robots finish the assembly of the bench by filling in the base of the seat (see Fig.14 for reference). One robot drops a voxel block into the empty space, and then steps on it to align it and the structure. The second robot then installs its voxel into the remaining space. This is possible because the alignment features on the voxels are permissive of a fairly large amount of error, the error is larger than the voxels, and the structure Figure 13: A single robot assembling a support staircase for the bench build. is able to slightly slide and deform. We cannot always guarantee that this will be the case, so for joining build fronts, it is likely necessary to consider alternative voxel types, such as ones that are slightly undersized or compliant, to account for the potential misfit. However, even though this is not a reliable method for joining a voxel seam, this block placement is taken to be infeasible in the prior planning systems for voxel assembly [Gregg et al. 2024], [Smith et al. 2024], [Jenett et al. 2019]. After installing the base of the bench, the two robots install the back of the seat. These two voxels are necessarily installed simultaneously, as to avoid collisions in the placement. Though for a small structure with only two robots, keeping synchronicity (or restoring it if lost) is not difficult, as the system scales to larger robot counts, it may be necessary to consider additional methods for synchronizing the movements of all robots at critical junctures, without needing to run the entire system at the pace of the slowest robot. Or, the methodology for voxel placement could be revisited, to consider more sophisticated obstacle avoidance, depending on the build state of the structure. Once completed, the bench is removed from the support stair- case, where the path planning has left a seam. The completed bench is then directly usable, as the voxels have decent mechanical perfor- mance, which is further discussed in Section 6.3. We qualitatively evaluated the bench by having two people sit on it, as shown in Fig.15. The bench supports this weight, though the overall design is a bit small for comfort— luckily, the voxels support disassembly and reassembly, so we can change the design. 6.2 Throughput and Fidelity As shown in Figure 16, we assess the performance of different vox- elization strategies by analyzing the trade-off between the number of voxels that make up the structure and geometric precision across various mesh types. The geometric precision can be defined as: 𝑝𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛= 𝑛𝑣𝑜𝑥𝑒𝑙∗𝑉𝑣𝑜𝑥𝑒𝑙/𝑉𝑚𝑒𝑠ℎ (2) Where 𝑛𝑣𝑜𝑥𝑒𝑙is the number of unit voxel with 𝑉𝑣𝑜𝑥𝑒𝑙being the volume of one unit voxel and 𝑉𝑚𝑒𝑠ℎthe volume of the input mesh. SCF ’25, November 20–21, 2025, Cambridge, MA, USA Smith et al. Figure 14: Two robots assembling a bench together. Figure 15: The assembled bench", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "d3f3f2d686effffc55ea8175137232cd6e5193c9905066339cee26d00f711701"}
{"doc_id": "arxiv:2510.13686#evaluation:part-3", "url": "https://arxiv.org/abs/2510.13686", "anchor": "#evaluation:part-3", "type": "paper", "title": "", "section": "Evaluation", "text": "vox- elization strategies by analyzing the trade-off between the number of voxels that make up the structure and geometric precision across various mesh types. The geometric precision can be defined as: 𝑝𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛= 𝑛𝑣𝑜𝑥𝑒𝑙∗𝑉𝑣𝑜𝑥𝑒𝑙/𝑉𝑚𝑒𝑠ℎ (2) Where 𝑛𝑣𝑜𝑥𝑒𝑙is the number of unit voxel with 𝑉𝑣𝑜𝑥𝑒𝑙being the volume of one unit voxel and 𝑉𝑚𝑒𝑠ℎthe volume of the input mesh. SCF ’25, November 20–21, 2025, Cambridge, MA, USA Smith et al. Figure 14: Two robots assembling a bench together. Figure 15: The assembled bench supports two people. The left panel shows how larger voxel patterns drastically reduce the number of elements required but come with a loss in precision. The right panel highlights this trade-off by plotting precision as a function of voxel count. Patterns like 2x3x1 and 2x2x1 achieve a good balance, while the hierarchical combination offers a com- pelling middle ground with high precision and low voxel count. Figure 17 highlights two key components to accelerate construc- tion: increasing the number of robots and improving the voxel carrying strategy. On the top row, simulation snapshots show how multiple robots parallelize the construction of a 16x16x16 cube, reducing total build time. While the benefit is clear, the speed-up Figure 16: Evaluating voxel pattern efficiency— hierarchical combination outperforms any single voxel type by breaking the linear trade-off between voxel size and precision. (Top: Voxel count and precision decrease linearly with increasing voxel size when voxelizing meter-scale meshes. Bottom: Pre- cision as a function of voxel count, highlighting the benefits of the hierarchical combination.) Hierarchical Discrete Lattice Assembly SCF ’25, November 20–21, 2025, Cambridge, MA, USA Figure 17: Impact of robot quantity and voxel carrying strategy on build time. (Top) Simulation snapshots of cubes of decreasing size (16x16x16, 8x8x8, and 4x4x4 voxels). (Bottom Left) Estimated build time as a function of cube size and number of robots. (Bottom Right) Time required to build the same structure depending on the number or size of voxels carried per trip. These graphs motivate the use of a larger robot fleet for assembling large structures, as they show a substantial decrease in assembly time. They also highlight the importance of maximizing robot payload, with each robot ideally carrying three 4×4×2 voxels rather than operating below full capacity. is not exactly proportional to the robot quantity due to each ro- bot building its own support stairs independently, without sharing intermediate infrastructure. The graph on the bottom left shows that construction time in- creases exponentially with structure size, but adding robots sig- nificantly flattens the curve. On the bottom right, we analyze the impact of voxel carrying capacity. Carrying multiple small voxels per trip already provides notable gains, but the greatest efficiency comes from transporting larger compound blocks (e.g., 4x2x2). This confirms that both scaling the number of robots and optimizing carried voxel configuration are critical for improving overall build throughput. 6.3 Hardware Performance Because other robotically assembled discrete lattices have been demonstrated, we can compare the performance of our system against those in literature. First, we consider the mechanical per- formance of the modified octet lattice we use. In evaluating this performance,", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "8990b271201d6784594e5f0fe359d6a5020cb8f609c50a0c1902b77751842b27"}
{"doc_id": "arxiv:2510.13686#evaluation:part-4", "url": "https://arxiv.org/abs/2510.13686", "anchor": "#evaluation:part-4", "type": "paper", "title": "", "section": "Evaluation", "text": "trip already provides notable gains, but the greatest efficiency comes from transporting larger compound blocks (e.g., 4x2x2). This confirms that both scaling the number of robots and optimizing carried voxel configuration are critical for improving overall build throughput. 6.3 Hardware Performance Because other robotically assembled discrete lattices have been demonstrated, we can compare the performance of our system against those in literature. First, we consider the mechanical per- formance of the modified octet lattice we use. In evaluating this performance, we consider the version of the lattice presented here to be a proof-of-concept demonstration, and the comparisons be- low to indicate, generally if the design decisions we have made are in desirable directions. Because of the anisotropy in FFF printed parts, the robustness of our voxel system can be low, and we do not perform a full mechanical characterization of the voxels under tensile and bending loads. Further, the overall interplay between vertically interconnected blocks under various loads is also still unknown, and beyond the scope of the current work. To determine the behavior of the lattice under compressive loads, we tested a single unit cell, a 2x2x2 lattice block, and a 3x3x3 lattice block on an Instron 4411 with a 5kN load cell for the single unit at the 2x2x2, and on an Instron 5985 with a 250kN load cell for the 3x3x3 at a compression rate of 10 mm/min. The results of this testing are summarized in Table 1. The lattices demonstrate good strength and stiffness at a reasonable weight (the average density of the lattice is 81.85 kg/m3). Under compression, the lattices can support very high loads— the 2x2x2 block reached a maximum load of 3445N, which is about 2,220x its own weight, while the 3x3x3 block reached a maximum load of 8712N, which is about the load exerted by the weight of an average cow. We can determine the mechanical efficiency of the lattice by looking at how its compressive modulus and density scale relative to the bulk material it is made from. The relative compressive modulus is given by 𝐸/𝐸𝑠where 𝐸is the compressive modulus of the lattice and 𝐸𝑠is the compressive modulus of the bulk material. Similarly, relative density is given as 𝜌/𝜌𝑠, where 𝜌is the density of the lattice and 𝜌𝑠is the density of the bulk material. Ideal stretch dominated SCF ’25, November 20–21, 2025, Cambridge, MA, USA Smith et al. behavior, that is, the most efficient scaling of modulus with density, is given by linear scaling 𝐸/𝜌. However, this is generally considered inaccessible [Schaedler and Carter 2016]. Instead, ideal bending dominated behavior is given by quadratic scaling 𝐸1/2/𝜌, while below 𝐸1/3/𝜌is accessible via foams. So, the target for the structural efficiency of an architected lattice is to surpass the quadratic scaling for its bulk material. We plot a comparison of the relative compressive modulus and relative density for this work, as well as the other discrete lattice as- sembly systems: [Gregg et al. 2024], [Smith et al. 2024], and [Jenett et al. 2019]. Both this work and [Gregg et al. 2024] achieve stretch dominated behavior,", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "c57763868f16ae88c7bb7a45ff20b29be1caf21e7158d44a9b71af791249a9a1"}
{"doc_id": "arxiv:2510.13686#evaluation:part-5", "url": "https://arxiv.org/abs/2510.13686", "anchor": "#evaluation:part-5", "type": "paper", "title": "", "section": "Evaluation", "text": "quadratic scaling 𝐸1/2/𝜌, while below 𝐸1/3/𝜌is accessible via foams. So, the target for the structural efficiency of an architected lattice is to surpass the quadratic scaling for its bulk material. We plot a comparison of the relative compressive modulus and relative density for this work, as well as the other discrete lattice as- sembly systems: [Gregg et al. 2024], [Smith et al. 2024], and [Jenett et al. 2019]. Both this work and [Gregg et al. 2024] achieve stretch dominated behavior, indicating efficient and desirable material us- age. Note that [Jenett et al. 2019] does not provide mechanical testing data; this is instead estimated using an ideal beam simu- lation of the presented lattice using the reported parameters and materials. Additionally [Jenett et al. 2019] uses a non-structural magnetic joint, but for this comparison we disregard that. We can additionally compare the assembly throughput and sys- tem cost for these systems. We use the reported volumetric assem- bly throughput for each system, with the exception of [Gregg et al. 2024], in which their reported throughput is for a hundred voxel structure, and so is an underestimate of the speed of the system as compared to the systems in [Jenett et al. 2019] and [Smith et al. 2024] which only demonstrate assembly of tens of voxels. Instead, we draw an average from the installation time per voxel for the first 15 voxels in their structure, to better align with the data we have here, resulting in a much higher throughput estimate than what they report. We find that our volumetric assembly throughput is 4,394,000 mm3/min for a single robot carrying two blocks of 4x2x2 voxels, as compared to 2,700,000 mm3/min for [Gregg et al. 2024], 751,879 mm3/min for [Jenett et al. 2019], and 343,281 mm3/min for [Smith et al. 2024]. Our higher volumetric throughput is owed to two main factors: we can carry multiple blocks of compounded voxels while moving at a comparable speed to [Jenett et al. 2019], and unlike [Gregg et al. 2024] the current implementation requires minimal installation/fastening time. This result indicates that we likely have budget to increase the time spent per voxel (such as by implementing a different connection system) while still staying competitive in volumetric throughput. We additionally compare these results against the cost of each robotic system. For [Smith et al. 2024] and [Jenett et al. 2019] these values are taken from their reported bills of materials, while for [Gregg et al. 2024] we estimate the cost based on the components reported in [Park et al. 2023], where the current figure is based on doubling the cost of the actuation system for one inchworm robot, to account for the need for both a carrying robot and an installing robot, with an additional $300 added as a low-end estimate for the cost of the third robot, as well as all of non-actuation components in the inchworm robots. Interestingly, there is a relatively linear relationship between system cost and assembly throughput for the prior single-voxel based systems, which this work sidesteps via implementing hierarchical material handling in an otherwise", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "7c26833c285ad064a1b6823438d044b4a0d23fe4d0e491975f3d332a66845782"}
{"doc_id": "arxiv:2510.13686#evaluation:part-6", "url": "https://arxiv.org/abs/2510.13686", "anchor": "#evaluation:part-6", "type": "paper", "title": "", "section": "Evaluation", "text": "of the actuation system for one inchworm robot, to account for the need for both a carrying robot and an installing robot, with an additional $300 added as a low-end estimate for the cost of the third robot, as well as all of non-actuation components in the inchworm robots. Interestingly, there is a relatively linear relationship between system cost and assembly throughput for the prior single-voxel based systems, which this work sidesteps via implementing hierarchical material handling in an otherwise very simple robot. Table 1: Summary of mechanical properties for different voxel block sizes 1x1x1 2x2x2 3x3x3 Stiffness [N/mm] 949 2278 4868 Maximum load [N] 602 3445 8712 Compressive Modulus [MPa] 14.6 17.5 24.9 Figure 18: A comparison of the relative compressive modulus (voxel modulus divided by bulk material modulus) and the relative density (voxel density divided by bulk density) for robotically assembled discrete lattices. This work achieves stretch dominated behavior, demonstrating material effi- ciency relative prior robotically assembled voxel systems. 7 Future Work and Limitations 7.1 Voxelization While our voxelization algorithm proved sufficient to demonstrate the feasibility of a complete robotic assembly pipeline across a defined set of structures, it remains limited in its generalizability and is sensitive to variations in input geometry. In future work, we plan to explore more robust and generalizable approaches. This includes developing enhanced rule-based algorithms inspired by an octree subdivision approach as developed in [Abdel-Rahman et al. 2022], which remain fully discrete and deterministic, as well as investigating learning-based methods, such as those proposed in [Pun et al. 2025], that leverage the power of large language model to Generate physically stable and buildable LEGO® structures. In- corporating a structure-aware strategy that accounts for physical constraints and evolving geometry could significantly improve the reliability and feasibility of voxel placement over time. And would extend the buildable structure to the one presented in Figure 20. Future work will also explore generating input geometries based on desired performance, such by integrating the system in [Kyaw Hierarchical Discrete Lattice Assembly SCF ’25, November 20–21, 2025, Cambridge, MA, USA Figure 19: A comparison of the volumetric throughput for different voxel assembly systems for low voxel count struc- tures, against the cost of the minimal robotic assembly unit. This work achieves the highest throughput at the second lowest cost. et al. 2025], or verifying structural performance, such as through built in FEA, as in [Smith et al. 2025]. Figure 20: Comparison of infeasible (left) and feasible (right) structures due to local connectivity constraints. 7.2 Feeder Management As discussed in Section 4.2, in the current setup, each robot is paired with a unique Feed location, allowing independent operation. This constraint simplifies coordination, but limits scalability and task flexibility. Future work will explore decoupling the number of robots from the number of sources, enabling more dynamic task allocation. Related work [Abdel-Rahman et al. 2022] has demonstrated that shared and flexible sourcing strategies can significantly improve resource utilization and system throughput. Additionally, in the current system, we manually feed voxels to the robot. In a fully automated set up, this would be accomplished using", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "5d34bee58d79a0898f68cb5fece60e750215e08f77014825cf7fd06e8fe8e743"}
{"doc_id": "arxiv:2510.13686#evaluation:part-7", "url": "https://arxiv.org/abs/2510.13686", "anchor": "#evaluation:part-7", "type": "paper", "title": "", "section": "Evaluation", "text": "unique Feed location, allowing independent operation. This constraint simplifies coordination, but limits scalability and task flexibility. Future work will explore decoupling the number of robots from the number of sources, enabling more dynamic task allocation. Related work [Abdel-Rahman et al. 2022] has demonstrated that shared and flexible sourcing strategies can significantly improve resource utilization and system throughput. Additionally, in the current system, we manually feed voxels to the robot. In a fully automated set up, this would be accomplished using e.g. conveyor belt to deliver voxels from their manufactured or stored location to the robots. 7.3 Robot Control As the system scales to multiple robots, hardware variability is expected to introduce differences in kinematics and precision. To ensure consistent behavior across platforms, we plan to develop a more adaptable control system with integrated per-robot calibra- tion. This includes implementing a tunable inverse kinematics (IK) model, allowing each robot to compensate for mechanical devia- tions and maintain precise control during assembly. Additionally, to fully realize voxel reconfiguration, the robots will need to eventually be able to disassemble voxel structures, which will require both additions to the robotic hardware as well as the path-planning tool. Eventually, we hope to be able to fully support the arbitrary disassembly and reconfiguration of a voxel structure using this system. 7.4 Voxel Material We use FFF printed voxel blocks in this work primarily because they are readily accessible and offer high performance along some loading directions. However, the anisotropies inherent in FFF plastic printing likely preclude its use for many practical applications, so future work will focus on developing alternate methods for pre- building the voxel blocks, such as through injection molded faces assembled together [Jenett et al. 2020] or formed metallic facets [Rubio et al. 2023]. Additionally, [Smith et al. 2025], [Cameron et al. 2022], and [Wang et al. 2023] present electrically active voxel systems, which could be integrated with this system in the future. Additionally, the system is likely mechanically anisotropic, and in future work we plan to develop additional tools simulated structure behavior, as well as more fully characterize the material system under bending and tensile loads. 8", "source": "arxiv_pdf", "published": "", "tokens": 355, "sha256": "2fae55286a035df4841565786048da204bd72b653c94560b5f0c7a40a6544632"}
{"doc_id": "arxiv:2510.13686#conclusion", "url": "https://arxiv.org/abs/2510.13686", "anchor": "#conclusion", "type": "paper", "title": "", "section": "Conclusion", "text": "This work demonstrates a novel approach to scalable digital fabrica- tion through the integration of architected lattice blocks, modular mobile robots, and a live digital twin system for coordinated assem- bly. By leveraging the strengths of small-scale digital fabrication to produce complex interlocking components, and combining them with simple, mobile robots capable of relative traversal and block placement, we enable the construction of meter-scale structures with reduced system complexity and cost. The hierarchical vox- elization strategy, modular assembly robot design, and real-time simulation interface collectively contribute to a flexible and scalable assembly pipeline. Our validation of the system through the success- ful fabrication of meter-scale structures highlights its potential for broader application, such as for architecture, infrastructure, or in- space assembly and manufacturing. This work establishes ground- work for further exploring multi-robot collaboration in hardware, the interplay between structural robotic systems, and, eventually, hierarchical and recursive robotic assembly. Acknowledgments This work was supported by MIT Center for Bits and Atoms Con- sortia funding.", "source": "arxiv_pdf", "published": "", "tokens": 163, "sha256": "4bd1730284863882e8a9bba139ffeb2247c616a4cb91b9e294eff6fb3fad516a"}
{"doc_id": "arxiv:2510.13108#method:part-1", "url": "https://arxiv.org/abs/2510.13108", "anchor": "#method:part-1", "type": "paper", "title": "", "section": "Method", "text": "Fine-tuning Accuracy EPDMS [13] ✗ 0.414 OpenAI-o3 (zero-shot) [20] ✗ 0.533 GPT-5 (zero-shot) [19] ✗ 0.552 Qwen2.5-VL-7B (zero-shot) ✗ 0.480 Supervised Pairwise Classifier ✓ 0.648 DriveCritic (ours) ✓ 0.760 B. Overall Comparison We first evaluate all methods on the DriveCritic dataset, using the verified test split as described in Sec. IV-A.2. The primary evaluation metric is accuracy, defined as the proportion of pairwise comparisons in which the model’s judgment agrees with the human-preferred trajectory. 1) Baselines: We compare DriveCritic against a wide range of baselines covering rule-based metrics, general- purpose LLMs, and controlled supervised models: Rule-based: EPDMS [13] serves as the SOTA rule-based benchmark. Since EPDMS outputs a scalar score per trajec- tory, we select the higher-scoring trajectory as its preference. General VLMs: We evaluate SOTA closed-source (OpenAI- o3 [20] and GPT-5 [19]) and open-source (Qwen2.5-VL- 7B [22]) VLMs under the same evaluation prompt as Drive- Critic. This baseline captures the out-of-the-box reasoning ability of frontier VLMs without domain-specific fine-tuning. Supervised Pairwise Classifier: We implement a supervised pairwise classifier as a data-driven baseline that does not rely on VLMs. The model employs ResNet-101 [48] encoders for stitched camera images and BEV maps with overlaid candidate trajectories, concatenated with feature encodings of the ego status and EPDMS sub-scores through an MLP-based fusion layer. The classifier is trained on the train split of the DriveCritic dataset with cross-entropy loss for 20 epochs, and results are reported from the best checkpoint. This TABLE V: Ablation on the DriveCritic training recipe. Checkmarks (✓) indicate enabled components. ‘Acc.’ under Rewards denotes an accuracy- based reward. Final column reports accuracy on the DriveCritic test set. ID SFT RL Rewards Accuracy GRPO DAPO Format Acc. A ✗ ✗ ✗ ✗ ✗ 0.480 B ✗ ✓ ✗ ✓ ✓ 0.464 C ✓ ✗ ✗ ✗ ✗ 0.645 D ✓ ✓ ✗ ✗ ✓ 0.739 E ✓ ✓ ✗ ✓ ✓ 0.750 F ✓ ✗ ✓ ✓ ✓ 0.760 ID legend: A = base Qwen2.5-VL-7B (zero-shot); B = GRPO only (format + accuracy rewards); C = SFT only; D = SFT + GRPO (accuracy reward); E = SFT + GRPO (format + accuracy rewards); F = SFT + DAPO (format + accuracy rewards). baseline provides a learning-based alternative, highlighting the benefits of a VLM backbone in DriveCritic. 2) Results: Table IV reports the overall accuracy of all baselines and DriveCritic on the DriveCritic test set. The rule-based EPDMS metric performs the weakest, reflecting the pressing need to improve the context awareness in rule-based driving metrics. General-purpose VLMs (GPT-5, OpenAI-o3, Qwen2.5-VL-7B) demonstrate stronger contex- tual awareness but remain less reliable than the proposed method. The Supervised Pairwise Classifier achieves higher accuracy than zero-shot VLMs, demonstrating that fine- tuning can help with aligning a model towards human prefer- ences. DriveCritic outperforms all baselines by a significant margin, reaching 76.0% accuracy, validating the effectiveness of the DriveCritic model and the proposed training paradigm. C. Qualitative Results In Fig. 3, we show two qualitative examples where correct context understanding leads to aligning to the ground truth in the DriveCritic dataset. These examples show that fixed", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "7a2143f75a8ae5dfde0f1031047e715a539756bcce36b2bb60276bc26a3254b9"}
{"doc_id": "arxiv:2510.13108#method:part-2", "url": "https://arxiv.org/abs/2510.13108", "anchor": "#method:part-2", "type": "paper", "title": "", "section": "Method", "text": "proposed method. The Supervised Pairwise Classifier achieves higher accuracy than zero-shot VLMs, demonstrating that fine- tuning can help with aligning a model towards human prefer- ences. DriveCritic outperforms all baselines by a significant margin, reaching 76.0% accuracy, validating the effectiveness of the DriveCritic model and the proposed training paradigm. C. Qualitative Results In Fig. 3, we show two qualitative examples where correct context understanding leads to aligning to the ground truth in the DriveCritic dataset. These examples show that fixed thresholds alone (e.g., lane offset, progress) can mis-rank trajectories in nuanced settings, while context-aware reason- TABLE VI: Robustness under trajectory-position flip on the DriveCritic test set. “No-flip acc.” and “flip acc.” are the standard accuracies before and after swapping the trajectory order. RR denotes robustness rate as defined above.", "source": "arxiv_pdf", "published": "", "tokens": 129, "sha256": "2a82ca31e1c505f821a07fc342fc84c6e64a6b1a8a60e050ddacedc5ff2ac4c6"}
{"doc_id": "arxiv:2510.13108#model:part-1", "url": "https://arxiv.org/abs/2510.13108", "anchor": "#model:part-1", "type": "paper", "title": "", "section": "Model", "text": "No-flip acc. Flip acc. RR (%) Supervised Pairwise Classifier 0.648 0.613 55.8 Qwen2.5-VL-7B (base) 0.480 0.487 74.9 Qwen2.5-VL-7B + SFT 0.645 0.649 78.0 DriveCritic (ours) 0.760 0.765 81.8 ing model (DriveCritic) can be used to complement rule- based evaluation methods in these challenging scenarios. We include more qualitative results in the supplementary video. D. Ablation Studies We further conduct an ablation study to break down the contributions of components in the DriveCritic model. We note that only applying RL fine-tuning (B) could reduce the accuracy, highlighting the need of the SFT training (C) to warm up the model’s ability. Building on SFT, all RL variants (D–F) yield clear gains, with the full DriveCritic recipe (F, SFT + DAPO [24] + format and accuracy rewards) achieving the best test accuracy on the DriveCritic dataset. E. Robustness Analysis An important requirement for a learning-based driving evaluator is to produce consistent judgments regardless of input ordering or formatting, a concern also raised in re- cent studies on LLM/VLM judges [36], [37]. To quantify robustness, we perform a position-flip test: for every test pair, we swap the order of Trajectory A and Trajectory B in the prompt and re-evaluate the model. Let yi be the original prediction and ˆyi the prediction after flipping. We follow [36] to compute the Robustness Rate (RR): RR = 1 |D| |D| X i=1 I \u0002 yi = ˆyi\u0003 , where |D| is the size of the DriveCritic test set and I[·] is the indicator function. We also report the standard accu- racy before (no-flip) and after (flip) swapping. As shown in Table VI, DriveCritic achieves the highest robustness rate (81.8%), consistently outperforming other models. We observe that robustness improves steadily through the two- stage training pipeline, indicating that both SFT and RL fine- tuning contribute to stronger invariance to trajectory-order perturbations. Moreover, all VLM-based models maintain their accuracy after the flip, whereas the supervised classifier exhibits a notable drop, underscoring the advantage of a VLM backbone for this evaluation task. VI. LIMITATIONS & OUTLOOK A. Limitations While DriveCritic demonstrates clear gains in aligning evaluation with human preferences, several limitations re- main. First, because the DriveCritic model relies on a VLM, it inherits typical VLM weaknesses: sensitivity to prompt design and domain shift, and occasional hallucination or inconsistent judgments when encountering scenes beyond its training distribution. These issues are expected to di- minish as stronger and more reliable VLMs emerge, and DriveCritic can directly benefit from such advances without architectural change. Second, the curated preference dataset, though targeted at ambiguous regimes, is relatively limited in scope (pairwise comparison) and diversity of driving patterns. Third, the current DriveCritic model does not leverage temporal information due to resource considera- tion, which means it may misinterpret scenarios such as changing traffic lights. Finally, running a VLM for selective adjudication incurs a non-negligible computational cost and carbon footprint. Although batching and caching help, the overhead can still be substantial at scale, posing practical and environmental challenges for large-scale deployment. B. Outlook Despite these limitations, we see several promising di- rections for future work. Expanding the", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "c13505477b2cf1f4b85afb4758311e2e8967af14c0737ee05d60f944d7f65e9c"}
{"doc_id": "arxiv:2510.13108#model:part-2", "url": "https://arxiv.org/abs/2510.13108", "anchor": "#model:part-2", "type": "paper", "title": "", "section": "Model", "text": "driving patterns. Third, the current DriveCritic model does not leverage temporal information due to resource considera- tion, which means it may misinterpret scenarios such as changing traffic lights. Finally, running a VLM for selective adjudication incurs a non-negligible computational cost and carbon footprint. Although batching and caching help, the overhead can still be substantial at scale, posing practical and environmental challenges for large-scale deployment. B. Outlook Despite these limitations, we see several promising di- rections for future work. Expanding the DriveCritic dataset across domains, evaluation modes, and driving styles [16] will strengthen its utility. Additionally, we think integrating the DriveCritic model to create a scalable human-aligned trajectory database with RL-based planners [42] is an in- teresting future direction to show that preference-aligned critics could guide RL fine-tuning of end-to-end planners. Furthermore, exploring lighter-weight models or knowledge distillation from large VLMs to smaller student critics may reduce compute cost and improve deployability of VLM- based driving evaluation solutions. VII. CONCLUSION In this work, we addressed the lack of context-awareness in state-of-the-art, rule-based metrics like EPDMS, which of- ten misaligns with expert human judgment in complex driv- ing scenarios. We introduced DriveCritic, a novel framework featuring a VLM evaluator and a new dataset of ambigu- ous scenarios annotated with pairwise human preferences. By fine-tuning the VLM with a two-stage supervised and reinforcement learning pipeline, our model learns to make human-aligned judgments. Our experiments validate this approach, showing DriveCritic achieves 76.0% accuracy in aligning with human preferences, significantly outperforming all baselines. The model also demonstrates high robustness to input permutations, confirming the effectiveness of our training strategy. Ultimately, DriveCritic represents a signifi- cant step toward developing more reliable and human-centric evaluation tools for autonomous driving.", "source": "arxiv_pdf", "published": "", "tokens": 282, "sha256": "1a0f0cb0befba1b1e3786f84ebbac9e187ef9e7db6ff20a1c6734fd5f059f1a7"}
{"doc_id": "arxiv:2510.13443#abstract:part-1", "url": "https://arxiv.org/abs/2510.13443", "anchor": "#abstract:part-1", "type": "paper", "title": "", "section": "Abstract", "text": "Electromyography (EMG) signals are widely utilized to predict body joint angles using Machine Learning (ML) and deep learning (DL) techniques. However, these techniques often face challenges such as real-time applicability due to non-real-time preprocessing, improper test set selection for real-world scenarios, suboptimal network structure, and the large datasets necessary for effective model tuning. This paper proposes a transfer-learning approach to predict knee joint angles requiring EMG signals from only a few gait cycles for new subjects. The transfer-learning approach employs three datasets, all of them containing four-channel EMG signals relevant to knee angle prediction. A lightweight attention-based CNN-LSTM network is designed to predict the knee joint angle. After network pre- training on the source dataset (Georgia Tech), a transfer-learning method is applied to the open-source dataset from the University of California, Irvine (UCI) Machine Learning Repository, in addition, to our own experimental dataset obtained from seven tests conducted in zero-force mode on the Sharif Mechatronic Lab Exoskeleton (SMLE). The ultimate objective is to attain high predictive accuracy for previously unseen subjects, even when trained with very few gait cycles, thereby ensuring the network's robustness and reliability for both short- and long-term rehabilitation applications The network is further validated on abnormal subjects as well as on subjects wearing an exoskeleton and performing non-periodic gait cycles. Based on the test results, the model achieved a Normalized Mean Absolute Error (NMAE) of 6.8% for one-timestep prediction and 13.7% for 50-timestep prediction on abnormal subjects using only EMG input in UCI dataset. These results encompass both short and extended prediction horizons, which are particularly relevant for rehabilitation applications. Similarly, for normal subjects, the NMAE was 7.2% and 15.6% for one-timestep and 50-timestep predictions on the target dataset, respectively. However, when both EMG signals and historical knee angle data were used as inputs, the NMAE significantly decreased to 3.1% and 3.5% for normal subjects, and to 2.8% and 7.5% for abnormal subjects in one-timestep and 50-timestep predictions, respectively. Notably, when transferring the network to the exoskeleton robot related to SMLE dataset, it was observed that EMG signals alone were insufficient. By incorporating EMG signals, kinematic data, and interaction forces as inputs, the model achieved a remarkable improvement, reaching an NMAE of 1.09% for one-timestep prediction and 3.1% for 50-timestep prediction. These results were obtained with a limited number of gait cycles from new subjects, including non-periodic gait patterns in both target datasets. Keywords: EMG, Transfer Learning, Knee Angle Prediction, Attention Mechanism, Rehabilitation, Exoskeleton. 1- Introduction Electromyography (EMG) measures electrical signals generated by contracting muscle fibers, reflecting neuromuscular activity. [1]. EMG is typically measured using electrodes placed on the skin's surface (surface Electromyography (sEMG)). Alternatively, electrodes may be inserted into the muscle tissue [2]. The frequency range of EMG signals is generally reported to be from 6 to 500 Hz, with most power concentrated between 20 and 250 Hz [3]. Analyzing EMG signals provides valuable information about muscle activation patterns, coordination, and fatigue levels. This information makes EMG signals invaluable in biomechanics, rehabilitation, and prosthetics [4]. EMG analysis has become applicable in various fields, particularly in the development", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "8905a041370d85bbf57473db0bf324290137e6408981df03b5ddb8b4beb3c041"}
{"doc_id": "arxiv:2510.13443#abstract:part-2", "url": "https://arxiv.org/abs/2510.13443", "anchor": "#abstract:part-2", "type": "paper", "title": "", "section": "Abstract", "text": "the skin's surface (surface Electromyography (sEMG)). Alternatively, electrodes may be inserted into the muscle tissue [2]. The frequency range of EMG signals is generally reported to be from 6 to 500 Hz, with most power concentrated between 20 and 250 Hz [3]. Analyzing EMG signals provides valuable information about muscle activation patterns, coordination, and fatigue levels. This information makes EMG signals invaluable in biomechanics, rehabilitation, and prosthetics [4]. EMG analysis has become applicable in various fields, particularly in the development and control of lower limb exoskeletons. Using EMG, exoskeletons can provide real-time assistance to individuals with mobility impairments to enhance their walking gait through adaptive control mechanisms [5]. Assisting a subject with abnormality in the lower limb to complete the walking phase is one the most important aspects of exoskeleton control application in rehabilitation. The gait cycle consists of two primary phases: the stance and the swing phase [6]. The stance phase accounts for approximately 60% of the gait cycle, and the swing phase constitutes the remaining 40%. The knee joint is a vital element in these two primary walking phases. So, accurate prediction of knee angles using EMG signals could be a crucial step in real-time environments such as exoskeleton control, facilitating a more natural and efficient gait pattern for users [6]. The knee joint's movement is influenced by the coordinated activity of several key muscles, each of which contributes to a specific part of knee flexion and extension during the walking phase. The coordinated activity of muscles, including the Vastus Medialis (VM), Vastus Lateralis (VL), Rectus Femoris (RF), Biceps Femoris (BF), Semitendinosus, Semimembranosus, and Gastrocnemius, plays a critical role in knee joint movement during the gait cycle. Some other muscles also affect the knee joint, although their effect is more superficial than the muscles reviewed ([7], [8]). Although there are numerous advantages to using EMG signals, there are also significant limitations, including signal noise, crosstalk, variability across individuals, electrode placement inconsistency, muscle fatigue effects, and the complexity of data acquisition protocols ([9],[10]). These challenges necessitate advanced computational techniques, such as machine learning (ML) and deep learning (DL), to process EMG signals effectively and extract meaningful patterns [11]. Numerous machine learning (ML) methods have been applied to EMG signals for various classification and regression tasks ([12], [13], [14], [15], [16], [17]). While a few of these methods achieve acceptable precisions in some specific scenarios and protocols, they often face limitations that make their application in real-time experiments challenging, if not impractical. These limitations include: 1. Preprocessing constraints: Many techniques rely on preprocessing steps that are not applicable in real-time scenarios. For example, preprocessing steps such as low-pass and de-noising filters are often applied to the entire test of the subject, requiring all samples from a test to be available beforehand, which is impractical in real-time applications ([13], [17]). 2. Cross-validation issues: Cross-validation methods often inadvertently allow models to access validation or test data from the same subjects included in the training process, particularly when EMG signals are used in solidarity, leading to overfitting and reduced generalizability in real-world scenarios ([13], [17])", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "abac6d8d44c74b4353f02264b838ce61a5417a83996d27e5327f4af90a895ea2"}
{"doc_id": "arxiv:2510.13443#abstract:part-3", "url": "https://arxiv.org/abs/2510.13443", "anchor": "#abstract:part-3", "type": "paper", "title": "", "section": "Abstract", "text": "example, preprocessing steps such as low-pass and de-noising filters are often applied to the entire test of the subject, requiring all samples from a test to be available beforehand, which is impractical in real-time applications ([13], [17]). 2. Cross-validation issues: Cross-validation methods often inadvertently allow models to access validation or test data from the same subjects included in the training process, particularly when EMG signals are used in solidarity, leading to overfitting and reduced generalizability in real-world scenarios ([13], [17]) 3. High sampling frequencies: Many studies process EMG signals at their original sampling frequencies (e.g., 1000 or 2000 Hz) ([17], [13], [14]). However, after some standard preprocessing, the effective frequency of the processed EMG signal is typically below 30 Hz. this is shown in the GitHub respiratory[18] Therefore, down sampling the signal to 100 Hz does not lead to aliasing [19], as the effective frequency content of the preprocessed EMG signal remains well below the Nyquist frequency (50 Hz), ensuring signal integrity is preserved. Reducing the input frequency can significantly decrease the size of the input data, making it more efficient for network processing. Some datasets have also provided EMG signals at lower frequencies, such as 100 Hz, to facilitate processing and real-time applications[20]. 4. Network optimality: Studies combining EMG and kinematic data as network inputs, usually report that the addition of EMG does not always lead to noticeable improvements [12]. This result is often obtained from suboptimal network designs for processing EMG signals. EMG is a complex signal, so the network must first be optimized on this signal before evaluating its synergistic effects with kinematic data. This prevents overfitting on the kinematic data. The most effective way to evaluate deep learning methods is by comparing results on similar datasets or standardized benchmarks. Our primary objective is to predict knee angles for both normal and abnormal subjects in the UCI dataset [21]. As will be detailed in Section 3, this dataset contains a highly limited number of walking gait cycles per subject. However, its inclusion of subjects with knee abnormalities makes it highly valuable for rehabilitation analysis. Due to the scarcity of available data, most studies have focused on classification tasks. Classification requires less training data, compared to regression tasks, due to less complexity, while regression- based analyses remain largely unexplored. Various ML and DL methods have been applied to this dataset for three-class classification (walking, flexion, and extension ([17], [16], [22]) and binary classification (normal vs. abnormal) ([23], [24]). However, our focus is on predicting knee angle data as a regression task. Two key studies that have conducted regression tasks on the UCI dataset are the following: MyoNet utilizes a Long-term Recurrent Convolutional Network (LRCN) with transfer learning for lower limb movement recognition and knee joint angle prediction from sEMG signals. The model achieved an average Mean Absolute Error (MAE) of 8.1% for healthy subjects and 9.2% for abnormal ones [13]. Multi-model fusion-based Ridge Regression (MMF-RR) has been introduced for knee joint angle prediction using EMG and historical joint angles [14]. The model integrates four machine learning algorithms and applies ridge", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "08604046d207606a6f0397801f06d9ea426bef6170646b06cf888cc78462d7a5"}
{"doc_id": "arxiv:2510.13443#abstract:part-4", "url": "https://arxiv.org/abs/2510.13443", "anchor": "#abstract:part-4", "type": "paper", "title": "", "section": "Abstract", "text": "the UCI dataset are the following: MyoNet utilizes a Long-term Recurrent Convolutional Network (LRCN) with transfer learning for lower limb movement recognition and knee joint angle prediction from sEMG signals. The model achieved an average Mean Absolute Error (MAE) of 8.1% for healthy subjects and 9.2% for abnormal ones [13]. Multi-model fusion-based Ridge Regression (MMF-RR) has been introduced for knee joint angle prediction using EMG and historical joint angles [14]. The model integrates four machine learning algorithms and applies ridge regression with penalty terms to improve prediction accuracy. Evaluated on the UCI dataset, MMF-RR achieved an MAE of 6.73% for healthy participants and 8.97% for individuals with knee joint abnormality, outperforming several existing methods in motion intention recognition for rehabilitation applications. The UCI dataset provides valuable data from both normal and abnormal subjects; however, it lacks enough training data to effectively be trained in deep learning models. In contrast, some other datasets, which are accessible upon reasonable request, contain a significantly larger volume of data per subject, making them more suitable for deep-learning applications [20]. Although large-scale datasets are typically available for normal subjects, as is the case here, they often lack data for abnormal subjects. The primary objective of this study is to leverage knowledge from a network trained on an extensive dataset and transfer it to a smaller dataset that contains valuable but limited information from rare abnormal subjects. This approach enhances the model's ability to generalize and improves prediction performance in real-world rehabilitation scenarios. In this study, we propose an attention-based CNN-LSTM network to predict real-time knee joint angle using EMG signals, both independently and in combination with kinematic data and the interaction forces (when the subject is with exoskeleton). By addressing challenges such as preprocessing constraints, cross-validation issues, and network optimization, this approach aims to perform real-time knee angle prediction according to EMG signals for new subjects utilizing transfer-learning. The network is first optimized using EMG signals alone, and then, kinematic and kinetic data are integrated to enhance prediction performance. The network's structure is designed to ensure low-volume network weights and reduce the size of the input data through carefully selected preprocessing steps. This lightweight design enables the network to be fine-tuned quickly and requires little data from the new subject, making it highly suitable for real-time applications. Technical Contributions: The main technical contributions of this study are as follows: • Efficient signal processing: The EMG signals are downsampled to 100 Hz, enabling the design of a compact and computationally efficient network. • Incorporation of attention mechanisms: An attention module is integrated into the network to enhance feature selection and improve predictive performance. • Optimized lightweight architecture: The network structure is carefully designed to have a minimal number of parameters, balancing accuracy with computational efficiency. • Data-efficient transfer learning: A transfer learning strategy is proposed that requires only a few gait cycles from new subjects, reducing the risk of overfitting and minimizing the need for extensive subject-specific data. • Optimizing the network with EMG input before incorporating kinematic data: Before adding kinematic data as input, the network is", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "cf3461b228aae2c1f1d726999de7372be828cd858d5628296e1fc256dd226595"}
{"doc_id": "arxiv:2510.13443#abstract:part-5", "url": "https://arxiv.org/abs/2510.13443", "anchor": "#abstract:part-5", "type": "paper", "title": "", "section": "Abstract", "text": "selection and improve predictive performance. • Optimized lightweight architecture: The network structure is carefully designed to have a minimal number of parameters, balancing accuracy with computational efficiency. • Data-efficient transfer learning: A transfer learning strategy is proposed that requires only a few gait cycles from new subjects, reducing the risk of overfitting and minimizing the need for extensive subject-specific data. • Optimizing the network with EMG input before incorporating kinematic data: Before adding kinematic data as input, the network is first optimized using only EMG signals. This strategy mitigates the risk of overfitting kinematic data, ensuring that the network effectively learns from the EMG features. • Selecting an EMG-based source dataset for pre-training: The source dataset is specifically chosen to include EMG signals, enabling meaningful transfer learning for the target task. In contrast, many existing studies adopt pre-trained models including convolutional neural networks, mostly used for image classification, which may not be optimal for EMG-based applications. • Transfer learning was performed on datasets obtained from subjects walking both independently and while equipped with the exoskeleton under non-periodic gait conditions: When the subjects operated with the exoskeleton, only the robotic joint angles measured by the encoders in zero-force mode were available, rather than the true human knee angles. Under these circumstances, the interaction forces introduced additional disturbances and modulated the EMG signal amplitudes, making the knee angle prediction task substantially more challenging. In summary, these contributions are combined to enable effective transfer learning with minimal data from new subjects, ensuring the method's applicability in real-world rehabilitation scenarios. The remainder of this paper is organized as follows: The proposed methodology, including the network architecture, dataset’s structure and transfer learning strategies, is described in Section 2. Experimental results are presented and analyzed in Section 3. Finally, conclusions and discussions are provided in Section 4.", "source": "arxiv_pdf", "published": "", "tokens": 301, "sha256": "b6cbec432f641fd98abbf2ac60c4e29213f33a684d72ed2248b0b87e2bc046a8"}
{"doc_id": "arxiv:2510.13443#2-method:part-1", "url": "https://arxiv.org/abs/2510.13443", "anchor": "#2-method:part-1", "type": "paper", "title": "", "section": "2 Method", "text": "This section provides a detailed explanation of the methodology employed in this study, outlining the dataset, prediction scenarios, preprocessing steps, network architecture, and transfer-learning approach. The flowchart diagram of the Method Section is shown in Fig. 2-1. 2-1 Dataset In this study, three datasets were utilized: the open‐source UCI dataset [21], the Georgia-tech dataset (obtained upon request) [20], and the SMLE dataset (acquired from our experimental laboratory tests). This dataset is not publicly available and will be provided upon request, with the citation of this paper for any use of the data. Our first target is the UCI dataset, which, despite having a limited number of samples per test, comprises both normal and abnormal tests. Accordingly, our strategy is to initially train the network on a comprehensive dataset (the Georgia-tech dataset) and subsequently transfer the trained model to the UCI dataset. Ultimately, the network will also be adapted to the experimental tests called SMLE dataset to further validate our network under conditions in which the subject is interacting with an exoskeleton. 2-1-1 Georgia-tech dataset The Georgia-tech dataset is comprehensive, containing enough training samples, making it well-suited for the development of deep neural networks [20]. However, it lacks data from abnormal subjects, whose EMG signal analysis is crucial to detect walking phase abnormalities, an important key aspect of rehabilitation. In this dataset, 22 healthy subjects participated in performing different movements. In our study the subset of dataset, corresponding to treadmill walking, was employed. In this subset, subjects walked at 28 distinct speeds ranging from 0.5 m/s to 1.85 m/s, with increments of 0.05 m/s. These speeds were distributed across seven trials, with four speeds per trial, resulting in approximately 50 gait cycles per test. the data from four channels, analogous to the UCI dataset, were selected, as stated in Section 2-1-2. 2-1-2 UCI dataset The UCI dataset includes both normal and abnormal subjects, making it valuable for rehabilitation-related studies. However, its primary limitation is the small amount of available data, which may hinder the network's ability to achieve acceptable precision [21]. A summary of the UCI dataset's characteristics is presented in Table 2-1. Table 2-1- Information of UCI dataset [21] Aspect Details EMG Sampling Frequency 1000 Hz Number of subjects 22 (11 healthy, 11 with knee abnormalities) Channels Biceps Femoris, Rectus Femoris, Semitendinosus, Vastus Medialis Activities Recorded Walking, Sitting with Knee Flexion, Standing with Knee Extension Kinematic Data Only knee angle measured by goniometer According to Table 2-1 in the UCI dataset, each subject performed three tests. In this study, only the data from walking test was employed. Also, the UCI dataset contains only four EMG channels. To ensure compatibility and enable transfer learning, only these four channels were considered from the Georgia-tech and the SMLE datasets described in Sections 2-1-1 and 2-1-3. Fig. 2-1- A flowchart of the proposed methodology 2-1-3 Sharif-Mechatronic lab Exoskeleton (SMLE) dataset (Experimental test) The data collection protocol in this section commenced with the placement of surface electromyography (EMG) electrodes on the right lower limb of a healthy subject. Electrodes were positioned over the muscle bellies of the", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "2311e591900cafa00c340d12da0616b86376a1d5ed878f6add2f0c55b7bd8b4a"}
{"doc_id": "arxiv:2510.13443#2-method:part-2", "url": "https://arxiv.org/abs/2510.13443", "anchor": "#2-method:part-2", "type": "paper", "title": "", "section": "2 Method", "text": "only four EMG channels. To ensure compatibility and enable transfer learning, only these four channels were considered from the Georgia-tech and the SMLE datasets described in Sections 2-1-1 and 2-1-3. Fig. 2-1- A flowchart of the proposed methodology 2-1-3 Sharif-Mechatronic lab Exoskeleton (SMLE) dataset (Experimental test) The data collection protocol in this section commenced with the placement of surface electromyography (EMG) electrodes on the right lower limb of a healthy subject. Electrodes were positioned over the muscle bellies of the Gluteus Maximus (GL), Vastus Lateralis (VL), Rectus Femoris (RF), Vastus Medialis (VM), Semitendinosus (ST), and Biceps Femoris (BF), and each channel was individually tested to ensure signal integrity. The subject was then carefully fitted with the lower-limb exoskeleton robot, with particular attention given to securing the straps without disturbing the underlying EMG electrodes. A zero-force controller was executed to minimize robotic impedance between the robot and the subject. In parallel, kinetic and kinematic data were collected, including thigh and shank interaction forces from embedded sensors and hip and knee joint angles from the exoskeleton encoders. For this trial, the subject’s left leg was stabilized on the stationary edge of the treadmill, while the instrumented right leg executed a walking motion at a constant treadmill speed of 0.5 km/h. From this dataset, only the four EMG channels that aligned with the requirements specified in Section 2.1.2 were utilized, together with the knee joint angle and the thigh and shank interaction forces, all in 1000HZ frequency. The interaction forces were measured using calibrated load cells integrated into the exoskeleton. The shank load cell was positioned approximately 18 cm below the knee joint, and the thigh load cell was located about 30 cm below the hip joint 1- Dataset 1-3- SMLE dataset 1-2- UCI dataset 1-1- Georgia-tech dataset 6- Transfer Learning 6-2- Transfer the network to the new subject from the Georgia-tech dataset 6-3- Transfer the network to the UCI dataset 6-4- Transfer the network to the SMLE dataset 6-1- Transfer the EMG network within Georgia-tech dataset with the added Kinematics data 3- Prediction Scenarios 2- Preprocessing Step Preprocessing Preprocessing Preprocessing 4- Network Architecture 5- Network primary Training Fig. 2-2- Experimental test setup conducted in the Sharif-Mechatronic Lab Figure 2-2 illustrates the human–robot interaction under zero-force mode control. During this experiment, IMU sensors were positioned on the shank and thigh, as shown in the figure; however, their data were excluded from the analysis. Only the EMG signals, the encoder- based knee and hip joint angles, and the shank and thigh interaction forces were retained for subsequent processing and analysis. 2-2 Preprocessing steps Preprocessing is a crucial step in EMG analysis due to the complexity of the signal. Effective preprocessing can make the network more efficient by reducing input size and improving validation accuracy. A key aspect of this study is that all preprocessing steps are performed on an online window after segmentation, making the approach feasible for real-time applications. The preprocessing steps employed in this study are as follows: 1. High-pass filtering: A cutoff frequency of 20 Hz is applied to remove low-frequency noise and", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "0b92d49dbb4e923f026f06067352e906f6f92dc995f163d695a1a312b3a7b32a"}
{"doc_id": "arxiv:2510.13443#2-method:part-3", "url": "https://arxiv.org/abs/2510.13443", "anchor": "#2-method:part-3", "type": "paper", "title": "", "section": "2 Method", "text": "crucial step in EMG analysis due to the complexity of the signal. Effective preprocessing can make the network more efficient by reducing input size and improving validation accuracy. A key aspect of this study is that all preprocessing steps are performed on an online window after segmentation, making the approach feasible for real-time applications. The preprocessing steps employed in this study are as follows: 1. High-pass filtering: A cutoff frequency of 20 Hz is applied to remove low-frequency noise and artifacts. 2. Recreation: All signals are converted to positive values by taking the absolute value of the filtered signal. 3. Low-pass filtering: A second-order Butterworth filter with a cutoff frequency of 3–6 Hz is applied to smooth the rectified signal. 4. Mean-variance normalization: The mean value is subtracted from the signal, and the result is divided by the standard deviation for each window. 5. Down-sampling: The sampling rate is reduced from 1000 Hz to 100 Hz to improve computational efficiency. A window size of 2 seconds is used for all three datasets (with a sampling frequency of 1000 Hz for EMG data collection). A 2000- sample window at 1000 Hz is preprocessed and reduced to 200 samples at 100 Hz. These preprocessing steps are commonly employed in studies that predict lower-limb joint angles using EMG signals ([12], [20]). 2-3 Prediction scenarios For all three datasets, four general scenarios were considered. These general scenarios involve predicting either a one-step or 50-step knee angle using (1) only EMG or (2) EMG in conjunction with knee angle data as inputs. Hereafter, we will refer to the first case as the single-input configuration (SIC) and the second case as the dual-input configuration (DIC). For the SMLE dataset, both the SIC and Dual- DIC scenarios were evaluated, with the interaction forces between the human and the exoskeleton (measured at the thigh and shank through embedded sensors) incorporated as additional inputs to the network. The details of splitting data into training, validation, and test sets were specified separately for each dataset in the following sections. 2-4 Network architecture The proposed network architecture employs three key components. Convolutional Neural Networks (CNNs), Long Short-Term Memory (LSTM) networks, and the attention mechanism. CNNs are a kind of deep learning model designed to process data with a grid-like structure, particularly images and time-series data. They employ convolutional layers to automatically learn spatial features that effectively capture local patterns in the input data [25]. LSTMs are a type of Recurrent Neural Network (RNN) designed to model sequential data by learning long-term dependencies [26]. The attention mechanism enhances neural network performance by selectively focusing on the most relevant parts of the input sequence. This dynamic weighting improves the network’s ability to capture important time and spatial information [27]. By integrating these components, the network is equipped with instruments to extract spatial features, model temporal dependencies, and dynamically prioritize critical input elements, resulting in superior predictive performance. The attention mechanism in our network is utilized only in DIC scenarios. In the structure used for SIC in this research, four EMG channels from each window are", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "d4559605d2e48e9a72126418ac342553b4fb03d5874e02aa616a1d9a70b698c3"}
{"doc_id": "arxiv:2510.13443#2-method:part-4", "url": "https://arxiv.org/abs/2510.13443", "anchor": "#2-method:part-4", "type": "paper", "title": "", "section": "2 Method", "text": "on the most relevant parts of the input sequence. This dynamic weighting improves the network’s ability to capture important time and spatial information [27]. By integrating these components, the network is equipped with instruments to extract spatial features, model temporal dependencies, and dynamically prioritize critical input elements, resulting in superior predictive performance. The attention mechanism in our network is utilized only in DIC scenarios. In the structure used for SIC in this research, four EMG channels from each window are fed to the network as the signal with a length of 200 (2s in 100Hz sample frequency). These four channels pass through the convolutional layers separately and are then concatenated. The concatenated layer is fed into two LSTM layers. After extracting time features with the LSTM layers, the last hidden state is fed to the final dense layer (containing 1 or 50 units according to the prediction horizon) to predict the knee angle. In DIC scenarios, the 200 kinematic data points are reshaped into a 50×4 matrix and fed into an LSTM layer, which processes the input across 50 consecutive time steps to extract temporal features. The network is designed such that the output of the CNN, before the concatenation layer, also produces 50 temporal features for each EMG channel. The attention mechanism operates between the hidden states of the LSTM, corresponding to the kinematic data, and the features extracted from the four EMG channels (before concatenation) across the 50-time segments. With this attention mechanism for each 50 time-steps, the model evaluates the significance of each channel to enhance feature extraction and improve prediction accuracy. The schematic of the network deployed in this research for DIC scenarios is shown in Fig. 2-3. Fig. 2-3- The schematic of the proposed network (dual-input configuration (DIC)) Figure 2-3 illustrates the DIC scenario, while the remaining configurations are omitted for brevity and to maintain conciseness in the manuscript. In the SIC scenario, the upper part of Figure 2-3 is fed directly to the LSTM and Dense layer. In the scenario where the subject walks with the exoskeleton and interaction forces at the shank and thigh are measured, these values are fed through a convolutional network for processing. After the preprocessing stage, the extracted features are concatenated with the outputs of the final layers preceding the dense layer. Detailed implementation and further information of all scenarios are provided in the GitHub repository [ 18 ] 2-5 Network primary training The partitioning methodology for the Georgia-tech dataset is designed to support real-time scenarios, particularly for new subjects (leave-one-out scenarios). The subjects of this dataset are divided into two groups: the first 21 subjects are allocated to the training and validation sets, while the remaining subject is considered as the test set. For the first 21 subjects, 80% of the data from each trial is used for training, and the remaining 20% is allocated for validation. This training is performed exclusively on the SIC scenario. We refer to this phase as the network’s primary training, and the trained model is subsequently utilized in the following sessions for transfer learning.", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "65ea435c106c7d336cfe10ecac6849f74650cb4371dc4ff5196ad8e41e5acda1"}
{"doc_id": "arxiv:2510.13443#2-method:part-5", "url": "https://arxiv.org/abs/2510.13443", "anchor": "#2-method:part-5", "type": "paper", "title": "", "section": "2 Method", "text": "groups: the first 21 subjects are allocated to the training and validation sets, while the remaining subject is considered as the test set. For the first 21 subjects, 80% of the data from each trial is used for training, and the remaining 20% is allocated for validation. This training is performed exclusively on the SIC scenario. We refer to this phase as the network’s primary training, and the trained model is subsequently utilized in the following sessions for transfer learning. It is important to note that validation data partitioning is not applied during the test phase, and it is only employed to prevent overfitting during the training process. 2-6 Transfer-learning The network trained from the Georgia-tech dataset is transferred to the new subjects from which the network has not seen any sample data. This means that the network is used with the weights from the previous training and will change with a lower learning rate compared to primary training to adapt to new data and use the history from past training in the network weights [28]. As discussed in Section 1, it is impossible to predict the joint angles of the new subjects in the SIC scenario without considering an adaptation phase on the new subjects. Due to unrepeatability of the EMG signals, the transfer-learning and fine-tuning of the network is mandatory. The transfer learning in this research is divided into 4 parts as follows: 2-6-1 Transfer the EMG network within the Georgia-tech dataset with the added Kinematics data (SIC to DIC) The trained network from the Georgia-tech dataset in Section 2-5 is transferred to the DIC scenario in the same dataset. The network components that process the EMG signals remain unchanged, while the components handling the kinematic data are trained from scratch. It is important to note that in the DIC scenarios, if the network is simultaneously trained with the EMG and kinematic data, it will be biased toward the kinematic data, and the EMG part of the network will not be well optimized. The optimization of the EMG part of the network before adding the kinematic data, is one of our method's merits that isolates this research from others. 2-6-2 Transfer the network to the new subject from the Georgia-tech dataset In this section, transfer-learning is used for a new subject (test subject) from the Georgia-tech dataset. Each trial in the new test set is divided into fine-tuning and evaluation sets with a ratio of 0.5 to ensure effective model adaptation and assessment. For the SIC scenarios, the network described in Section 2-5 is transferred to the new subject. In the DIC scenarios, the pre-trained network in Section 2-6-1 is utilized. As illustrated in Figure 2-1, the flowchart includes two arrows originating from Sections 2.5 and 2.6.1, representing their corresponding connections in the subsequent scenarios. The network is fine-tuned using the fine-tuning data and evaluated using the evaluation data. 2-6-3 Transfer the network to the UCI dataset Since this study focuses on predicting knee joint angle data during walking, only the walking class was considered in the UCI dataset. This", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "614a4116640a9916ef8eecccac4388ee625a82ed26a10e2a057bdc69a3c0429e"}
{"doc_id": "arxiv:2510.13443#2-method:part-6", "url": "https://arxiv.org/abs/2510.13443", "anchor": "#2-method:part-6", "type": "paper", "title": "", "section": "2 Method", "text": "the DIC scenarios, the pre-trained network in Section 2-6-1 is utilized. As illustrated in Figure 2-1, the flowchart includes two arrows originating from Sections 2.5 and 2.6.1, representing their corresponding connections in the subsequent scenarios. The network is fine-tuned using the fine-tuning data and evaluated using the evaluation data. 2-6-3 Transfer the network to the UCI dataset Since this study focuses on predicting knee joint angle data during walking, only the walking class was considered in the UCI dataset. This results in 11 tests for normal subjects and 11 tests for abnormal subjects (one test per subject). All four general scenarios outlined in Section 2-3 were implemented to predict knee angles under two specific conditions: (1) fine-tuning exclusively on data from normal subjects and (2) fine-tuning exclusively on data from abnormal subjects. In each scenario, tests from 10 subjects were allocated to the training and validation sets, with a separation ratio of 0.8 for training and 0.2 in each test for the validation sets. The remaining test subject (11th) was considered as the test set. Furthermore, the test set was divided into fine-tuning and evaluation subsets, with a split ratio of 0.5. First, the network primarily trained with the Georgia-tech dataset, is re-trained with the training set and validated on the validation set to adapt to the new conditions (training on the population). The first training is referred to as first-stage training. Then, the network from the first stage is transferred to the new subject in the second-stage training with fine-tuning and evaluation sets. The pre-trained network from Section 2-5 is transferred for SIC scenarios, and the one in Section 2-6-1 is utilized for DIC scenarios. 2-6-4 Transfer the network to the SMLE dataset In addition to EMG and kinematic data, shank and thigh force sensors were also available in the (SMLE) dataset as described in section 2-1-3. Four scenarios were considered: SIC, DIC, SIC-Forces, and DIC-Forces. In the SIC-Forces and DIC-Forces scenarios, the corresponding SIC and DIC setups were extended by including only the shank and thigh forces as additional inputs to the network. One healthy subject performed seven different tests. Six tests were used with an 80/20 data split in a shuffled manner for training and validation, allowing the network to adapt to the new experimental conditions. The final test was split evenly, with 50% of the data used for fine-tuning and 50% for evaluation. Accordingly, the scenarios corresponding to the first- and second-stage training, as described in Section 2.5.3, were considered in this analysis. The pre-trained network from Section 2.5 was transferred in all four scenarios, where the EMG portion of the network was fine-tuned with a lower learning rate, while the kinematic and force components were trained from scratch. So, as shown in Figure 2-1, the flowchart contains a single arrow originating from Section 2.5, with no corresponding connection from Section 2.6.1.", "source": "arxiv_pdf", "published": "", "tokens": 476, "sha256": "f381dd07471085cf055f8e7755e4fd69b7552bd63f3957ef7d6e0981357e4e22"}
{"doc_id": "arxiv:2510.13443#3-results:part-1", "url": "https://arxiv.org/abs/2510.13443", "anchor": "#3-results:part-1", "type": "paper", "title": "", "section": "3 Results", "text": "This section presents the results corresponding to each scenario outlined in Section 2. The primary objective of this study is to achieve acceptable prediction accuracy on target subjects (new subjects in the UCI dataset and experimental data collected in the SMLE dataset). To maintain brevity and clarity in the main text, the complete training and testing results for Sections 2.5, 2.6.2, 2.6.3, and 2.6.4 are presented in the supplementary tables in the GitHub respiratory[18] ,while Section 3.3 includes only the second-stage evaluation results for Sections 2.6.3 and 2.6.4. 3-1 Simulation parameters As discussed in Section 2, all EMG data undergoes preprocessing and is subsequently down sampled to 100 Hz. A 2000 ms (millisecond) window with 200 samples is then fed into the network, with an overlap of 40 ms (4 samples) between consecutive windows. The network contains 72,361 parameters in the SIC scenario and 80,208 parameters the DIC one. For the SMLE dataset, the total number of trainable parameters amounted to 77,469 in the SIC-Forces scenario and 90,069 in the DIC-Forces scenario, respectively. Thus, the network is significantly lighter compared to deep learning architectures which typically contain parameters in the order of millions. In the primary training the batch size was 2000 (in transfer-learning scenarios, according to the number of training samples, a smaller batch size is considered), and the number of epochs is set between 40 and 80, depending on the scenario. Training is terminated if validation loss does not decrease after 5 consecutive epochs. The Adam optimizer is utilized with an initial learning rate of 0.001 for primary training (Sections 2-5 and 2-6-1). For transfer learning scenarios, the learning rate is reduced to either 0.0001 (1/10 of the original) or 0.0005 (1/5 of the original), depending on the specific scenario. By gradually reducing the learning rate, the network preserves essential features from the pre-trained model while allowing adaptation to the new data. 3-2 Performance evaluation criteria To evaluate the network's performance across the scenarios outlined in Section 2, the metrics considered are Normalized Root Mean Squared Error (NRMSE), Normalized Mean Absolute Error (NMAE), and the Coefficient of Determination (R²). These metrics are widely adopted in related studies for assessing EMG-based kinematic predictions ([12], [13],[14] (. 3-3 Results for transfer-learning In this section, the results for the target dataset (UCI dataset) in second-stage transfer-learning and the experimental SMLE dataset will be shown in the two following sections. 3-3-1 Source and Target Data Ratio It is crucial to understand the distribution of data across source and target datasets for a clearer interpretation of the results. The primary dataset, Georgia-tech, consists of data from 22 subjects, each evaluated at 28 different speeds, comprising over 120 gait cycles per subject. This vast dataset provides a substantial foundation for the initial training phase. In contrast, the UCI dataset includes 11 subjects, with abnormal data containing between 6 to 40 gait cycles per test, and the normal data ranging from 4 to 6 cycles. The SMLE dataset consists of 7 tests, with each test having between 4 and 15 gait cycles. After preprocessing, the training and", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "0f183159a28c0d14150fed73558a9600c748318909ab88304c86f71ec9c154ea"}
{"doc_id": "arxiv:2510.13443#3-results:part-2", "url": "https://arxiv.org/abs/2510.13443", "anchor": "#3-results:part-2", "type": "paper", "title": "", "section": "3 Results", "text": "from 22 subjects, each evaluated at 28 different speeds, comprising over 120 gait cycles per subject. This vast dataset provides a substantial foundation for the initial training phase. In contrast, the UCI dataset includes 11 subjects, with abnormal data containing between 6 to 40 gait cycles per test, and the normal data ranging from 4 to 6 cycles. The SMLE dataset consists of 7 tests, with each test having between 4 and 15 gait cycles. After preprocessing, the training and test samples were defined as follows: Georgia-tech includes 414,446 samples for training and validation, and 19,209 samples for testing, which incorporates both fine-tuning and evaluation with a 50% split ratio. For the UCI dataset, the normal subjects have 1,135 samples for training and validation, with 87 samples for testing. The abnormal subjects in the UCI dataset have 5,460 samples for training and validation, and 479 samples for testing. Finally, the SMLE dataset consists of 12,811 samples for training and validation, and 1,890 samples for testing 3-3-2 Results for transferring the network to the UCI dataset As mentioned in Section 2-6, the first stage transfer-learning is to adapt the network to the new environment. This adaptation (fine- tuning) is performed with 10 subject tests (training and validation sets). The fine-tuned network is transferred again to the new subject in the second stage (fine-tuning and evaluation sets). The results of the first stage fine-tuning on the UCI dataset are presented in supplementary tables in the GitHub respiratory[18]. For the second fine-tuning on the new normal subject, the results corresponding to one-step and 50-step predictions, are presented in Tables 3-1 and 3-2, respectively. Similarly, for the new abnormal subject, the results for one-step and 50-step predictions are provided in Tables 3-3 and 3-4, respectively. Table 3-1- Second network transfer to UCI dataset (normal subject and one-step prediction) Scenario", "source": "arxiv_pdf", "published": "", "tokens": 305, "sha256": "296e36d3ad31bff01ce3b6e2c39d47a091519ee4bc6dbdfbee9c4ef57d315606"}
{"doc_id": "arxiv:2510.13443#evaluation:part-1", "url": "https://arxiv.org/abs/2510.13443", "anchor": "#evaluation:part-1", "type": "paper", "title": "", "section": "Evaluation", "text": "R2 SIC*…… 0.189 0.156 0.697 DIC**…. 0.113 0.075 0.801 Tables 3-1 to 3-4 demonstrate the improvement in loss and error in the DIC scenarios rather than SIC ones. The effect of adding kinematic data is more pronounced in normal subjects compared to abnormal cases. This discrepancy may be attributed to the less periodic nature of abnormal gait cycles and noting that kinematic data provides greater predictive benefits in more periodic gait patterns. These results also indicate that EMG signals play a crucial role in predicting knee angles for abnormal subjects, where gait irregularities reduce the impact of kinematic inputs alone. Overall, the results indicate that predictions for normal subjects are generally more accurate than for abnormal subjects. This trend persists even when more training data is available for abnormal subjects in the UCI dataset; suggesting that knee angle prediction for abnormal subjects presents a greater challenge due to the increased variability and irregularities in pathological gait patterns. Additionally, the R² criterion in Tables 3-2 and 3-4, show that EMG alone does not provide acceptable prediction accuracy in the 50-step prediction scenario. A comparison between Tables 3-1 to 3-4 with Tables related to the first adaptation to population in UCI dataset (detailed in supplementary tables in the GitHub respiratory[18] reveals that first-stage transfer learning (using 10 subjects) achieves better performance than second-stage transfer learning (using one subject data with limited fine-tuning data from only three walking gait cycles for normal subjects and seven for abnormal ones). Figure 3-1 presents the results of knee angle prediction using EMG signals in normal test cases in a one-step prediction scenario. Fig. 3-1- Fine-tuning and evaluation data for the new normal subject in a one-step prediction (UCI dataset) The lower panel of Figure 3-1 represents the fine-tuning data, while the upper panel illustrates the real and predicted values for the evaluation data in both SIC and DIC cases. It should be noted that the figures may not fully resemble a complete walking gait cycle due to the selection of an overlap exceeding one step, which is necessary for real-time scenarios. Figure 3 -2 shows the results of knee angle prediction using EMG signals for abnormal test cases in a one-step prediction scenario. -10.00 0.00 10.00 20.00 30.00 40.00 50.00 60.00 0 10 20 30 40 Knee Angle (Degrees) Sample (in 100 HZ frequency) Evaluation Data (Original Scale) GroundTruth Predictions (SIC) Predictions (DIC) -20.00 -10.00 0.00 10.00 20.00 30.00 40.00 50.00 60.00 70.00 0 5 10 15 20 25 30 35 40 45 Knee Angle (Degrees) Sample (100 HZ) Fine-tuning Data (Original Scale) Fig. 3-2- Fine-tuning and evaluation data for the new abnormal subject in a one-step prediction (UCI dataset) In Figure 3-2, the lower panel represents the fine-tuning data, while the upper panel illustrates the real and predicted values for the evaluation data in both SIC and DIC cases. As shown in Figures 3-1 and 3-2, the network demonstrates the ability to predict knee angle data using only EMG signals with very limited fine-tuning data (approximately three walking gait cycles for normal subjects and seven for abnormal", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "53dd1bfbfca489d76ba512e84ebc9362eeb066acad793b4701608eac9fd8d9ee"}
{"doc_id": "arxiv:2510.13443#evaluation:part-2", "url": "https://arxiv.org/abs/2510.13443", "anchor": "#evaluation:part-2", "type": "paper", "title": "", "section": "Evaluation", "text": "data for the new abnormal subject in a one-step prediction (UCI dataset) In Figure 3-2, the lower panel represents the fine-tuning data, while the upper panel illustrates the real and predicted values for the evaluation data in both SIC and DIC cases. As shown in Figures 3-1 and 3-2, the network demonstrates the ability to predict knee angle data using only EMG signals with very limited fine-tuning data (approximately three walking gait cycles for normal subjects and seven for abnormal subjects). These results highlight the model's capability to be effectively transferred to new subjects across different datasets, reinforcing its adaptability for real-world applications. Additionally, as observed in Figure 3-2, the irregular and non-periodic nature of gait cycles in abnormal subjects is evident. At the end of this section, the last subject (out of 11) was replaced with a different subject to evaluate whether the network's transfer learning performance remains consistent across different new subjects. This experiment evaluated the robustness of the model, ensuring that its effectiveness is not highly sensitive to the specific new subject introduced during fine-tuning. The results of this part are also added in supplementary tables in the GitHub respiratory[18]. The results show that the NMAE of the new subjects in different scenarios does not change more than 2 % in normal and abnormal subjects. It is worth emphasizing that the UCI dataset used in the transfer- learning stage constitutes only a minor portion of the Georgia Tech dataset employed during initial training. Specifically, the training data for the normal subjects account for about 0.27 %, while those for the abnormal subjects account for approximately 1.32 % of the total Georgia Tech training samples as stated in Section 3-3-1. -10.00 0.00 10.00 20.00 30.00 40.00 50.00 60.00 70.00 0 50 100 150 200 Knee Angle (Degrees) Sample (in 100 HZ frequency) Evaluation Data (Original Scale) GroundTruth Predictions (SIC) Predictions (DIC) -20.00 -10.00 0.00 10.00 20.00 30.00 40.00 50.00 60.00 70.00 0 50 100 150 200 Knee Angle (Degrees) Sample (100 HZ) Fine-tuning Data (Original Scale) 3-3-3 Comparing the results with previous work As reviewed in Section 2, very few studies have conducted regression tasks on the UCI dataset. In this section, the results are compared with two primary regression studies on the UCI dataset ([13],[14]). Studies [13] and [14] implemented the SIC and DIC scenarios, respectively, for predicting knee joint angles from EMG signals. Accordingly, the corresponding scenarios were adopted in this work to enable a direct and consistent comparison with their methodologies. However, a direct comparison with these studies is not straightforward due to differences in methodology and evaluation approaches. The MyoNet study[13] performed regression using a CNN-LSTM network in the SIC scenario, employing a K-fold cross-validation method, whereas our work utilizes a leave-one-out validation approach that is more applicable in real-time scenarios. Additionally, in MyoNet, a 256 ms prediction window was used, while in our research, predictions were made for 10 ms and 500 ms windows. To compare our results with the Myonet we also consider a 26-step prediction (260 ms) to be more similar to the", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "22b346afb39432b5cac7a062083560b52b6b37f477cdd7f7ffcb687739c22914"}
{"doc_id": "arxiv:2510.13443#evaluation:part-3", "url": "https://arxiv.org/abs/2510.13443", "anchor": "#evaluation:part-3", "type": "paper", "title": "", "section": "Evaluation", "text": "evaluation approaches. The MyoNet study[13] performed regression using a CNN-LSTM network in the SIC scenario, employing a K-fold cross-validation method, whereas our work utilizes a leave-one-out validation approach that is more applicable in real-time scenarios. Additionally, in MyoNet, a 256 ms prediction window was used, while in our research, predictions were made for 10 ms and 500 ms windows. To compare our results with the Myonet we also consider a 26-step prediction (260 ms) to be more similar to the Myonet. The entire network in the SIC scenario from Section 2.5 is used, except for the final dense layer. A new dense layer with 26 units replaces the original dense layer with 1 unit. Subsequently, transfer learning is applied to adapt the network to the new configuration. To enable a more realistic and direct comparison with MyoNet, an additional evaluation was conducted using a 3-fold cross-validation strategy, aligning with the validation methodology adopted in MyoNet. This experiment specifically targeted the 260 ms prediction horizon, allowing for a consistent and meaningful comparison of performance under similar conditions. Table 3-5 presents a comparative NMAE analysis between our work and the MyoNet, highlighting key differences in methodology and performance. Table 3-5- A comparative NMAE analysis between our work and Myonet (SIC* scenario) [13], [14] Methodogy Our method (leave-one-out scenario**) Our method (3-Fold cross validation***) Myonet Normal subjects 7.4 % in 260 ms prediction 3.4 % in 260 ms prediction 8.1 % in 256 ms prediction Abnormal subjects 9.29% in 260 ms prediction 2.7 % in 260 ms prediction 9.2 % in 256 ms prediction * Single-Input Configuration ** Considering new subjects for the test *** Selecting 3 random data from the whole dataset as validation data As shown in Table 3-5, our method achieves lower error rates compared to MyoNet, even under the more challenging leave-one-out validation scenario, except for the abnormal cases at the 260 ms prediction horizon. Notably, under the 3-fold cross-validation setting, our model significantly outperforms MyoNet, underscoring the effectiveness and optimality of the proposed architecture for knee angle prediction. Furthermore, as emphasized in this section, our study uniquely conducts evaluation on new, unseen subjects, representing a key advancement in the SIC scenario. As shown in Table 3-5, the results obtained under the 3-fold cross-validation scenario are notably better than those from the leave-one-out validation. However, it is important to note that while 3-fold cross-validation offers higher accuracy, it is less representative of real-world deployment, where the model must generalize to completely unseen subjects, as evaluated in the leave-one-out scenario. The study reviewed in Section 2 [14] conducted the DIC scenario on the UCI dataset. Similar to MyoNet, it did not evaluate the model on new subjects. Instead, it utilizes an 80-20 training-validation split for model training in a random shuffling manner. Furthermore, according to the study, preprocessing was performed before segmentation, which is not suitable for real-time applications. Additionally, the model predicted knee angles at 1000 Hz (1 ms prediction intervals), whereas in our study, the shortest prediction horizon was one step ahead (equivalent to a 10 ms prediction interval).To allow", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "4d37387b4d0936c158369963cd7fd8d4156b24975436dca0bfbed7f893dcaf57"}
{"doc_id": "arxiv:2510.13443#evaluation:part-4", "url": "https://arxiv.org/abs/2510.13443", "anchor": "#evaluation:part-4", "type": "paper", "title": "", "section": "Evaluation", "text": "the UCI dataset. Similar to MyoNet, it did not evaluate the model on new subjects. Instead, it utilizes an 80-20 training-validation split for model training in a random shuffling manner. Furthermore, according to the study, preprocessing was performed before segmentation, which is not suitable for real-time applications. Additionally, the model predicted knee angles at 1000 Hz (1 ms prediction intervals), whereas in our study, the shortest prediction horizon was one step ahead (equivalent to a 10 ms prediction interval).To allow a more meaningful comparison with MMF-RR [14], we designed an additional evaluation scenario in which the model predicts 10 ms ahead, and the validation sets are selected using a 3-fold cross- validation approach, consistent with the methodology used in [14]. Table 3-6 presents a comparative NMAE analysis between our approach and the average performance across different subjects from this study, which employed the Multi-Model Fusion-based Ridge Regression (MMF-RR) method (outperformed by several of their different ML and DL methods [14] Table 3-6- A comparative NMAE analysis between our work and MMF-RR (DIC* scenario) [14] Methodogy Our method(leave-one-out**) Our method (3-Fold cross validation***) MMF-RR Normal subjects 3.1 % in 10 ms prediction 1.5% in 10 ms prediction 6.58 % in 1ms prediction Abnormal subjects 3.5% in 10 ms prediction 1.4% in 10 ms prediction 9.18% in 1 ms prediction * Double-Input Configuration ** Considering new subjects for the test *** Selecting 3 random data from the whole dataset as validation data As shown in Table 3-6, the NMAE of our approach surpasses that of MMF-RR, even when predicting 10 ms knee angles and evaluating new subjects, highlighting a significant achievement in this study. Once again, the results under the 3-fold cross-validation setting exhibit improved performance; however, this evaluation strategy is generally considered less reliable for real-world applications compared to leave-one-out validation. 3-3-4 Results for transferring the network to the SMLE dataset It is important to note that the actual human knee angles were not directly available from the dataset. As mentioned in Section 2-6-4 and only the knee joint encoder data from the exoskeleton was accessible. Consequently, the angles provided to the network as inputs and outputs in the different scenarios correspond to the robot’s measurements rather than the subject’s true joint kinematics. In the SML dataset, the interaction forces at the thigh and shank were also measured. The corresponding kinematic and kinetic profiles for Test 7 (the test set) are illustrated in Figure 3-3. -5.00 0.00 5.00 10.00 15.00 20.00 25.00 30.00 35.00 40.00 0 1000 2000 3000 4000 5000 6000 7000 8000 Robot's Knee Angle (Degree) Sample (100 HZ) -10.00 -8.00 -6.00 -4.00 -2.00 0.00 2.00 4.00 0 1000 2000 3000 4000 5000 6000 7000 8000 Shank Interaction Torque (Nm) Sample (100 HZ) Fig. 3-3- the profile of kinematic and kinetic data sample in SMLE dataset As illustrated in Figure 3-3, the kinetic data (shank and thigh interaction torques) are synchronized with the kinematic data (robot knee joint encoder angles). To enhance the interpretability of the dataset, Figure 3-3 depicts the corresponding joint torques at the shank and thigh segments rather", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "2e44105838c16f03daec90990cddef8214ccc6d012e98ed401a89a65e5e973f2"}
{"doc_id": "arxiv:2510.13443#evaluation:part-5", "url": "https://arxiv.org/abs/2510.13443", "anchor": "#evaluation:part-5", "type": "paper", "title": "", "section": "Evaluation", "text": "-6.00 -4.00 -2.00 0.00 2.00 4.00 0 1000 2000 3000 4000 5000 6000 7000 8000 Shank Interaction Torque (Nm) Sample (100 HZ) Fig. 3-3- the profile of kinematic and kinetic data sample in SMLE dataset As illustrated in Figure 3-3, the kinetic data (shank and thigh interaction torques) are synchronized with the kinematic data (robot knee joint encoder angles). To enhance the interpretability of the dataset, Figure 3-3 depicts the corresponding joint torques at the shank and thigh segments rather than the raw interaction forces. The torques were obtained by multiplying each measured force by its respective moment arm distance from the corresponding robot joint. The data collected in the SMLE dataset were originally sampled at 1000 Hz; however, to ensure consistency with the preprocessed data used for network training, all signals were resampled to 100 Hz, and Figure 3-3 illustrates the results at this normalized frequency. As described in Section 2.6.5, the DML dataset is divided into training, validation, and test subsets as UCI dataset in Section 2-6-3. It is worth emphasizing that the SMLE dataset used in the transfer-learning stage represents only a small fraction of the Georgia Tech dataset utilized during the initial training phase. As stated in Section 3-3-1, the SMLE dataset contains approximately 3.1 % of the total source-domain data. The network was trained in two stages: initially on the training data, followed by fine-tuning on the designated fine-tuning subset. The outcomes for the four scenarios SIC, SIC-Forces, DIC, and DIC-Forces in the second stage of transfer-learning are presented in Tables 3-8 to 3-11, respectively, and include only the evaluation results. The results of the first-stage training and the fine-tuning data of the second stage are provided in the supplementary tables in the GitHub respiratory[18]. Table 3-7- Network transfer to the SMLE dataset (one-step prediction) Scenario", "source": "arxiv_pdf", "published": "", "tokens": 301, "sha256": "f38f41e0c20a33080df415194b3d4d6adb72ac7963d0e6da852afa67bac1fe2d"}
{"doc_id": "arxiv:2510.13443#evaluation", "url": "https://arxiv.org/abs/2510.13443", "anchor": "#evaluation", "type": "paper", "title": "", "section": "Evaluation", "text": "R2 SIC* 0.1392 0.1101 0.6728 SIC + Interaction Forces 0.0976 0.0758 0.8393 DIC** 0.0171 0.0126 0.9951 DIC + Interaction Forces 0.0142 0.0109 0.9966 -5.00 -4.00 -3.00 -2.00 -1.00 0.00 1.00 2.00 3.00 4.00 5.00 0 1000 2000 3000 4000 5000 6000 7000 8000 Thigh Interaction Torque (Nm) Sample (100 HZ) Table 3-8- Network transfer to the SMLE dataset ((50-step prediction)) Scenario", "source": "arxiv_pdf", "published": "", "tokens": 61, "sha256": "6871c0576638f81ba0f96716a0f9e276f61fb9c433f6e3638ec0ff0da3ef6964"}
{"doc_id": "arxiv:2510.13443#5-conclusion:part-1", "url": "https://arxiv.org/abs/2510.13443", "anchor": "#5-conclusion:part-1", "type": "paper", "title": "", "section": "5 Conclusion", "text": "This paper presents a transfer learning-based approach for predicting knee joint angles using EMG data, with the goal of facilitating rehabilitation applications. The target of the paper is to leverage information from an extensive dataset (Georgia-Tech), which is typically available for normal subjects, to enhance predictive performance on a clinically valuable dataset (UCI dataset) containing rare abnormal subject data with knee pathologies. To address the challenge of limited abnormal subject data, the proposed method eliminates the need for data augmentation, which is often not appropriate for abnormal cases due to the uniqueness and variability of pathological conditions. The proposed transfer-learning framework was also validated on robot-assisted laboratory experiments conducted using a lower-limb exoskeleton, demonstrating its robustness in practical scenarios. This highlights the method’s applicability to real-time rehabilitation environments, where marker-based motion capture systems, IMUs, and other external sensors may not be available for direct measurement of human joint angles An attention-based CNN- LSTM network was initially trained using only EMG data from the Georgia Tech dataset (source dataset). Subsequently, the transfer learning was applied within the same dataset, integrating knee angle data. This strategy avoids overfitting kinematic inputs, ensuring the network maintains its generalization, which is one of the key distinctions of this study compared to previous works. Then, the transfer-learning approach was conducted on the target datasets (UCI and SMLE dataset) in two stages: Adaptation to the new conditions and fine-tuning to new subjects (in SMLE performed on a new test from the same subject), where the fine-tuning data consisted of only 3 to 7 gait cycles per test, which is practical in real scenarios. For the UCI dataset, the model achieved an NMAE of 6.8% and 13.7% for abnormal subjects in one-step and 50-step predictions, respectively. While for normal subjects, the corresponding NMAE values were 7.2% and 15.6%, respectively. These results were achieved using only EMG as the network input (SIC scenario), distinguishing this study by demonstrating exceptional predictive accuracy with minimal fine-tuning data. In the SMLE dataset, where complex human-robot interaction dynamics are present, accurate knee-angle prediction was not attainable under the SIC configuration. incorporating of the DIC configuration and interaction forces from the shank and thigh significantly enhanced the model’s predictive capability, achieving an NMAE of 1.09 % for one-step prediction and 3.1 % for 50-step prediction, thereby demonstrating the effectiveness of the proposed transfer-learning framework under realistic exoskeleton conditions involving dynamic force exchange between the human and the robot. The overall findings across both datasets highlight the proposed network’s strong potential for real-time deployment in assistive and rehabilitation scenarios. further investigations will focus on extending the transfer-learning framework to abnormal subjects performing walking tasks with the exoskeleton . 6 References [1] J. R. Cram, G. S. Kasman, and J. Holtz, “Introduction to surface electromyography,” (No Title), 1998. [2] C. J. De Luca, “Surface Electromyography: Detection and Recording. DelSys Incorporated (2002),” 2002, Forschungsbericht. [3] D. Farina, R. Merletti, and R. M. Enoka, “The extraction of neural strategies from the surface EMG,” J Appl Physiol, vol. 96, no. 4, pp. 1486–1495, 2004. [4] P. Konrad, “The ABC of EMG:", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "db74ee8cee3a1e845247ccdd6ac5724f42ca685a341fa318035e272136125c2d"}
{"doc_id": "arxiv:2510.13443#5-conclusion:part-2", "url": "https://arxiv.org/abs/2510.13443", "anchor": "#5-conclusion:part-2", "type": "paper", "title": "", "section": "5 Conclusion", "text": "framework to abnormal subjects performing walking tasks with the exoskeleton . 6 References [1] J. R. Cram, G. S. Kasman, and J. Holtz, “Introduction to surface electromyography,” (No Title), 1998. [2] C. J. De Luca, “Surface Electromyography: Detection and Recording. DelSys Incorporated (2002),” 2002, Forschungsbericht. [3] D. Farina, R. Merletti, and R. M. Enoka, “The extraction of neural strategies from the surface EMG,” J Appl Physiol, vol. 96, no. 4, pp. 1486–1495, 2004. [4] P. Konrad, “The ABC of EMG: A practical introduction to kinesiological electromyography,” 2005. [5] L. M. Vaca Benitez, M. Tabie, N. Will, S. Schmidt, M. Jordan, and E. A. Kirchner, “Exoskeleton Technology in Rehabilitation: Towards an EMG‐Based Orthosis System for Upper Limb Neuromotor Rehabilitation,” Journal of Robotics, vol. 2013, no. 1, p. 610589, 2013. [6] M. Jacquelin Perry, “Gait analysis: normal and pathological function,” New Jersey: SLACK, 2010. [7] D. A. Neumann, “Kinesiology of the musculoskeletal system,” St. Louis: Mosby, pp. 25–40, 2002. [8] P. Konrad, “The abc of emg,” A practical introduction to kinesiological electromyography, vol. 1, no. 2005, pp. 30– 35, 2005. [9] R. Merletti and P. J. Parker, Electromyography: physiology, engineering, and non-invasive applications, vol. 11. John Wiley & Sons, 2004. [10] C. J. De Luca, “Surface electromyography: Detection and recording,” DelSys Incorporated, vol. 10, no. 2, pp. 1–10, 2002. [11] D. Xiong, D. Zhang, X. Zhao, and Y. Zhao, “Deep learning for EMG-based human-machine interaction: A review,” IEEE/CAA Journal of Automatica Sinica, vol. 8, no. 3, pp. 512–533, 2021. [12] S. M. Moghadam, T. Yeung, and J. Choisne, “A comparison of machine learning models’ accuracy in predicting lower-limb joints’ kinematics, kinetics, and muscle forces from wearable sensors,” Sci Rep, vol. 13, no. 1, p. 5046, 2023. [13] A. Gautam, M. Panwar, D. Biswas, and A. Acharyya, “MyoNet: A transfer-learning-based LRCN for lower limb movement recognition and knee joint angle prediction for remote monitoring of rehabilitation progress from sEMG,” IEEE J Transl Eng Health Med, vol. 8, pp. 1–10, 2020. [14] J. Han, H. Wang, and Y. Tian, “Multi Model Fusion Based Prediction of Human Joint Angles Using sEMG and Historical Angles,” in 2023 42nd Chinese Control Conference (CCC), IEEE, 2023, pp. 3174–3179. [15] J. Zhang et al., “Physics-informed deep learning for musculoskeletal modeling: Predicting muscle forces and joint kinematics from surface EMG,” IEEE Transactions on Neural Systems and Rehabilitation Engineering, vol. 31, pp. 484–493, 2022. [16] A. Vijayvargiya, V. Gupta, R. Kumar, N. Dey, and J. M. R. S. Tavares, “A hybrid WD-EEMD sEMG feature extraction technique for lower limb activity recognition,” IEEE Sens J, vol. 21, no. 18, pp. 20431–20439, 2021. [17] A. Vijayvargiya, Khimraj, R. Kumar, and N. Dey, “Voting-based 1D CNN model for human lower limb activity recognition using sEMG signal,” Phys Eng Sci Med, vol. 44, no. 4, pp. 1297–1309, 2021. [18] “https://github.com/MojtabaHosseiniie/.” [19] M. W. Maciejewski, H. Z. Qui, I. Rujan, M. Mobli, and J. C. Hoch, “Nonuniform sampling and spectral aliasing,” Journal of Magnetic Resonance, vol. 199, no. 1, pp. 88–93, 2009. [20] J. Camargo, A. Ramanathan, W. Flanagan, and A. Young, “A comprehensive, open-source dataset of lower", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "8427160743fbc92f01995e873d61d100211286306086391a7ed374338d116a33"}
{"doc_id": "arxiv:2510.13443#5-conclusion:part-3", "url": "https://arxiv.org/abs/2510.13443", "anchor": "#5-conclusion:part-3", "type": "paper", "title": "", "section": "5 Conclusion", "text": "Vijayvargiya, Khimraj, R. Kumar, and N. Dey, “Voting-based 1D CNN model for human lower limb activity recognition using sEMG signal,” Phys Eng Sci Med, vol. 44, no. 4, pp. 1297–1309, 2021. [18] “https://github.com/MojtabaHosseiniie/.” [19] M. W. Maciejewski, H. Z. Qui, I. Rujan, M. Mobli, and J. C. Hoch, “Nonuniform sampling and spectral aliasing,” Journal of Magnetic Resonance, vol. 199, no. 1, pp. 88–93, 2009. [20] J. Camargo, A. Ramanathan, W. Flanagan, and A. Young, “A comprehensive, open-source dataset of lower limb biomechanics in multiple conditions of stairs, ramps, and level-ground ambulation and transitions,” J Biomech, vol. 119, p. 110320, 2021. [21] O. F. A. Sanchez, J. L. R. Sotelo, M. H. Gonzales, and G. A. M. Hernandez, “Emg dataset in lower limb data set,” UCI machine learning repository, vol. 2, 2014. [22] F. H. Daryakenari, M. Mollahossein, A. Taheri, and G. R. Vossoughi, “Classification of lower limb electromyographical signals based on autoencoder deep neural network transfer learning,” in 2022 10th RSI International Conference on Robotics and Mechatronics (ICRoM), IEEE, 2022, pp. 323–328. [23] A. Vijayvargiya, C. Prakash, R. Kumar, S. Bansal, and J. M. R. S. Tavares, “Human knee abnormality detection from imbalanced sEMG data,” Biomed Signal Process Control, vol. 66, p. 102406, 2021. [24] A. Altıntaş and D. Yılmaz, “Classification of Knee Abnormality Using sEMG Signals with Boosting Ensemble Approaches,” Computer Science, no. Special, pp. 48–52, 2021. [25] X. Zhao, L. Wang, Y. Zhang, X. Han, M. Deveci, and M. Parmar, “A review of convolutional neural networks in computer vision,” Artif Intell Rev, vol. 57, no. 4, p. 99, 2024. [26] I. D. Mienye, T. G. Swart, and G. Obaido, “Recurrent neural networks: A comprehensive review of architectures, variants, and applications,” Information, vol. 15, no. 9, p. 517, 2024. [27] Z. Dai, D. Li, and S. Feng, “Attention Mechanism with Spatial‐Temporal Joint Deep Learning Model for the Forecasting of Short‐Term Passenger Flow Distribution at the Railway Station,” J Adv Transp, vol. 2024, no. 1, p. 7985408, 2024. [28] J. A. Raj, L. Qian, and Z. Ibrahim, “Fine-tuning--a Transfer Learning approach,” arXiv preprint arXiv:2411.03941, 2024.", "source": "arxiv_pdf", "published": "", "tokens": 345, "sha256": "dfeb1066ecf8e805db36ce7913abc8737c98b4a644f64016e13933af305512b1"}
{"doc_id": "arxiv:2510.13287#method", "url": "https://arxiv.org/abs/2510.13287", "anchor": "#method", "type": "paper", "title": "", "section": "Method", "text": "Absolute pose error [m] Relative pose error [m] Mean Max RMSE Stdev. Mean Max RMSE Stdev. KISS-ICP [4] 6.83 19.05 8.72 5.41 0.10 0.94 0.14 0.10 CT-ICP [6] 44.18 60.14 45.66 11.55 0.19 7.15 0.68 0.65 DLO [29] 7.69 27.99 9.09 4.86 0.26 22.74 1.32 1.29 Point-to-point ICP [30] 6.83 19.05 8.72 5.41 0.10 0.94 0.14 0.10 Point-to-plane ICP [31] 32.84 40.88 33.16 4.55 0.13 12.50 0.68 0.67 GENZ-ICP [10] 1.69 4.32 1.99 1.04 0.06 0.73 0.09 0.07 Ours 1.47 4.08 1.72 0.89 0.12 0.97 0.17 0.13 TABLE II: Quantitative results for the Corridor1 and Corridor2 sequences of the Ground-Challenge dataset [32]. Some comparison results have been reproduced from [10] Sequence", "source": "arxiv_pdf", "published": "", "tokens": 111, "sha256": "bb6462ea9fce21e83caff0b6f846bd53a27bca91ebc658ff10c4eaaa0c112a9c"}
{"doc_id": "arxiv:2510.13324#results:part-1", "url": "https://arxiv.org/abs/2510.13324", "anchor": "#results:part-1", "type": "paper", "title": "", "section": "results", "text": "show that high-dimensional force- distribution information significantly enhances task success, particularly in tasks requiring nuanced force interactions. By capturing both shear and normal forces, FARM provides superior control and contact state awareness −15 −10 −5 0 Force (N) 0.0 0.1 0.2 0.3 0.4 0.5 Weighted Density Demos FARM (ours) −15 −10 −5 0 Force (N) 0.0 0.1 0.2 0.3 0.4 0.5 Weighted Density Demos Force-Aware −15 −10 −5 0 Force (N) 0.0 0.1 0.2 0.3 0.4 0.5 Weighted Density Demos Tactile-Aware −15 −10 −5 0 Force (N) 0.0 0.1 0.2 0.3 0.4 0.5 Weighted Density Demos Vision-Only Fig. 6: Weighted empirical force distributions for the screw tightening task, comparing demonstrations and rollouts for the FARM framework against force-aware, tactile-aware, and vision-only baselines. compared to scalar force signals or raw tactile images. This advantage is most pronounced in tasks where temporal and multi-dimensional force dynamics are critical. 3) How do these approaches compare in the force domain for the time-dependent task of screw tightening?: To answer this question, we compare FARM in the force domain against the force-aware, tactile-aware, and vision-only baselines in the time-dependent screw tightening task. To quantify how closely the applied forces from each approach match those of the demonstrations, we use the Wasserstein-1 distance W1 [35]. Since trajectories differ in duration, simply pooling all samples would bias the statistics toward longer trajectories. For that reason, we use an equal-mass weighting scheme, ensuring that each demonstration or policy rollout contributes equally regardless of trajectory length. For trajectory m with nm samples, each sample receives the weight wk,m = 1/M · nm, where M is the number of trajectories. The W1 distance is then defined as W1(u, v) = inf π∈Γ(u,v) Z R×R |x −y|dπ(x, y), (1) for weighted empirical force distributions of demonstrations u and policy rollouts v. The W1 value corresponds to the average amount the force distributions from one set would need to be shifted to match the other, expressed in the same unit as the force (N). Smaller values, therefore, indicate that the forces measured during rollouts are more similar to those of the demonstrations, while larger values indicate greater differences in either magnitude or distributional shape. As shown in Fig. 6, FARM’s weighted empirical force distributions closely align with those of the demonstrations. This is also represented by the Wasserstein-1 distance of 0.7538 N. The force-aware baseline follows with a distance of 1.6580 N, showing reasonable, but less precise force matching. Although the force-aware baseline was able to produce similar forces, it nevertheless failed in the task success because it lacked the necessary contact state in- formation from just a single force value. The tactile-aware baseline, with a Wasserstein-1 distance of 4.3801 N, tends to apply insufficient forces, while the vision-only baseline, at 5.0515 N, consistently applies excessive forces, leading to significant deviations from the demonstration force profiles. These results show that incorporating force distributions into the observation embedding enables FARM to outperform the baselines in the force domain. The larger W1 distances for force-aware, tactile-aware, and vision-only baselines reflect their limitations in capturing the complex,", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "89cf99d83a55ce2c5f9aa9c727bbdfaf4f4179efe48dc497ad63709c4b83c249"}
{"doc_id": "arxiv:2510.13324#results:part-2", "url": "https://arxiv.org/abs/2510.13324", "anchor": "#results:part-2", "type": "paper", "title": "", "section": "results", "text": "formation from just a single force value. The tactile-aware baseline, with a Wasserstein-1 distance of 4.3801 N, tends to apply insufficient forces, while the vision-only baseline, at 5.0515 N, consistently applies excessive forces, leading to significant deviations from the demonstration force profiles. These results show that incorporating force distributions into the observation embedding enables FARM to outperform the baselines in the force domain. The larger W1 distances for force-aware, tactile-aware, and vision-only baselines reflect their limitations in capturing the complex, time-varying force profiles required for precise screw tightening. V. CONCLUSION This work investigated how tactile sensing can be in- corporated not only as an observation modality but also directly into the action space of imitation learning policies for contact-rich manipulation. We thus introduced FARM, a visuotactile-conditioned diffusion policy that predicts target grip width and target grip force jointly with the robot pose. Within FARM, the tactile-conditioned contact information is used both as an observation and as a basis for generating force-based action sequences. Demonstrations were collected using an adapted hand-held UMI gripper equipped with a GelSight Mini sensor at one fingertip, with normal and shear force distributions estimated via the FEATS model. To enable policy transfer, we developed the Actuated UMI gripper, whose geometry and kinematics match the hand-held UMI gripper. We executed gripper actions through a dual-mode controller that switches between grip width position control and closed-loop force control. The real-world experimental results and comparison with several baselines show the effectiveness of our proposed method and underline the importance of both leveraging a force-based action space, as well as physically-grounded, high-dimensional tactile infor- mation as an observation modality. In addition to improved reliability and success rates, our proposed method also aligns more closely with the demonstration data w.r.t. the applied forces. Future work should explore generalizing the method to bimanual manipulation, incorporating anthropomorphic hands, and employing a flow-matching objective to enhance the reactivity of the learned policies.", "source": "arxiv_pdf", "published": "", "tokens": 318, "sha256": "2a01e9bc9bd2a76025f72b1d8efee49b9980f9f033575b4399cde5ccc4b3290a"}
{"doc_id": "arxiv:2510.13626#abstract", "url": "https://arxiv.org/abs/2510.13626", "anchor": "#abstract", "type": "paper", "title": "", "section": "Abstract", "text": "Visual–Language–Action (VLA) models report impressive success rates on robotic manip- ulation benchmarks, yet these results may mask fundamental weaknesses in robustness. We perform a systematic vulnerability analysis by introducing controlled perturbations across seven dimensions: objects layout, camera viewpoints, robot initial states, language instructions, light conditions, background textures and sensor noise. We comprehensively an- alyzed multiple state-of-the-art models and revealed consistent brittleness beneath apparent competence. Our analysis exposes critical weaknesses: models exhibit extreme sensitivity to perturbation factors, including camera viewpoints and robot initial states, with performance dropping from 95% to below 30% under modest perturbations. Surprisingly, models are largely insensitive to language variations, with further experiments revealing that models tend to ignore language instructions completely. Our findings challenge the assumption that high benchmark scores equate to true competency and highlight the need for evaluation practices that assess reliability under realistic variation. 1", "source": "arxiv_pdf", "published": "", "tokens": 141, "sha256": "40bb1734d41f111279444fa6e37306133c90d3e0722a75e3644e689b98f4bf03"}
{"doc_id": "arxiv:2510.13626#introduction:part-1", "url": "https://arxiv.org/abs/2510.13626", "anchor": "#introduction:part-1", "type": "paper", "title": "", "section": "Introduction", "text": "Recent advances in Visual–Language–Action (VLA) models have led to impressive performance on standard- ized benchmarks, with many systems achieving near-perfect success rates on tasks in controlled simulation environments (Kim et al., 2024; 2025; Li et al., 2025; Black et al.; Pertsch et al., 2025; Hung et al., 2025; Cen et al., 2025; Tan et al., 2025). However, these headline numbers often conceal critical deficiencies in the underlying models. In fact, a closer inspection reveals that contemporary VLA systems tend to exhibit a fragile robustness, struggling to maintain performance when faced with even minor variations in environmental conditions or task parameters. The prevailing evaluation methodologies (Liu et al., 2023; Li et al., 2024c) focus on aggregate success rates under static, ideal conditions. While such metrics provide valuable baselines for comparing different approaches, they fail to capture the stability and reliability of learned policies under realistic variations. This approach tends to obscure the models’ inability to handle subtle variations that are intrinsic to any realistic task setting (Wang et al., 2025; Müller, 2019; Zhang et al., 2024)—even if those tasks remain within the realm of simulation. For example, models trained to excel under fixed camera angles or consistent illumination often fail to generalize when confronted with slight shifts in viewpoint or minor changes in the robot’s initial configuration. This gap is especially problematic for VLA models, which must integrate information across multiple modalities and maintain coherent behavior despite perturbations in any of these input channels. †Joint First Authors, ‡Joint Second Authors, ∗Project Lead. BCorresponding Authors. 1 arXiv:2510.13626v1 [cs.RO] 15 Oct 2025 LIBERO-Plus: In-depth Robustness Analysis of Vision-Language-Action Models To uncover these hidden vulnerabilities, we conduct a comprehensive analysis of contemporary VLA models using the LIBERO (Liu et al., 2023) benchmark as a diagnostic tool. By systematically varying key factors such as camera viewpoints, robot initial states, language instructions, light conditions, background textures, sensor noise, and object layout, we expose the brittle nature of these models. Our analysis shows that even nominal modifications can lead to steep drops in performance. This indicates that, rather than achieving true multimodal understanding, current VLA architectures rely on overfitting to specific, narrowly defined cues provided during training. Our study highlights several core weaknesses in contemporary VLA models: Vulnerability to Visual Shifts: an over-reliance on fixed visual features leads to failure under variations in camera angle or illumination; Inadequate Kinematic Reasoning: limited generalization across different initial robot configurations reflects a lack of deep kinematic understanding; Superficial Language Interaction: linguistic inputs are often underutilized or even completely ignored, as shown by the minimal impact of instruction variation. Through this work, we provide: 1. A detailed vulnerability analysis of current VLA models through systematic parameter variation. 2. A diagnostic framework for identifying and quantifying the impact of perturbations on model performance. 3. Critical insights into the mismatch between apparent multimodal competence and actual robust understanding. Our findings challenge the assumption that high benchmark scores equate to true competency, urging the community to re-evaluate current evaluation practices and focus on building models that are robust in the face of inherent variability. This work", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "aca92e61127b43f567fb3834daffb4e15e03cb610532e89fe4c0d9ef41c30676"}
{"doc_id": "arxiv:2510.13626#introduction:part-2", "url": "https://arxiv.org/abs/2510.13626", "anchor": "#introduction:part-2", "type": "paper", "title": "", "section": "Introduction", "text": "provide: 1. A detailed vulnerability analysis of current VLA models through systematic parameter variation. 2. A diagnostic framework for identifying and quantifying the impact of perturbations on model performance. 3. Critical insights into the mismatch between apparent multimodal competence and actual robust understanding. Our findings challenge the assumption that high benchmark scores equate to true competency, urging the community to re-evaluate current evaluation practices and focus on building models that are robust in the face of inherent variability. This work is a step toward developing VLA systems that are not only high-performing but also genuinely reliable and adaptable. 2 How Do Single-Dimension Perturbations Affect VLA Models? 2.1 Perturbation Factors We systematically evaluate how different perturbation factors affect VLA performance and study seven common single-dimension perturbations applied to the evaluation episodes: (1) Objects Layout: add confounding objects and/or shift the target object’s position. (2) Camera Viewpoints: change the viewpoint/pose and field-of-view of the third-person camera. (3) Robot Initial States: change the manipulator’s initial pose. (4) Language Instructions: rewrite task instructions to increase linguistic richness and complexity. (5) Light Conditions: vary illumination intensity, direction, color, and shadow patterns. (6) Background Textures: modify table/scene textures and materials. (7) Sensor Noise: inject photometric distortions (e.g., jitter, Gaussian blur) into input images. Full per-factor specifications are provided in Appendix A. 2.2 Models We analyze a series of representative open-checkpoint models spanning diverse architectures (autoregressive vs. diffusion-based) and training paradigms (web-data co-training, world modeling, reinforcement learning, etc): (1) OpenVLA (Kim et al., 2024) and its variants (2) OpenVLA-OFT (Kim et al., 2025), (3) OpenVLA-OFT_w (third-view-only version), (4) OpenVLA-OFT_m (mix-sft version, trained on all 4 suites), (5) π0 (Black et al.), (6) π0-fast (Pertsch et al., 2025), (7) Nora (Hung et al., 2025), (8) WorldVLA (Cen et al., 2025), (9) UniVLA (Bu et al., 2025) and (10) RIPT-VLA (Brohan et al., 2022). Please refer to Appendix B for further details. 2.3", "source": "arxiv_pdf", "published": "", "tokens": 314, "sha256": "adba80ae4bdbb7cfaaed783895525cf4961739439b90aa409847ccb7b60a9e99"}
{"doc_id": "arxiv:2510.13626#results", "url": "https://arxiv.org/abs/2510.13626", "anchor": "#results", "type": "paper", "title": "", "section": "Results", "text": "We present the main experimental results in Table 1 and Figure 1, which collectively reveal a significant fragility in the generalization capabilities of current VLAs. As shown, even minor perturbations can lead to drastic performance degradation. Below we analyze the specific robustness patterns across perturbation dimensions, models, and tasks. 2 LIBERO-Plus: In-depth Robustness Analysis of Vision-Language-Action Models Table 1: Model performance under different perturbations. For each model, the first row reports the task success rate (%) under each perturbation dimension, with the \"Original\" column indicating the performance on unperturbed inputs. The second row (denoted by ↓) shows the corresponding absolute performance drop. The results highlight significant variations in robustness across models and perturbation types. Original Camera Robot Language Light", "source": "arxiv_pdf", "published": "", "tokens": 119, "sha256": "a813e3a01c1dcc63eb095df23f46d69d3ee4637567069ec69d1671c3171ccba2"}
{"doc_id": "arxiv:2510.13626#background:part-1", "url": "https://arxiv.org/abs/2510.13626", "anchor": "#background:part-1", "type": "paper", "title": "", "section": "Background", "text": "Noise Layout OpenVLA 76.5 1.1 4.1 26.8 4.4 25.3 19.3 31.6 ↓75.4 ↓72.4 ↓49.7 ↓72.1 ↓51.2 ↓57.2 ↓44.9 OpenVLA-OFT 97.1 59.7 37.2 81.5 85.8 92.4 76.7 77.1 ↓37.4 ↓59.9 ↓15.6 ↓11.3 ↓4.7 ↓20.4 ↓20.0 OpenVLA-OFT_w 95.3 16.8 43.7 73.2 68.2 92.5 51.4 72.3 ↓78.5 ↓51.6 ↓22.1 ↓27.1 ↓2.8 ↓43.9 ↓23.0 OpenVLA-OFT_m 97.6 57.9 30.6 83.6 91.6 83.6 76.3 73.2 ↓39.7 ↓67.0 ↓14.0 ↓6.0 ↓14.0 ↓21.3 ↓24.4 π0 94.2 15.8 6.6 61.0 79.6 78.5 79.4 70.4 ↓78.4 ↓87.6 ↓33.2 ↓14.6 ↓15.7 ↓14.8 ↓23.8 π0-fast 85.5 66.4 24.8 63.3 73.0 67.7 75.8 70.3 ↓19.1 ↓60.7 ↓22.2 ↓12.5 ↓17.8 ↓9.7 ↓15.2 Nora 87.9 4.0 41.1 67.0 31.0 50.5 17.6 63.9 ↓83.9 ↓46.8 ↓20.9 ↓56.9 ↓37.4 ↓70.3 ↓24.0 WorldVLA 79.1 0.3 30.2 44.2 29.4 14.5 12.2 39.4 ↓78.8 ↓48.9 ↓34.9 ↓49.7 ↓64.6 ↓66.9 ↓39.7 UniVLA 95.2 4.3 50.3 71.8 59.1 80.0 25.3 34.3 ↓90.9 ↓44.9 ↓23.4 ↓36.1 ↓15.2 ↓69.9 ↓60.9 RIPT-VLA 97.5 58.3 36.7 80.1 87.9 90.4 73.8 76.5 ↓39.2 ↓60.8 ↓17.4 ↓9.6 ↓7.1 ↓23.7 ↓21.0 Finding 1: Significant Overall Fragility to Perturbations Across all perturbation factors, current VLAs exhibit brittle generalization. Performance degrades significantly under various input perturbations, particularly with changes in camera viewpoint and robot initial state. Finding 2: Robustness varies considerably by perturbation type. Models are most vulnerable to changes in camera viewpoint and robot initial state, which require a high-level understanding of spatial geometry and proprioception. In contrast, they show relative resilience to lighting and background variations, which constitute more superficial, low-level visual changes. Finding 3: Minor Impact of Language Perturbation. Contrary to expectations, language perturbations result in the second smallest average performance drop (-25.3) across most models. This apparent robustness is counter-intuitive and merits deeper investigation. As we explore in Section 4, this phenomenon is unlikely to stem from superior linguistic generalization. A more plausible hypothesis, which we have proven empirically, is that models may be relying less on the language instruction than anticipated, potentially leveraging task cues from the visual context. Finding 4: Model robustness is dictated by architecture and training paradigm. Specifically, models incorporating a first-person wrist camera (e.g., OpenVLA-OFT) demonstrate superior generalization, especially to camera viewpoint changes, compared to those reliant solely on a third-person view (e.g., OpenVLA-OFT_w). Furthermore, training strategies that emphasize diversity and co-training (e.g., π0,π0-fast ) consistently yield more robust models across multiple perturbation types, highlighting the importance of exposure to varied data distributions. 3 Do contemporary VLA Models truly pay attention to visual inputs? While the overall trends reveal substantial fragility, we observe two particularly interesting patterns in the data: (1) models exhibit surprising resilience to background changes, and (2) several models show limited 3 LIBERO-Plus: In-depth Robustness Analysis of Vision-Language-Action Models sensitivity to light variations. These observations raise important questions about what representations the models are actually learning. Do they genuinely understand task-relevant object semantics, or are they relying on superficial visual cues? To answer these questions, we conduct finer-grained analyses of object layout and illumination robustness. NORA OpenVLA OFT 0 0-Fast RIPT-VLA UniVLA WorldVLA 0 20 40 60 80 Accuracy (%) Confounding Displacement Overall Figure 1: Robustness to object layout perturbations. Comparison", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "e4d83223314628ed38a6ddad4ac0b7d67981e06922d76640a270a8c9d0738893"}
{"doc_id": "arxiv:2510.13626#background:part-2", "url": "https://arxiv.org/abs/2510.13626", "anchor": "#background:part-2", "type": "paper", "title": "", "section": "Background", "text": "3 LIBERO-Plus: In-depth Robustness Analysis of Vision-Language-Action Models sensitivity to light variations. These observations raise important questions about what representations the models are actually learning. Do they genuinely understand task-relevant object semantics, or are they relying on superficial visual cues? To answer these questions, we conduct finer-grained analyses of object layout and illumination robustness. NORA OpenVLA OFT 0 0-Fast RIPT-VLA UniVLA WorldVLA 0 20 40 60 80 Accuracy (%) Confounding Displacement Overall Figure 1: Robustness to object layout perturbations. Comparison of different models under confounding and displacement perturbations, as well as their overall robustness. Do Models Genuinely Attend to Task-Relevant Objects? We are pleasantly surprised to observe that the models are relatively insensitive to changes in the Background setting. To further investigate whether the models truly focus on the core interac- tive objects and genuinely understand the high-level semantics and spatial information relevant to the task, we decomposed the Object Layout perturbation into two subcategories: (1) adding confounding objects, and (2) changing the placement and pose of the tar- get objects. We then evaluated all models under these conditions, and the results are shown in Fig- ure 1. It can be seen that for π0, π0-Fast, RIPT-VLA, UniVLA, and WorldVLA, the success rate decreases only marginally when confounding objects are added, indicating that these models, through training, indeed manage to focus their attention on the target objects. However, when the target objects’ placement is altered, the performance of the models drops significantly, suggesting that the current models may have merely learned the positional information of the target objects rather than truly capturing the high-level task-relevant semantics. Origin Light 3rd Black All Black 0 20 40 60 80 100 Accuracy (%) 97.1 86.5 43.6 0.0 94.2 79.3 43.0 2.0 85.5 73.1 67.3 0.0 OpenVLA-OFT -Fast Figure 2: Illumination robustness and extreme ablation tests. The term Light denotes the condition with light perturbation applied. 3rd Black and All Black rep- resent conditions where only the third-view image is masked and where images from both views are masked, respectively. How Do Models Maintain Performance Under Il- lumination Changes? We observe that for several models, the performance drop under light perturba- tions is limited to around 10 points, suggesting a surprising insensitivity to illumination changes. To investigate this phenomenon, we design an extreme ablation test: (i) all-black, where all camera inputs are replaced with black frames, and (ii) 3rd-black, where only the third-person view is masked while the wrist camera is preserved. In the all-black con- dition, performance collapses to nearly zero across models, confirming a strong reliance on visual input. In contrast, under the 3rd-black setting, the same models still achieve accuracies of 43.6, 43.0, and 67.3, respectively, demonstrating that the wrist view alone provides critical and stable close-range geo- metric and contact cues. This explains why standard light perturbations cause only minor degradation: il- lumination changes primarily affect the third-person view and global appearance, whereas the wrist view remains relatively stable. Consistently, models such as OpenVLA, Nora, and WorldVLA—which depend exclusively on third-person observations—suffer se- vere drops under light perturbations (often exceeding 60 points).", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "ca8e736cdcb96d6a5e036d3203d772780cc2f46b395aa38660e3fb613c58632c"}
{"doc_id": "arxiv:2510.13626#background:part-3", "url": "https://arxiv.org/abs/2510.13626", "anchor": "#background:part-3", "type": "paper", "title": "", "section": "Background", "text": "setting, the same models still achieve accuracies of 43.6, 43.0, and 67.3, respectively, demonstrating that the wrist view alone provides critical and stable close-range geo- metric and contact cues. This explains why standard light perturbations cause only minor degradation: il- lumination changes primarily affect the third-person view and global appearance, whereas the wrist view remains relatively stable. Consistently, models such as OpenVLA, Nora, and WorldVLA—which depend exclusively on third-person observations—suffer se- vere drops under light perturbations (often exceeding 60 points). Based on our deeper investigation into object layout and illumination robustness, we can conclude: Finding 5: Current VLAs exhibit positional bias rather than genuine semantic understanding of objects. While models demonstrate an ability to ignore distracting objects, they fail to generalize when target objects are displaced, indicating that they rely on memorized positional cues rather than learning invariant object semantics. 4 LIBERO-Plus: In-depth Robustness Analysis of Vision-Language-Action Models Spatial Object Goal Long 0 20 40 60 80 100 Accuracy (a) Instruction Retained (light) vs. Removed (dark) Object-Var 0 20 40 60 80 100 (b) Modified Target OpenVLA-OFT NORA WorldVLA UniVLA 0 0-Fast Figure 3: Accuracy of different models on instruction removed (a) and target modified (b) tasks. Light bars: original success rate with language instruction; (a) dark bars: success rate after removing the instruction; (b) Dark bars: success rate under altered task goal and instruction (task substitution). Finding 6: Wrist cameras provide critical robustness to illumination changes. The relative stability of performance under light perturbations is largely attributable to the wrist camera’s close-range perspective, which provides illumination-invariant geometric cues. Models lacking wrist-camera inputs show significantly greater vulnerability to lighting variations. 4 Do Contemporary VLA Models Truly Follow Language Instructions? In the experiments presented in Section 2, we observed an intriguing phenomenon: when introducing language perturbations, the overall performance of the OpenVLA-OFT model was barely affected and remained close to the baseline level. To further investigate the potential underlying reasons, we propose the following three hypotheses: (1) The model may possess strong generalization capabilities in the language domain, allowing it to remain robust even when instructions are perturbed. (2) The model may extract limited keywords from the input instruction for matching and decision-making, rather than genuinely understanding the full semantic structure. However, this is unlikely because our perturbations include a commonsense subclass that performs keyword commonsense rewrite, yet the performance drop remains nearly negligible. (3) The model may not fully utilize the language modality, instead relying primarily on visual or other non- linguistic signals to complete tasks. In such a scenario, language inputs would be functionally redundant, and even significant perturbations would have minimal impact. To verify which of the above hypotheses is more plausible, we conducted additional analysis experiments. 4.1 What If We Remove Language, Does Performance Drop? We introduced a blank instruction experiment. In this setting, the language input provided to the model was entirely replaced with an empty value, i.e., no linguistic information was supplied during inference. This approach directly tests whether the absence of language leads to a substantial performance degradation. We conducted experiments on all four suites of", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "9325c5b044d5090bce1d163126b3ec30074e21e736234ff557cac88c4b2d570e"}
{"doc_id": "arxiv:2510.13626#background:part-4", "url": "https://arxiv.org/abs/2510.13626", "anchor": "#background:part-4", "type": "paper", "title": "", "section": "Background", "text": "minimal impact. To verify which of the above hypotheses is more plausible, we conducted additional analysis experiments. 4.1 What If We Remove Language, Does Performance Drop? We introduced a blank instruction experiment. In this setting, the language input provided to the model was entirely replaced with an empty value, i.e., no linguistic information was supplied during inference. This approach directly tests whether the absence of language leads to a substantial performance degradation. We conducted experiments on all four suites of LIBERO, and the results are shown in Figure 3(a). Surprisingly, even without any valid language input, the performance of OpenVLA-OFT on the object suite remained largely unchanged, with significant degradation observed only on the long suite. We attribute this to the greater reliance on instruction guidance in long-horizon tasks, which forces the model to attend to the language modality. This finding is highly revealing: although the model is nominally designed as a Vision-Language-Action (VLA) framework, in practice it degenerates into a form that disregards language, behaving more like a Vision-Action (VA) model. 5 LIBERO-Plus: In-depth Robustness Analysis of Vision-Language-Action Models 4.2 What If We Replace Goals with OOD Objects, Do Models Fail? We further designed a goal replacement task to directly examine whether models genuinely possess language instruction-following ability. Specifically, for the layout suite, where the issue appeared most pronounced, we replaced the target object in the instruction and the task goal with alternatives within the same scene. For instance, the original task instruction pick up the alphabet soup was replaced with pick up the tomato sauce, and similarly, a series of new goal instructions were constructed. As shown in Figure 3(b), the experimental results revealed two key findings: Finding 7: VLA models do not possess strong cross-object instruction-following generalization. In tasks with replaced targets, the model’s success rate dropped nearly to zero, with the degradation particularly severe for OpenVLA-OFT. The apparent “robustness” observed in prior language perturbation experiments did not stem from a deep modeling of language but rather from ignoring linguistic inputs altogether, leading to a superficially stable performance under perturbations. Finding 8: VLA models appear to rely more on fixed vision–action mappings than on fully exploiting language signals in task decision-making. By analyzing rollout cases, we observed that even when the target in the instruction was explicitly changed, the model still tended to execute the original target action rather than adjust its behavior according to the new instruction. More details can be found in Appendix E. 5 Does There Exist Compositional Generalization Gap Across Multi-Dimensional Perturbations? Generalization results under single-dimension perturbations demonstrate the model’s robustness against isolated factors. However, these dimensions may not be independent, and different types of perturbations are likely to exhibit complex dependencies. In this study, we refer to such performance as compositional generalization. To ensure scientific rigor, we define the problem from a statistical perspective as follows. 5.1 Statistical Definition of the Compositional Generalization Gap We define the random variables Di as Di = \u001a1, if the i-th type of perturbation is applied, 0, otherwise, (1) and similarly for Dj. For a single trial, we", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "91c5e9df0ef9b2882e8854bcf972bbcc4848a06f6d3e5af6fca2a8d13e5f0841"}
{"doc_id": "arxiv:2510.13626#background:part-5", "url": "https://arxiv.org/abs/2510.13626", "anchor": "#background:part-5", "type": "paper", "title": "", "section": "Background", "text": "these dimensions may not be independent, and different types of perturbations are likely to exhibit complex dependencies. In this study, we refer to such performance as compositional generalization. To ensure scientific rigor, we define the problem from a statistical perspective as follows. 5.1 Statistical Definition of the Compositional Generalization Gap We define the random variables Di as Di = \u001a1, if the i-th type of perturbation is applied, 0, otherwise, (1) and similarly for Dj. For a single trial, we define the success indicator variable Y = \u001a1, if the task is successfully executed, 0, otherwise. (2) The success rate can be defined in terms of conditional probability as s(Di = di, Dj = dj) = P(Y = 1 | Di = di, Dj = dj), di, dj ∈{0, 1}. (3) We further estimate the joint probability between Di and Dj conditioned on Y = 1, p(Di = di, Dj = dj | Y = 1) = s(Di = di, Dj = dj) ∑a,b∈{0,1} s(Di = a, Dj = b) (4) which represents the probability that the combination Di = di and Dj = dj occurs among all successful cases. Similarly, the marginal probabilities are p(Di = 1 | Y = 1) = p(Di = 1, Dj = 0 | Y = 1) + p(Di = 1, Dj = 1 | Y = 1) = s(Di = 1, Dj = 0) + s(Di = 1, Dj = 1) ∑a,b∈{0,1} s(Di = a, Dj = b) , (5) p(Dj = 1 | Y = 1) = p(Di = 0, Dj = 1 | Y = 1) + p(Di = 1, Dj = 1 | Y = 1) = s(Di = 0, Dj = 1) + s(Di = 1, Dj = 1) ∑a,b∈{0,1} s(Di = a, Dj = b) . (6) 6 LIBERO-Plus: In-depth Robustness Analysis of Vision-Language-Action Models Intuitively, p(Di = 1 | Y = 1) reflects the probability that the i-th perturbation occurs among all successful cases. It measures the contribution of the i-th perturbation to the overall successful outcomes. A high value indicates that the perturbation frequently co-occurs with successful trials, suggesting the model is robust to this perturbation, while a low value indicates sensitivity to this perturbation. Similarly, p(Di = 1, Dj = 1 | Y = 1) = s(Di = 1, Dj = 1) ∑a,b∈{0,1} s(Di = a, Dj = b) (7) represents the proportion of successful cases under the \"double perturbation\" scenario. A high probability suggests the model maintains performance under joint perturbations, whereas a low probability indicates that the combination severely affects success. In this study, we focus on the Compositionality Gap which is also the covariance between variable Di and Dj given that Y = 1: ∆ij ≜Cov(Di, Dj | Y = 1) = E[DiDj | Y = 1] −E[Di | Y = 1] E[Dj | Y = 1] = p(Di = 1, Dj = 1 | Y = 1) −p(Di = 1 | Y = 1) p(Dj = 1 | Y = 1). (8) The sign of ∆ij correctly reflects the correlation of the contributions", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "a7c6343d8523cdad3aef867d409b33ca4d2201b5c3ddc665ab38b9338622d406"}
{"doc_id": "arxiv:2510.13626#background:part-6", "url": "https://arxiv.org/abs/2510.13626", "anchor": "#background:part-6", "type": "paper", "title": "", "section": "Background", "text": "focus on the Compositionality Gap which is also the covariance between variable Di and Dj given that Y = 1: ∆ij ≜Cov(Di, Dj | Y = 1) = E[DiDj | Y = 1] −E[Di | Y = 1] E[Dj | Y = 1] = p(Di = 1, Dj = 1 | Y = 1) −p(Di = 1 | Y = 1) p(Dj = 1 | Y = 1). (8) The sign of ∆ij correctly reflects the correlation of the contributions of the two perturbations to successful outcomes. Specifically: ∆ij > 0 indicates positive correlation, meaning the model can jointly handle both per- turbations. ∆ij < 0 indicates negative interaction, meaning that the combination introduces additional difficulty beyond independent effects. ∆ij = 0 indicates no interaction, satisfying the independence assumption. 5.2 Experimental Setup and Results Analysis Layout", "source": "arxiv_pdf", "published": "", "tokens": 136, "sha256": "b6cb992e7de1c95f14ffd10b4cb3ecbd30d83b7bd6760ee9b548a1f5c3f3d69c"}
{"doc_id": "arxiv:2510.13626#background", "url": "https://arxiv.org/abs/2510.13626", "anchor": "#background", "type": "paper", "title": "", "section": "Background", "text": "Light Camera Robot Noise 18.93 18.92 14.62 11.31 16.63 18.29 (-0.64) 20.68 15.19 12.59 17.97 18.56 (-0.35) 20.21 (-0.48) 15.48 12.50 18.40 13.72 (-0.90) 13.57 (-1.62) 14.36 (-1.12) 9.83 14.75 10.50 (-0.81) 11.89 (-0.70) 11.96 (-0.54) 8.96 (-0.86) 10.86 15.64 (-0.99) 16.72 (-1.26) 17.72 (-0.68) 13.98 (-0.77) 9.64 (-1.22) 10 12 14 16 18 20 Figure 4: Heatmap of conditional probabilities under pairwise perturbations. Upper triangular entries represent independence-based products of single-dimension probabilities, while lower trian- gular entries show actual joint outcomes. We perform 2000 independent repeated experiments to ensure high statistical significance. As noted in the previ- ous section, the performance of the VLA model on LLM- Based Language Rewrites is somewhat limited by the model’s language-following ability, and its scores may be somewhat “deceptive”. Therefore, when analyzing compo- sitional generalization, we select single-dimension pertur- bations objects spanning, environment sampling, Illumina- tion Variations, camera-sphere shifts, Robot Initialization perturbations, sensor noise and use the OpenVLA-OFT model for testing. In the experiments, we perform independent tests for each type of single-dimension perturbation and pairwise per- turbations, recording the success rate over 2000 repeated trials, which can be found in Appendix F. The final experimental results are presented in a heatmap shown in Figure 4. The values in the upper-triangular ma- trix Aij (1 ≤i < j ≤6) are the product of the conditional probabilities of two single-dimension perturbations. The values in the lower-triangular matrix Aij (1 ≤j < i ≤6) represent the actual probabilities when applying joint per- turbations. Additionally, we calculate the compositional generalization gap ∆ij = Aij −Aji (1 ≤j < i ≤6) and verify the statistical significance of the results using a chi-squared test, as shown in Appendix F. Finding 9: Generalization is intrinsically non-decomposable. The consistent negative compositionality gap reflects interaction effects among perturbations, where co-occurring shifts act as coupled noise sources in feature space and expose entanglement in the learned representations. The findings indicate that current models lack mechanisms to capture higher-order dependencies, leading to pronounced robustness degradation under complex perturbation combinations. 7 LIBERO-Plus: In-depth Robustness Analysis of Vision-Language-Action Models L1 L2 L3 L4 L5 0 20 40 60 80 100 Accuracy (%)", "source": "arxiv_pdf", "published": "", "tokens": 357, "sha256": "576232e784d7bd874ac9449b96f510829c0d753c92ec1a59bb50b58baf4dd574"}
{"doc_id": "arxiv:2510.13626#related-work", "url": "https://arxiv.org/abs/2510.13626", "anchor": "#related-work", "type": "paper", "title": "", "section": "Related Work", "text": "7.1 Vision-Language-Action Models Recent advancements in Vision-Language-Action (VLA) models have expanded the paradigm of foundation models from language and vision into robotics, motivating unified architectures. Autoregressive approaches (Brohan et al., 2022; Kim et al., 2024; Pertsch et al., 2025; Li et al., 2025; Wen et al., 2025a; Li et al., 2024b) discretize robot actions into tokens and train end-to-end policies on large-scale demonstrations, while diffusion- based models (Black et al.; Bjorck et al., 2025; Li et al., 2024a; Wen et al., 2025b) generate continuous trajectories via generative diffusion experts. More recently, reinforcement learning methods (Tan et al., 2025; Liu et al., 2025; Lu et al., 2025; Guo et al., 2025) move beyond supervised fine-tuning, emphasizing robustness and downstream adaptability through reinforcement learning objectives. However, while these models show “zero-shot” performance in familiar settings, their success often reflects interpolation rather than true generalization. As a result, existing benchmarks lack comprehensive insights into model performance under distribution shifts, highlighting the need for systematic and fine-grained robustness evaluations. 7.2 Generalization Robotic Manipulation Evaluations Table 3: Comparison of different simulated generalization evaluation benchmarks for VLA models.", "source": "arxiv_pdf", "published": "", "tokens": 182, "sha256": "2cf2906071e0abb6f46e8c2a68969a9a704d7d9756799aeb409132d8a3df6cfc"}
{"doc_id": "arxiv:2510.13626#conclusion", "url": "https://arxiv.org/abs/2510.13626", "anchor": "#conclusion", "type": "paper", "title": "", "section": "Conclusion", "text": "This work systematically analyzes modern VLA models, exposing a significant generalization problem in contrast to their almost saturated performance on benchmarks such as LIBERO. Our findings reveal that most of the contemporary VLA models remain brittle, showing particular vulnerability to camera and robot state changes, almost all models ignore the language instructions, and some of the models execute with a bare memorization of the trajectory instead of relying on visual feedback. We also identify positional bias and negative combinatorial generalization gaps under combined perturbations. We urge the community to prioritize the true diversity of embodied tasks in evaluation and develop architectures capable of robust generalization beyond limited benchmark environments.", "source": "arxiv_pdf", "published": "", "tokens": 109, "sha256": "67c3a0a93df1a3f066885e87c47af6655752df05d8f00dec68191a15ebb107b1"}
{"doc_id": "arxiv:2510.13778#model:part-1", "url": "https://arxiv.org/abs/2510.13778", "anchor": "#model:part-1", "type": "paper", "title": "", "section": "Model", "text": "Deployment Point Grounding Multi-Modal Understanding Q: Generate a description based on the given image.. A: The image shows a street scene with a traffic light displaying a red signal. Above the traffic light, there is ... 𝜋𝜋0 𝜋𝜋0-fast Figure 1. InternVLA-M1 integrates spatial grounding into the vision–language–action training pipeline. Given a task instruction, the VLM planner produces latent plans through explicit spatial prompting, which then effectively guides the action expert to generate control signals. (2023); Khazatsky et al. (2024); Wu et al. (2024) to directly learn robot control. However, these models tend to overfit fine-grained motor behaviors while under-generalizing to high-level linguistic instructions that involve absolute or relational positions, thereby failing to fully incorporate spatial priors into execution. Core spatial priors such as object recognition, affordance grounding, visual trajectory reasoning, relative localization, and scaling provide transferable knowledge across robotic platforms. Once these priors are established, embodiment-specific learning can focus on concrete control strategies (e.g., manipulator joints, end-effector trajectories, humanoid locomotion, or mobile navigation). Such a division clarifies the role of spatial priors as general-purpose foundations while leaving embodiment-specific details to downstream adaptation, thereby bridging the gap between abstract instruction following and grounded physical execution. Building on the separation between spatial priors and embodiment-specific control, we introduce InternVLA-M1, a dual-system vision–language–action framework that unifies high-level reasoning with grounded execution. InternVLA-M1 consists of a spatial prior VLM planner that interprets linguistic instructions and reasons about spatial relations, and an action expert that translates these grounded representations into executable motor commands. To achieve this, we construct over 3M multimodal training samples, including 2.3M spatial grounding data and 0.7M multimodal understanding data collected from web, real-world, and simulated sources. To leverage the above pre-training data, we propose spatially guided two-stage training recipes: (i) spatial grounding pre-training for the VLM, which establishes transferable spatial understanding through large-scale multimodal supervision on points, boxes, and traces; and (ii) spatially guided action post-training, which specializes these priors 2 for embodiment-specific control under joint supervision. This design bridges abstract goal reasoning with concrete physical execution, enabling robust instruction following across diverse and complex environments. To validate these capabilities, we conduct a comprehensive evaluation across multiple bench- marks in both simulated environments and real-world settings. InternVLA-M1 demonstrates strong generalization and robust performance across diverse scenarios: • On SimplerEnv (Google Robot and WidowX), InternVLA-M1 achieves a new state of the art, surpassing its variant by improving the average success rate by up to +5.9% and +9.8%, respectively. It also demonstrates strong spatial reasoning capabilities across box, point, and trace prediction tasks. Further analysis shows that spatially guided action post-training effectively transfers spatial reasoning ability to motor control. • For the generalizable pick-and-place 200 tabletop scenarios, our model exhibits strong generalization to unseen objects and instructions under few-demonstration fine-tuning, achieving an average improvement of 6.2% over prior works. • In real-world settings, InternVLA-M1 demonstrates strong instruction-following capability, achieving a +20.6% success rate on unseen objects and novel setups in clustered pick-and-place tasks. It also maintains robust long-horizon performance under perturbations (e.g., physical interference, task replanning), outperforming baselines such as GR00T and 𝜋0 by large", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "2ed80531753d3b2157033ce8681b633c6a8a7603821463bcd55634357b87bb71"}
{"doc_id": "arxiv:2510.13778#model:part-2", "url": "https://arxiv.org/abs/2510.13778", "anchor": "#model:part-2", "type": "paper", "title": "", "section": "Model", "text": "ability to motor control. • For the generalizable pick-and-place 200 tabletop scenarios, our model exhibits strong generalization to unseen objects and instructions under few-demonstration fine-tuning, achieving an average improvement of 6.2% over prior works. • In real-world settings, InternVLA-M1 demonstrates strong instruction-following capability, achieving a +20.6% success rate on unseen objects and novel setups in clustered pick-and-place tasks. It also maintains robust long-horizon performance under perturbations (e.g., physical interference, task replanning), outperforming baselines such as GR00T and 𝜋0 by large margins. 2. InternVLA-M1 We propose InternVLA-M1, a dual-system, end-to-end vision–language–action (VLA) framework. It integrates both a language head and an action head within a single model (Section 2.1). The language head establishes instruction-to-visual grounding through spatial pretraining and co-training, while the action head conditions on these learned spatial priors to generate embodiment-specific motor commands(Section 2.2). This joint design bridges abstract linguistic goals with grounded execution, enabling robust instruction following across diverse and complex scenes. 2.1. Model Architecture Dual-System. InternVLA-M1 is a dual-system, end-to-end VLA framework pre-trained on large-scale spatial grounding data collected from diverse sources. InternVLA-M1 employs the Qwen2.5-VL- 3B-instruct Bai et al. (2025a) as the multimodal encoder for System 2, which is to capture spatial priors. It adopts the diffusion policy Chi et al. (2023) (86 M) as the Action Expert (System 1, the fast executor), which effectively models embodiment-specific control. This expert is built on the DINOv2 visual encoder Oquab et al. (2023) (21 M) and a lightweight state encoder (0.4 M), forming a compact vision–action model. In total, InternVLA-M1 comprises approximately 4.1B parameters. During inference, the system runs on a single RTX 4090 GPU with around 12 GB of memory usage. With FlashAttention, the VLM component achieves inference speeds of approximately 10 FPS. Action execution can be further accelerated via chunking and KV caching. Dual-Supervision. The dual-system architecture supports both multimodal supervision and action supervision during training. In each training step, batches from both data types are jointly pro- cessed, and the model computes losses from the two supervision signals. The resulting gradients are aggregated and applied in a single optimization update, ensuring that perception and control are co-adapted rather than learned in isolation. Specifically, the VLM planner is aligned with a broad range of spatial grounding data, both real and synthetic, covering tasks such as object detection, affordance recognition, and visual trajectory planning. In parallel, the Action Expert is trained on 3 Stage 2 | Spatially Guided Action Post-training VLM - Planner Stage 1 | Spatial Grounding Pre-training Multimodal understanding robot observation Spatial Grounding (box / point / trace) multi-modal web data [box] [trace] VLM real & sim robot data Organize the table. Noisy Actions Actions DiT - Actor Conditioned State (opt) Your task is to {instruction}. Figure out how to execute it, then locate the key object needed. Sub-Task Planning Collect snacks. [point] Give the box coordinates according to the instruction… Your answer should be formatted as a list of tuples … Based on the task description, predict the trajectory that the end effector should take… Figure 2. Overview of InternVLA-M1. InternVLA-M1 adopts a spatially guided two-stage", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "242f4e90686200f42739d08472638f0632f540252beba7c00a45a58f72800fa3"}
{"doc_id": "arxiv:2510.13778#model:part-3", "url": "https://arxiv.org/abs/2510.13778", "anchor": "#model:part-3", "type": "paper", "title": "", "section": "Model", "text": "robot data Organize the table. Noisy Actions Actions DiT - Actor Conditioned State (opt) Your task is to {instruction}. Figure out how to execute it, then locate the key object needed. Sub-Task Planning Collect snacks. [point] Give the box coordinates according to the instruction… Your answer should be formatted as a list of tuples … Based on the task description, predict the trajectory that the end effector should take… Figure 2. Overview of InternVLA-M1. InternVLA-M1 adopts a spatially guided two-stage training pipeline. Stage 1 (spatial grounding pre-training): the VLM is trained on large-scale multisource multimodal spatial grounding data to learn embodiment-agnostic spatial priors. Stage 2 (spatially guided action post-training): the VLM Planner, functioning as a slow but reliable System 2 reasoner, generates latent planning tokens via spatial prompting as the condition to the action expert (instanti- ated as a DiT Actor) to execute as a fast System 1 controller. robot demonstration data, enabling it to specialize these priors into embodiment-specific motor commands. This dual-supervision strategy establishes a cohesive link between high-level semantic perception and low-level motion control, which is essential for robust instruction following in both simulation and real-world settings. Latent planning via spatial prompting. To connect the VLM Planner with the action expert, we adopt a lightweight querying transformer (8.7 MB) conditioned on the latent planning embeddings produced by the VLM Planner. The querying transformer stabilizes expert learning and inference by mapping variable-length input tokens into a fixed set of learnable query tokens. It is implemented as a 𝑘-layer cross-attention module, where the query tokens selectively attend to 𝑘intermediate layers of the VLM (e.g., 𝑘= 1 attends only to the final layer). To explicitly activate the spatial perception capability learned during spatial grounding pre-training, we employ spatial prompting. For instance, in general object manipulation tasks, we append simple prompts such as “Figure out how to execute it, then locate the key object needed.” after the task instruction. The extracted feature embeddings provide the planner with explicit spatial cues that facilitate more reliable grounding. Motivated by prior studies Bjorck et al. (2025); Driess et al. (2025); Zhou et al. (2025b) showing that direct gradient flow between action and VLM modules may distort multimodal knowledge, we introduce a gradient decay factor within the querying transformer. This attenuates the gradients propagated from the Action Expert back to the VLM (e.g., by a factor of 0.5), thereby preserving the Planner’s semantic reasoning ability while still enabling effective joint optimization. 2.2. Training Recipe To leverage spatial priors for stronger embodiment-specific control in instruction following, InternVLA- M1 adopts a spatially guided two-stage training pipeline: Stage 1: Spatial grounding pre-training. As shown in Figure 2, the first stage optimizes only the 4 VLM. The objective is not generic vision–language pre-training, but stronger spatial reasoning and planning ability essential for robotics. We combine internet-scale multimodal corpora with robot- specific datasets such as RefCOCO, RoboRefIt Lu et al. (2023b), A0 Xu et al. (2025b), MolmoAct Lee et al. (2025), and Pixmo-Points Deitke et al. (2024). All robot datasets are reformatted into a unified QA-style structure covering bounding-box detection, trajectory prediction,", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "2288785dd2bd8cbb28c9a4ed59b145aa85322571e4be3be71683bac9ea18b1d1"}
{"doc_id": "arxiv:2510.13778#model:part-4", "url": "https://arxiv.org/abs/2510.13778", "anchor": "#model:part-4", "type": "paper", "title": "", "section": "Model", "text": "grounding pre-training. As shown in Figure 2, the first stage optimizes only the 4 VLM. The objective is not generic vision–language pre-training, but stronger spatial reasoning and planning ability essential for robotics. We combine internet-scale multimodal corpora with robot- specific datasets such as RefCOCO, RoboRefIt Lu et al. (2023b), A0 Xu et al. (2025b), MolmoAct Lee et al. (2025), and Pixmo-Points Deitke et al. (2024). All robot datasets are reformatted into a unified QA-style structure covering bounding-box detection, trajectory prediction, affordance recognition, and chain-of-thought reasoning. Aligning them with web-scale data enables training under the same supervised fine-tuning framework as conventional VLMs. Stage 2: Spatially guided action post-training. In this stage, both the VLM and Action Expert are jointly optimized on demonstration data, ensuring semantic understanding and motion generation remain tightly integrated. Two strategies are employed: • Spatial prompting. Before predicting actions, we prepend a spatial cue to the task instruction to elicit structured reasoning about object relationships and task constraints. For example, the instruction “store all toys into the toy box” can be augmented with: “Identify all relevant toys and their spatial relationships to the container.” Although the VLM does not explicitly output a response to this auxiliary cue, its inclusion improves spatial awareness and generalization in manipulation tasks. • Co-training with spatial grounding data. Training alternates between robot trajectory data and grounding data. For trajectory data, both the VLM backbone and the action Expert are optimized with an L2 loss between predicted and ground-truth noise. For spatial grounding data, only the VLM backbone is updated via next-token prediction. This co-training scheme reinforces spatial reasoning while supporting efficient end-to-end optimization. 3. Data This section introduces the datasets used in InternVLA-M1, covering pre-training, mid-training, and post-training stages. For VLM pre-training, we construct large-scale spatial grounding datasets with point, box, and trajectory annotations to enhance spatial perception and vision-language alignment. Mid-training employs synthetic manipulation data to bridge pre-training knowledge and robotic execution. Post-training uses both simulated and real-world instruction-following data, including large-scale tabletop tasks and real-robot demonstrations for long-horizon manipulation. 3.1. Spatial Grounding Data for Pre-training The multimodal training dataset for our model comprises over 3M data, categorized into four distinct types: General QA, Box QA, Trajectory QA, and Point QA, as shown in Figure 3. Notably, more than 2.3M of these data are dedicated to spatial reasoning datasets. These categories ensure robust multimodal understanding while supporting adaptation to embodied tasks in tabletop robotic scenarios. Below, we describe each category: • General QA. Sourced from LLaVA-OneVision Li et al. (2024a) and InternVL3 Chen et al. (2024); Zhu et al. (2025), this category is sampled to cover diverse multimodal tasks, including image captioning, visual question answering (VQA), optical character recognition (OCR), knowledge grounding, and creative writing, resulting in approximately 637K samples in total. • Box QA. We curate a diverse collection of multimodal grounding datasets, including RefCOCO Mao et al. (2016); Yu et al. (2016), ASv2 Wang et al. (2024), and COCO-ReM Singh et al. (2024), sourced from InternVL3 Chen et al. (2024); Zhu et al. (2025). Additionally, we incorporate the InternData-M1 dataset, generated via", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "270548f472c20d39563340ddf9f2f34656dd6b3249213695612d1631a9c8e681"}
{"doc_id": "arxiv:2510.13778#model:part-5", "url": "https://arxiv.org/abs/2510.13778", "anchor": "#model:part-5", "type": "paper", "title": "", "section": "Model", "text": "to cover diverse multimodal tasks, including image captioning, visual question answering (VQA), optical character recognition (OCR), knowledge grounding, and creative writing, resulting in approximately 637K samples in total. • Box QA. We curate a diverse collection of multimodal grounding datasets, including RefCOCO Mao et al. (2016); Yu et al. (2016), ASv2 Wang et al. (2024), and COCO-ReM Singh et al. (2024), sourced from InternVL3 Chen et al. (2024); Zhu et al. (2025). Additionally, we incorporate the InternData-M1 dataset, generated via scalable synthetic data generation as Section 3.3, and the RoboRefIt dataset Lu et al. (2023b), a specialized dataset for robotics grounding, resulting in 5 approximately 879K samples in total. • Trajectory QA. This category integrates the A0 ManiSkill subset Xu et al. (2025a), the InternData- M1 waypoint dataset, and the MolmoAct dataset Lee et al. (2025) to enable precise end-effector trajectory prediction. The A0 ManiSkill subset provides high-quality, object-centric trajectory data, where small objects move in coordination with the robotic arm’s gripper. These trajectories can be approximated as end-effector movements for tabletop manipulation tasks, resulting in approximately 684K samples in total. • Point QA. For precise point localization, we integrate multiple datasets, including the Pixmo-Points dataset Deitke et al. (2024), the RoboPoint dataset Yuan et al. (2024), the RefSpatial dataset Zhou et al. (2025a), and a point subset extracted from the InternData-M1 dataset, each subjected to tailored preprocessing. Specifically, the Pixmo-Points dataset is filtered to exclude images with resolutions exceeding 1024 pixels and restricted to a maximum of 10 points per image. Additionally, we prioritize the extraction of object reference and region reference data from the RoboPoint and RefSpatial datasets to enhance grounding accuracy, resulting in approximately 832K samples in total. All point coordinates are converted to absolute coordinates Bai et al. (2025a). Predicted coordi- nates are formatted in JSON and XML to support robust learning and adaptive processing of spatial instructions for diverse robotic tasks. Q: Provide a scene graph caption of the given image. A: In the image, three <ref> women </ref> <box> [[67, …… General VQA Data Q: How many types of animals are represented in this picture? A: 2 Q: Could you find the light brown lion. A: {\"response\": \"sure, I have found the light brown lion now.\", “subtask\": “Pick up the light brown lion<box>[[304, 275, 346, 356]]</box> in the table.\"} Q: Put the lettuce on the plate. A: {\"response\": \"Sure, I'm placing the lettuce on the plate now.\", “subtask\": \"Place the green lettuce<box>[[365, 278, 483, 381]]</box> on the table.\"} Spatial Grounidng Data Pre-training Data 3,032K Trajectory-QA Point-QA BOX-QA VQA Figure 3. Overview of the pre-training data for the vision-language model. The data comprises two main parts: general VQA data to maintain the model’s general multimodal capabilities, and spatial VQA data focusing on robotic-related grounding and spatial perception in a VQA format. 3.2. Synthetic Data For Action Post-Pre-training To bridge the gap between VLM and VLA, we introduce a Post-Pre-Training phase, where large-scale simulated data is used to pre-train the VLA after VLM pre-training. This stage initializes the action head and facilitates the learning of action representations. Post-Pre-Training", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "eaec9fd5b055f0bfe1f2f3681c75c644d86989612daef776cf9db1e2ca27f6d0"}
{"doc_id": "arxiv:2510.13778#model:part-6", "url": "https://arxiv.org/abs/2510.13778", "anchor": "#model:part-6", "type": "paper", "title": "", "section": "Model", "text": "the vision-language model. The data comprises two main parts: general VQA data to maintain the model’s general multimodal capabilities, and spatial VQA data focusing on robotic-related grounding and spatial perception in a VQA format. 3.2. Synthetic Data For Action Post-Pre-training To bridge the gap between VLM and VLA, we introduce a Post-Pre-Training phase, where large-scale simulated data is used to pre-train the VLA after VLM pre-training. This stage initializes the action head and facilitates the learning of action representations. Post-Pre-Training requires maintaining diversity both at the instruction and object levels. Consistent with the InternVLA-M1-Interface Data, we leverage GenManip as our data synthesis pipeline to construct a large-scale pick-and-place dataset, the InternData M1 dataset, which comprises 244K closed-loop samples. Specifically, we adopt the same object set and positional distributions as in InternVLA-M1-Interface Data, and process them through our scalable data pipeline. Each synthesized sample is rigorously validated to ensure correctness and consistency. To further enhance visual diversity, we introduce controlled randomization in lighting conditions and texture mappings. 6 Grasp Generation Motion Planning cuRobo | MPLib Isaac Sim Physics Rollout 14716 Objects 80+ Lights 1676 Textures 200+ Tables Meta Data Layout Initial State <2D Trace> <2D Box> <2D Point> Object Pose Joint State Joint Action Gripper Action Gripper State Planning Data Layout generation Physics Simulation Visual Rendering for VLM and VLA data synthesis Task Generation Simulation Assets Help me pick up and open the plastic bottle. Synthetic Spatial Grounding QA {\"response\": \"Sure thing.\", “subtask\": “Pick up the white bottle <box> [8, 176, 106, 294]] </box> and open it.\"} Question: Answer: Synthetic Demonstration 244K Episodes with GT annotations Isaac Sim Visual Rendering Grasp Point RGB 2D Box 2D Trace Depth Large-scale Simulation for generalizable pick-place: Put <Obj1> to the <Relation> of <Obj2> The day is late, help me light the lantern now. {\"response\": \"Sure, lighting the lantern now.\", “subtask\": “Ignite the lantern <box> [[398, 150, 426, 240]] </box>\"} Question: Answer: random camera Figure 4. Simulation data synthesis pipeline. The pipeline generates diverse robotic manipulation data from a large asset library, converts intermediate representations into VQA data, and separates physics from rendering to reduce wasted failures and improve efficiency. 3.3. Scalable Synthetic Data Engine for Instruction-Following To support large-scale end-to-end data generation for VLM pre-training, we build a highly scalable, flexible, and fully automated simulation pipeline on top of GenManip Gao et al. (2025) and Isaac Sim Makoviychuk et al. (2021). Automatic task synthesis for generalizable pick-and-place. We develop a scalable simulation pipeline (shown in Figure 4) that generates diverse manipulation trajectories from randomized object layouts and lighting conditions. By leveraging privileged simulation signals including object poses, object meshes, and robot arm state, the system rapidly generates scene layouts via a scene graph solver and computes candidate grasps based on object meshes Liang et al. (2019). Each candidate trajectory is then executed once in physics for closed-loop verification, after which a scene-graph validator checks whether the task goals are achieved. Only trajectories that both execute successfully and pass validation are accepted, ensuring that all collected data are physically feasible and task-complete. Synthesis of VLM data and", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "a97bf9bfc3933e32be30af20547eefeeddff06fec69c43f5356d56df351cf495"}
{"doc_id": "arxiv:2510.13778#model:part-7", "url": "https://arxiv.org/abs/2510.13778", "anchor": "#model:part-7", "type": "paper", "title": "", "section": "Model", "text": "poses, object meshes, and robot arm state, the system rapidly generates scene layouts via a scene graph solver and computes candidate grasps based on object meshes Liang et al. (2019). Each candidate trajectory is then executed once in physics for closed-loop verification, after which a scene-graph validator checks whether the task goals are achieved. Only trajectories that both execute successfully and pass validation are accepted, ensuring that all collected data are physically feasible and task-complete. Synthesis of VLM data and VLA data for spatial grounding. For higher efficiency, robot planning and rendering are fully decoupled in our framework. The planner records structured scene and trajectory data, including joint states, object positions, and action information, which are later replayed by the renderer under randomized lighting, materials, and viewpoints. To align the simulation with real world, we calibrate all cameras using ArUco markers, ensuring that their intrinsic and extrinsic parameters match those of real-world cameras, thus maintaining consistent viewpoint geometry. In addition to high-resolution images, the renderer produces rich intermediate outputs, such as object bounding boxes and 2D end-effector trajectories. These signals provide dense supervision for action learning and facilitate the creation of auxiliary datasets for tasks such as spatial grounding, affordance reasoning, and trajectory prediction. Our asset library includes 14K annotated objects, 211 tables, 1.6K textures, and 87 dome lights, offering data with high visual and physical diversity—critical for developing generalizable models. 7 4. Experiments We conducted extensive experiments to evaluate the performance of InternVLA-M1 in both simu- lation and real-world settings. First, we assess the performance on public simulated benchmarks (Section 4.1). Next, we fully evaluate the instruction-following of InternVLA-M1 for generalizable pick- and-place using Isaac-Sim (Section 4.2). Finally, we examine real-robot performance on long-horizon manipulation tasks to study instruction-following in real-world deployment (Section 4.2.2). 4.1. Experiments on Public Benchmarks We use two established simulation suites: • SimplerEnv is designed to probe robustness to visual appearance shifts. It includes both WidowX and Google Robot platforms, short-horizon atomic tasks, and controlled changes in lighting, color, surface texture, and camera pose. We report results on three task sets: Google Robot-VM (visual matching under viewpoint and lighting changes), Google Robot-VA (visual aggregation with varying textures and colors), and WidowX-VM (cross-robot generalization). • LIBERO is a language-conditioned manipulation suite built on a Franka arm with diverse scenes and expert demonstrations. We evaluate four task sets: LIBERO-Spatial (same objects, different spatial layouts), LIBERO-Object (fixed layout, different objects), LIBERO-Goal (fixed objects and layout, different goals), and LIBERO-Long (also known as LIBERO-10; longer tasks that span multiple objects, layouts, and operations). Google Robot Models Co-Train Pick Coke Can Move Near Open/Close Drawer Open Top Drawer and Place Apple Avg Visual Matching RT-1 Brohan et al. (2022) ✗ 85.7 44.2 73.0 6.5 52.4 RT-1-X Collaboration et al. (2023) ✗ 56.7 31.7 59.7 21.3 42.4 RT-2-X Brohan et al. (2023) ✓ 78.7 77.9 25.0 3.7 46.3 OpenVLA Kim et al. (2024) ✗ 18.0 56.3 63.0 0.0 34.3 CogACT Li et al. (2024c) ✗ 91.3 85.0 71.8 50.9 74.8 SpatialVLA Qu et al. (2025) ✗ 86.0 77.9 57.4 - 75.1 𝜋0 Black", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "f34b89808ce33fb5f999c9790225d51005d0e9387807eced666f36f4f3a937c4"}
{"doc_id": "arxiv:2510.13778#model:part-8", "url": "https://arxiv.org/abs/2510.13778", "anchor": "#model:part-8", "type": "paper", "title": "", "section": "Model", "text": "Near Open/Close Drawer Open Top Drawer and Place Apple Avg Visual Matching RT-1 Brohan et al. (2022) ✗ 85.7 44.2 73.0 6.5 52.4 RT-1-X Collaboration et al. (2023) ✗ 56.7 31.7 59.7 21.3 42.4 RT-2-X Brohan et al. (2023) ✓ 78.7 77.9 25.0 3.7 46.3 OpenVLA Kim et al. (2024) ✗ 18.0 56.3 63.0 0.0 34.3 CogACT Li et al. (2024c) ✗ 91.3 85.0 71.8 50.9 74.8 SpatialVLA Qu et al. (2025) ✗ 86.0 77.9 57.4 - 75.1 𝜋0 Black et al. (2024) ✗ 72.7 65.3 38.3 - 58.8 𝜋0-FAST Pertsch et al. (2025) ✗ 75.3 67.5 42.9 - 61.9 GR00T N1.5∗Bjorck et al. (2025) ✗ 51.7 54.0 27.8 7.4 35.2 Magma Yang et al. (2025a) ✓ 83.7 65.4 56.0 6.4 52.9 Vanilla VLA ✗ 90.0 69.8 52.5 52.2 66.1 InternVLA-M1 ✓ 95.3 90.0 75.5 62.0 80.7 Δ +5.3 +20.2 +23.0 +9.8 +14.6 Variant Aggregation RT-1 Brohan et al. (2022) ✗ 89.8 50.0 32.3 2.6 43.7 RT-1-X Collaboration et al. (2023) ✗ 49.0 32.3 29.4 10.1 30.2 RT-2-X Brohan et al. (2023) ✓ 82.3 79.2 35.3 20.6 54.4 OpenVLA Kim et al. (2024) ✗ 60.8 67.7 28.8 0.0 39.3 CogACT Li et al. (2024c) ✗ 89.6 80.8 28.3 46.6 61.3 SpatialVLA Qu et al. (2025) ✗ 88.0 82.5 41.8 - 70.7 𝜋0 Black et al. (2024) ✗ 75.2 63.7 25.6 - 54.8 𝜋0-FAST Pertsch et al. (2025) ✗ 77.6 68.2 31.3 - 59.0 GR00T N1.5 Bjorck et al. (2025) ✗ 69.3 68.7 35.8 4.0 44.5 Magma Yang et al. (2025a) ✓ 68.8 65.7 53.4 18.5 51.6 Vanilla VLA ✗ 92.3 80.3 50.1 31.4 63.5 InternVLA-M1 ✓ 86.1 82.0 72.0 64.0 76.0 Δ -6.2 +1.7 +21.9 +32.6 +12.5 Table 1. Result comparisons of robotic manipulation on SimplerEnv (Google-Robot) benchmark. The underlined scores indicate the best results excluding InternVLA-M1. Numbers are officially reported; otherwise, we reimplement and mark such entries with ∗. 8 Baselines. We compare to state-of-the-art open VLA systems, including 𝜋0 Black et al. (2024), GR00T Bjorck et al. (2025), OpenVLA Kim et al. (2024), CogACT Li et al. (2024c), and etc. We also include a Vanilla VLA built on QwenVL-2.5-3B-Instruct with a DiT action head. When available, we use official reported numbers; otherwise, we reimplement and mark such entries with ∗. We keep training data, observation spaces, and action type aligned with the most popular setups Li et al. (2024c) to ensure a fair comparison. 4.1.1. SimplerEnv Benchmark Experiment setup. As described in Section 2.2, we post-train InternVLA-M1 on a subset of Open-X Embodiment (OXE) (including fractal_rt_1 and bridge_v1), with co-training on spatial ground- ing data (Section 3.1). The VLM takes the primary observation image, task instruction, and an auxiliary spatial prompt as input, while the action expert predicts actions with an action chunk size of 16. For multimodal data, the model follows an SFT-style question-answering format. Training is performed on 16 NVIDIA A100 GPUs for 50k steps (around 2.5 epochs), with total batch sizes of 256 for robot data and 64 for multimodal data, optimized with a summed loss over both data types. All evaluations are conducted within SimplerEnv using", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "0b6b4103d99884894d45239d3e7a6ada02fa269d509e23837c5cebd66bfc2283"}
{"doc_id": "arxiv:2510.13778#model:part-9", "url": "https://arxiv.org/abs/2510.13778", "anchor": "#model:part-9", "type": "paper", "title": "", "section": "Model", "text": "the primary observation image, task instruction, and an auxiliary spatial prompt as input, while the action expert predicts actions with an action chunk size of 16. For multimodal data, the model follows an SFT-style question-answering format. Training is performed on 16 NVIDIA A100 GPUs for 50k steps (around 2.5 epochs), with total batch sizes of 256 for robot data and 64 for multimodal data, optimized with a summed loss over both data types. All evaluations are conducted within SimplerEnv using its official evaluation protocol. WidowX Robot Models Co-Train Put Spoon on Towel Put Carrot on Plate Stack Green Block on Yellow Block Put Eggplant in Yellow Basket Avg Visual Matching RT-1-X Brohan et al. (2022) ✗ 0.0 4.2 0.0 0.0 1.1 Octo-Base Octo Model Team et al. (2024) ✗ 15.8 12.5 0.0 41.7 17.5 Octo-Small Octo Model Team et al. (2024) ✗ 41.7 8.2 0.0 56.7 26.7 OpenVLA Kim et al. (2024) ✗ 4.2 0.0 0.0 12.5 4.2 CogACT Li et al. (2024c) ✗ 71.7 50.8 15.0 67.5 51.3 SpatialVLA Qu et al. (2025) ✗ 16.7 25.0 29.2 100.0 42.7 𝜋0 Black et al. (2024) ✗ 29.1 0.0 16.6 62.5 27.1 𝜋0-FAST Pertsch et al. (2025) ✗ 29.1 21.9 10.8 66.6 48.3 GR00T N1.5 Bjorck et al. (2025) ✗ 75.3 54.3 57.0 61.3 61.9 Magma Yang et al. (2025a) ✓ 37.5 31.0 12.7 60.5 35.8 Vanilla VLA ✗ 56.6 63.3 27.0 71.8 54.7 InternVLA-M1 ✓ 87.5 67.9 31.3 100.0 71.7 Δ +30.9 +4.6 +4.3 +28.2 +17.0 Table 2. Result comparisons of robotic manipulation on SimplerEnv (WidowX) benchmark. The underlined scores indicate the best results excluding InternVLA-M1. Result analysis. The main experimental results are presented in Table 1 and Table 2. Compared with prior state-of-the-art models, it attains a 5.9% gain in Google Robot Visual Matching, a 5.3% gain in Visual Aggregation, and a 9.8% gain on the WidowX benchmark. These results highlight the strong competitiveness of InternVLA-M1 within the community. Compared to the Vanilla VLA based on Qwen2.5-VL-3B-Instruct, InternVLA-M1 achieves substantial improvements: a 14.6% increase in Google Robot Visual Matching and a 12.4% increase in Visual Aggregation, along with a 17.0% improvement on the WidowX benchmark. These results demonstrate the effectiveness of our spatially guided pre-training and action post-training strategies. Ablation study on dual-supervision co-training. Figure 5 presents a comparative analysis of manipulation performance (WindowX) and perception performance (RefCOCO+) across training steps. The results demonstrate that omitting spatial data and spatially guided prompting during training leads to rapid degradation of spatial grounding capabilities and slower convergence in manipulation tasks. In contrast, spatially guided action post-training accelerates convergence, substantially improves manipulation success rates, and enhances spatial grounding accuracy as shown in Figure 5. To further analyze the relationship between the spatial grounding objective and the action manipulation objective, we compute the Projection-space Similarity (PSS) Raghu et al. (2017) using Singular Value Decomposition (SVD). As shown in Figure 5(c), vanilla co-training of action data 9 with spatial data yields a PSS of only 0.25, indicating significant misalignment between the gradient subspaces. In contrast, our spatially guided training approach increases the PSS to 0.42, demonstrating", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "d3c34cef1f0ec454402fd24540c7c4710918bcb94bc1585e8fe51055b30e169a"}
{"doc_id": "arxiv:2510.13778#model:part-10", "url": "https://arxiv.org/abs/2510.13778", "anchor": "#model:part-10", "type": "paper", "title": "", "section": "Model", "text": "and enhances spatial grounding accuracy as shown in Figure 5. To further analyze the relationship between the spatial grounding objective and the action manipulation objective, we compute the Projection-space Similarity (PSS) Raghu et al. (2017) using Singular Value Decomposition (SVD). As shown in Figure 5(c), vanilla co-training of action data 9 with spatial data yields a PSS of only 0.25, indicating significant misalignment between the gradient subspaces. In contrast, our spatially guided training approach increases the PSS to 0.42, demonstrating substantially improved optimization consistency. This enhanced alignment correlates with better preservation of spatial perception capabilities and faster convergence in manipulation tasks. 0 10 20 30 40 50 Training steps (k) 0 20 40 60 80 IoU@0.5 on RefCOCO (a) Spatial grounding Vanilla VLA + Spatially guided action post-training 0 10 20 30 40 50 Training steps (k) 0 20 40 60 80 SR on SimplerEnv WindowX (b) Robot manipulation Vanilla VLA + Spatially guided action post-training 0 5 10 15 20 Training steps (k) 0.15 0.20 0.25 0.30 0.35 0.40 0.45 0.50 0.55 Projection-space similarity (c) Projection-space similarity Vanilla co-train + Spatially guided action post-training Figure 5. Ablation study on the effect of auxiliary spatial prompting for co-training robot manipulation with spatial grounding. From left to right: (a) spatial grounding performance (IoU@0.5 on RefCOCO- g); (b) manipulation performance (SimplerEnv-WidowX SR); (c) shows the gradient similarity of the spatial grounding and manipulation objectives. We conduct a comprehensive study of VLA training strategies and their effects across three distinct task categories: multi-modal understanding, spatial grounding, and robot manipulation performance, which is a type of generalist VLA. Specifically, we evaluate: • Multi-modal understanding: MME Zhang et al. (2021), MMVet Yu et al. (2023), TextVQA Singh et al. (2019), POPE Li et al. (2023), COCO Caption Chen et al. (2015) • Spatial grounding: RefCOCO-g Mao et al. (2016) (Box IoU0.5), Refit-testB Lu et al. (2023a) (Box IoU0.5), Where2Place Yuan et al. (2024) (evaluated by accuracy of predicted points with respect to ground-truth free space), and A0-maniskill Gu et al. (2023b) (evaluated using trajectory MAE, measuring mean absolute error between predicted and reference waypoints). • Robot manipulation: Google-Robot Visual Matching (VM), Variant Aggregations (VA), and WindowX Visual Matching (VM). As shown in Table 3, our InternVLA-M1 achieves superior robotic manipulation performance while simultaneously preserving stronger multimodal understanding and spatial grounding capabilities compared to vanilla fine-tuning from VLM to VLA (Vanilla VLA) and direct co-training with spatial grounding data (vanilla co-train). Table 3. Ablation study of VLA training strategies across multi-modal understanding, spatial grounding, and robotic manipulation tasks. Multi-modal Understanding Spatial Grounding Robotic Manipulation Models MME MMVet TextVQA POPE COCO Caption BLEU/ROUGE RefCOCO-g Box IoU0.5↑ Refit-testB Box IoU0.5↑ Where2place Point Acc↑ A0-maniskill Traj. MAE ↓ Google Robot VM/VA WindowX VM Vanilla VLA - - - - - - - - - 66.1/63.5 54.7 Vanilla co-train 1106 19.2 20.5 78.0 10.4/15.1 47.1 66.7 21.4 6.4 70.2/66.5 61.1 InternVLA-M1 1411 23.3 28.6 86.2 13.0/13.4 71.2 74.3 25.5 5.1 80.7/76.0 71.7 10 4.1.2. LIBERO Benchmark Experimental setups. Following Kim et al. (2025), we filter out failed demonstrations and pause frames. During", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "3e09adb1e200cb0e5dd75996cd114fb4773fcb6b0df037d3364eabc35734b1e0"}
{"doc_id": "arxiv:2510.13778#model:part-11", "url": "https://arxiv.org/abs/2510.13778", "anchor": "#model:part-11", "type": "paper", "title": "", "section": "Model", "text": "POPE COCO Caption BLEU/ROUGE RefCOCO-g Box IoU0.5↑ Refit-testB Box IoU0.5↑ Where2place Point Acc↑ A0-maniskill Traj. MAE ↓ Google Robot VM/VA WindowX VM Vanilla VLA - - - - - - - - - 66.1/63.5 54.7 Vanilla co-train 1106 19.2 20.5 78.0 10.4/15.1 47.1 66.7 21.4 6.4 70.2/66.5 61.1 InternVLA-M1 1411 23.3 28.6 86.2 13.0/13.4 71.2 74.3 25.5 5.1 80.7/76.0 71.7 10 4.1.2. LIBERO Benchmark Experimental setups. Following Kim et al. (2025), we filter out failed demonstrations and pause frames. During training, the policy takes as input both wrist-mounted and third-person camera views. We fine-tune the model on each suite independently using 8 A100 GPUs with a batch size of 128 and an action chunk size of 8. Training runs for roughly 30K steps, lasting about 20 hours. Each suite is evaluated with 500 trials. Models Spatial Objects Goal Long Avg OpenVLA Kim et al. (2024) 84.7 88.4 79.2 53.7 76.5 SpatialVLA Qu et al. (2025) 88.2 89.9 78.6 55.5 78.1 CoT-VLA Zhao et al. (2025) 87.5 91.6 87.6 69.0 83.9 GR00T N1 Bjorck et al. (2025) 94.4 97.6 93.0 90.6 93.9 𝜋0 Black et al. (2024) 96.8 98.8 95.8 85.2 94.2 𝜋0-FAST Pertsch et al. (2025) 96.4 96.8 88.6 60.2 85.5 𝜋0.5-KI Driess et al. (2025) 98.0 97.8 95.6 85.8 94.3 Vanilla VLA 98.8 98.0 81.4 88.0 91.6 InternVLA-M1 98.0 99.0 93.8 92.6 95.9 Table 4. Result comparisons of robotic manipulation on LIBERO (Franka) benchmark. Result analysis. The primary experimental results on the LIBERO benchmark are presented in Table 4. Compared to previous strong baselines, such as GR00T N1 and 𝜋0, the InternVLA-M1 framework achieves notable improvements, particularly on the spatial and long-horizon tracks, with success rates of 98.0% and 92.6%, respectively. These results demonstrate the efficacy of our proposed method in managing complex, multi-step manipulation tasks. Specifically, for object placement, InternVLA-M1 attains a 99.0% SR, which highlights its robust object grounding capability. 4.2. Experiments on Instruction-Following in In-house Environment 4.2.1. Evaluation in Simulated Large-scale Pick-and-place Existing benchmarks such as SimplerEnv and LIBERO are limited in scale, which restricts the com- prehensive evaluation of instruction-following manipulation in diverse and cluttered settings. To more rigorously assess generalization capabilities, we conduct an experimental study on a large-scale simulation evaluation with enhanced object diversity and layout variation. Experimental setups. We constructed 200 pick-and-place tasks based on Isaac-Sim Gao et al. (2025), where the manipulated objects in each task are mutually distinct. Including background objects, the benchmark covers over 3K items and containers in total. Each task was executed once through the data generation pipeline to ensure its executability. Furthermore, for each of the 200 tasks, we additionally collected 5 trajectories with identical object sets but randomized layouts, which were used for post-training. The observation space comprises two RGB images: one captured from a fixed third-person viewpoint and the other from a first-person camera mounted on the Franka end-effector. Both images are resized to 224 × 224 before being fed into the model. We fine-tune the model on each suite independently using 16 A100 GPUs, with a total batch size of 256 and an action chunk", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "efb36180fe236f9c0a5b979d3b1f6f16afdaccf31d8eeb94c3e6294be8510b17"}
{"doc_id": "arxiv:2510.13778#model:part-12", "url": "https://arxiv.org/abs/2510.13778", "anchor": "#model:part-12", "type": "paper", "title": "", "section": "Model", "text": "we additionally collected 5 trajectories with identical object sets but randomized layouts, which were used for post-training. The observation space comprises two RGB images: one captured from a fixed third-person viewpoint and the other from a first-person camera mounted on the Franka end-effector. Both images are resized to 224 × 224 before being fed into the model. We fine-tune the model on each suite independently using 16 A100 GPUs, with a total batch size of 256 and an action chunk size of 16. Training is conducted for 20K steps. Both our model and all baseline models are trained using delta joint space control. Result analysis. As shown in Figure 6, we evaluate InternVLA-M1 under four generalization settings: 11 In-distribution, Unseen Object, New Background, and Unseen Instruction. For each setting, we report two variants of the model: w/o mid-train, which is fine-tuned using only five trajectories per task, and w/ mid-train, which is additionally mid-trained on InternData M1 prior to fine-tuning. The results, summarized in Figure 7, show that across all settings, both variants outperform the baseline 𝜋0, while InternVLA-M1 w/ mid-train consistently surpasses GR00T N1.5. Although InternVLA-M1 w/o mid-train exhibits slight variance in certain settings, the mid-trained variant achieves a consistent advantage, with an average gain of +6.2% over GR00T N1.5. The performance on unseen objects highlights the benefit of simulation-enhanced visual gen- eralization, enabling the model to handle novel instances beyond the training distribution. When evaluated under new backgrounds with randomized textures and layouts, both variants maintain strong performance, and the improvements from mid-training indicate increased robustness to scene- level shifts. Furthermore, under paraphrased instructions involving attribute-level or commonsense rewrites, InternVLA-M1 w/ mid-train demonstrates reliable instruction grounding, reflecting strong language generalization beyond templated expressions. New Background Unseen Instruction Transfer the item with the red lid on the barrel. Drop the green object into the middle of shallow metal bowl. Move the bottle to the top of the first aid kit. Move the blue bottle to the top of the wooden barrel. Unseen Object Move the flower to the top of the bowl. Move the microphone to the top of the microwave oven. In-distribution Move the yellow bottle to the top of the board. Move the flashlight to the top of the speaker. Figure 6. Evaluation settings for generalizable pick-and-place in large-scale simulation. 51 75 Success Rate (%) 76 47 75 69 𝜋! GR00T N1.5 InternVLA-M1 w/ mid-train InternVLA-M1 w/o mid-train 80 78 In distribution 42 65 62 68 Unseen objects Unseen instruction 33 59 62 72 New background Figure 7. Result comparison of 200 simulated benchmarks in instruction-following pick-and-place. 4.2.2. Evaluation in Real-world Cluttered-scene Pick-and-Place Experimental setup. To evaluate our model’s instruction-following capability in real-world scenarios, we employ a Franka Research 3 robotic arm equipped with a Robotiq 2F-85 gripper. The setup includes two Intel RealSense D435 cameras for RGB visual input—one mounted on the end-effector 12 and another positioned at a rear, third-person perspective. We assess the model across a variety of manipulation tasks, including short-range pick-and-place, long-horizon object sorting, drawer opening/closing, and sandwich assembly. For quantitative evaluation, we design", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "7649bc535515e60376d64cfd0e7803757644ffbf3d946dae40db062919c800f3"}
{"doc_id": "arxiv:2510.13778#model:part-13", "url": "https://arxiv.org/abs/2510.13778", "anchor": "#model:part-13", "type": "paper", "title": "", "section": "Model", "text": "in Real-world Cluttered-scene Pick-and-Place Experimental setup. To evaluate our model’s instruction-following capability in real-world scenarios, we employ a Franka Research 3 robotic arm equipped with a Robotiq 2F-85 gripper. The setup includes two Intel RealSense D435 cameras for RGB visual input—one mounted on the end-effector 12 and another positioned at a rear, third-person perspective. We assess the model across a variety of manipulation tasks, including short-range pick-and-place, long-horizon object sorting, drawer opening/closing, and sandwich assembly. For quantitative evaluation, we design a real-world object- sorting benchmark consisting of single-horizon pick-and-place tasks within a 60 × 90 cm tabletop workspace. The benchmark features 23 seen objects and 5 seen containers (listed in Figure 8). In each episode, three containers are fixed at designated positions, while diverse objects are scattered randomly among them. The model must follow natural language instructions to pick specific objects and place them into the correct containers. To support post-training, we collect 6 hours of teleoperated demonstrations using only objects and containers from the predefined “seen” set. We compare two variants of InternVLA-M1, w/o co-train and w/ co-train, against GR00T N1.5 and 𝜋0 across five evaluation regimes on this benchmark. InternVLA-M1 w/o co-train is fine-tuned solely on real-world demonstrations, while InternVLA-M1 w/ co-train jointly trains on both the real-world data and the simulation dataset InternData-M1. The two RGB views are resized to 224 × 224 and used as model inputs. For both variants of our model, we fine-tune each on individual suites independently using 16 A100 GPUs, with a batch size of 256 and an action chunk size of 16. Training is performed for 20K steps. All models, including baselines, are trained and executed using delta end-effector space control in real-world experiments. 23 Seen Objects 6 Unseen Containers 27 Unseen Objects 5 Seen Containers Figure 8. Overview of objects and containers used in instruction-following pick-and-place. Evaluation settings. To evaluate generalization, we further partition all available object and container assets into disjoint seen and unseen sets, as illustrated in Figure 8. Only the seen set is included in the training data, while both seen and unseen sets are evaluated during testing to measure the model’s ability to generalize to novel objects. As shown in Figure 9, we evaluate instruction- following capabilities of various models on real-world pick-and-place tasks under the below settings: In-Distribution, Unseen Objects, Unseen Object Position, Unseen Object Orientation, and Unseen Instructions. We report success rate, defined as the fraction of trials in which the specified object is placed into the designated container. Higher SR indicates better performance. For each model, we conducted a total of 300 rollout evaluations. Each trial corresponds to one or more testing settings, and we ensured that each individual setting was evaluated at least 50 times. To ensure fair comparisons across models, we fixed the positions of the objects and containers for each task during testing. Result analysis. As shown in Figure 10, both variants of InternVLA-M1 demonstrate superior performance under the in-distribution setting, consistently outperforming GR00T N1.5 and 𝜋0 when evaluated on objects and containers seen during training. This indicates strong instruction-following capabilities within", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "1b079e3d1460eec3b39a10d07e1e678bc9844b3365bb3672ac016517e2fe600b"}
{"doc_id": "arxiv:2510.13778#model:part-14", "url": "https://arxiv.org/abs/2510.13778", "anchor": "#model:part-14", "type": "paper", "title": "", "section": "Model", "text": "trial corresponds to one or more testing settings, and we ensured that each individual setting was evaluated at least 50 times. To ensure fair comparisons across models, we fixed the positions of the objects and containers for each task during testing. Result analysis. As shown in Figure 10, both variants of InternVLA-M1 demonstrate superior performance under the in-distribution setting, consistently outperforming GR00T N1.5 and 𝜋0 when evaluated on objects and containers seen during training. This indicates strong instruction-following capabilities within familiar contexts. Beyond this, the inclusion of Interndata-M1 during co-training significantly enhances the model’s visual generalization, enabling improved performance on novel objects not encountered during training. This suggests that synthetic data serves as an effective complement to limited real-world demonstrations. Additionally, because real-world data collection cannot exhaustively cover the spatial workspace, simulation data enriches the distribution of object positions and orientations. This leads to substantially better generalization to unseen configurations in terms of both object placement and pose. Finally, InternVLA-M1 maintains robust performance when given novel instructions, highlighting its ability to generalize across diverse linguistic expressions 13 Unseen object Unseen instruction Spatial reasoning: Put the object closest to the robot base into the brown box. 1 2 3 4 Attribute Identification: Move the green fruit in the white fruit plate. 1 2 3 4 Commonsense: Transfer the object related to sports into the brown basket. 1 2 3 4 New object instance: Put the small chips into the brown fruit plate. 1 2 3 4 Similar distractor: Put the blue original Oreo into the purple fruit plate. 1 2 3 4 New background: Move the pear in the top of the pink fruit plate. 1 2 3 4 Figure 9. Evaluation settings showcase for real-world instruction-following manipulations. beyond those seen during training. 4.2.3. Evaluation in Long-horizon and Reasoning Manipulation A key strength of our dual-system framework is its ability to leverage a high-level planner (System 2) to decompose long-horizon, reasoning-heavy tasks into a sequence of atomic actions, which are then robustly executed by a low-level action model (System 1). To evaluate this capability, we design a series of tasks that require not only multi-step planning but also the ability to reason about object attributes, monitor progress, and adapt to changes. As illustrated in Figure 11, these include: • Desktop Sorting. The Franka robot is tasked with sorting objects into containers based on high- level semantic categories, aiming to ensure that all items on the desktop are eventually placed into 14 𝜋! GR00T N1.5 InternVLA-M1 w/ co-train InternVLA-M1 w/o co-train 45 78 88 Success Rate (%) In distribution 92 34 56 63 67 Unseen instruction 37 59 68 73 By attribute 31 53 58 61 By spatial 29 44 49 58 Unseen objects 32 46 50 62 New object instance 25 40 45 49 Similar distractor 27 47 52 New background 63 18 20 24 52 Unseen object position 32 40 50 72 Unseen object orientation Success Rate (%) Figure 10. Result comparison in real-world instruction-following pick-and-place. the correct containers. Both objects and containers are scattered within a 60×90 cm region in front", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "aecd3c57ff0e832c8c1dc6ff6f319a731065140dccd76747174626d87d284533"}
{"doc_id": "arxiv:2510.13778#model:part-15", "url": "https://arxiv.org/abs/2510.13778", "anchor": "#model:part-15", "type": "paper", "title": "", "section": "Model", "text": "Unseen instruction 37 59 68 73 By attribute 31 53 58 61 By spatial 29 44 49 58 Unseen objects 32 46 50 62 New object instance 25 40 45 49 Similar distractor 27 47 52 New background 63 18 20 24 52 Unseen object position 32 40 50 72 Unseen object orientation Success Rate (%) Figure 10. Result comparison in real-world instruction-following pick-and-place. the correct containers. Both objects and containers are scattered within a 60×90 cm region in front of the robot base. The setup includes five seen containers and five object categories: fruits, toys, vegetables, bottles, and snacks. Each evaluation instance involves sorting objects from one to three categories into their respective containers. Each trial consists of three pick-and-place actions, and we report success rates consistent with the metric used for pick-and-place under clustered environments. • Sorting Items into Drawers. The Franka robot is required to (i) open a designated drawer (either lower or upper), (ii) place the target objects into it, and (iii) close the drawer. This task demands precise temporal reasoning and articulated manipulation. The objects are placed within a 35×35 cm area located to the front-right of the robot base. We report stepwise execution success, where a step is considered valid only if all preceding steps have succeeded. • Making Sandwiches. The Franka robot is instructed to assemble sandwiches following a predefined meal recipe. Ingredients and plates are placed within a 50×70 cm region in front of the robot base. We define five types of sandwich recipes as the seen set: [ bread–lettuce–bread ], [ bread–lettuce– meat–bread ], [ bread–meat–lettuce–meat–bread ], [ bread–meat–meat–bread ], and [ bread–meat– bread ]. We report success rates on both the seen set and an unseen set involving real-time environment interaction, using the same success definition as in the drawer sorting task. • Math Calculation. The Franka robot is prompted to solve a math problem and press the color-coded button (red, yellow, or blue) that corresponds to the correct answer based on arithmetic reasoning. The buttons are randomly placed within a 40×40 cm area in front of the robot base. • Goods Purchase. The ARX LIFT2 dual-arm robot is tasked with identifying and placing into a basket the object bearing the correct price tag, given a numerical cue ranging from 1 to 9. We report the success rate of correctly placing the item corresponding to the queried price into the basket. Experimental setup. To support fine-grained training for these long-horizon tasks, we collect a total of 22 hours of high-quality long-horizon and reasoning teleoperated demonstrations, amounting to approximately 500 demonstrations per task. Each collected trajectory is segmented into subtasks and annotated with corresponding atomic actions. For example, a “make a classic sandwich” task is decomposed into four subtasks: (1) “Put a piece of bun on the plate.” →(2) “Put a piece of meat on the plate.” →(3) “Put a piece of lettuce on the plate.” →(4) “Put a piece of bun on the plate.” 15 Math calculation: Solve math and press button. (1 step) Long horizon and customized tasks: Make a classic", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "29d33c1803b5930a2466e448453e9eea0d79946150cfdda420c68e1eca34bf2f"}
{"doc_id": "arxiv:2510.13778#model:part-16", "url": "https://arxiv.org/abs/2510.13778", "anchor": "#model:part-16", "type": "paper", "title": "", "section": "Model", "text": "Each collected trajectory is segmented into subtasks and annotated with corresponding atomic actions. For example, a “make a classic sandwich” task is decomposed into four subtasks: (1) “Put a piece of bun on the plate.” →(2) “Put a piece of meat on the plate.” →(3) “Put a piece of lettuce on the plate.” →(4) “Put a piece of bun on the plate.” 15 Math calculation: Solve math and press button. (1 step) Long horizon and customized tasks: Make a classic sandwich and add another meat. (6-12 steps) Put a piece of lettuce on the plate. Put a piece of bun on the plate. Stop. Put a piece of bun on the plate. Long-horizon sorting: Sort snacks, toys, and fruits into respective bins. (6-14 steps) Put all the snacks into the white basket. Put all the fruits into the wooden basket. Start. Stop. Put all the toys into the brown basket. Put all the toys into the brown basket. -9 x 7 =· -72 · -63 · -54 Press the blue botton. Task replanning: Sort the brush into a closed drawer, then sort another hippo toy on sudden human request. (2-8 steps) Collect the brush to the upper drawer. Open the upper drawer. Close the upper drawer. Clear the hippo toy to the upper drawer. 1 2 3 4 1 2 3 4 5 6 7 8 1 2 3 4 1 2 Recognize price tags: Goods purchase. (2 steps) Pick the object that costs $4. 1 Put the object to the basket 2 Human: Wait, also clear the hippo toy into the drawer. Figure 11. Showcase for long-horizon instruction-following manipulation. Each sub-instruction is paired with a specific segment of the demonstration. To enable subtask-level transition, we introduce zero-action vectors padding after each subtask segment. This allows the model to stop upon subtask completion and then be prompted to predict the transition to the next subtask. In addition, to improve temporal consistency and ensure smooth inference, we remove frames in which the robot arm exhibits clear pauses or idle behavior. In contrast to prior VLA models that depend on an additional VLM to serve as a task planner for long-horizon or reasoning-intensive tasks, our unified model architecture is trained jointly on multimodal inputs encompassing task decomposition, subtask identification, numerical reasoning, and action supervision. This joint training paradigm enables a single model to seamlessly integrate task planning, reasoning, and action prediction in an 16 𝜋! GR00T N1.5 InternVLA-M1 29 52 59 In distribution 16 41 54 Physical interference 26 45 57 Task replanning Success Rate (%) 51 56 70 In distribution 45 51 67 Physical interference 52 56 69 Task replanning Sorting desk items into drawers Make sandwiches 43 60 67 In distribution 30 55 63 Physical interference 38 57 62 Task replanning Desktop Sorting Figure 12. Result comparison in real-world long-horizon task planning for manipulation. end-to-end fashion. The training recipe follows that of real-world short-horizon pick-and-place tasks. As shown in Table 5, despite GPT-5’s strong reasoning capability, additional post-training of the unified model notably improves performance on long-horizon and reasoning-intensive tasks, underscoring the", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "d4af2163663f53168293fdddb73070a1e5db3f3d7f8e75cf15ca754fa2ff64ad"}
{"doc_id": "arxiv:2510.13778#model:part-17", "url": "https://arxiv.org/abs/2510.13778", "anchor": "#model:part-17", "type": "paper", "title": "", "section": "Model", "text": "Physical interference 52 56 69 Task replanning Sorting desk items into drawers Make sandwiches 43 60 67 In distribution 30 55 63 Physical interference 38 57 62 Task replanning Desktop Sorting Figure 12. Result comparison in real-world long-horizon task planning for manipulation. end-to-end fashion. The training recipe follows that of real-world short-horizon pick-and-place tasks. As shown in Table 5, despite GPT-5’s strong reasoning capability, additional post-training of the unified model notably improves performance on long-horizon and reasoning-intensive tasks, underscoring the necessity of post-training for effective high-level task planning. Models Long-horizon tasks Reasoning tasks Sort into Drawers Make Sandwiches Desktop Sorting Math calculation Goods Purchase Gemini-2.5 Pro 57 62 83 53 61 GPT-5 75 67 62 79 82 GPT-4o 37 57 35 39 41 Qwen2.5-VL-72B 31 71 34 33 29 Qwen2.5-VL-3B 30 49 52 41 38 Ours-3B 90 91 91 93 92 Table 5. Task scheduling performance of VLM planner in long-horizon and reasoning scenarios. Evaluation settings. We evaluate model performance under three distinct settings: In-distribution, Physical Interference, and Task Replanning: • Physical interference. External disturbances are introduced during task execution. For example, during the sorting items into drawers task, the drawer is manually closed after the robot opens it, or the target object is displaced during grasping. This evaluates the model’s ability to perceive environmental changes and adapt accordingly. • Task replanning. New instructions are issued mid-execution. For instance, after placing an object in the drawer but before closing it, the robot is told: “Also put the cow toy into the top drawer.” This tests the model’s ability to incorporate new subgoals and dynamically adjust its plan. Results analysis. As shown in Figure 12, across long-horizon tasks, InternVLA-M1 consistently outperforms the baselines, enabled by its unified subtask planning mechanism. In the in-distribution setting, it achieves more reliable execution than GR00T N1.5 and 𝜋0, showing stronger grounding of high-level goals into actionable steps. Under physical interference, the model demonstrates robust adaptability: for example, in desktop sorting when containers are unexpectedly moved, InternVLA-M1 can track the new container locations and complete the placement. Moreover, when task replanning is required, such as when additional instructions are introduced during execution, InternVLA-M1 is able to revise its subtask sequence on the fly and continue with correct actions. This adaptability leads to minimal performance degradation under stress conditions, while the baselines exhibit much larger declines, underscoring the model’s resilience to dynamic environments and shifting instructions. 17 5. Related work Hierarchical robot system. A key challenge in embodied AI is bridging high-level instructions with low-level actions, often addressed by generating intermediate representations (IRs) that range from formal symbolic structures to learned embeddings Xie et al. (2019). Inspired by Chain-of- Thought (CoT) reasoning, many approaches train vision-language-action (VLA) models to generate explicit textual plans before acting, which improves both interpretability and performance on complex tasks Zawalski et al. (2024). Beyond textual plans, research has explored more structured or physically grounded IRs. Historically, systems relied on direct perceptual outputs, such as bounding boxes from object detectors for manipulation Griffin (2023), specific 3D points for grasp planning from point clouds Ten Pas", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "598afbbd7d7061da8da3eea24d57f701c7552635a7627d19bd884a24c1f973ad"}
{"doc_id": "arxiv:2510.13778#model:part-18", "url": "https://arxiv.org/abs/2510.13778", "anchor": "#model:part-18", "type": "paper", "title": "", "section": "Model", "text": "to learned embeddings Xie et al. (2019). Inspired by Chain-of- Thought (CoT) reasoning, many approaches train vision-language-action (VLA) models to generate explicit textual plans before acting, which improves both interpretability and performance on complex tasks Zawalski et al. (2024). Beyond textual plans, research has explored more structured or physically grounded IRs. Historically, systems relied on direct perceptual outputs, such as bounding boxes from object detectors for manipulation Griffin (2023), specific 3D points for grasp planning from point clouds Ten Pas and Platt (2017), or dense correspondence fields derived from self-supervised features learned for control, such as DINO features Laskin et al. (2020); Nair et al. (2022). Some methods construct persistent 3D scene graphs as comprehensive world models that LLMs can query to ground long-horizon plans Rana et al. (2023). Others emphasize action-centric IRs, for example by conditioning policies on visual affordances that specify the robot’s end-effector pose at key moments in a task Nasiriany et al. (2024). A growing trend involves generating explicit spatial localizers directly consumable by robot controllers Gu et al. (2023a); Huang et al. (2025b); Li et al. (2025c). Large-scale foundation models Luo et al. (2025); Team et al. (2025) unify perception and planning by outputting not only plans but also affordance predictions as bounding boxes. For tasks requiring higher precision, specialized models such as RoboRefer Zhou et al. (2025a) employ dedicated architectures and reinforcement learning to predict exact 3D coordinates from complex spatial language. In contrast, our method provides a unified latent modeling framework that integrates spatial guidance into downstream action training, enabling end-to-end optimization with direct feedback from real-world deployment. Embodied reasoning and planning in VLA. Chain-of-Thought prompting has proven effective for improving reasoning in large language models Wei et al. (2022), and its success has inspired extensions to embodied AI. In Vision-Language-Action (VLA) models, generating intermediate reasoning steps before acting enables agents to handle complex, long-horizon tasks. Early approaches emphasized linguistic reasoning: ECOT Zawalski et al. (2024) elicits explicit text-based plans and sub-tasks to enhance performance and interpretability; RT-H Belkhale et al. (2024) introduces a fine-grained “action language” for hierarchical policies and human intervention; InstructVLA Yang et al. (2025b) jointly optimizes reasoning and action through VLA-IT, improving generalization; OneTwoVLA Lin et al. (2025) adaptively alternates between “thinking” and execution; RAD Clark et al. (2025) leverages action-free human videos to derive reasoning guides; and 𝜋0.5 Intelligence et al. (2025) trains on heterogeneous data before fine-tuning for subtask prediction. Later work has also explored visual and spatial modalities, such as graph-based representations for spatial reasoning Huang et al. (2025a). Despite their differences, these approaches all generate intermediate steps such as textual, visual, or spatial representations during inference. While effective, this adds computational overhead. In contrast, we propose a post-training phase that directly unlocks the VLM’s intrinsic reasoning capacity, removing the need for explicit generative reasoning. Generalist robot policy. Recent research in general-purpose robotics has seen the emergence of several mainstream technical paradigms. Monolithic VLA models utilize a single end-to-end network to directly map multimodal inputs to tokenized low-level actions, as demonstrated by systems Brohan et al. (2023); Kim", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "628374007f41308a66fde11a32a12d76fdd0c3ed5096681b4689b0a35cd4b758"}
{"doc_id": "arxiv:2510.13778#model:part-19", "url": "https://arxiv.org/abs/2510.13778", "anchor": "#model:part-19", "type": "paper", "title": "", "section": "Model", "text": "such as textual, visual, or spatial representations during inference. While effective, this adds computational overhead. In contrast, we propose a post-training phase that directly unlocks the VLM’s intrinsic reasoning capacity, removing the need for explicit generative reasoning. Generalist robot policy. Recent research in general-purpose robotics has seen the emergence of several mainstream technical paradigms. Monolithic VLA models utilize a single end-to-end network to directly map multimodal inputs to tokenized low-level actions, as demonstrated by systems Brohan et al. (2023); Kim et al. (2024); Lee et al. (2025); Yang et al. (2025a). In contrast, unified archi- tectures decouple high-level cognition from low-level action, allowing for greater modularity and interpretability. This category has seen extensive exploration Black et al. (2024); Li et al. (2025a, 2024c) leveraging specialized generative models for action synthesis. Other notable approaches in this vein Cheang et al. (2025); Intelligence et al. (2025); Shukor et al. (2025); Song et al. (2025); 18 Yang et al. (2025b); Zhou et al. (2025b), which uses an LLM to break down high-level language commands into intermediate action plans. A third paradigm is based on world models, which learn a predictive model of the environment’s dynamics to enable planning and control. These models allow for simulating future outcomes, often facilitating planning via search in a learned latent space or by conditioning a separate policy. While powerful, this approach can be computationally intensive. Representative works Bjorck et al. (2025); Bu et al. (2025b); Cen et al. (2025); Li et al. (2025b); Liao et al. (2025); Lv et al. (2025); Tian et al. (2024); Wang et al. (2025); Ye et al. (2025) exem- plify this forward-predictive approach to decision-making. Our model adopts a typical dual-system approach, building upon the VLA with unified architectures, then introducing additional planning design, thereby achieving better adaptability to real-world environments. 6. Discussion and conclusion In this work, we presented InternVLA-M1, a unified vision-language-action framework that leverages spatial grounding priors to bridge high-level multimodal reasoning with low-level robotic execution. By combining large-scale multimodal pre-training with spatially guided post-training, our model effectively transfers spatially grounded understanding into embodied control, achieving strong gener- alization to unseen objects, instructions, and environments. Extensive evaluations across simulation and real-world settings demonstrate that InternVLA-M1 surpasses existing VLA models and specialized systems in instruction following, long-horizon manipulation, and multimodal grounding, highlighting spatial reasoning as a unifying substrate for scalable and reliable generalist robots. 19", "source": "arxiv_pdf", "published": "", "tokens": 395, "sha256": "fe5a502bd8fc5fbf380c024af0d2ab2f8dfc8404a84068722f8edaa41f9399e4"}
{"doc_id": "arxiv:2510.13546#method:part-1", "url": "https://arxiv.org/abs/2510.13546", "anchor": "#method:part-1", "type": "paper", "title": "", "section": "Method", "text": "Fig. 2. High-level overview of the hardware-accelerated ICE-BA pipeline. Best viewed in color. C. Evaluation Figure 2 presents a high-level overview of the accelerated ICE-BA pipeline. Modules colored in green are executed on the Arm processors of the considered SoCs. The module colored in red (feature detector) is accelerated on the em- bedded GPU or FPGA. On the Versal VCK190, the FAST and Harris FPGA accelerators, from the Vitis Vision Library [63], are implemented on programmable logic, whereas the SuperPoint FPGA accelerator, from the Vitis AI Library [64], is implemented using both programmable logic and the AMD AI Engine. On the Nvidia Jetson Orin, the SuperPoint implementation from [65] is accelerated on the embedded GPU with TensorRT, whereas the recent Faster than FAST (FTFast) [38] is selected as the most optimized GPU im- plementation for FAST. For the Harris GPU accelerator, we selected the implementation from the Nvidia VPI library with the CUDA backend. An Intel system is also utilized to provide software baselines using FAST and Harris imple- mentations from OpenCV. Both SuperPoint models from [64] and [65] are pre-trained using the MS COCO 2014 dataset [66]. We did not limit the number of feature points that can be detected by the FAST, Harris, and SuperPoint accelerators. Each MH sequence is pre-loaded completely into DRAM before the processing starts. We execute ICE-BA in its monocular mode. In the ICE-BA mapping thread, the local and global bundle adjust- ments are executed in parallel on two different processor threads. Table IV summarizes the algorithmic parameters of the FAST and Harris feature detectors. The hardware accelerators share the same algorithmic parameters as the software baseline. Table V summarizes the configurations of the Nvidia Jetson Orin. We used two different configurations of the Orin during the evaluation. The Orin max configuration enables maximum performance by enabling all twelve Arm cores, and fixing the clock frequency of the Arm cores and the GPU to 2.2 GHz and 1.3 GHz, respectively. The number of online Arm processor cores on Orin is configured by modifying the Linux kernel files, to bring several selected Arm cores online/offline. The clock frequency of the Arm processors and the GPU is fixed by disabling Dynamic Voltage and Frequency Scaling (DVFS) and modifying the Linux kernel files to apply the frequency we need. The Orin Versal Similar (VS) configuration aims to ap- TABLE III SUMMARY OF HARDWARE AND SOFTWARE SPECIFICATIONS AND CONFIGURATIONS. Intel Xeon Nvidia Jetson AGX Orin AMD Versal VCK190 Processor Intel Xeon W-2123 (8 cores, 3.6 GHz) Arm Cortex-A78AE (12 cores, 2.2 GHz, 64 KB L1i1and L1d2, 3 MB L2, 6 MB L3) Arm Cortex-A72 (2 cores, 1.2 GHz, 48 KB L1i, 32 KB L1d, 1 MB L2) Accelerator N/A Nvidia Ampere GPU (2048 CUDA cores, 64 Tensor cores, 1.3 GHz) Vitis Vision FAST, 150 MHz, Vitis AI SuperPoint, 1.3 GHz DRAM 64 GB DDR4 32 GB LPDDR5 8 GB DDR4 OS Ubuntu 20.04 Ubuntu 20.04 Petalinux 2023.2 Compiler GCC-9.4 GCC-9.4, NVCC GCC-9.3, Vitis HLS 2023.2 Compiler Flags -O3, -fno-math-errno, -funroll-loops, -fno-finite-math-only ISA Intel AVX & SSE Arm NEON Library OpenCV", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "e6705b3736ae2562f4603465d598922fecd3885b074026028ef92db7ac4caaa3"}
{"doc_id": "arxiv:2510.13546#method:part-2", "url": "https://arxiv.org/abs/2510.13546", "anchor": "#method:part-2", "type": "paper", "title": "", "section": "Method", "text": "MB L3) Arm Cortex-A72 (2 cores, 1.2 GHz, 48 KB L1i, 32 KB L1d, 1 MB L2) Accelerator N/A Nvidia Ampere GPU (2048 CUDA cores, 64 Tensor cores, 1.3 GHz) Vitis Vision FAST, 150 MHz, Vitis AI SuperPoint, 1.3 GHz DRAM 64 GB DDR4 32 GB LPDDR5 8 GB DDR4 OS Ubuntu 20.04 Ubuntu 20.04 Petalinux 2023.2 Compiler GCC-9.4 GCC-9.4, NVCC GCC-9.3, Vitis HLS 2023.2 Compiler Flags -O3, -fno-math-errno, -funroll-loops, -fno-finite-math-only ISA Intel AVX & SSE Arm NEON Library OpenCV 4.5.5 OpenCV 4.5.4, CUDA 11.4, TensorRT 8.4 OpenCV 4.5.5, Vitis Vision 2022.1, Vitis AI 3.0 1 L1i stands for L1 instruction cache. 2 L1d stands for L1 data cache. TABLE IV THE ALGORITHMIC PARAMETERS OF FAST AND HARRIS Parameters FAST Harris FAST Type FAST-9 N/A Threshold 10 Sensitivity K N/A 0.04 NMS? Yes NMS Window Size 3×3 2×2 Feature Score Computation Method Maximum Threshold N/A Sobel Filter Size N/A 7×7 Neighbor Block Size N/A 7×7 TABLE V CONFIGURATIONS OF NVIDIA AGX ORIN Configuration Orin Versal Similar (VS) Orin max # of Processor Cores 2 12 Processor Clock Frequency 600 MHz 2.2 GHZ GPU Clock Frequency 150 MHz (FTFast and VPI Harris) / 1.3 GHz (SuperPoint) 1.3 GHz proximate the processing cores and accelerator configuration of the Versal VCK190 on the Orin platform, for a fair comparison. This is because the Orin has 12 Arm A78 cores at 2.2 GHz, whereas the VCK190 only has 2 Arm A72 cores at 1.2 GHz. Note that the Arm A78 has a more advanced microarchitecture than the Arm A72 (see Table III). The following experiments are conducted to determine the Arm processor configuration. ICE-BA is executed on VCK190 and Orin using only two Arm cores and FAST implementation from OpenCV, the run-time of its localization thread is mea- sured. ICE-BA is executed using different clock frequencies of the Arm processors. We use frequencies available with both the AMD Petalinux 2023.2 OS and the Ubuntu 20.04 OS on Orin, which are 300 MHz, 400 MHz, 600 MHz, and 1.2 GHz. The dataset we used in this experiment is the MH01 sequence. Table VI illustrates the different run-time achieved by the ICE-BA localization thread under different clock frequencies with 2 Arm A72/A78 cores. Note for the 600 MHz frequency, the Arm Cortex A78 cores deliver 14.9 ms, while the Arm TABLE VI RUN-TIME (MS) OF THE ICE-BA LOCALIZATION THREAD. Clock Frequency (MHz) Arm A72 (max 1.2 GHz) Arm A78 (max 2.2 GHz) 300 46.3 34 400 46.3 20.6 600 31 14.9 1200 15.8 7.4 2200 N/A 4.4 TABLE VII POWER COMPARISON AMONG GPU- AND FPGA-ACCELERATED FEATURE DETECTORS AND THE SOFTWARE BASELINE Feature Detectors Implementations and System Configs Power (W) Processor Accelerator Total FAST OpenCV + Xeon 1203 N/A 120 FTFast + Orin VS 0.8 5.6 12.6 FTFast + Orin max 4.4 8.8 20.4 Vitis FAST 4.9 10.8 16.7 Harris OpenCV + Xeon 120 N/A 120 VPI + Orin VS 3.8 6 17.5 VPI + Orin max 4.4 8.8 21.5 Vitis Harris 4.6 10.3 16.5 SuperPoint Orin VS 2.4 8.8 18.5 Orin max 4.4 9.6 22 Vitis SuperPoint 5.2", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "d1d9ecf7eb28ec846e9897c4d046945bf29c6afcabe870f7c13f53b90ad91ee3"}
{"doc_id": "arxiv:2510.13546#method:part-3", "url": "https://arxiv.org/abs/2510.13546", "anchor": "#method:part-3", "type": "paper", "title": "", "section": "Method", "text": "THE SOFTWARE BASELINE Feature Detectors Implementations and System Configs Power (W) Processor Accelerator Total FAST OpenCV + Xeon 1203 N/A 120 FTFast + Orin VS 0.8 5.6 12.6 FTFast + Orin max 4.4 8.8 20.4 Vitis FAST 4.9 10.8 16.7 Harris OpenCV + Xeon 120 N/A 120 VPI + Orin VS 3.8 6 17.5 VPI + Orin max 4.4 8.8 21.5 Vitis Harris 4.6 10.3 16.5 SuperPoint Orin VS 2.4 8.8 18.5 Orin max 4.4 9.6 22 Vitis SuperPoint 5.2 21.1 30 3 The TDP (Thermal Design Power) of the Intel Xeon W-2123 system. Cortex A72 cores deliver 15.8 ms. Thus, for the Orin VS configuration, the number of power-on Arm cores is 2. The clock frequency of the two Arm cores is 600 MHz. The GPU clock frequency is 150 MHz under this configuration when executing FTFast, to be in line with the frequency of the FAST FPGA accelerator. V. EXPERIMENTAL RESULTS AND ANALYSIS This section presents the run-time performance, power, and energy efficiency results of the FPGA- and GPU- accelerated FAST, Harris, and SuperPoint. The run-time performance, accuracy, power, and energy efficiency results of ICE-BA integrated with different hardware-accelerated feature detectors are also presented. Fig. 3. Speedups and energy efficiency improvements of different FAST and Harris accelerators w.r.t OpenCV FAST + Xeon. Best viewed in color. Fig. 4. Run-time and energy efficiency of different SuperPoint accelerators. Best viewed in color. A. Results of the Feature Detector Hardware Accelerators Figure 3 summarizes the results of speedup and energy efficiency improvement of the GPU- and FPGA-accelerated FAST and Harris, with respect to the FAST software baseline on Xeon. Regarding the FAST accelerators, FTFast+Orin max achieves the best run-time performance in 4 out of 5 se- quences (except for MH01), and the best energy efficiency (only 2.2 - 2.3 mJ/Frame) across all MH sequences, followed by the Vitis FAST accelerator on VCK190, which is 12.8% - 15.2% slower. However, note that the GPU clock frequency is 1.3 GHz under the Orin max configuration, whereas the clock frequency of the FAST FPGA accelerator is 150 MHz. Compared with FTFast+Orin VS, where the GPU clock frequency is the same as the FPGA accelerators, the Vitis FAST accelerator can achieve 5.9× - 7.7× speedups and 3.3× - 4.3× improvements in energy efficiency. The FAST FPGA accelerator yields better run-time performance than FTFast+Orin VS because it prioritizes latency over scalability by processing 8 pixels per clock cycle. Further- more, the Vitis FAST implementation utilizes approximation, reduced precision, and function overlapping. The Vitis FAST accelerator uses shift operations, which are less compu- tationally expensive and require fewer hardware resources to implement, to approximate multiplication and division operations. Furthermore, the NMS function overlaps with the FAST score computation function, and fixed-point numbers are used instead of floating-point numbers. Compared with the OpenCV software baseline on an Intel Xeon processor, FTFast+Orin max achieves 3.7× - 7.6× speedups and 52× - 104× improvements in energy efficiency, while the Vitis FAST accelerator achieves 5.1× - 8.5× speedups and 38.3× - 95.6× improvements in energy efficiency. In terms of the Harris accelerators, the", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "7d01ed7755605cb82ab16f645b3b0ded1b2da136233fd3cec1015cc4a6c676ee"}
{"doc_id": "arxiv:2510.13546#method:part-4", "url": "https://arxiv.org/abs/2510.13546", "anchor": "#method:part-4", "type": "paper", "title": "", "section": "Method", "text": "resources to implement, to approximate multiplication and division operations. Furthermore, the NMS function overlaps with the FAST score computation function, and fixed-point numbers are used instead of floating-point numbers. Compared with the OpenCV software baseline on an Intel Xeon processor, FTFast+Orin max achieves 3.7× - 7.6× speedups and 52× - 104× improvements in energy efficiency, while the Vitis FAST accelerator achieves 5.1× - 8.5× speedups and 38.3× - 95.6× improvements in energy efficiency. In terms of the Harris accelerators, the Vitis Harris ac- celerator achieves the best run-time performance across all MH sequences, with 10.6× - 11× speedups against the OpenCV baseline on an Intel Xeon processor, and 1.01× - 1.1× speedups against the VPI Harris+Orin max. The Vitis Harris accelerator is slightly faster than the VPI Harris+Orin max due to the use of reduced precision numbers. On the other hand, VPI Harris+Orin max achieves the best energy efficiency across all MH sequences, with 136× - 146× improvements against the software baseline on the Intel Xeon, and 1.02× - 1.1× improvements against the Vitis Harris accelerator. Figure 4 summarizes the run-time and energy efficiency of different SuperPoint GPU and FPGA accelerators. The Vitis SuperPoint accelerator with VCK190 achieves the best run-time performance (28 FPS) and energy efficiency (except for MH04, 745 - 758 mJ/Frame) across all MH sequences, with 2× - 3.1× speedups and 1.2× - 1.4× improvements in energy efficiency, compared with SuperPoint+Orin max. Note that the Vitis SuperPoint accelerator is the only ac- celerator that can achieve real-time performance, whereas SuperPoint+Orin max can only yield up to 14 FPS. The Vitis SuperPoint accelerator can achieve better run-time performance because it is quantized to use INT8 precision, whereas the SuperPoint accelerator from [65] is quantized with FP16 precision. Compared with the FAST accelerators, Harris accelerators on both hardware platforms exhibit worse runtime and energy efficiency, especially for the GPU implementations. This is because FAST is a more efficient algorithm than Harris, as demonstrated in [40]. Furthermore, Harris accelerators have similar power consumption to the FAST accelerators, as reported in Table VII. Compared with the FAST and Harris accelerators, Su- perPoint accelerators on both hardware platforms exhibit worse run-time and energy efficiency. This is expected since SuperPoint is more computationally expensive than FAST and Harris. Further, according to Table VII, the SuperPoint accelerators have a higher power consumption than both the FAST and Harris accelerators, especially for FPGA (21.1 W vs 10.8 W vs 10.3 W). This is because the power of an FPGA accelerator is proportional to its clock frequency and the area it occupies. The AMD Deep Learning Processor Unit (DPU) on which the Vitis SuperPoint executes, operates at a higher frequency (1.3 GHz vs 150 MHz) and occupies more area (FF: 28% vs 0.82% vs 0.97%, LUT: 45% vs 2.91% vs 2.01%, DSP: 42% vs 0% vs 0%, BRAM: 73% vs 1.24% vs 3.62%, AIE: 48% vs 0% vs 0%) than the Vitis FAST and Harris accelerators. B. Results of the Hardware-accelerated ICE-BA Figure 5 summarizes the speedup and energy efficiency improvements of the ICE-BA pipeline integrated with GPU-", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "559f14c811f805ad43f6867c7987abe2e511321a3c1b36f29f1b19383a98ba6b"}
{"doc_id": "arxiv:2510.13546#method:part-5", "url": "https://arxiv.org/abs/2510.13546", "anchor": "#method:part-5", "type": "paper", "title": "", "section": "Method", "text": "(DPU) on which the Vitis SuperPoint executes, operates at a higher frequency (1.3 GHz vs 150 MHz) and occupies more area (FF: 28% vs 0.82% vs 0.97%, LUT: 45% vs 2.91% vs 2.01%, DSP: 42% vs 0% vs 0%, BRAM: 73% vs 1.24% vs 3.62%, AIE: 48% vs 0% vs 0%) than the Vitis FAST and Harris accelerators. B. Results of the Hardware-accelerated ICE-BA Figure 5 summarizes the speedup and energy efficiency improvements of the ICE-BA pipeline integrated with GPU- and FPGA-accelerated FAST and Harris, with respect to the software baseline (ICE-BA+OpenCV FAST+Xeon). Figure 7 summarizes the accuracy (in RMSE ATE) of the ICE- BA pipeline integrated with different FAST, Harris, and SuperPoint accelerators. Regarding ICE-BA integrated with the FAST accelerators, ICE-BA+FTFast+Orin max achieves the best run-time per- formance and energy efficiency across all MH sequences. The run-time performance and energy efficiency of the pipeline can be as low as 9 ms (111 FPS) and 183 mJ/Frame, respectively. ICE-BA integrated with the Vitis FAST accel- erator yields worse performance and therefore worse energy efficiency, due to the disadvantages in processor microarchi- tecture (Arm A72 vs Arm A78), the number of processor cores (2 vs 12), and the clock frequency (1.2 GHz vs 2.2 GHz), between the Orin and VCK190. However, compared with ICE-BA+FTFast+Orin VS, ICE-BA with the Vitis FAST accelerator can achieve comparable run-time performance, with slightly better performance in the MH03, MH04, and MH05 sequences. However, ICE-BA with the FAST FPGA accelerator yields worse energy efficiency compared to ICE- BA+FTFast+Orin VS, due to higher power consumption (12.6 W vs 16.7 W, see Table VII). Compared with the soft- ware baseline on Xeon, ICE-BA+FTFast+Orin max pipeline achieves 2.1× - 10.5× speedups and 11.9× - 57.3× improve- ments in energy efficiency. ICE-BA pipeline integrated with the Vitis FAST accelerator achieves 3× - 25.1× improve- ments in energy efficiency when compared to the software baseline. Regarding accuracy, in general, ICE-BA+FTFast yields slightly better accuracy than the software baseline, whereas ICE-BA integrated with the Vitis FAST accelerator exhibits worse accuracy than ICE-BA+FTFast, except for MH05. This is mainly due to the use of approximation and reduced-precision numbers, where shift operations are used to approximate multiplication and division operations, and fixed-point numbers are used instead of floating-point numbers. In terms of ICE-BA integrated with the Harris accelerators, ICE-BA+VPI Harris+Orin max achieves the best run-time performance in the MH01 and MH02 sequences, whereas the ICE-BA pipeline integrated with the Vitis Harris accelerator achieves the best run-time performance in the MH03, MH04, and MH05 sequences. It is surprising to see ICE-BA+Vitis Harris demonstrates better run-time performance than ICE- BA+VPI Harris+Orin max in ”medium” and ”difficult” dataset sequences with fast motion and poor illumination, while having a disadvantage in the Arm processor mi- croarchitecture. In terms of energy efficiency, ICE-BA+VPI Harris+Orin VS is the most energy efficient one in the MH01 and MH02 sequences, while ICE-BA+Vitis Harris is the most energy efficient implementation in the MH03, MH04, and MH05 sequences. With regards to accuracy, ICE-BA integrated with the Harris FPGA accelerator is more accurate than the GPU counterpart in", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "00daad6973332851c425e2ff6a31d56645807f601dc0e45037ac9cbd900cb62a"}
{"doc_id": "arxiv:2510.13546#method:part-6", "url": "https://arxiv.org/abs/2510.13546", "anchor": "#method:part-6", "type": "paper", "title": "", "section": "Method", "text": "BA+VPI Harris+Orin max in ”medium” and ”difficult” dataset sequences with fast motion and poor illumination, while having a disadvantage in the Arm processor mi- croarchitecture. In terms of energy efficiency, ICE-BA+VPI Harris+Orin VS is the most energy efficient one in the MH01 and MH02 sequences, while ICE-BA+Vitis Harris is the most energy efficient implementation in the MH03, MH04, and MH05 sequences. With regards to accuracy, ICE-BA integrated with the Harris FPGA accelerator is more accurate than the GPU counterpart in ”easy” sequences (MH01 and MH02), whereas ICE-BA integrated with the Harris GPU accelerator is more accurate in ”medium” and ”difficult” sequences (MH03, MH04, and MH05). Compared with the software baseline on Xeon, ICE-BA+VPI Harris+Orin max pipeline achieves 2.2× - 3.6× speedups and 12.2× - 20× improvements in energy efficiency. ICE-BA pipeline integrated with the Vitis Harris accelerator achieves 1.7× - 4.4× speedups and 12.6× - 33.4× improvements in energy efficiency when compared to the software baseline. Figure 5 summarizes the run-time performance and energy efficiency of the ICE-BA pipeline integrated with GPU- and FPGA-accelerated SuperPoint. It is interesting to see that ICE-BA+Vitis SuperPoint can achieve the best run-time performance and energy efficiency with sequences MH01 and MH02, despite the limited Arm cores and their frequency on the VCK190. We believe this is because MH01 and MH02 are ”easy” sequences that represent scenes with good tex- tures. Compared with ICE-BA+SuperPoint+Orin max, ICE- BA+Vitis SuperPoint achieves up to 1.5× speedups and 1.1× improvements in energy efficiency with MH01 and MH02 se- quences. ICE-BA+Vitis SuperPoint can also achieve compa- rable run-time performance with ICE-BA+SuperPoint+Orin max in the rest of the sequences. ICE-BA+SuperPoint+Orin max yields the best run-time performance (up to 7 FPS) and energy efficiency with sequences MH03, MH04, and MH05. In terms of accuracy, ICE-BA integrated with the SuperPoint GPU accelerator is more accurate than ICE-BA+Vitis Super- Point in general, except for MH04. ICE-BA+Vitis SuperPoint Fig. 5. Speedups and energy efficiency improvements of ICE-BA integrated with different FAST and Harris accelerators w.r.t ICE-BA + OpenCV FAST + Xeon. Best viewed in color. Fig. 6. Run-time and energy efficiency of ICE-BA integrated with different SuperPoint accelerators. Best viewed in color. TABLE VIII RUN-TIME PERFORMANCE OF FAST GPU AND FPGA ACCELERATORS, ICE-BA LOCALIZATION THREAD AND PIPELINE, AS WELL AS ICE-BA ACCURACY ACROSS ALL MH SEQUENCES Dataset Sequences Implementations and System Configs Run-time (ms) RMSE ATE (m) Feature Detector Localization Thread Pipeline MH01 OpenCV FAST + Orin max 2.41 4.03 30.69 0.26 FTFast + Orin max 0.26 2.84 12.77 0.22 OpenCV FAST + Versal 7.35 15.82 128.19 0.23 Vitis FAST 0.23 10.96 42.07 0.34 MH02 OpenCV FAST + Orin max 1.88 4.03 31.19 0.27 FTFast + Orin max 0.26 2.9 13.19 0.16 OpenCV FAST + Versal 7.15 15.61 128.63 0.29 Vitis FAST 0.3 11.19 64.72 0.29 MH03 OpenCV FAST + Orin max 1.66 3.64 24.26 0.22 FTFast + Orin max 0.26 2.58 8.97 0.21 OpenCV FAST + Versal 6.34 14.47 96.12 0.15 Vitis FAST 0.3 9.54 28.16 0.62 MH04 OpenCV FAST + Orin max 1.27 3.39 50.68 0.46 FTFast + Orin max 0.26 2.51 9.97 0.21 OpenCV", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "a548b58803d545104a2c7f74bbca21c1e904790804f5b3b7ea576cb777a16d71"}
{"doc_id": "arxiv:2510.13546#method:part-7", "url": "https://arxiv.org/abs/2510.13546", "anchor": "#method:part-7", "type": "paper", "title": "", "section": "Method", "text": "+ Orin max 1.88 4.03 31.19 0.27 FTFast + Orin max 0.26 2.9 13.19 0.16 OpenCV FAST + Versal 7.15 15.61 128.63 0.29 Vitis FAST 0.3 11.19 64.72 0.29 MH03 OpenCV FAST + Orin max 1.66 3.64 24.26 0.22 FTFast + Orin max 0.26 2.58 8.97 0.21 OpenCV FAST + Versal 6.34 14.47 96.12 0.15 Vitis FAST 0.3 9.54 28.16 0.62 MH04 OpenCV FAST + Orin max 1.27 3.39 50.68 0.46 FTFast + Orin max 0.26 2.51 9.97 0.21 OpenCV FAST + Versal 5.07 13.44 157.78 0.36 Vitis FAST 0.29 9.09 29.92 0.39 MH05 OpenCV FAST + Orin max 1.26 3.22 79.28 0.82 FTFast + Orin max 0.25 2.61 15.94 0.56 OpenCV FAST + Versal 5.02 12.9 318.93 1.1 Vitis FAST 0.3 9.5 44.47 0.48 is less accurate since the Vitis SuperPoint is quantized using INT8 precision, while the SuperPoint GPU accelerator from [65] uses FP16 precision. In general, ICE-BA integrated with FAST GPU acceler- ators is more high-performance, energy efficient, and ac- curate than ICE-BA with Harris GPU accelerators. How- ever, ICE-BA+Vitis FAST shows slightly worse performance and energy efficiency than ICE-BA+Vitis Harris, also being less accurate in ”easy” and ”medium” sequences such as MH01, MH02, and MH03. Furthermore, despite being the configuration that yields the worst run-time performance and energy efficiency, ICE-BA+SuperPoint is not always more Fig. 7. Accuracy of ICE-BA integrated with different FAST, Harris, and SuperPoint accelerators. Best viewed in color. accurate than either ICE-BA+FAST or ICE-BA+Harris. For example, on both hardware platforms, ICE-BA+SuperPoint is only more accurate than ICE-BA+FAST and ICE-BA+Harris on the MH01 ”easy” sequence that has good texture and illumination. We also discovered that the use of hardware accelerators for feature detection, might have a positive effect on run- time performance for the downstream ICE-BA modules in its mapping thread, especially for the global bundle adjustment module. Table VIII summarizes the run-time of the feature detection module and the localization thread, as well as the run-time and accuracy of the ICE-BA pipeline integrated with different FAST implementations. According to Table VIII, after replacing the FAST implementation from OpenCV with FTFast and Vitis FAST, the decrease in run-time for the feature detection module is 1.01 ms - 2.15 ms (GPU) and 4.72 ms - 7.12 ms (FPGA), respectively, while the decrease in run-time for the localization thread is 0.61 ms - 1.19 ms (GPU) and 3.4 ms - 4.93 ms (FPGA), respec- tively. However, the decrease in run-time for the pipeline is much larger, i.e., 15.29 ms - 63.34 ms (GPU) and 63.91 ms - 274.46 ms (FPGA), respectively. Considering the localization thread runs in parallel with the local and global bundle adjustment modules in the mapping thread, and the global bundle adjustment module is empirically the most time-consuming module within a V-SLAM pipeline [4], we believe that the use of hardware accelerators for feature detection can affect the performance of the global bundle adjustment module. Further investigation shows that, when using FTFast or Vitis FAST, global bundle adjustment is invoked less frequently, compared with using OpenCV FAST, which leads to a decrease in run-time. Global", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "24d6a02bf36733e94b433f85f07bbca1239839e90170425236f028b9f912699a"}
{"doc_id": "arxiv:2510.13546#method:part-8", "url": "https://arxiv.org/abs/2510.13546", "anchor": "#method:part-8", "type": "paper", "title": "", "section": "Method", "text": "with the local and global bundle adjustment modules in the mapping thread, and the global bundle adjustment module is empirically the most time-consuming module within a V-SLAM pipeline [4], we believe that the use of hardware accelerators for feature detection can affect the performance of the global bundle adjustment module. Further investigation shows that, when using FTFast or Vitis FAST, global bundle adjustment is invoked less frequently, compared with using OpenCV FAST, which leads to a decrease in run-time. Global bundle adjustment is a non-linear least squares system solver that jointly optimizes all the landmarks and the feature points that can be observed from each landmark in the global map, to further reduce the accumulated translation and rotation error, thus improving accuracy. It is surprising to find that, despite invoking global bundle adjustment less frequently, ICE-BA+FTFast is more accurate than ICE-BA+OpenCV FAST across all MH sequences. Although ICE-BA+Vitis FAST is, in general, less accurate than the software baseline, we believe this is because of the use of reduced-precision numbers and approximations in the Vitis FAST accelerator design. VI. CONCLUSIONS This paper is the first study of feature detectors consid- ering a V-SLAM on state-of-the-art SoCs with FPGA/GPU. The evaluation shows that when using a non-learning-based feature detector such as FAST and Harris, FTFast and Harris from the Nvidia VPI library, as well as ICE-BA+FTFast and ICE-BA+VPI Harris, can achieve better run-time per- formance and energy efficiency than the Vitis FAST and Harris accelerators as well as the FPGA-accelerated ICE- BA. However, when considering a learning-based detector such as SuperPoint, the Vitis SuperPoint accelerator can achieve better run-time performance and energy efficiency (up to 3.1× and 1.4× improvements, respectively) than its GPU counterpart. ICE-BA+Vitis SuperPoint can also achieve comparable run-time performance compared to ICE-BA inte- grated with the SuperPoint GPU accelerator, with better FPS in 2 out of 5 dataset sequences. However, when considering the accuracy, the results show that the GPU-accelerated ICE- BA is more accurate than the FPGA-accelerated ICE-BA in general. We also discovered that the use of hardware acceler- ation for feature detection could further improve the run-time of the V-SLAM pipeline by having global bundle adjustment (typically the most time-consuming module) invoked less frequently, while not sacrificing the accuracy. ACKNOWLEDGEMENT This work is partially funded by the UK Industrial Strat- egy Challenge Fund (ISCF) under the Digital Security by Design (DSbD) Programme delivered by UKRI as part of the Soteria (75243) projects and EPSRC EP/T026995/1 (EnnCore project). Mikel Luj´an is supported by a Royal Society Wolfson Fellowship and an Arm/RAEng Research Chair Award.", "source": "arxiv_pdf", "published": "", "tokens": 426, "sha256": "a00b19d555b64858e5aec619c6843b4416535b6381bbc30bc20fe5f5d050b653"}
{"doc_id": "arxiv:2510.13625#abstract", "url": "https://arxiv.org/abs/2510.13625", "anchor": "#abstract", "type": "paper", "title": "", "section": "ABSTRACT", "text": "Within the field of robotics, computer vision remains a significant barrier to progress, with many tasks hindered by inefficient vision systems. This research proposes a generalized vision module leveraging YOLOv9, a state-of-the-art framework optimized for computationally constrained environments like robots. The model is trained on a dataset tailored to the FIRA robotics Hurocup. A new vision module is implemented in ROS1 using a virtual environment to enable YOLO compatibility. Performance is evaluated using metrics such as frames per second (FPS) and Mean Average Precision (mAP). Performance is then compared to the existing geometric framework in static and dynamic contexts. The YOLO model achieved comparable precision at a higher computational cost then the geometric model, while providing improved robustness. Laurentian Intelligent Mobile Robotic Lab Keywords Computer Vision · Machine Learning · YOLO · ROS. 1", "source": "arxiv_pdf", "published": "", "tokens": 134, "sha256": "9c61f0a04fecc011dada0e3f0fc72e15d6c0b62db814e718ee7a6bf354199d7b"}
{"doc_id": "arxiv:2510.13625#introduction:part-1", "url": "https://arxiv.org/abs/2510.13625", "anchor": "#introduction:part-1", "type": "paper", "title": "", "section": "Introduction", "text": "In computer vision, AI (Artificial Intelligence) powered frameworks have significantly increased performance in both speed and accuracy. Conventional object detection strategies have been rendered increasingly obsolete by object detection AI, such as CNNs (Convolutional Neural Networks). In robotics, applications of CNNs have remained limited due to their large computational cost on constrained systems. Machine learning frameworks typically require large datasets and a powerful GPU (Graphical Processing Unit) to achieve good performance. Robotic environments rarely have access to large amounts of memory, and their GPUs tend to be limited, or even non-existent. Recently, CNN frameworks have seen substantial performance improvements. Specifically, the latest YOLO (You Only Look Once) model, YOLOv9, outperforms almost every other framework in most aspects Wang et al. [2024]. This paper presents the design and optimization of a lightweight, general-purpose vision module optimized for human robotics using YOLOv9. To evaluate its utility, we examine performance within the context of the FIRA (Federation of International Robot-Sports Association) Hurocup, a competition in which robots are tasked with performing visually intensive tasks such as archery, weightlifting, and basketball. The vision module is designed to run on a CPU-only platform, reflecting the limitations commonly found in humanoid robots Our objectives are threefold : (1) assess whether YOLO-based detection can operate efficiently under the hardware constraints typical of humanoid robots; (2) compare its performance to traditional vision techniques; and (3) discuss the broader implications of integrating object detection frameworks in robotics. By demonstrating the viability of real-time YOLO-based detection on constrained systems, this work contributes to closing the gap between cutting-edge computer vision research and its deployments in field robotics. arXiv:2510.13625v1 [cs.RO] 15 Oct 2025 A PREPRINT 2 Literature Review 2.1 Object Detection Computer vision encompasses many subfields, with object detection being a critical area due to its growing relevance in real-time applications such as autonomous vehicles Arnold et al. [2019], agriculture Wang et al. [2020], Salazar-Gomez et al. [2021], Khan and AlSuwaidan [2022], surveillance Murugan et al. [2018], Angadi and Nandyal [2020], and healthcare Ahmed et al. [2025], Wang et al. [2022a]. Modern object detection involves both classifying objects and localizing them with bounding boxes—tasks essential for systems requiring fast, accurate decisions. Early methods relied on feature-based techniques like Histogram of Oriented Gradients (HOG), which showed high accuracy in pedestrian detection Suard et al. [2006] and facial recognition Déniz et al. [2011]. Scale-Invariant Feature Transform (SIFT) improved matching through keypoint detection Kasiselvanathan et al. [2020], Meng et al. [2013], while Speeded-Up Robust Features (SURF) enhanced performance for real-time tasks Li et al. [2012], He et al. [2019]. The rise of deep learning and convolutional neural networks (CNNs) revolutionized object detection. Two-Stage Detectors like Fast R-CNN emphasize accuracy and have shown success in pavement crack detection Xu et al. [2022] and facial expression recognition Li et al. [2017]. One-Stage Detectors, including SSD and the YOLO family, prioritize speed, making them ideal for real-time use. While AI-powered vision has been widely applied in areas like surgical robotics Ai et al. [2023] and infrastructure monitoring Ward et al. [2021], humanoid robotics lags behind. Most research in that", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "0cd2317fefbc95f0bacd8033ea9d39347116e45307ad2d8efffc569b8cc8e64c"}
{"doc_id": "arxiv:2510.13625#introduction:part-2", "url": "https://arxiv.org/abs/2510.13625", "anchor": "#introduction:part-2", "type": "paper", "title": "", "section": "Introduction", "text": "neural networks (CNNs) revolutionized object detection. Two-Stage Detectors like Fast R-CNN emphasize accuracy and have shown success in pavement crack detection Xu et al. [2022] and facial expression recognition Li et al. [2017]. One-Stage Detectors, including SSD and the YOLO family, prioritize speed, making them ideal for real-time use. While AI-powered vision has been widely applied in areas like surgical robotics Ai et al. [2023] and infrastructure monitoring Ward et al. [2021], humanoid robotics lags behind. Most research in that domain emphasizes interaction and learning Kanda et al. [2002], Oztop et al. [2004], Bao et al. [2023], Hester et al. [2010], rather than detection. This paper addresses that gap by developing a robust vision module suited for humanoid platforms, leveraging recent advances in real-time object detection. 2.2 YOLO: You Only Look Once The YOLO framework is a leading one-stage object detector used in both research and industry Chen et al. [2023], Hussain [2023]. As a CNN model, YOLO processes images through convolutional, pooling, and fully connected layers to detect and localize objects with labeled bounding boxes and confidence scores. YOLOv1, introduced in 2015, pioneered the single-stage detection approach and achieved 45 FPS with 63.4% average precision (AP) Redmon et al. [2016]. YOLOv2 followed with major improvements, reaching 78.6% AP at 40 FPS Redmon and Farhadi [2016], and YOLOv3 further refined the model in 2018 Redmon and Farhadi [2018]. YOLOv4 (2020) introduced key techniques such as mosaic augmentation and drop block regularization Bochkovskiy et al. [2020]. Subsequent versions, including YOLOv5 and YOLOv6, enhanced performance further; YOLOv6 notably achieved 35.9% AP at 1234 FPS Li et al. [2022]. YOLOv7 improved both speed and accuracy Wang et al. [2022b]. The latest version, YOLOv9, introduces programmable gradient information and the GELAN architecture, pushing the framework to new performance heights. Though widely adopted across domains, YOLOv9 has yet to be implemented on a humanoid robot. 2.3 Robotic Applications & Vision Modules Vision modules are critical to robotic systems, serving as the interface between sensory input and decision-making. Their primary function is to interpret visual data for tasks such as navigation and object detection. Earlier systems relied on classical techniques like segmentation, edge detection, and template matching. While effective in some cases, these methods struggle under real-world variability and noise. Recent advances in AI have greatly improved the adaptability and accuracy of vision modules, making research in this area crucial for modern robotics. Classical approaches—such as color segmentation and template matching—were once standard for robotic vision Al-Jarrah et al. [2018] Hurtado and Valada [2022]. These methods are fast and require little processing power, but they lack robustness. Variability in lighting, background clutter, and object similarity can easily confuse such systems. For instance, similarly colored objects or reflective surfaces can lead to detection failures and hinder performance. The emergence of deep learning revolutionized robotic vision. CNN-based frameworks like YOLO, Faster R-CNN, and SSD (Single-shot Detectors) allow for more precise detection in complex scenes Redmon et al. [2016] Liu et al. [2016] Ren et al. [2016]. These models, trained on large datasets, generalize far better than traditional methods. However, their", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "cbe3fc113a4bbde3e12dbc1f7a2eede9dbe876bbc235bee74f86623653fa9250"}
{"doc_id": "arxiv:2510.13625#introduction:part-3", "url": "https://arxiv.org/abs/2510.13625", "anchor": "#introduction:part-3", "type": "paper", "title": "", "section": "Introduction", "text": "in lighting, background clutter, and object similarity can easily confuse such systems. For instance, similarly colored objects or reflective surfaces can lead to detection failures and hinder performance. The emergence of deep learning revolutionized robotic vision. CNN-based frameworks like YOLO, Faster R-CNN, and SSD (Single-shot Detectors) allow for more precise detection in complex scenes Redmon et al. [2016] Liu et al. [2016] Ren et al. [2016]. These models, trained on large datasets, generalize far better than traditional methods. However, their implementation on robots introduces new challenges. Most robots lack GPUs, making real-time CPU inference difficult. Additionally, building large, diverse training datasets is time-consuming and resource-intensive. Variations in lighting, 2 A PREPRINT Figure 1: LIMRL’s two humanoid robots. ROBOTIS-OP3, also known as Oscar (pictured left), will be used for this implementation. The vision module will also be installed on Polaris (pictured right). occlusion, and clutter still pose issues Kasiselvanathan et al. [2020]. Designing vision modules for humanoid robots thus requires carefully balancing accuracy, speed, and hardware constraints. 2.4 Research Gap This research serves to address a critical gap in the intersection of Computer Vision, YOLO-based frameworks, and robotic vision. While YOLO has been extensively used for tasks such as autonomous driving and surveillance, its applications in humanoid robotics remain very limited. Current implementations of vision modules rely on task-specific configurations and traditional techniques. This can lead to struggles in tasks where a more generalized approach is required or in unpredictable environments such as FIRA competitions. This paper leverages YOLO to create a generalized vision module that balances efficiency and accuracy, enabling its deployment onto resource-constrained humanoid robots. This will contribute to the field by addressing the dual challenge of computational constraints and the lack of general-purpose vision solutions for humanoid robots. By employing various optimization tools and making the most of performance improvements, the YOLO framework might finally be accessible for humanoid robotics. 3 Methodology 3.1 Hardware Configuration The humanoid robot used in this paper is the ROBOTIS-OP3 (See Fig. 1), a 51 cm tall, 3.5 kg platform equipped with 20 XM430-W350-R actuators and a Logitech C920 HD Pro Webcam. It’s powered by an 11.1V 1800mA LiPo battery or a direct cable connection. Minor modifications include 3D-printed plastic hands and feet. The OP3 runs ROS1 Kinetic, enabling smooth hardware interaction, while its computational efficiency and real-time processing make it suitable for object detection tasks. Local image processing reduces latency, though its modular architecture allows easy transition to external computing if needed. Connections can be made via a VNC client or an Ethernet cable. 3.2 YOLOv9 Preparation and Training As previously outlined, this project integrates the YOLO object detection framework into a humanoid robot to enable real-time detection within hardware constraints and ensure fast communication between vision and control systems. The software stack combines pre-trained models, open-source libraries, and middleware to maintain both flexibility and reproducibility. The implementation is built on the Robot Operating System (ROS) for its modular design and rich library support. ROS enables seamless integration of the camera, YOLO inference, and actuator control. Python is used for development, leveraging PyTorch and", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "3364c30c943d7c60bd915134712573cf9bd5ae6a97a8883d3bfe0af0439b3964"}
{"doc_id": "arxiv:2510.13625#introduction:part-4", "url": "https://arxiv.org/abs/2510.13625", "anchor": "#introduction:part-4", "type": "paper", "title": "", "section": "Introduction", "text": "integrates the YOLO object detection framework into a humanoid robot to enable real-time detection within hardware constraints and ensure fast communication between vision and control systems. The software stack combines pre-trained models, open-source libraries, and middleware to maintain both flexibility and reproducibility. The implementation is built on the Robot Operating System (ROS) for its modular design and rich library support. ROS enables seamless integration of the camera, YOLO inference, and actuator control. Python is used for development, leveraging PyTorch and OpenCV—both essential for YOLO-based computer vision tasks. 3 A PREPRINT Table 1: Table describing which objects will be included within the datasets, organized by event. YOLOv9 Training Object Examples Model Name Description Image Basketball Ball A ping pong ball is used for the kid size, and a larger tennis ball for the adult size. Basket A bright basketball where the robot must be able to score points on. The kid-sized basket has a diameter of 10cm, while the adult-sized basket has a diameter of 30cm. Archery Target An archery target with a diameter of 50cm. Marathon Line A red guiding line which the robot is tasked with following. Right Arrow An arrow whose tip points to the right. Left Arrow An arrow whose tip points to the left. Forward Arrow An arrow whose tip points straight ahead. YOLO was chosen for its strong balance between speed and accuracy. The latest version, YOLOv9, introduces Programmable Gradient Information (PGI) and the GELAN architecture, enhancing inference time and accuracy, making it well-suited for robotic deployment. This paper applies YOLO to three FIRA Hurocup events: Archery, Marathon, and Basketball. Table 1 shows an example of the objects used in training. In Archery, the model detects a moving target and fires an arrow at its center. In Marathon, the robot follows a red line and classifies directional arrow markers (left, right, forward), which may appear rotated. For Basketball, the robot detects an orange ball, retrieves it, locates a red net, approaches it, and shoots when close enough. The training process begins with collecting and preprocessing task-specific datasets. To maintain performance and efficiency, multiple smaller models are trained for individual tasks rather than a single large one. Each dataset corresponds to specific FIRA Hurocup events—for example, balls and nets for basketball or arrow markers for the marathon (see Table 3.1). Datasets are annotated using bounding boxes in Roboflow’s YOLOv9 format, where labels include class ID and normalized bounding box coordinates. Roboflow also handles augmentation tasks—such as 4 A PREPRINT Figure 2: Flowchart showing the interaction between ROS Nodes within the vision model’s various environments. The ROS Bridge WebSocket is essential to ensure that both environments can communicate properly. rotation, lighting changes, and scaling—to improve generalization Pellegrino et al. [2024], P et al. [2023], Gupta et al. [2023]. Training was conducted using PyTorch on an Nvidia RTX 1650 GPU. Models were trained over 100 epochs with a batch size of six and a close mosaic threshold of 50. To avoid overfitting, a patience flag of five was set, halting training if no improvement occurred within five epochs—an optimal value", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "5f005768d4fe6f262110cf20e6b312b99eb0c5cc64f34803533a092e54457ce1"}
{"doc_id": "arxiv:2510.13625#introduction:part-5", "url": "https://arxiv.org/abs/2510.13625", "anchor": "#introduction:part-5", "type": "paper", "title": "", "section": "Introduction", "text": "is essential to ensure that both environments can communicate properly. rotation, lighting changes, and scaling—to improve generalization Pellegrino et al. [2024], P et al. [2023], Gupta et al. [2023]. Training was conducted using PyTorch on an Nvidia RTX 1650 GPU. Models were trained over 100 epochs with a batch size of six and a close mosaic threshold of 50. To avoid overfitting, a patience flag of five was set, halting training if no improvement occurred within five epochs—an optimal value determined through prior experimentation. Images were processed at 640×640 resolution, and training began from pre-trained MSCOCO weights Ultralytics [2024]. Three architectures were used: YOLOv9-Tiny, YOLOv9-Small, and YOLOv9-Medium—the most lightweight variants. Training was executed via command line on an external laptop to reduce onboard computational load. 3.3 ROS Integration The vision module integrates tightly with the Robot Operating System (ROS), ensuring seamless communication between the camera, inference pipeline, and OP3’s actuators. The architecture consists of three core components: the YOLO node, a listener node, and event-specific nodes. ROS uses a publisher/subscriber communication model, allowing nodes to exchange data asynchronously through named topics. This modular approach enables system flexibility—nodes can operate independently while exchanging data efficiently. In this system, the YOLO node publishes detection results to a topic, which the listener node subscribes to, formats, and routes to the appropriate event modules. Due to compatibility issues—ROS relying on Python 2.7, OpenCV 3.3.1, and YOLO requiring Python 3—the YOLO node is run in a Python 3.12 virtual environment with OpenCV 4.11. This isolation allows it to leverage optimized libraries like PyTorch for model inference. The node subscribes to a camera feed, processes video frames resized to the model’s input dimensions, and performs object detection. Detections (bounding boxes, labels, and confidence scores) are published via WebSocket as JSON. The listener node then receives this data and republishes it on ROS topics suitable for each event. To optimize performance, inference can be limited to 20 detections per second rather than processing every frame. Fig. 2 provides a visual representation of the various environments used. The aforementioned issues between YOLO and Python 2.7 introduce the need for a separate listener node to receive detection data via WebSocket and republish it to the /yolo/bboxes topic. This node enables event-specific modules to access detection results in a ROS-compatible format without direct integration with the YOLO node. Each event node 5 A PREPRINT subscribes to /yolo/detections and uses the data to identify targets and initiate actions such as approach or avoidance. Object tracking is handled by pre-written Python logic, while actuator control is managed through the Dynamixel SDK. Given the limited CPU power onboard OP3, real-time inference performance remains a challenge. To mitigate this, training is performed externally, while onboard inference is optimized using frame skipping and resizing inputs to 640×640 resolution. Since OP3 lacks a GPU, the YOLO model is converted to OpenVINO for CPU efficiency. This involves first converting the model to ONNX format, where quantization reduces computational load. The modular nature of ROS further allows for easy substitution or scaling with alternative detection models. 3.4 Performance Evaluation To", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "96b381c1dbdac5f68f598a03541a90c713dba1a6c9324f3563e61a4db43411ae"}
{"doc_id": "arxiv:2510.13625#introduction:part-6", "url": "https://arxiv.org/abs/2510.13625", "anchor": "#introduction:part-6", "type": "paper", "title": "", "section": "Introduction", "text": "CPU power onboard OP3, real-time inference performance remains a challenge. To mitigate this, training is performed externally, while onboard inference is optimized using frame skipping and resizing inputs to 640×640 resolution. Since OP3 lacks a GPU, the YOLO model is converted to OpenVINO for CPU efficiency. This involves first converting the model to ONNX format, where quantization reduces computational load. The modular nature of ROS further allows for easy substitution or scaling with alternative detection models. 3.4 Performance Evaluation To ensure reliable performance, the new vision module will undergo comprehensive testing in both static and dynamic environments. Initial evaluations will take place in a controlled laboratory setting with minimal background interference and fixed lighting. From a stationary position, the robot’s ability to detect objects will be assessed in order to measure frame rate and latency in a reproducible environment. Following static tests, the vision module will be evaluated in real-world conditions by completing three FIRA Hurocup events: Archery, Basketball, and Marathon. Performance will be assessed using quantitative metrics such as FPS, Precision, Recall, mAP50, and mAP50–95. Static and dynamic FPS measurements will help evaluate latency and consistency. Distance estimation will be tested by comparing the vision module’s output to a geometric approach and ground truth measurements. This comparison will highlight the system’s accuracy and its advantages over previous methods. Extended testing will ensure robustness and guide iterative improvements to detection accuracy in dynamic conditions. 4 Implementation 4.1 Model Deployment The deployment of the YOLO model involved a lengthy training process, beginning with dataset preparation and culminating in real-time inference. The dataset was composed of no more than 150 images per class to attain a balance between speed and accuracy. The data sets were manually labeled using RoboFlow to help the model focus on key objects, such as basketballs or arrow markers. Data augmentation techniques such as rotation, shear, exposure adjustment, and Gaussian blur were applied to improve overall robustness. For real-time inference, a video feed is captured through the ROBOTIS-OP3’s webcam. Each captured frame was processed using a letterbox resizing function to maintain the image’s aspect ratio while fitting the required input size (640px x 640px). Several techniques were utilized to help optimize the model for a CPU-based inference, including model conversion into OpenVINO format. The bounding boxes are drawn only if they are over a confidence threshold, and non-max suppression (NMS) is applied to filter overlapping bounding boxes, retaining only the most accurate detections. Bounding boxes were then scaled back to the original image dimensions, correcting for the webcam’s aspect ratio and avoiding distorted bounding boxes. This step was crucial to ensure accurate placement of detections in the video feed. OpenCV 4.11 was used to draw bounding boxes and display class labels on the detected objects, along with a confidence score. 4.2 ROS Node Developpement The YOLO object detection system is implemented using two separate ROS nodes due to Python version incompatibili- ties. ROS Kinetic requires Python 2.7, whereas YOLOv9 and its dependencies (e.g., PyTorch) need Python 3.8 or newer. To resolve this, the detection node runs in a Python 3.12", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "63166da48461d6099b60f27bb0ab8e84f233ad1b48aa73265c99dfca5c9b029f"}
{"doc_id": "arxiv:2510.13625#introduction:part-7", "url": "https://arxiv.org/abs/2510.13625", "anchor": "#introduction:part-7", "type": "paper", "title": "", "section": "Introduction", "text": "ensure accurate placement of detections in the video feed. OpenCV 4.11 was used to draw bounding boxes and display class labels on the detected objects, along with a confidence score. 4.2 ROS Node Developpement The YOLO object detection system is implemented using two separate ROS nodes due to Python version incompatibili- ties. ROS Kinetic requires Python 2.7, whereas YOLOv9 and its dependencies (e.g., PyTorch) need Python 3.8 or newer. To resolve this, the detection node runs in a Python 3.12 virtual environment, isolated from ROS. A second node—the listener node—operates in Python 2.7 and receives the detection results via WebSocket through a ROS bridge. This bridge enables seamless communication between the two environments. The detection node performs inference using PyTorch and Ultralytics’ YOLOv9, sending bounding box data as JSON messages over WebSocket. Each JSON includes object class, coordinates, size, and confidence score. OpenCV 4.11 is used to overlay detections on the video feed for debugging. The node also includes a cleanup routine to release camera resources and close WebSocket connections on shutdown. The listener node receives the JSON data and republishes it as ROS-compatible messages. Running within the ROS environment, it ensures that all other event-specific nodes can subscribe to detection outputs without needing to interact directly with YOLO. The node logs incoming data to rospy for debugging and includes error handling for malformed 6 A PREPRINT Figure 3: Graph showing the pipeline utilized by Event scripts. The graph shown is for the basketball event. messages or connection failures. WebSocket lifecycle events are managed to allow reconnections and diagnostic logging. This dual-node architecture ensures modern deep learning tools can be used without disrupting ROS compatibility. The modular design promotes maintainability and scalability, allowing future upgrades to the detection pipeline without altering the core ROS control structure. 4.3 Pipeline and Behavior Integration The new vision module is integrated into the robot’s framework using a structured pipeline built on ROS architecture. This setup enables efficient data exchange between the vision system and behavioral modules, particularly for FIRA events, allowing real-time object detection to drive robot actions. The main implementation challenge stemmed from YOLO’s Python 3 requirement conflicting with ROS Kinetic’s Python 2.7. To solve this, a WebSocket bridge was created to connect a Python 3.12 virtual environment (hosting the YOLO node) with the ROS system. Initial setup issues—such as the virtual environment defaulting to Python 2.7 libraries like OpenCV 3.3.1 and PIP—were resolved by restructuring the environment with the –no-site-packages flag. This allowed the isolated vision node to communicate with the ROS system without dependency conflicts. The YOLO vision node sends detection data as JSON messages over WebSocket. These are received by a listener node, which repackages the data into ROS-compatible messages. This decoupled, asynchronous communication ensures low-latency performance—critical for the fast reactions required in FIRA events. Event nodes can now easily subscribe to the updated detection topics. Launching an event is as simple as updating the roslaunch file to point to the new vision node. Once active, these nodes interpret bounding box data to determine object location and trigger behaviors such as tracking,", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "1d92d1b04e8b04bb7729788e4cb130d5a89d1b70f660f7baf99ad9d85aad611a"}
{"doc_id": "arxiv:2510.13625#introduction:part-8", "url": "https://arxiv.org/abs/2510.13625", "anchor": "#introduction:part-8", "type": "paper", "title": "", "section": "Introduction", "text": "JSON messages over WebSocket. These are received by a listener node, which repackages the data into ROS-compatible messages. This decoupled, asynchronous communication ensures low-latency performance—critical for the fast reactions required in FIRA events. Event nodes can now easily subscribe to the updated detection topics. Launching an event is as simple as updating the roslaunch file to point to the new vision node. Once active, these nodes interpret bounding box data to determine object location and trigger behaviors such as tracking, turning, or avoidance, depending on the object’s position and proximity. The ROS publisher-subscriber model ensures each component processes only the data it needs, improving efficiency and scalability. This modular structure preserves ROS core stability while supporting extensibility, making it ideal for future upgrades. The separation of perception from decision-making enhances maintainability and ensures the system can support additional tasks beyond the FIRA competition. An example of the complete pipeline during an event can be found in Fig. 3 7 A PREPRINT 4.4 Optimization Optimizing the YOLOv9 model is essential to achieving real-time performance on the ROBOTIS-OP3’s CPU-only system. Several strategies—frame skipping, input resizing, model conversion, and quantization—were employed to reduce inference latency while preserving detection accuracy. One of the most effective methods was frame skipping. Rather than processing every incoming frame, the system processes every nth frame, where n is a configurable parameter. This significantly reduces computational load while maintaining responsiveness, allowing the robot to track and react to objects without overloading the CPU. Another crucial optimization was input resizing. Camera frames are downscaled to 640×640 pixels—the native YOLO input size—prior to inference. This ensures efficient processing without unnecessary high-resolution computations, speeding up preprocessing and inference while retaining detection reliability. To further reduce inference time, the YOLOv9 model was converted from PyTorch to ONNX (Open Neural Netowrk Exchange), and then from ONNX to OpenVINO, Intel’s toolkit optimized for CPU performance. The ONNX conversion ensures model portability, while OpenVINO accelerates inference by leveraging CPU-specific instructions and pruning redundant operations. Finally, quantization was applied to reduce model size and inference time. PyTorch’s quantization tools lowered weights and activations from 32-bit floats to 8-bit integers.This step greatly decreased memory usage and sped up matrix operations—the core bottleneck of inference. Together, these optimizations enable the YOLOv9 system to function efficiently within OP3’s hardware limitations. By combining model-level and hardware-specific techniques, the system strikes a strong balance between speed and accuracy, allowing real-time object detection in demanding robotic environments. 5", "source": "arxiv_pdf", "published": "", "tokens": 405, "sha256": "145ea6c44690277eee6cad2b3bf14120b1271117e7417f5f5d8d7f5172968b93"}
{"doc_id": "arxiv:2510.13625#results:part-1", "url": "https://arxiv.org/abs/2510.13625", "anchor": "#results:part-1", "type": "paper", "title": "", "section": "Results", "text": "5.1 YOLO Training Results Training the YOLOv9 models was a crucial step in ensuring high detection accuracy while maintaining real-time performance. For this implementation, nine models were trained—three per FIRA event—using the YOLOv9-Tiny (T), Small (S), and Medium (M) architectures. This multi-model strategy allowed for a balance between accuracy and speed, essential for deployment on the limited hardware of the ROBOTIS-OP3. To improve model accuracy, all models employed weights that were pre-trained on the MSCOCO dataset. These weights can be found at Ultralytics [2024]. Additionally, all models were trained with a patience of 5, as to avoid overfitting. Results for the training can be found in Table 2 Table 2: Results of YOLO model training. All models were trained on 100 epochs with a patience factor of 5 YOLOv9 Training Results Model Name mAP@0.5 mAP@0.5-0.95 Precision Recall Basketball YOLOv9-T 0.994 0.92536 0.98565 0.98383 YOLOv9-S 0.995 0.96225 0.98253 0.99809 YOLOv9-M 0.99472 0.95518 0.9859 0.98571 Archery YOLOv9-T 0.95946 0.92109 0.95934 1 YOLOv9-S 0.98321 0.91407 0.95782 1 YOLOv9-M 0.99393 0.82793 0.95612 1 Marathon YOLOv9-T 0.75063 0.68905 0.67078 0.89904 YOLOv9-S 0.78545 0.71538 0.61382 1 YOLOv9-M 0.73501 0.61436 0.5923 0.87498 8 A PREPRINT For this implementation, nine models were trained—three per FIRA event—using the YOLOv9-Tiny (T), Small (S), and Medium (M) architectures. This multi-model strategy allowed for a balance between accuracy and speed, essential for deployment on the limited hardware of the ROBOTIS-OP3. Training results varied by event. In the Basketball event, all three models achieved high scores across mAP, precision, and recall metrics, indicating strong generalization and reliable detection across frames. In contrast, Archery models exhibited mixed results. Despite high recall and mAP0.5, the mAP0.5–0.95 scores varied significantly. This, along with elevated recall scores, suggests overfitting: the models successfully identify targets seen during training but fail to generalize well to new data. Further investigation or dataset augmentation may be necessary to improve their robustness. The Marathon models performed the weakest overall, showing lower mAP and precision across all three architectures. While recall remained relatively high, the models struggled with classification accuracy. This was largely due to the similar appearance of visual markers used in the event, particularly the left and right arrows, which the models frequently confused. Analysis of the confusion matrix in Fig. 4 confirmed this as a consistent error pattern. Figure 4: The confusion matrix for YOLOv9-T’s marathon model. The matrix suggests the model is frequently confusing left and right arrows due to their similar appearance. The primary limiting factor in model performance was dataset generalization. Given the OP3’s limited computing resources, the training dataset had to be kept small to allow for real-time inference. This constraint often led to overfitting, with models memorizing training samples rather than learning robust feature representations. Although this tradeoff limited accuracy in complex events such as Marathon, it was necessary to preserve real-time operability. Given the superior performance in terms of frame rate and efficiency, the Tiny (YOLOv9-T) models were selected for deployment. They offered the best balance of speed and acceptable accuracy, making them ideal for the real-time requirements of the robot’s vision system. Benchmark tests, conducted", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "0530596632ccb835c731bab798415a8dcdb1c942724e967f09cd05a2200b203d"}
{"doc_id": "arxiv:2510.13625#results:part-2", "url": "https://arxiv.org/abs/2510.13625", "anchor": "#results:part-2", "type": "paper", "title": "", "section": "Results", "text": "inference. This constraint often led to overfitting, with models memorizing training samples rather than learning robust feature representations. Although this tradeoff limited accuracy in complex events such as Marathon, it was necessary to preserve real-time operability. Given the superior performance in terms of frame rate and efficiency, the Tiny (YOLOv9-T) models were selected for deployment. They offered the best balance of speed and acceptable accuracy, making them ideal for the real-time requirements of the robot’s vision system. Benchmark tests, conducted with all optimizations applied and with no objects in view, confirmed that the Tiny models consistently delivered the highest frame rates under idle conditions (see Table 3). As such, the performance tests for the YOLO Vision Module will be conducted using the YOLOv9-T models. Future work may focus on leveraging transfer learning or lightweight augmentation strategies to enhance model generalization without sacrificing speed. 9 A PREPRINT Figure 5: All experiments, both static and dynamic, will be conducted on the LIMRL’s turf field, similar to the ones utilized at the FIRA competition. This photo shows the setup for the Basketball Event. Table 3: Results for the idle FPS testing. Values were averaged over a minute with no detections for a clear baseline. Idle Frame Rate Test Results Architecture Basketball Archery Marathon YOLOv9-T 7.99 FPS 8.01 FPS 7.98 FPS YOLOv9-S 2.75 FPS 2.75 FPS 2.74 FPS YOLOv9-M 0.86 FPS 0.86 FPS 0.84 FPS 5.2 Experimental Setup and Procedure Two distinct types of experiments were conducted to evaluate the performance of the YOLOv9 module in a robotic setting: a static test and a dynamic test. These tests aim to measure the system’s inference speed, real time performance, and accuracy in a controlled setting and in an active use case. Additionally, both tests will be compared to the geometric approach currently implemented on the robot. This is to serve as a baseline for assessing the vision based model’s effectiveness. All tests will be conducted inside the LIMRL as shown in Fig. 5 The static test isolates the vision system’s performance by processing frames while the robot remains stationary. Key metrics include inference time and frames per second, providing a baseline for real-time feasibility and computational load. The dynamic test evaluates the system under operational conditions, where the robot uses its vision module to interact with its environment. This reveals how movement, lighting, and real-time demands affect detection and responsiveness. Both tests are compared to a geometric baseline that uses color segmentation and rule-based logic. This comparison quantifies the trade-offs between speed, accuracy, and practicality, helping assess the benefits of the YOLO-based system in robotic applications. 10 A PREPRINT Table 4: Results for the static testing of the new vision module. FPS measures the speed, the Precision represents the percentage of correct detections. Static Experiment Results Module Speed(FPS) Precision Basketball YOLO Module 5,92 91.6% Geometric Module 12.99 91.7% Archery YOLO Module 6.22 92.24% Geometric Module 10.64 96.6% Marathon YOLO Module 6.12 91.67%* Geometric Module 8.21 50.8% Figure 6: The figure shows an example of a case where the Geometric Node gets thrown off by background noise in", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "317226276ca4fc3229db48b56de46dff7d73a96ecef308b42a2fa3207efd66f7"}
{"doc_id": "arxiv:2510.13625#results:part-3", "url": "https://arxiv.org/abs/2510.13625", "anchor": "#results:part-3", "type": "paper", "title": "", "section": "Results", "text": "A PREPRINT Table 4: Results for the static testing of the new vision module. FPS measures the speed, the Precision represents the percentage of correct detections. Static Experiment Results Module Speed(FPS) Precision Basketball YOLO Module 5,92 91.6% Geometric Module 12.99 91.7% Archery YOLO Module 6.22 92.24% Geometric Module 10.64 96.6% Marathon YOLO Module 6.12 91.67%* Geometric Module 8.21 50.8% Figure 6: The figure shows an example of a case where the Geometric Node gets thrown off by background noise in the static experiment. Due to a change in lighting conditions, the node detects background objects as part of the ball, resulting in errors. The YOLO node is much more resilient to this kind of noise 5.3 Static Experiments In the static experiments (see Table 4), the YOLOv9 models were able to record comparable precision levels to those achieved by the existing geometric model. In the marathon event, the YOLO model has exceeded the precision levels of the geometric model. While the YOLO model made more consistent detections, it frequently confused arrow marker types, resulting in false detections. Therefore, the number of true detections achieved by the YOLO module is likely closer to the geometric model than the numbers show. In terms of FPS, the geometric approach still achieves better performance, sometimes reaching speeds 50% faster than the YOLO module. Even with the optimizations, the YOLO module still performs much slower than the geometric one, however, the YOLO module has several key advantages. Frameworks like YOLO don’t require any calibration post-training, unlike the current color-segmentation-based approach, which requires a lengthy fine-tuning period to achieve good results. Secondly, through data augmentation, the YOLO module is much more resistant to environmental changes. Dataset augmentations, such as brightness or rotation adjustments, allow YOLO to achieve a level of robustness that the geometric approach cannot match. While the geometric approach often struggles with background noise (See Fig. 6), the YOLO node ignores it completely. 11 A PREPRINT 5.4 Dynamic Experiments In terms of performance, the dynamic experiment produced results similar to those of the static experiment (see Table 5). The FPS is lower for both modules; however, the gap in between them remains similar. The exception to this is the marathon event, where the YOLO module can compete with the geometric module because in the marathon event, the geometric module is required to process more complex tasks to detect the arrow markers. Table 5: Results for the Dynamic testing of the new vision module. FPS measures the speed, the precision represents the percentage of correct detections. Dynamic Experiment Results Module Speed(FPS) Precision Basketball YOLO Module 6.68 83,71% Geometric Module 12.30 50.9% Archery YOLO Module 4.41 90.1% Geometric Module 9.24 93.25% Marathon YOLO Module 5.90 39.77% Geometric Module 8.49 55.5% Precision varies drastically between events due to their various requirements. In basketball, the YOLO module outperformed the Geometric module. The geometric module for basketball previously used contour detection, and it struggles with detecting spherical objects due to variances in lighting conditions. The ball detection is set up into two different segmentations and combined to account for the light;", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "b020758c597b72911eb831c6136fe171066476e723dfffd5d7ed48306a239573"}
{"doc_id": "arxiv:2510.13625#results:part-4", "url": "https://arxiv.org/abs/2510.13625", "anchor": "#results:part-4", "type": "paper", "title": "", "section": "Results", "text": "Geometric Module 12.30 50.9% Archery YOLO Module 4.41 90.1% Geometric Module 9.24 93.25% Marathon YOLO Module 5.90 39.77% Geometric Module 8.49 55.5% Precision varies drastically between events due to their various requirements. In basketball, the YOLO module outperformed the Geometric module. The geometric module for basketball previously used contour detection, and it struggles with detecting spherical objects due to variances in lighting conditions. The ball detection is set up into two different segmentations and combined to account for the light; however, the approach remains cumbersome. The YOLO module is much better equipped to deal with this variability. In Archery, both modules performed similarly, and in Marathon, the Geometric module outperformed the YOLO module. Marathon requires the detection of arrow markers, and the YOLO module has a hard time differentiating between marker types. Figure 7 shows the output from the basketball event. Figure 7: YOLO is much more robust, and therefore more resilient to background noise than traditional segmented approaches 12 A PREPRINT 5.5 Discussion Throughout static testing, the geometric module consistently outperformed YOLO in frames per second (FPS), averaging 50% higher across all scenarios due to its lower computational cost. However, its advantage diminished in complex tasks. YOLO maintained a stable frame rate across tests, suggesting optimization potential through pruning or hardware acceleration. A breakdown of per-frame processing time could clarify where each model incurs computational costs. Precision results were more event-dependent. Basketball and Archery yielded comparable scores, while Marathon showed YOLO achieving higher detection rates—but with frequent misclassifications between similar arrow markers, diminishing its effective accuracy. This suggests YOLO’s confidence scores may not always reflect real-world reliability. In contrast, the geometric model’s consistency made it less prone to such false positives, especially with repetitive object shapes. Further tests under distortions like motion blur could assess robustness more thoroughly. In dynamic tests, with movement and real-time decision-making, FPS trends remained consistent: the geometric module led in speed but with a narrowing gap in Marathon due to detection complexity. Event-specific challenges influenced performance. YOLO excelled in Basketball, handling lighting changes and background clutter better than the geometric module, which struggled with occlusions. Marathon again exposed YOLO’s weakness in distinguishing similar markers, while the geometric module maintained higher precision. Archery offered balanced conditions where both methods performed well. Precision varied significantly by event. YOLO outperformed in Basketball due to its adaptability to visual noise. In Archery, both modules achieved high accuracy, with the geometric node slightly ahead. In Marathon, YOLO’s precision dropped sharply due to misclassification. These outcomes emphasize that the optimal module depends on task context: geometric methods suit controlled environments or speed-critical applications, while YOLO thrives in dynamic, unpredictable scenes with adequate resources. Overall, the results highlight a trade-off: the geometric module is fast and effective in simple tasks but lacks adaptability, while YOLO handles complex scenes at the cost of computational load. A hybrid solution—YOLO for detection, geometric for refinement—could combine their strengths. While deep learning tools continue to evolve, both methods remain valid, with choice driven by task demands and hardware limitations. 6", "source": "arxiv_pdf", "published": "", "tokens": 505, "sha256": "3f9d11b34e7a2ad3bb8f09bd58237c7b008d92d7fb1a2b79d5d811a24fad5957"}
{"doc_id": "arxiv:2510.13625#conclusions:part-1", "url": "https://arxiv.org/abs/2510.13625", "anchor": "#conclusions:part-1", "type": "paper", "title": "", "section": "Conclusions", "text": "6.1 Summary of Findings This research successfully demonstrated the feasibility of deploying a YOLO-based vision module on a humanoid robot, overcoming computational challenges through targeted optimizations. By conducting both static and dynamic experiments, the study provides a comprehensive evaluation of the trade-offs between deep-learning-based object detection and the traditional geometric approach. The results do not show that one of these modules is superior, instead, they highlight the strengths and limitations of each method, offering insights for robotic applications. The static experiments confirmed that YOLO can achieve high detection accuracy, often matching the geometric approach in precision. However, the geometric module remains considerably faster, outperforming YOLO in terms of computational efficiency and processing frames at nearly twice the speed in some scenarios. Although expected, this efficiency gap highlights the need for further optimization, especially given OP3’s hardware limitations. The dynamic experiments further emphasized this trade-off. While YOLO showed much better robustness, its real-time performance suffered further. The geometric approach retained its speed advantage, however, it struggled with certain complex object types. This indicates that while deep learning models introduce significant computation overhead, their ability to generalize across varying conditions can significantly enhance real-world usability 6.2 Limitations and Challenges Despite the system’s success in achieving real-time detection under controlled conditions, several limitations must be acknowledged. Firstly, the OP3 platform lacks a dedicated GPU, requiring extensive optimizations to maintain acceptable performance. Even with the quantization and model conversions, inference speed remained a considerable bottleneck in certain high-demand scenarios. Secondly, due to the necessity of maintaining a small dataset, the models exhibited signs of overfitting, particularly in tasks requiring difficult object differentiation. While the system performed well in controlled settings, variations in lighting and background clutter still slightly impacted detection accuracy in real-world scenarios. Even with augmentations, the change in conditions can be so drastic that the need for more adaptive training data augmentation strategies becomes apparent. Addressing these challenges will require a 13 A PREPRINT combination of hardware improvements, additional software optimizations, and refinements to the training process to enhance generalization across diverse operating conditions. 6.3 Future Work This study opens several directions for future research. One key area is hardware acceleration. Leveraging edge computing devices like Intel Neural Compute Sticks or Raspberry Pi AI accelerators could offload inference and reduce computational strain. In networked environments, cloud-based inference or GPU/FPGA-optimized implementations may offer significant performance gains, especially where low-latency is critical. Expanding the dataset to include diverse environmental conditions and using transfer learning can also enhance model generalization. A larger or more varied dataset—potentially aided by synthetic data generation—would help address the overfitting seen with small samples. Another promising avenue is multi-modal sensing. Combining YOLOv9 with depth sensors or LiDAR could improve object localization, especially in occluded or noisy scenes. Sensor fusion techniques like Kalman Filtering may reduce uncertainty and enhance detection robustness. Finally, adaptive inference strategies—such as dynamic frame skipping, resolution scaling, or model switching based on system load—could optimize performance. Reinforcement learning or self-supervised methods could allow models to adapt to real-world conditions without full retraining. In conclusion, this work demonstrates the viability", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "8af0663df1ebf087241879e18dd11dbd115bcbfaefe4fa8e430746270d54c791"}
{"doc_id": "arxiv:2510.13625#conclusions:part-2", "url": "https://arxiv.org/abs/2510.13625", "anchor": "#conclusions:part-2", "type": "paper", "title": "", "section": "Conclusions", "text": "small samples. Another promising avenue is multi-modal sensing. Combining YOLOv9 with depth sensors or LiDAR could improve object localization, especially in occluded or noisy scenes. Sensor fusion techniques like Kalman Filtering may reduce uncertainty and enhance detection robustness. Finally, adaptive inference strategies—such as dynamic frame skipping, resolution scaling, or model switching based on system load—could optimize performance. Reinforcement learning or self-supervised methods could allow models to adapt to real-world conditions without full retraining. In conclusion, this work demonstrates the viability of a YOLOv9-based vision system on a humanoid robot, overcoming hardware constraints through targeted optimizations. It offers a flexible foundation for integrating deep learning in robotics, with potential across fields like autonomous navigation, assistive tech, and industrial automation.", "source": "arxiv_pdf", "published": "", "tokens": 118, "sha256": "7a59603f1af2c8f83bfd43db3e4c0e5f00c9853087bb6cbd4a73e41ed92160d0"}
{"doc_id": "arxiv:2510.13619#abstract", "url": "https://arxiv.org/abs/2510.13619", "anchor": "#abstract", "type": "paper", "title": "", "section": "ABSTRACT", "text": "In this paper we introduce a visualization methodology to aid a human analyst in classifying adversity modes that impact lidar scan matching. Our methodology is intended for offline rather than real-time analysis. The method generates a vector-field plot that characterizes local discrepancies between a pair of registered point clouds. The vector field plot reveals patterns that would be difficult for the analyst to extract from raw point-cloud data. After introducing our methodology, we apply the process to two proof-of-concept examples: one a simulation study and the other a field experiment. For both data sets, a human analyst was able to reason about a series of adversity mechanisms and iteratively remove those mechanisms from the raw data, to help focus attention on progressively smaller discrepancies. 1", "source": "arxiv_pdf", "published": "", "tokens": 125, "sha256": "7709273e16be866c226259768b0297963da3578401537800cd978c5a9fde1aa9"}
{"doc_id": "arxiv:2510.13619#introduction:part-1", "url": "https://arxiv.org/abs/2510.13619", "anchor": "#introduction:part-1", "type": "paper", "title": "", "section": "INTRODUCTION", "text": "This paper introduces a visualization technique for offline identification and characterization of lidar adversities that degrade scan matching. Our approach works iteratively, calculating a discrepancy-vector field from a pair of point clouds to aid a human observer in proposing a discrepancy-mitigation model, which might account for the patterns observed in the vector-field plot. Our work is motivated by the applications of lidar to automated driving. Lidar sensors have been used in many commercial and prototype automated vehicles as a component within a larger sensing suite, often including GNSS, INS, wheel odometry, and sometimes also cameras or radar (Ilci & Toth, 2020). Lidar sensors are advantageous in that they can create a high-resolution 3D representation of the surroundings (Olson, 2009); this 3D representation of the scene can be used for localization (Hassani & Joerger, 2021), simultaneous localization and mapping (SLAM) (Cadena et al., 2016), and for collision avoidance (Wei et al., 2018). In this paper, we will focus on using lidar for localization, by registration of one lidar image with another (relative localization) or with a high-definition map (absolute localization). When lidar registration infers vehicle pose, meaning its rotation and translation, random noise and systematic biases propagate into errors in the estimated vehicle pose. To date, several error mechanisms have been identified for lidar registration (Rife & McDermott, 2024) as well as change detection in 3D scanning (Salimpour et. al., 2022; Nurunnabi et. al., 2015). There has also been previous work regarding mitigation of various error mechanisms related to scene geometry (Li & Wang, 2020), continuous change of a scene such as moveable objects (Jeong & Kim 2023), and historical terrain anomalies (Storch et. al., 2023). Adverse weather and off-road environments have also been shown to degrade lidar scan-matching (Fu & Yu, 2020; Nagai et. al., 2024). Identifying additional error mechanisms for lidar scan registration, or scan matching, remains an open research topic. To aid in identifying various adversities in lidar data that lead to registration errors, we describe in this paper a new process for human- in-the-loop characterization. In this paper, we assert that each adversity mechanism will broadly impact all scan matching algorithms, even though specific error levels may vary from one scan-matching algorithm to another. For example, iterative closest point or ICP methods (Segal et al., 2009), normal distributions transform or NDT (Biber & Straßer, 2003), iterative closest ellipsoidal transform or ICET (McDermott & Rife, 2023), and lidar odometry and mapping or LOAM (Shan & Englot, 2018) emphasize different aspects of lidar point clouds in order to perform scan matching, so they each obtain different results and different errors when estimating pose. Nonetheless, all these methods suffer errors due to the presence of moving objects, such as pedestrians or motor vehicles, unless the corresponding points are pruned from lidar data sets prior to scan matching. With the idea that point-cloud adversities impact a wide-range of algorithms, we propose an offline, human-in-the-loop methodology to identify and characterize them. Our key contribution is to introduce discrepancy-vector fields to help the human analyst to visualize and reason about possible lidar adversities. Our implementation", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "82dde5a6f23aaf9ed1c767831e5c01d21a157e49160c99cfdff8f60df9f2f897"}
{"doc_id": "arxiv:2510.13619#introduction:part-2", "url": "https://arxiv.org/abs/2510.13619", "anchor": "#introduction:part-2", "type": "paper", "title": "", "section": "INTRODUCTION", "text": "estimating pose. Nonetheless, all these methods suffer errors due to the presence of moving objects, such as pedestrians or motor vehicles, unless the corresponding points are pruned from lidar data sets prior to scan matching. With the idea that point-cloud adversities impact a wide-range of algorithms, we propose an offline, human-in-the-loop methodology to identify and characterize them. Our key contribution is to introduce discrepancy-vector fields to help the human analyst to visualize and reason about possible lidar adversities. Our implementation starts by aligning a pair of point clouds using ground truth (e.g., using carrier-phase differential GNSS for experimental data). Next, a voxel grid is introduced. For the points within each voxel, a mean location is computed for each of the two clouds. The voxel-based means are differenced to create a discrepancy vector. A vector field is then created by compiling the discrepancy vectors over all voxels. A human analyst observes this vector-field visualization to identify patterns in the field. The human observer then hypothesizes the mechanism causing the largest observed discrepancies and proposes a pruning rule (or other transformation) to mitigate the discrepancy. The hypothesized rule can then be tested to check whether it indeed reduces the magnitude of the discrepancy vectors in the region of interest. In short, this methodology provides a systematic though labor-intensive offline characterization of discrepancies between a pair of point clouds. The main goal of the method is to troubleshoot and identify impactful discrepancies within a specific scene. A secondary goal of the method is to visualize representative adversities in lidar point clouds, to aid in their explanation. In prior work, it has been common to hypothesize error modes and mitigate them in the context of a specific scan-matching algorithm, with validation conducted by statistical analysis on the improvement in the pose estimate (Rife & McDermott, 2024; Joerger et al., 2022). Our approach can provide a statistical description, but also an interpretable visualization, that is intended to be somewhat algorithm agnostic. In the next section of the paper, we provide more detail on the proposed methodology for visualizing adversities in lidar data. Subsequently, we apply the method to two types of data: (i) simulated data where exact truth is known and (ii) experimental data where ground truth must be inferred. A brief discussion and summary conclude the paper. 2 METHODOLOGY In this paper we introduce a visualization approach to aid in identifying and reasoning about sources of lidar adversity. The basic idea of the visualization is to align a pair of point clouds (or a point cloud and a high-definition map) and then look for local discrepancies between the clouds. The details of our approach are illustrated as a block diagram in Figure 1. In the diagram, the key feature is the human inspection step, in which an analyst examines a vector-field plot to localize and characterize patterns of discrepancies in the lidar data. The human analyst proposes adversity models with the goal of accounting for the patterns observed in the vector-field plot. As shown in the block diagram, hypotheses are iteratively evaluated, with the human analyst", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "a2bcc85dd077e864fe08fe64545563a72839daaa92cf804ffdace221e6fc6e24"}
{"doc_id": "arxiv:2510.13619#introduction:part-3", "url": "https://arxiv.org/abs/2510.13619", "anchor": "#introduction:part-3", "type": "paper", "title": "", "section": "INTRODUCTION", "text": "between the clouds. The details of our approach are illustrated as a block diagram in Figure 1. In the diagram, the key feature is the human inspection step, in which an analyst examines a vector-field plot to localize and characterize patterns of discrepancies in the lidar data. The human analyst proposes adversity models with the goal of accounting for the patterns observed in the vector-field plot. As shown in the block diagram, hypotheses are iteratively evaluated, with the human analyst rejecting invalid hypotheses and preserving valid ones. The key data that mediates decision making on each iteration is the discrepancy-vector field. In order to understand the discrepancy-vector visualization, it makes sense to start with the input data, which consist of a pair of point clouds, labeled as Pt. Cloud 1 and Pt. Cloud 2 in Figure 1. A point cloud represents an image composed of thousands of lidar returns, with each point i described by a three-dimensional vector {𝑥𝑖, 𝑦𝑖,𝑧𝑖}. The first point cloud represents a current snapshot of a scene; the second point cloud represents a past snapshot of the scene, either taken by the same vehicle (e.g., to support dead reckoning) or drawn from a high-definition map (e.g., to support absolute positioning). FIGURE 1 Flow diagram displaying the process of vector-field visualizations. The methodology takes two LIDAR point clouds as inputs and produces a discrepancy-vector field for visualization by a human analyst. If the point clouds capture roughly the same scene, then they can be aligned so that static features appear at the same coordinates in both point clouds. This alignment, or registration, process requires determination of pose, meaning the relative orientation and translational offset between the two point-clouds. By default, each point cloud has its origin at the center of the lidar unit, and its basis vectors attached to the lidar unit, such that the coordinate system changes whenever the lidar undergoes rigid body translation and/or rotation. The registration process ideally uses truth (for simulated data sets) or ground truth (for experimental data) to obtain the alignment. For instance, in an experiment where GPS and inertial data are available, the second image can be transformed to align with the first image using relative position data from the GPS and relative orientation data from the inertial navigation system. In the event that ground truth is not available, registration can also be conducted using the lidar data only, with a scan- matching algorithm aligning image features as well as possible, for instance through algorithms like NDT (Biber & Straßer, 2003), ICP (Segal et al., 2009), or ICET (McDermott & Rife, 2023). Once the second image is aligned to the first, any prior hypothesis-driven models can be applied to the data to mitigate discrepancies (via the “Remove Hypothesized Adversity” block in Figure 1). During the first iteration through the figure, the human analyst has not seen the data, so hypothesis-driven models don’t yet exist, and this block just acts as a pass through. As a next step, local discrepancies are computed through a voxelization process. The voxelization divides the entire scene into", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "3420fb178c4c2b837e477e7c2d641b9f0138897d2a9e5bb295e331c73ba5e28e"}
{"doc_id": "arxiv:2510.13619#introduction:part-4", "url": "https://arxiv.org/abs/2510.13619", "anchor": "#introduction:part-4", "type": "paper", "title": "", "section": "INTRODUCTION", "text": "Once the second image is aligned to the first, any prior hypothesis-driven models can be applied to the data to mitigate discrepancies (via the “Remove Hypothesized Adversity” block in Figure 1). During the first iteration through the figure, the human analyst has not seen the data, so hypothesis-driven models don’t yet exist, and this block just acts as a pass through. As a next step, local discrepancies are computed through a voxelization process. The voxelization divides the entire scene into equal sized volumetric bins and places the lidar points from each scan within a specific bin. Although the volumetric bins could be cubes, defined in Cartesian Coordinates {𝑋, 𝑌, 𝑍}, we use spherical volume elements, or voxels, defined in radius, azimuth, and elevation {𝑟, 𝜃, 𝜙}. After lidar points are binned, each voxel contains one subset of returns from Point Cloud 1 and a second subset from Point Cloud 2. Ideally, with perfectly aligned scenes and no discrepancies, the two sets of returns appear to be identical and, by extension, the mean location of points within a given voxel should be the same for Point Cloud 1 and Point Cloud 2. Due to noise and other adversities, the voxel means are not identical in practice. The local level of misalignment within voxel i can be computed as the difference between the mean { 𝑋̅ 1 𝑖, 𝑌̅ 1 𝑖, 𝑍̅ 1 𝑖} for Point Cloud 1 and the mean { 𝑋̅ 2 𝑖, 𝑌̅ 2 𝑖, 𝑍̅ 2 𝑖} for Point Cloud 2. Based on this definition, the components of the discrepancy vector are {𝑈𝑖, 𝑉𝑖, 𝑊𝑖} = { 𝑋̅ 2 𝑖, 𝑌̅ 2 𝑖, 𝑍̅ 2 𝑖} −{ 𝑋̅ 1 𝑖, 𝑌̅ 1 𝑖, 𝑍̅ 1 𝑖} (1) Since a local discrepancy vector can be computed within each voxel, a vector field can be visualized by plotting the discrepancy {𝑈𝑖, 𝑉𝑖, 𝑊𝑖} over all voxels i. A vector field is a concept from calculus (Anton, et. al., 2020) that defines a vector value at each point in a continuous space. An example in physics is the velocity field of a fluid flow, such as for the airflow around a wing, where the air at each point in space has a 3D velocity vector that describes its local direction and speed of motion. In the case of a velocity field, the data is a continuum. In the case of our voxelized point clouds, equation (1) generates a field of discrepancy vectors, but because the space is discretized, the field of vectors is not a continuum. Rather, the discrepancy field is discrete, with vectors defined at the centroid of the points from cloud 1 within that voxel. In this sense, the tail of each visualized vector represents the centroid from point cloud 1, and the tip of the vector represents the centroid of point cloud 2 within that voxel. As a means of conceptualizing a discrepancy-vector field, consider three example cases shown in Figure 2. Each case shows a hypothetical 3x3 grid of nine voxels total. The first example case (left) features uniform", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "97ca74d7a7437afae71177fb47400e97d2f0206623b1226d928553beeba97f07"}
{"doc_id": "arxiv:2510.13619#introduction:part-5", "url": "https://arxiv.org/abs/2510.13619", "anchor": "#introduction:part-5", "type": "paper", "title": "", "section": "INTRODUCTION", "text": "vectors defined at the centroid of the points from cloud 1 within that voxel. In this sense, the tail of each visualized vector represents the centroid from point cloud 1, and the tip of the vector represents the centroid of point cloud 2 within that voxel. As a means of conceptualizing a discrepancy-vector field, consider three example cases shown in Figure 2. Each case shows a hypothetical 3x3 grid of nine voxels total. The first example case (left) features uniform discrepancy vectors across the grid; this special case represents a bad registration, where the discrepancies can be fixed by shifting the point clouds, moving the second cloud opposite the direction of the discrepancy vectors to reduce their length. The second example case (middle) is one in which large opposing vectors are shown along the top and bottom rows. This is a case of a discrepancy that cannot be explained by bad registration, since there is no translation or rotation that will reduce the magnitude of the discrepancies. Instead, a human analyst might identify this as a systematic error, perhaps a calibration bias that introduces an incorrect scaling into one point cloud but not the other. If the human analyst can successfully reason about the cause of discrepancies, then the human analyst can mitigate the large biases such that only small residual vectors remain. This is the case in the third example (right side of Figure 2), where discrepancy vectors have small magnitudes and point in seemingly random directions. FIGURE 2 Three examples of a 2D vector field, defined over a three-by-three voxel grid. (Left) Idealized discrepancy vectors for pure translation, which can be corrected by better registration; (Middle) Discrepancy vectors that cannot be corrected by registration, such that the human analyst will generate a hypothesis about the physical basis for the discrepancy; (Right) A random vector field of small magnitude – the ideal outcome after removal of the hypothesized condition. In the visualization-driven methodology illustrated in Figure 1, the human analyst is the workhorse that converts the discrepancy-vector field into a hypothesis about the physical basis for the discrepancies. That hypothesis can be used to generate a mitigation method for the proposed physical mechanism, for instance, a method for detecting the issue and excluding data from corrupted voxels. The mitigation method (or rather a series of all validated mitigations) is applied to the raw data via the “Remove Hypothesized Adversity” block in Figure 1. After mitigating adversities, the discrepancy-vector field is recomputed and shown to the human analyst. Three things may happen at this point in the process. First, the magnitude of the discrepancy field may be somewhat reduced, but with salient patterns remaining. In this case, the proposed mitigation method was effective, but the scene still exhibits evidence of other adversities. Second, the magnitude and patterns of the discrepancy field may be unchanged, in which case the analyst deems the proposed mitigation ineffective. Finally, it is possible that the vector field contains only small-magnitude discrepancy vectors oriented randomly, as shown on the right side of Figure 2. In this case new", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "ac2631c9893ebab89e2f15bf9cc8c2bdd3d634c158dfdf30a79b1c351f918426"}
{"doc_id": "arxiv:2510.13619#introduction:part-6", "url": "https://arxiv.org/abs/2510.13619", "anchor": "#introduction:part-6", "type": "paper", "title": "", "section": "INTRODUCTION", "text": "the discrepancy field may be somewhat reduced, but with salient patterns remaining. In this case, the proposed mitigation method was effective, but the scene still exhibits evidence of other adversities. Second, the magnitude and patterns of the discrepancy field may be unchanged, in which case the analyst deems the proposed mitigation ineffective. Finally, it is possible that the vector field contains only small-magnitude discrepancy vectors oriented randomly, as shown on the right side of Figure 2. In this case new patterns are not discernable, so the iteration process stops. 3 APPLICATIONS The effectiveness of a vector-field visualization was explored through application to two lidar examples: one simulated and one experimental. The simulated scene allows for an analysis in which the ground truth is exact and the registration therefore ideal; the experimental scene allows for an analysis where the data includes real-world noise and distortions that do not appear in simulation. This section explains the two examples, their discrepancy-vector fields, and the process of identifying lidar- imaging adversities that introduce discrepancies between point clouds pairs formed at slightly different locations or at different times. 3.1 Generation of a Simulated Scene As a first means of evaluating our vector-field methodology, a simulated scene was created using engineering drawing software (SOLIDWORKS). As shown in Figure 3, the simulated scene roughly models a road intersection featuring four “buildings,” three modeled as rectangular prisms and one as a cylinder. The four structures are ten meters in height with rectangular footprints of 5x5 m2, 8x5 m2, 7x5 m2 for the prisms and with a 5 m diameter for the cylinder. The dimensions of the ground plane are set arbitrarily to 26x26 m2. Synthetic lidar data were generated for this scene using MATLAB. The lidar sensor was simulated in the middle of the scene, shown by the red star marker in Figure 3. The lidar is positioned three meters above the ground (at the roof height of a large truck or van). The MATLAB sensor model featured 80 channels separated by 0.4 degrees (spanning elevations between -22° and 10°). The resulting point cloud is shown in Figure 4. The point cloud captures the ground and part of the buildings (but not their roofs) because of the simulated lidar’s field-of-view constraints in the elevation direction. To emphasize adversities due to geometric structures in the scene, the MATLAB simulation was configured to suppress random measurement noise. FIGURE 3 An image of the SOLIDWORKS simulated scene, marking the placement of a theoretical lidar sensor. FIGURE 4 Simulated point cloud generated at initial lidar location, shown (A) superimposed on a rendering of the scene and (B) absent the rendered scene. For the purpose of visualizing discrepancies, we generated a pair of point clouds from the scene to represent motion of the lidar for a vehicle rounding a curve. The initial position (red star in Figure 3) was {𝑥1, 𝑦1,𝑧1} = {0,0,3} 𝑚 and the second position was {𝑥2, 𝑦2,𝑧2} = {1,1,3} 𝑚 after a pitch-roll-yaw rotation of {𝜙, 𝜃, 𝜓} = {0,0,0.05} 𝑟𝑎𝑑. The MATLAB simulation tool assumed the entire point cloud was", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "6298dd09950cd66d9c39f2c362770ab86752bd5aaac5e352b74006aef85b88c5"}
{"doc_id": "arxiv:2510.13619#introduction:part-7", "url": "https://arxiv.org/abs/2510.13619", "anchor": "#introduction:part-7", "type": "paper", "title": "", "section": "INTRODUCTION", "text": "scene and (B) absent the rendered scene. For the purpose of visualizing discrepancies, we generated a pair of point clouds from the scene to represent motion of the lidar for a vehicle rounding a curve. The initial position (red star in Figure 3) was {𝑥1, 𝑦1,𝑧1} = {0,0,3} 𝑚 and the second position was {𝑥2, 𝑦2,𝑧2} = {1,1,3} 𝑚 after a pitch-roll-yaw rotation of {𝜙, 𝜃, 𝜓} = {0,0,0.05} 𝑟𝑎𝑑. The MATLAB simulation tool assumed the entire point cloud was captured instantaneously, which is a reasonable assumption for low-speed motion, but X Z Y (A) (B) which neglects the fact that today’s rotating lidars typically collect lidar points using a rotating mechanism that completes a full rotation over a period of about 0.1 seconds. The finite rotation speed of the lidar rotor introduces motion distortion effects sometimes called rolling shutter (McDermott & Rife, 2023). 3.2 Application of Visualization Methodology to Simulated Data The visualization methodology of Figure 1 was subsequently applied to the pair of point clouds generated from the simulated scene of Figure 3. Since the displacement and rotation of the lidar between images is known exactly, the “Registration” block can transform the second point cloud precisely to align with the first, such that static features in the scene are coincident. The aligned point clouds (red and blue) are shown in Figure 5. The alignment is evident in that the edges of the buildings are crisply defined for both point clouds. Note that the circular “hole” in the middle of the point clouds does not correspond to a feature in the environment; rather, this hole represents the elevation angle constraints of the lidar unit, which cannot sample the ground plane below an angle of -22°. The center of the circular hole corresponds to the lidar location, which was different when sampling each point cloud. FIGURE 5 Registration of two simulated point clouds (red and blue) shown (A) from above and (B) in perspective. The next major step of the methodology involves voxelization. (Note that the “Remove Hypothesized Adversity” block in Figure 1 acts as a pass-through initially, since the human analyst has not yet reasoned about the visualization or proposed any hypotheses to explain discrepancies.) In the voxelization process, we selected a spherical grid centered at the origin of the first point cloud. Each grid cell is a frustrum, meaning a semi-infinite wedge extending outward from the origin in the range direction, bounded within a side-to-side band of azimuth angles and a top-to-bottom band of elevation angles. Specifically, the full measurement space was divided into 36 azimuth bins (each of 10° in width, covering a full 360° azimuth range) and 9 elevation bins (each of 7.5° in height, covering a 37.5° elevation range, slightly larger than the sensor’s 30° field-of-view cutoff in elevation). Based on empirical observation, discrepancy vectors are more easily characterized using a spherical voxel grid rather than a Cartesian grid. After voxelization, the discrepancy-vector field is computed using (1). The resulting difference field is shown in Figure 6. A salient detail is that discrepancy vectors computed with (1)", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "29599f2f65dc33c1889f08a0e684e9414e4a6801b1d16f0ce916326f5f633c15"}
{"doc_id": "arxiv:2510.13619#introduction:part-8", "url": "https://arxiv.org/abs/2510.13619", "anchor": "#introduction:part-8", "type": "paper", "title": "", "section": "INTRODUCTION", "text": "in width, covering a full 360° azimuth range) and 9 elevation bins (each of 7.5° in height, covering a 37.5° elevation range, slightly larger than the sensor’s 30° field-of-view cutoff in elevation). Based on empirical observation, discrepancy vectors are more easily characterized using a spherical voxel grid rather than a Cartesian grid. After voxelization, the discrepancy-vector field is computed using (1). The resulting difference field is shown in Figure 6. A salient detail is that discrepancy vectors computed with (1) use cluster-mean locations framed in Cartesian coordinates, even though the cluster boundaries are defined using spherical voxels. (A) (B) FIGURE 6 Initial discrepancy vector field shown (A) from above and (B) in perspective. Discrepancies indicate differences between the mean position of red and blue points (see Figure 5) within each spherical voxel. Red dashed contours highlight the most prominent discrepancies considered by the analyst. Next, the human inspection step takes place. The human analyst identifies prominent patterns within the vector field. In the case of Figure 6, the human analyst marked two groups of discrepancy-vectors with a red dashed contour. One contour forming a ring-like pattern in Figure 6 appears to correspond to the field-of-view boundary for the sensor. Based on this observation, the analyst hypothesized that the discrepancy magnitude might be reduced by excluding points from one cloud that were outside the field of view of the other. The points excluded by this hypothesis are shown in red and blue in Figure 7, while the preserved points are shown in gray. The excluded points either fall on the ground plane (at the lower elevation limit) or high on buildings (at the upper elevation limit). FIGURE 7 Points in each cloud identified as lying outside the field-of-view associated with the other cloud, shown (A) from above and (B) in perspective. After implementing this exclusion rule, removing points from one cloud that are not within the field-of-view associated with the other, the pair of point clouds can be processed by the “Remove Hypothesized Adversity” block in Figure 1. Progressing around the processing loop, the human analyst is presented with a new discrepancy-vector field. The comparison of the original and new vector fields is illustrated in Figure 8. The large discrepancies visible in the initial vector field (left side of figure) are removed in the revised vector field (right side). (A) (B) (A) (B) FIGURE 8 Comparison of discrepancy-vector field (A) before and (B) after field-of-view mitigation. The question remains if other discrepancies can be identified by drilling down farther. To answer this question, the vectors in Figure 8B can be scaled. After magnification, patterns are again visible in the vector field, as shown in Figure 9A and B. The largest vectors seen in the magnified plot occur where the buildings meet the ground. To better visualize one of these regions, we identify the voxels generating the large arrows in the lower-left corner of Figure 9A. These voxels are shown as wireframes (dashed lines) in Figure 9C and D. FIGURE 9 Information used in second iteration, where the analyst identifies clusters as red dashed", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "95042a68b1b6bda986e2b8025e8593bc48437c1739ab852a825ddfde7d0e80ca"}
{"doc_id": "arxiv:2510.13619#introduction:part-9", "url": "https://arxiv.org/abs/2510.13619", "anchor": "#introduction:part-9", "type": "paper", "title": "", "section": "INTRODUCTION", "text": "patterns are again visible in the vector field, as shown in Figure 9A and B. The largest vectors seen in the magnified plot occur where the buildings meet the ground. To better visualize one of these regions, we identify the voxels generating the large arrows in the lower-left corner of Figure 9A. These voxels are shown as wireframes (dashed lines) in Figure 9C and D. FIGURE 9 Information used in second iteration, where the analyst identifies clusters as red dashed contours, shown (A) from above and (B) in perspective. The voxels containing the large discrepancies are shown as wireframes, with voxel boundaries superimposed on the registered point clouds, shown (C) from above and (D) in perspective. (A) (B) (A) (B) (C) (D) Looking at the voxels with the largest discrepancies (see Figure 9C and D) indicates a potential adversity associated with the edges of the shadow cast by the cylindrical structure. Shadowing errors, that occur where one object casts a shadow on another object or on the ground plane, are known to introduce spurious features (McDermott & Rife, 2022). Shadow errors can occur in lidar imaging of curved surfaces, when part of an object blocks another part of the same object from view (Rife & McDermott, 2024). Previous work highlights solutions to target and mitigate occlusion and shadowing effects (McDermott & Rife, 2022; Shimojo et. al., 1989). Focusing on the shadows cast by the cylinder, the analyst implemented the shadow-mitigation algorithm proposed by McDermott and Rife in the “Remove Hypothesized Adversity” block in Figure 1. The shadow-mitigation algorithm removes points from one point cloud if they might be blocked from sight from the lidar location for the second point cloud. The additional points removed in this step are shown in Figure 10, where removed points are shown in color and preserved points in gray. On the third loop through Figure 1, the “Remove Hypothesized Adversity” block thus contained two mitigations: the shadow mitigation proposed in the second loop as well as the field-of-view mitigation proposed in the first. FIGURE 10 Points in each cloud identified as lying in a shadow associated with the other cloud, shown (A) from above and (B) in perspective. This second mitigation step eliminated the discrepancy vectors associated with the cylinder, but not those associated with the building on the opposite corner (upper right of Figure 9A). The improvement is visualized in Figure 11, which shows the vector field after mitigation. The improvement is evident in the lower-left corner of Figure 11A, where the large vectors associated with the cylinder shadow have disappeared. FIGURE 11 Third discrepancy-vector field, after shadow removal. (A) (B) (A) (B) While discrepancy vectors remain in the vector field shown in Figure 11, a considerable impact has been made to reason about and mitigate adversities. In concept, the iterations could be continued, with the next goal of shrinking the large residual discrepancies identified in the upper-right corner of Figure 11 Figure 11A. 3.3 Experimental Data To further evaluate the proposed analysis process, we applied the methodology to real-world data. In particular, we used USDOT-furnished", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "3704c8b60f0c65d8b6354142dc6c2d0f3b04e0a33f37e01c133d420a0f2da688"}
{"doc_id": "arxiv:2510.13619#introduction:part-10", "url": "https://arxiv.org/abs/2510.13619", "anchor": "#introduction:part-10", "type": "paper", "title": "", "section": "INTRODUCTION", "text": "after shadow removal. (A) (B) (A) (B) While discrepancy vectors remain in the vector field shown in Figure 11, a considerable impact has been made to reason about and mitigate adversities. In concept, the iterations could be continued, with the next goal of shrinking the large residual discrepancies identified in the upper-right corner of Figure 11 Figure 11A. 3.3 Experimental Data To further evaluate the proposed analysis process, we applied the methodology to real-world data. In particular, we used USDOT-furnished data depicting a parking lot located near Fairfax, VA. The scene is shown in Figure 12. The data collection system (Wassaf et al., 2021) used a Velodyne VLP-16 lidar, with sixteen vertical channels spanning a field-of-view from -15° to +15° elevation. We considered one image pair from a larger dataset. This point-cloud pair were separated by an interval of 1 s, during which the vehicle moved in the y-direction at approximately 3 m/s. Due to limitations in GPS accuracy, the scans were aligned using NDT lidar scan matching (Biber & Straßer, 2003). The aligned scans are shown in Figure 13. FIGURE 12 Parking lot where experimental lidar data were acquired, with approximate vehicle location indicated by a red star. FIGURE 13 Two experimental point clouds (red and blue), after registration, shown (A) from above and (B) in perspective. 3.4 Application of Visualization Methodology to Experimental Data The methodology of Figure 1 was again applied. Given the smaller number of vertical channels, a grid was chosen with 36 spherical voxels in the azimuth direction and 5 in the elevation direction. A discrepancy-vector field was computed across the two experimentally acquired point clouds. Figure 14 shows a visual representation of the initial discrepancy field. (A) (B) FIGURE 14 Initial discrepancy-vector field generated from experimental data. The maximum discrepancy vector magnitude is 17.6 m. In the initial discrepancy field, the largest vectors are near the center of the point cloud, where lidar beams can reflect from the roof of the data collection van. Previous research shows that own-vehicle removal before the scan matching process increases localization accuracy (Jeong et al., 2023). Own-vehicle removal was conducted by eliminating points within a radius of 3 m from the lidar center; the result was a cleaner discrepancy-vector field, as shown in Figure 15. FIGURE 15 Second discrepancy-vector field, with own-vehicle removed. The maximum discrepancy vector magnitude is 13.3 m. After own-vehicle removal, several large discrepancies persist near domain boundaries. These discrepancies form a familiar ring pattern around the field-of-view boundary. This ring is identified in Figure 15 with a red-dashed contour. As in the simulation-based analysis, points were removed at the domain boundaries for one scan if those points fell outside the lidar field-of-view for the location of the second scan. After domain-boundary mitigation, a third set of discrepancy vectors was computed, as shown in Figure 16A. The large discrepancy vectors have disappeared within the analyst-defined contours of Figure 15 (contours that are shown in green in Figure 16A). In the third discrepancy-vector field, the analyst identified a pattern of large discrepancies on or around object contours, suggesting", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "da42a4e3439091b204dd7330731b7765dc016772ff0c35ded734beb2d030574d"}
{"doc_id": "arxiv:2510.13619#introduction:part-11", "url": "https://arxiv.org/abs/2510.13619", "anchor": "#introduction:part-11", "type": "paper", "title": "", "section": "INTRODUCTION", "text": "removed at the domain boundaries for one scan if those points fell outside the lidar field-of-view for the location of the second scan. After domain-boundary mitigation, a third set of discrepancy vectors was computed, as shown in Figure 16A. The large discrepancy vectors have disappeared within the analyst-defined contours of Figure 15 (contours that are shown in green in Figure 16A). In the third discrepancy-vector field, the analyst identified a pattern of large discrepancies on or around object contours, suggesting shadowing effects. Voxels seemingly associated with shadow effects are identified by red-dashed contours in Figure 16B. After applying the shadow-mitigation method of McDermott and Rife (2022), a fourth discrepancy-vector field resulted (see Figure 17). (B) (A) (A) (B) FIGURE 16 Third discrepancy-vector field, after field-of-view mitigation. (A) The discrepancy vectors in the ring-shaped contours are eliminated by field-of-view mitigation. (B) A pattern of residual errors is evident near the edge of objects, like trees, as indicated by a new set of contours. The maximum discrepancy vector magnitude is 11.3 m. FIGURE 17 Fourth discrepancy-vector field, after shadow mitigation. The maximum discrepancy vector magnitude is 8.1 m. While several lidar adversities have been addressed through this process, there remain a number of large discrepancies in Figure 17. Specifically, a set of long discrepancy vectors pointed radially inward or outward are visible. Another round of reasoning would be required on the part of the human analyst to identify and, ideally, mitigate this effect. 4 DISCUSSION Developing reliable uncertainty quantification for lidar scan-matching algorithms remains an open question, in part because of the large number of systematic errors that appear during processing. Many of these adversities are related to the assumption that point clouds that depict the same static scene from different viewpoints are equivalent, given that they are correctly registered via a rigid transformation. It is not easy to spot these discrepancies when viewing raw point-cloud data, as seen in Figure 13; by contrast, the discrepancies become much more salient when viewed in the format of a vector field, as seen in Figure 14 through Figure 17. In this sense, our two examples provide a basic proof-of-concept for the utility of analyzing discrepancy-vector fields. One salient point is that neither proof-of-concept example was iterated to completion. Ideally the final vector field would be small in magnitude and random in direction, as hypothesized in Figure 2; however, large, patterned discrepancies remained visible on the final iteration (third iteration for simulation, fourth iteration for experimental data). As context, we acknowledge the proof-of-concept examples demonstrated mitigation of known adversities, previously described in the research literature. (A) (B) (A) (B) These adversities included ego-vehicle, field-of-view, and shadowing effects. Recognizing and characterizing new adversities is significantly more challenging, and so we leave that process to future work. A curious aspect about implementing the algorithm is that the process appears to be relatively insensitive to the order in which adversities are identified and mitigated. In general, we expect that the human analyst will tend to focus on the largest visible discrepancies. However, there may not be a clear adversity mode", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "a4c2bbea178ff371712d320d8b8e02657edfb7da96fac0930acdb5d2b5527e19"}
{"doc_id": "arxiv:2510.13619#introduction:part-12", "url": "https://arxiv.org/abs/2510.13619", "anchor": "#introduction:part-12", "type": "paper", "title": "", "section": "INTRODUCTION", "text": "(A) (B) These adversities included ego-vehicle, field-of-view, and shadowing effects. Recognizing and characterizing new adversities is significantly more challenging, and so we leave that process to future work. A curious aspect about implementing the algorithm is that the process appears to be relatively insensitive to the order in which adversities are identified and mitigated. In general, we expect that the human analyst will tend to focus on the largest visible discrepancies. However, there may not be a clear adversity mode that explains all of the largest discrepancy vectors. As seen from the experimental data in Figure 15A, for example, domain-boundary effects do not necessarily have larger magnitude discrepancies than the shadowing effects in Figure 16B. Fortunately, either of these issues can be addressed before the other without substantial impact on the form of the vector field once both are removed. A final observation is that discrepancy patterns appear to become more complex as lidar resolution decreases. The simulation example considers a lidar sensor model with 80 horizontal beams across 32 degrees of vertical channels. By comparison, the experimental data utilizes a Velodyne VLP-16 lidar with 16 vertical beams across a similar elevation range. In effect, the elevation-direction resolution of the experimental data is roughly five times coarser than that of the simulation data. The relatively low angular resolution of the VLP-16 results in larger discrepancies in point patterns when imaging the same region of space from different vantage points. Though high-resolution may mitigate some adversities, low-resolution analysis may actually be beneficial for identification of lidar error sources, since adversities are more pronounced. Additional future work may include the conversion of our error visualization strategy into a real-time failure diagnosis methodology. This would entail leveraging new rules and hypotheses from a human analyst into a direct pipeline for an online mitigation algorithm. The integrity of a real time mitigation system would improve by continuously adding new tools and deploying validated hypotheses within real-world datasets. 5 SUMMARY In summary, we have presented an innovative technique for offline identification of lidar adversities that degrade scan matching, through a vector field visualization. The main contribution of this method includes the ability to troubleshoot and assist in localizing impactful discrepancies within a specific scene. Also, this method presents the ability to visualize representative adversities in lidar point clouds, to aid in their explanation. Our approach provides an interpretable visualization which differs from previous work in error mitigation strategies, which have generally proposed mitigations and then tested the effect on pose estimation, the final outcome of scan matching, rather than on the lidar scan itself, which is the input to scan matching. ACKNOWLEDGEMENTS The authors wish to acknowledge and thank the U.S. Department of Transportation Joint Program Office (ITS JPO) and the Office of the Assistant Secretary for Research and Technology (OST-R) for sponsorship of this work. We also gratefully acknowledge SAIC and Tufts University, which supported specific aspects of this research. Opinions discussed here are those of the authors and do not necessarily represent those of the DOT, SAIC, or other affiliated agencies. REFERENCES Anton, H., Davis,", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "2ee51fe1d3901b6708ecfa4a97dfc2219bcf73506800069a1bb6e06add493942"}
{"doc_id": "arxiv:2510.13619#introduction:part-13", "url": "https://arxiv.org/abs/2510.13619", "anchor": "#introduction:part-13", "type": "paper", "title": "", "section": "INTRODUCTION", "text": "input to scan matching. ACKNOWLEDGEMENTS The authors wish to acknowledge and thank the U.S. Department of Transportation Joint Program Office (ITS JPO) and the Office of the Assistant Secretary for Research and Technology (OST-R) for sponsorship of this work. We also gratefully acknowledge SAIC and Tufts University, which supported specific aspects of this research. Opinions discussed here are those of the authors and do not necessarily represent those of the DOT, SAIC, or other affiliated agencies. REFERENCES Anton, H., Davis, S., & Bivens, I. C., (April 2020) Calculus: Early Transcendentals (11th ed.). Wiley Biber, P., & Straßer, W. (2003). The normal distributions transform: A new approach to laser scan matching. In Proceedings 2003 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2003)(Cat. No. 03CH37453) (Vol. 3, pp. 2743-2748). IEEE https://doi.org/10.1109/IROS.2003.1249285 Cadena, C., Carlone, L., Carrillo, H., Latif, Y., Scaramuzza, D., Neira, J., Reid, I., & Leonard, J., Past, Present, and Future of Simultaneous Localization and Mapping: Toward the Robust-Perception Age, in IEEE Transactions on Robotics, vol. 32, no. 6, pp. 1309-1332, Dec. 2016, doi: 10.1109/TRO.2016.2624754. Fu, H. & Yu, R. LIDAR Scan Matching in Off-Road Environments. Robotics 2020, 9, 35. https://doi.org/10.3390/robotics9020035 Hassani, A., & Joerger, M. (2021) A New Point-Cloud-Based LiDAR/IMU Localization Method with Uncertainty Evaluation, Proceedings of the 34th International Technical Meeting of the Satellite Division of The Institute of Navigation (ION GNSS+ 2021) (pp. 636-651), St. Louis, Missouri, September 2021, pp. 636-651. https://doi.org/10.33012/2021.17905 Ilci, V., & Toth, C. (2020) High Definition 3D Map Creation Using GNSS/IMU/LiDAR Sensor Integration to Support Autonomous Vehicle Navigation. Sensors, 20, 899. https://doi.org/10.3390/s20030899 Jeong, S., Ko, M., & Kim, J. (2023) LiDAR Localization by Removing Moveable Objects. Electronics 2023, 12, 4659. https://doi.org/10.3390/electronics12224659 Joerger M., Wang, J., & Hassani, A. (2022) On Uncertainty Quantification for Convolutional Neural Network LiDAR Localization, IEEE Intelligent Vehicles Symposium (IV), Aachen, Germany, 2022, pp. 1789-1794, doi: 10.1109/IV51971.2022.9827445. Li, Z., & Wang, N., DMLO: Deep Matching LiDAR Odometry, 2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Las Vegas, NV, USA, 2020, pp. 6010-6017, doi: 10.1109/IROS45743.2020.9341206. McDermott, M., & Rife, J.H. (2022). Mitigating Shadows in LIDAR Scan Matching Using Spherical Voxels. IEEE Robotics and Automation Letters, 7, 12363-12370. McDermott, M., & Rife, J. (2023). ICERT online accuracy characterization for geometric based laser scan matching of 3D point clouds. Submitted to NAVIGATION McDermott, M., & Rife, J., (2024), Correcting Motion Distortion for LIDAR Scan-to-Map Registration, in IEEE Robotics and Automation Letters, vol. 9, no. 2, pp. 1516-1523, Feb. 2024, doi: 10.1109/LRA.2023.3346757 Nagai, K., Ahmed, S., & Pervan, B. (2024) Integrity with LiDAR Incorrect Extraction Faults in Adverse Weather Conditions, Proceedings of the 2024 International Technical Meeting of The Institute of Navigation, Long Beach, California, January 2024, pp. 1085-1094. https://doi.org/10.33012/2024.19535 Nurunnabi, A., West, G., & Belton, D. (2015) Outlier Detection and Robust Normal-Curvature Estimation in Mobile Laser Scanning 3D Point Cloud Data, Pattern Recognition, Volume 48, Issue 4, 2015, Pages 1404-1419, ISSN 0031-3203, https://doi.org/10.1016/j.patcog.2014.10.014. Olson, E.B. Real-time correlative scan matching, 2009 IEEE International Conference on Robotics and Automation, Kobe, Japan, 2009, pp. 4387-4393, doi: 10.1109/ROBOT.2009.5152375. Rife, J. H., & McDermott, M. (2024) Characterizing", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "a1f16d7cd8f9992a450904e446d6481e5eb311f98dc3a2cdabb8ac7edb542ac9"}
{"doc_id": "arxiv:2510.13619#introduction:part-14", "url": "https://arxiv.org/abs/2510.13619", "anchor": "#introduction:part-14", "type": "paper", "title": "", "section": "INTRODUCTION", "text": "2024 International Technical Meeting of The Institute of Navigation, Long Beach, California, January 2024, pp. 1085-1094. https://doi.org/10.33012/2024.19535 Nurunnabi, A., West, G., & Belton, D. (2015) Outlier Detection and Robust Normal-Curvature Estimation in Mobile Laser Scanning 3D Point Cloud Data, Pattern Recognition, Volume 48, Issue 4, 2015, Pages 1404-1419, ISSN 0031-3203, https://doi.org/10.1016/j.patcog.2014.10.014. Olson, E.B. Real-time correlative scan matching, 2009 IEEE International Conference on Robotics and Automation, Kobe, Japan, 2009, pp. 4387-4393, doi: 10.1109/ROBOT.2009.5152375. Rife, J. H., & McDermott, M. (2024) Characterizing Perspective Error in Voxel-Based LIDAR Scan Matching. The International Technical Meeting of The Institute of Navigation, 71(1), 2024. Web. doi:10.33012/2023.18670 Salimpour, S., Queralta, J. P., & Westerlund, T. (2022) Self-Calibrating Anomaly and Change Detection for Autonomous Inspection Robots, 2022 Sixth IEEE International Conference on Robotic Computing (IRC), Italy, 2022, pp. 207-214, doi: 10.1109/IRC55401.2022.00042. Segal, A., Haehnel, D., & Thrun, S. (2009). Generalized-icp. In Robotics: science and systems 2(4). Robotics: Science and Systems V - Online Proceedings (roboticsproceedings.org) Shan, T., & Englot, B. (2018). Lego-loam: lightweight and ground-optimized lidar odometry and mapping on variable terrain. In 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) (pp. 4758-4765). IEEE. https://doi.org/10.1109/IROS.2018.8594299 Shimojo, S., Silverman, G. H., & Nakayama, K. (1989) Occlusion and the solution to the aperture problem for motion. Vision research 29.5, pp. 619–626. Storch, M., de Lange, N., Jarmer, T., & Waske, B., (2023) Detecting Historical Terrain Anomalies With UAV-LiDAR Data Using Spline-Approximation and Support Vector Machines, in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing, vol. 16, pp. 3158-3173, 2023, doi: 10.1109/JSTARS.2023.3259200. Wassaf, H., Bernazzani, K., Gandhi, P., Lu, J., Van Dyke, K., Shallberg, K., Ericson, S., Flake, J., & Herman, M., (2021) Highly Automated Vehicle Absolute Positioning Using LiDAR Unique Signatures, Proceedings of the 34th International Technical Meeting of the Satellite Division of The Institute of Navigation (ION GNSS+ 2021), St. Louis, Missouri, September 2021, pp. 22-52. https://doi.org/10.33012/2021.17878 Wei, P., Cagle, L., Reza, T., Ball, J., & Gafford, J. (2018) LiDAR and Camera Detection Fusion in a Real-Time Industrial Multi- Sensor Collision Avoidance System. Electronics. 2018; 7(6):84. https://doi.org/10.3390/electronics7060084", "source": "arxiv_pdf", "published": "", "tokens": 344, "sha256": "de800abb6d90f65fba539b70842ac29ad71d11add4a61de7ebfc4747f8cdb77d"}
{"doc_id": "arxiv:2510.13594#abstract", "url": "https://arxiv.org/abs/2510.13594", "anchor": "#abstract", "type": "paper", "title": "", "section": "ABSTRACT", "text": "The operation of humanoid robotics is an essential field of research with many practical and com- petitive applications. Many of these systems, however, do not invest heavily in developing a non- expert-centered graphical user interface (GUI) for operation. The focus of this research is to develop a scalable GUI that is tailored to be simple and intuitive so non-expert operators can control the robot through a FIRA-regulated obstacle course. Using common practices from user interface develop- ment (UI) and understanding concepts described in human-robot interaction (HRI) and other related concepts, we will develop a new interface with the goal of a non-expert teleoperation system. Laurentian Intelligent Mobile Robotic Lab Keywords HuroCup · User Friendly · ROS · GUI 1", "source": "arxiv_pdf", "published": "", "tokens": 119, "sha256": "717ef746f331c8c6155b26741e4142baaad1ef3024a45c4cad3d8ac0ff8ef5b9"}
{"doc_id": "arxiv:2510.13594#introduction:part-1", "url": "https://arxiv.org/abs/2510.13594", "anchor": "#introduction:part-1", "type": "paper", "title": "", "section": "Introduction", "text": "Teleoperation in humanoid robotics has grown rapidly over the past decade, driven by advances in robotic movement and control. These systems are essential in domains like healthcare, mining, and space exploration Liu et al. (2018); Godelius (2021); Gómez et al. (2021). Competitions such as the FIRA RoboWorld Cup—and specifically its HuroCup teleoperation event—serve as testing grounds for teleoperated humanoid systems Baltes et al. (2025). A core challenge in this event is navigating an obstacle course without contact, which demands a clear and intuitive user interface. Designing such a GUI requires strong visual feedback and simplified control schemes, making the system accessible to users with no prior experience. This is especially important in team settings like FIRA, where new participants frequently inherit systems built by others. A usable interface not only enhances operator performance but also reduces the need for retraining and lowers cognitive workload White et al. (2020); Rea and Seo (2022). This research aims to create a user-friendly GUI tailored for the FIRA HuroCup obstacle run. It will prioritize camera-based visualization, clear communication of robot state, and scalable design for broader applications. The GUI will be evaluated qualitatively against a previous version using standard competition-style obstacle courses, though testing with non-experts and quantitative metrics remains outside the current scope due to time constraints Zhang et al. (2023); Sankar et al. (2023). 2 Literature Review Prior research on teleoperated humanoid robots has emphasized the importance of user interface design, particularly in the context of competitions like the FIRA RoboWorld Cup. Despite ongoing advancements, many systems still rely on expert operators. Studies underline the need for GUIs that support fast and safe control by novice users, thereby improving accessibility Rea and Seo (2022). The current interface, lacking clear labels, intuitive layouts, key actions like crawl, coherent camera placement, and on-the-fly motion tuning, illustrates these shortcomings. Teleoperation arXiv:2510.13594v1 [cs.RO] 15 Oct 2025 A PREPRINT bridges human intent and robot action Darvish et al. (2023). Effective systems build situational awareness, manage operator workload, and mitigate error Steinfeld et al. (2006). Shared-control schemes further ease cognitive load by blending autonomous and manual control Adamides et al. (2014). Intuitive visual feedback and adaptive GUIs cut training time for non-experts Goodrich et al. (2013), while task-specific visuals condense complex manoeuvres into simple commands Sankar et al. (2023). Scalable architectures, such as the DRC-HUBO interface, show how one design can span competitions and industry Zucker et al. (2015). Performance is typically evaluated through error rates and user-satisfaction metrics Steinfeld et al. (2006). Classic usability texts reinforce these ideas: maintain consistent navigation and colour schemes, hide rarely used controls, provide immediate feedback, and adapt fluidly to different screens Beaird et al. (2020); Boss and Teague (2016); Karray et al. (2013). Applying such rules to teleoperation reduces novice frustration and errors. Evidence from high-risk domains underscores the value of friendly teleoperation. In mining, remote control shields workers from danger Godelius (2021). Surgical robots hinge on intuitive human–robot interaction to ensure safety Sun et al. (2023). Competitions amplify these needs: DARPA Robotics Challenge teams navigated disaster scenarios with interfaces balancing shared autonomy", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "397de19acd6f3982ad2c469c26edda7f0e3f453bae43596a875ef93462d8c72f"}
{"doc_id": "arxiv:2510.13594#introduction:part-2", "url": "https://arxiv.org/abs/2510.13594", "anchor": "#introduction:part-2", "type": "paper", "title": "", "section": "Introduction", "text": "immediate feedback, and adapt fluidly to different screens Beaird et al. (2020); Boss and Teague (2016); Karray et al. (2013). Applying such rules to teleoperation reduces novice frustration and errors. Evidence from high-risk domains underscores the value of friendly teleoperation. In mining, remote control shields workers from danger Godelius (2021). Surgical robots hinge on intuitive human–robot interaction to ensure safety Sun et al. (2023). Competitions amplify these needs: DARPA Robotics Challenge teams navigated disaster scenarios with interfaces balancing shared autonomy and operator clarity Marion et al. (2018), and FIRA rules similarly demand fast, reliable teleop Baltes et al. (2025). Iterative, user-centred design (UCD) drives better GUIs Mao et al. (2005) and high cognitive load invites confusion and errors Goodrich and Schultz (2007); Chen et al. (2007), so refinements that lighten mental effort—such as cleaner layouts or automated feedback loops—improve speed and accuracy Tagliamonte and Maccaline (2024). Although System Usability Scale (SUS) questionnaires and similar tools can quantify these gains, formal user trials may not always be feasible Steinfeld et al. (2006). Despite progress, most teleoperation research still targets trained specialists. Interfaces explored in joystick, VR, or wearable studies prioritise raw capability over novice accessibility Franz et al. (2013); Ryu et al. (2004); Fritsche et al. (2015). Few offer GUIs that are simultaneously simple, extensible, and competition-ready. This project, therefore, proposes a redesigned interface that merges HRI guidelines, web-design best practices, and shared-control paradigms to enable non-expert teams to pilot humanoid robots effectively on the FIRA obstacle course and beyond. 3 Methodology This research aims to develop a user-friendly GUI for non-expert operators to control humanoid robots in the FIRA HuroCup teleoperated obstacle run. The GUI addresses a gap in existing research by prioritizing usability and intuitive design for users with no prior experience in teleoperation systems Rea and Seo (2022). Relying primarily on camera input, the interface emphasizes effective visualization of the robot’s environment and operations. Development is guided by an iterative cycle: • Create wireframes and identify key features per iteration. • Implement and adapt features throughout development. • Test feature usability and assess layout comfort. • Refine based on observations before the next cycle. This structured, iterative methodology ensures consistent improvement across interface versions, enhancing usability, layout, and functionality. 3.1 GUI Platform and Tools Interface development typically involves multiple design iterations to ensure that each added feature enhances function- ality without compromising overall usability. This iterative approach, widely used in UI and web development, supports consistency and continuous refinement Wynn and Eckert (2017). While earlier versions of the GUI relied on Bootstrap and other pre-built components, the current project focuses on custom development using HTML, CSS, and JavaScript to achieve greater control and flexibility. The GUI is built for a bipedal mobile robot classified under FIRA’s kid-size humanoid standards Baltes (2025). Inputs are limited to the camera and terminal console, with no external sensors available. This limitation highlights the importance of optimizing camera-based environmental awareness for the operator. The system runs on Ubuntu Mint 16.07 with ROS1, using Python 2.7 and C++ in a publisher-subscriber model. A ROS node for", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "9a1be4e64f6752b8f9062253646592584a26801c75bf3470da54ee55f1842a86"}
{"doc_id": "arxiv:2510.13594#introduction:part-3", "url": "https://arxiv.org/abs/2510.13594", "anchor": "#introduction:part-3", "type": "paper", "title": "", "section": "Introduction", "text": "on custom development using HTML, CSS, and JavaScript to achieve greater control and flexibility. The GUI is built for a bipedal mobile robot classified under FIRA’s kid-size humanoid standards Baltes (2025). Inputs are limited to the camera and terminal console, with no external sensors available. This limitation highlights the importance of optimizing camera-based environmental awareness for the operator. The system runs on Ubuntu Mint 16.07 with ROS1, using Python 2.7 and C++ in a publisher-subscriber model. A ROS node for teleoperation exists, developed by a past student, and provides a basic browser-based GUI with limited functionality. Existing buttons allow for basic motion, while features like crawling or lateral movement are either absent or inconsistently implemented. Some elements, such as an obstacle mapping widget, are partially functional but incomplete. The interface communicates through a server-based node, with front-end development mimicking a standard web application using HTML, CSS, and JavaScript. 2 A PREPRINT 3.2 Development and Testing Procedures Development began with the implementation of essential controls, such as movement and camera views, ensuring they were both intuitive and responsive. Additional features were added, including crawling functionality, voltage indicators, and diagnostic logs. Due to FIRA’s restriction on external sensors, all feedback is derived solely from internal data and camera visuals. Wireframes were used to guide the layout of core elements, which were continuously refined throughout the development process. Interactivity is handled using JavaScript, while HTML and CSS define the layout and styling. To maintain full control and support iterative improvements, no external libraries were used. Communication with the robot uses HTTP for commands and WebSockets for real-time feedback, with JSON used for message formatting. Testing was performed on a regulation-compliant turf track with obstacles to simulate competition conditions. Usability evaluations were based on task success and user workflow efficiency. Features were retained or revised based on their contribution to task performance. Iterative development enables continuous improvements and refinements, supporting a more user-friendly and task-oriented interface over time. 3.3 Ethical and Practical Constraints Due to time constraints, no external user testing or ethics approval was pursued, and evaluation was conducted solely by the developer. Future work could incorporate third-party evaluations and quantitative testing. Additionally, inconsistent robot behavior, caused by modifications from other users, sometimes affected testing. These irregularities, while not part of the GUI design, were accounted for during evaluations by focusing on ideal performance conditions. 4 Implementation The main goal of the implementations is to develop a user-friendly interface for teleoperation in the FIRA RoboWorld Cup. Focusing on making it as easy for operators to navigate and utilize the robot for its intended use in the obstacle run, as shown in Fig. 1. Figure 1: Visual of kid-size humanoid robot navigating a replica of the FIRA obstacle run event. The implementation will closely follow the design philosophy and concepts that were previously outlined in the methodology chapter. Such as focusing on good user interface development tactics, an iterative development approach that uses testing in realistic application environments, and a focus on the camera as the main user feedback for robotic movements and environmental situations. 4.1", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "c79f2f10047d7c8451753523eecf73fc9b1c6ba787f0ed75213e14e5b1be550f"}
{"doc_id": "arxiv:2510.13594#introduction:part-4", "url": "https://arxiv.org/abs/2510.13594", "anchor": "#introduction:part-4", "type": "paper", "title": "", "section": "Introduction", "text": "the obstacle run, as shown in Fig. 1. Figure 1: Visual of kid-size humanoid robot navigating a replica of the FIRA obstacle run event. The implementation will closely follow the design philosophy and concepts that were previously outlined in the methodology chapter. Such as focusing on good user interface development tactics, an iterative development approach that uses testing in realistic application environments, and a focus on the camera as the main user feedback for robotic movements and environmental situations. 4.1 System Architecture and Communication The development of the software to complete the task is highly dependent on the operating systems and software the robot already utilizes. In this case, the robot uses a distribution of Ubuntu mini with the ROS (Robot Operating System) 3 A PREPRINT installed for the operation of the implemented code. The operating system specifically utilizes version Mint 16.04 of the Ubuntu operating system, which allows for a lightweight operating system to perform actions on. As for the ROS software, it is what is mostly worked with during the development of the UI, where ROS is the middleware that utilizes a node-based structure for the execution of robotic actions. This means that development involves updating features of the remote-control node system. The node system utilizes Python 2.7 for the development of the code. In this specific application, we activate the remote server node, which runs the communication for the actions the users perform in the interface to the actual robotic actions that they correlate to. This node structure sets up the local server, which can be accessed via the IP 10.42.0.1 in the browser. At this point, the front end created by the node is visible, utilizing HTML/CSS and JavaScript for user interactions. Allowing for the development of the UI to be similar to the web development structure in terms of coding and conventions. As for possible JavaScript library utilization, the use of jQuery for certain UI interactions, and mjpegcanvas.js for streaming video is utilized. mjpegcanvas allows users to utilize the img tag and output the live camera feed to the element for the user to view and interact with. Jquery is used within the JavaScript for DOM manipulation, handling button presses, and hiding or showing UI elements such as submenus. Its purpose is to reduce the amount of boilerplate code used to select and modify elements of the page. The robot utilizes ROSbridge to bridge communication between ROS and the web-based GUI. ROSbridge runs on the robot as a WebSocket server, exposing ROS topics and services in JSON format. On the client side, roslib.js is used to establish a WebSocket connection with ROSBridge, allowing the web interface to publish movement commands and subscribe to feedback topics. This setup removes the complexity of ROS network protocols and enables real-time control and monitoring from a standard browser without requiring native ROS installations. For this web-based node, it utilizes both rosbridge and roslib.js. ROSbridge is what allows the node to create the local server (websocket server), and roslib.js, included in the web page, opens a WebSocket connection to Rosbridge. With both", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "2775b159388c025caf21a9e32fd3c34c9834d8114d79b23970256a00277147cc"}
{"doc_id": "arxiv:2510.13594#introduction:part-5", "url": "https://arxiv.org/abs/2510.13594", "anchor": "#introduction:part-5", "type": "paper", "title": "", "section": "Introduction", "text": "a WebSocket connection with ROSBridge, allowing the web interface to publish movement commands and subscribe to feedback topics. This setup removes the complexity of ROS network protocols and enables real-time control and monitoring from a standard browser without requiring native ROS installations. For this web-based node, it utilizes both rosbridge and roslib.js. ROSbridge is what allows the node to create the local server (websocket server), and roslib.js, included in the web page, opens a WebSocket connection to Rosbridge. With both of these utilities, we can send ROS messages over the WebSocket to have effective communication with the user and the robot. This setup abstracts away the complexity of handling ROS network protocols (like roscpp or rospy), making it possible to control and monitor the robot from a standard web page. The process in which the user communicates with the robot follows the path of the user first interacting with the robot. In this case, it would be the UI, and then that action is converted into a readable JSON for ROS by roslib.js and sent over the Rosbridge WebSocket. At this point, that command is read and then performed as an action by the robot. These possible messages will contain all the needed actions to perform the obstacle run event successfully and will require the user to utilize the proper commands given a situation. The Messages contain the action that is to be performed, which is then sent to the remote control node that is subscribed to the actions. The node reads each incoming message and interprets it as a request to walk (forward/backward), turn, or shift the robot’s body. 4.2 GUI development process Due to the utilization of an iterative process during development, the first version of the GUI draft contained the goals of displaying content in a non-cluttered way to the user and making sure the very basic movement (not everything needed to complete the obstacle course) was possible, and newer features were to be added when the first development was done. It is also important to note that a basic version of this already existed in the robot, but it contained critical functionality flaws, unintuitive design, and missing features. There was a feature that was partially implemented that allows the user to create a map that shows the robot’s location along with the location of some obstacles in a rudimentary fashion, which is elaborated upon shortly. Also note that for each iteration of this development process, A rough draft of a wireframe model was created to allow the development process to follow some structure as features were implemented, as shown in Fig. 2. The first draft allowed us to see what was possible with the UI and utilized the simple technique of putting similar operations in the same area on the screen. This is known as proximity or grouping Lidwell et al. (2010). In this case, the application of grouping involved putting the controls for basic movement on screen together, along with making 2 screens for the setup of the map and the actual control environment of the robot.", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "59f59937b7579538a9114edb7d30f5bbe491c06ca592765cc6b9171b0d14ac32"}
{"doc_id": "arxiv:2510.13594#introduction:part-6", "url": "https://arxiv.org/abs/2510.13594", "anchor": "#introduction:part-6", "type": "paper", "title": "", "section": "Introduction", "text": "shown in Fig. 2. The first draft allowed us to see what was possible with the UI and utilized the simple technique of putting similar operations in the same area on the screen. This is known as proximity or grouping Lidwell et al. (2010). In this case, the application of grouping involved putting the controls for basic movement on screen together, along with making 2 screens for the setup of the map and the actual control environment of the robot. This process is continually utilized for a cohesive interface throughout the rest of the development. Another important feature during this iteration is the new placement of the camera at the forefront of the user’s attention, this was done as it is the most important interaction between the human and robot in terms of communication and the fact that in the previous system, it was not scaled to a perspective that was easy for the user to preserve without difficulty in some scenarios. The final thing for this iteration to focus on would be to implement the basic shifting movements for left and right movement, which were not present in 4 A PREPRINT Figure 2: Wireframe depiction set before each draft 5 A PREPRINT the old system. Adding these features will complete the umbrella of basic movement and provide the user with intuitive movement to navigate the obstacle course. This first iteration provided good results and showed that the placement of the movement controls for the robot and the addition of the left and right shift movements proved as a great success in the tests, working as expected. Through testing, one point of concern was the camera size. As the camera increases in size on the screen, the throughput decreases, and depending on the state, it becomes detrimental to operation. So future iterations would scale down the camera and utilize the additional space to focus emphasis on controls to balance the robot’s vision with controls. With this, the second iteration of implementation was set. The preceding iteration involved mapping the controls to common keyboard controls, so that, along with pressing the buttons on the screen, the keyboard can be utilized. This requires adding all the actions needed and mapping them to logical keys. Due to the need for the movement of the camera along with the movement of the robot, the utilization of 2 directional inputs via keyboard where needed so due to the convention in the modern moment in keyboards being attributed to the W, A, S, and D buttons, those where selected to be used for the robot movement and the arrow keys for camera movement Krug (2005). Alternative actions also needed to be added, such as the get-up and crawl actions. This, along with resizing the camera for optimal size/throughput rate for operations, was the main goal of this iteration. Also, due to the notes in the last chapter based on the camera, we would move the camera to have its own section of the screen without the controls being overlaid on top of it. This would reduce the camera", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "000044c1ed1eb0c099e587e11c72b9f04a9bdbaa11329d7a51971b0a352ded49"}
{"doc_id": "arxiv:2510.13594#introduction:part-7", "url": "https://arxiv.org/abs/2510.13594", "anchor": "#introduction:part-7", "type": "paper", "title": "", "section": "Introduction", "text": "arrow keys for camera movement Krug (2005). Alternative actions also needed to be added, such as the get-up and crawl actions. This, along with resizing the camera for optimal size/throughput rate for operations, was the main goal of this iteration. Also, due to the notes in the last chapter based on the camera, we would move the camera to have its own section of the screen without the controls being overlaid on top of it. This would reduce the camera resolution and improve the throughput of the live image over the network. There would also be the translation of basic control instructions for new users that were present in the previous system. For this iteration, the camera was demonstrated to be much better in performance, and the binding of the movement buttons proved to be beneficial in simplifying the movement controls. During testing, there was an exception made for this button binding, and that was the action buttons, as they were added as selectable buttons on the screen rather than being bound to keys. This was done for a few reasons. The first is that they are less commonly used in the application of obstacle runs than the other movements; thus, their priority is less prudent. With this priority in mind, their prevalence in screen real estate is also much smaller than the other controls. This follows the design approach in UI development known as visual hierarchy, which gives the distinction of more important things to be more prevalent on the screen for the user Johnson (2010). And my second reason is due to the sub-goal of making this project scalable. If these actions are mapped to a key bind, other actions are later added and mapped. Eventually, it can become confusing to the user what button completes what actions, and could lead to operator overload and human error in actions Goodrich and Crandall (2013). So with this in mind, the actions are simply left as selectable buttons. And with this iteration, we have a UI that can complete the obstacle course, but can be improved upon. Another refinement added during the third iteration was the inclusion of start and reset buttons on the right-hand sides. These buttons set the position of the robot in their respective manners. The start button puts the robot in the default starting position for the event, and the reset puts the robot in a default standing position. This is useful if the operator is getting confused as to what the current positioning of the robot is; they can simply reset the position so that they can continue the operation from a point they already understand. The third iteration for development would look into adding some of the internal senses to the UI or moving them so that the user could access these during operation. This would allow the user to gain more information about the status of their robot and possibly troubleshoot the problem on their own. These include the rendering frames per second (FPS), battery life, and log of backbend messages. The battery voltage indicator", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "8ea537252cb7d615502eebe71fd36c2fff2c085aeba58883685706b0db707f8b"}
{"doc_id": "arxiv:2510.13594#introduction:part-8", "url": "https://arxiv.org/abs/2510.13594", "anchor": "#introduction:part-8", "type": "paper", "title": "", "section": "Introduction", "text": "continue the operation from a point they already understand. The third iteration for development would look into adding some of the internal senses to the UI or moving them so that the user could access these during operation. This would allow the user to gain more information about the status of their robot and possibly troubleshoot the problem on their own. These include the rendering frames per second (FPS), battery life, and log of backbend messages. The battery voltage indicator and log were both present in the old system, and just need to be reformatted to fit the new layout. After developing these features, minimal testing was needed to see their application validity. They did not hinder current operations and provided useful information on demand, so keeping these features proved to be beneficial. The fourth iteration had the goal of improving the visual experience of all the components within the GUI. So some of the methods implemented to improve this were High-Contrast Visuals, which allow the controls to be more visible and prudent compared to the general camera feed. Along with utilizing colours to correlate different portions of the screen. Dynamic screen sizing was also important for this phase a,s the previous iterations had been developed on a full 1080p display but in competition applications that will likely be a much smaller window to conserve as much processing power as possible and to fit on the smaller laptop screens, so dynamic sizing web principles are important. Now, the placement and labelling of certain features on the display also become more prominent in development. The addition of collapsible menus can also allow users to only open the menus needed for operations. For example, the control instruction menu can be collapsed so that returning users can minimize the menu so they don’t need to have the menu take up space that can be otherwise used for controls. First, the blue and orange were selected as contrasting colours so that different features could be associated with colour. In the case of this application, blue and orange were selected as the colours as they are in most applications, visible 6 A PREPRINT when compared to the screen live. The buttons for controls are located in bright orange on the screen and are larger and brighter than their surrounding components, as they are more important for the operation of the telescope. Labelling was also elaborated and refined with simple descriptions and a pop-up instruction option so users could choose to have it on screen. Additionally, on the final iteration of the development cycle, a new system on the starting screen was created, where the user can make micro-adjustments to the duration of the walking movements on the fly. Because the system uses JS listeners, and individual button presses without holding the button produce one movement. So if you’re on an obstacle course and need to only move forward a unit smaller than any derivative the the default value, you can just lower the value to be smaller so the user can make the desired distance move without", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "07872ee480edab7eeff892f5093fbbc21fd1d1d5c1b24f9a77ee717c1f5f106d"}
{"doc_id": "arxiv:2510.13594#introduction:part-9", "url": "https://arxiv.org/abs/2510.13594", "anchor": "#introduction:part-9", "type": "paper", "title": "", "section": "Introduction", "text": "the starting screen was created, where the user can make micro-adjustments to the duration of the walking movements on the fly. Because the system uses JS listeners, and individual button presses without holding the button produce one movement. So if you’re on an obstacle course and need to only move forward a unit smaller than any derivative the the default value, you can just lower the value to be smaller so the user can make the desired distance move without affecting the result. This feature utilizes a subscriber-listener model where JavaScript event listeners capture the user’s adjustments to walking movement coefficients in real time. Specifically, when a user modifies the distance or angle values in the input fields on the GUI, a listener function stores these updated values locally in the browser’s session storage. Upon pressing a movement button, another listener retrieves the current coefficients and constructs a movement command message. This message is then published over a WebSocket connection using roslib.js to the robot’s ROSbridge server. On the robot’s side, a ROS node subscribes to the movement topic, reads the received message, and then executes the movement using the user-defined values. This model allows seamless, on-the-fly customization of robotic movements, allowing the user to fine-tune actions without restarting the system. 4.3 Refinements and Iterations During the iterative process of development, there were many refinements to the old code and the newly written code. The following depicts the changes of some of these features and fine tuning that was completed during the development period. Firstly, one of the main features that was present in the old system was the layout map as shown in Fig. 3. This map would be created and modified by the user and would depict a version of the obstacle cores from a bird-eye perspective. In this map, you would be able to modify the obstacles and the robot’s position within it. When movements on the robot were completed, it would also depict the robot moving as the user entered commands. This feature was present on the same screen as the operation, and depending on the map size, it could vastly over-clutter the operation area. This feature, although good in practice, would clutter the screen and work only sometimes in the application. To make this feature more user-friendly, it was moved to the first screen where it has its fixed-size location on screen as shown in Fig. 4. As well as making it available on the second screen, should the user want to use it. While this optional second screen feature is not completed due to time constraints, the new layout of it being on the first screen reduces clutter and allows the user to focus either on creating the map or operating the robot, rather than having a feature you are not using being constantly on your screen. Another Major refinement between the old and the new versions of the UI is the dynamic page sizing as shown in Fig. 8. This feature will be analysed in the coming chapter, but just know that the new system", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "31677bf3615f0f730ab922b2b26899bef7674fc0c62b046e4795030fafbc5ba5"}
{"doc_id": "arxiv:2510.13594#introduction:part-10", "url": "https://arxiv.org/abs/2510.13594", "anchor": "#introduction:part-10", "type": "paper", "title": "", "section": "Introduction", "text": "the new layout of it being on the first screen reduces clutter and allows the user to focus either on creating the map or operating the robot, rather than having a feature you are not using being constantly on your screen. Another Major refinement between the old and the new versions of the UI is the dynamic page sizing as shown in Fig. 8. This feature will be analysed in the coming chapter, but just know that the new system provides a system for dynamic window sizing in a simple and user-friendly manner. Whereas the old system just compressed everything within the window, causing loss of page structure and readability. Seeing as shrinking this window in competition is a user-friendly to save processing power is common, this feature refinement will prove to be critical for the user experience. The next general refinement of the code involved the ROS communication for the movement of the robot. The old system did not provide any actions such as left and right walks, even though the buttons were present, and the mapping of the configurations of these walks were not present, so updates to these walking configurations were added. Along with this, a restructuring of this message mapping code present in rosactions.js was refactored to be more readable and expandable in case more motions are added in the future. The buttons for the action buttons, such as crawling and recovery, were also implemented in a way such that developers in the future may be able to just add another button to the sub-menu and map it to its action in ROS without much hassle. This addition helps achieve the secondary goal of allowing the code to be scalable for future events. On the final iteration of development, the addition of many of the buttons present on the right side being activated to turn into sub-menus provides users with less clutter, as shown in Fig. 9. This allows the operator to be more focused on the task at hand and reduce the cognitive load of performing the tasks, helping user performance in general applications Norman (2013); Tidwell (2010). This also has the advantage of allowing more advanced users to keep tabs like the control list closed so they can focus on the task, and new users can open this legend of controls. We added the additional feature of tooltips to the buttons for the system, where almost every button has a descriptive tooltip that appears if operators hover over the button, as shown in Fig. 6. This visual labelling ensures that even non-expert users can immediately recognize the function without needing prior training. This aligns with best common practices and is in user-friendly development Cooper et al. (2014). 7 A PREPRINT Figure 3: Image of original screen with the map on the left side of the screen and operation buttons on the bottom Figure 4: Image of new map layout with map on left and operation buttons on the right 8 A PREPRINT The old system utilized a slider bar to represent the position in terms of", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "97d1055b5503749fb0600f094aefd269f69e2c8b1a303cb5b2aacc8f8faa0d0e"}
{"doc_id": "arxiv:2510.13594#introduction:part-11", "url": "https://arxiv.org/abs/2510.13594", "anchor": "#introduction:part-11", "type": "paper", "title": "", "section": "Introduction", "text": "the function without needing prior training. This aligns with best common practices and is in user-friendly development Cooper et al. (2014). 7 A PREPRINT Figure 3: Image of original screen with the map on the left side of the screen and operation buttons on the bottom Figure 4: Image of new map layout with map on left and operation buttons on the right 8 A PREPRINT The old system utilized a slider bar to represent the position in terms of how far its head is turned in a certain direction. This representation of the head’s position proved to be valuable as the user can easily see if the head is aligned with the body without any guesswork involved. Tweaks to these sliders were made in terms of visual appeal and location on the screen with labelling to make it more clear to the user in terms of their use 9. Additionally, the implementation of a reset button on the slider proved useful as it allowed the user to quickly reset the camera position without additional time consumed. 4.4 Comparison setup The newly developed UI is evaluated by directly comparing it to the old system previously used on the robot. This comparison focuses on how each interface adheres to established UI development practices and maintains consistency. An interface that incorporates these practices more thoroughly is presumed to be more user-friendly. Although this approach does not offer definitive proof of improvement, it provides a qualitative measure to gauge which UI might better enhance the user experience. 5 Results and Discussion This chapter reviews the completed teleoperation GUI, contrasts it with the legacy interface, and highlights key gains, remaining limits, and practical implications for competition use. 5.1 Presentation of Results Figure 5 shows the former interface; Figure 9 depicts the new one. Figure 5: Legacy teleoperation screen. Layout & Navigation The new UI divides functions across two pages and clearly grouped panels, so related controls stay close and unused tools stay hidden. Separate “operate” and “map-edit” pages to remove clutter. Colour Scheme Dark blue backgrounds reduce glare, while bright-orange buttons stand out against both the back- ground and live video, guiding the eye. Camera Emphasis The live feed is now central and larger. Paired with keyboard shortcuts (WASD for motion, arrows for camera), operators can watch and act simultaneously. 9 A PREPRINT On-the-fly Tuning Figure 6 shows sliders for step length and turn angle; values persist locally so users adjust without restarting. Figure 6: Movement-coefficient controls. Responsive Design Flexbox scaling keeps panels legible on small windows (compare Figures 7–8). Scrollbars appear only when components outsize their areas. 10 A PREPRINT Figure 7: Old UI on a small display. Figure 8: New UI on the same footprint. 11 A PREPRINT Collapsible Menus Sidebar buttons open drop-downs inside the same frame (Figure 9); each panel gets its own scrollbar to keep controls usable even in very narrow views. Figure 9: Sidebar with expanded sub-menu. 5.2 Discussion Notable advances include: • Fine-tune sliders for step/turn coefficients—absent before—let operators adapt mid-run. • Added motions (crawl, self-stand) enable recovery and", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "31657c26d7e99b1c101c667c30f8f9946c99f38868cd920933087f008ba51500"}
{"doc_id": "arxiv:2510.13594#introduction:part-12", "url": "https://arxiv.org/abs/2510.13594", "anchor": "#introduction:part-12", "type": "paper", "title": "", "section": "Introduction", "text": "their areas. 10 A PREPRINT Figure 7: Old UI on a small display. Figure 8: New UI on the same footprint. 11 A PREPRINT Collapsible Menus Sidebar buttons open drop-downs inside the same frame (Figure 9); each panel gets its own scrollbar to keep controls usable even in very narrow views. Figure 9: Sidebar with expanded sub-menu. 5.2 Discussion Notable advances include: • Fine-tune sliders for step/turn coefficients—absent before—let operators adapt mid-run. • Added motions (crawl, self-stand) enable recovery and obstacle negotiation, impossible with the old tool. • Relative to manual ROS commands or the legacy GUI, the new layout offers far less clutter, labelled controls, and reliable head-pan sliders, all reinforcing novice usability. 5.3 Limitations Two key constraints remain: (i) minimal human-subject testing during iterations, limiting empirical validation of “user-friendly” claims; and (ii) tight timelines that postponed extra features that could further streamline competition performance. 6", "source": "arxiv_pdf", "published": "", "tokens": 147, "sha256": "ea49fb4de8a7dd7023b6bfb5fcd80b728115380a12b065b488cc552e51d6094f"}
{"doc_id": "arxiv:2510.13594#conclusion:part-1", "url": "https://arxiv.org/abs/2510.13594", "anchor": "#conclusion:part-1", "type": "paper", "title": "", "section": "Conclusion", "text": "The objective of this project was to create a user-friendly GUI designed for non-expert users to teleoperate with a humanoid robot through a FIRA-rated obstacle course. This was done by addressing issues with the previous system used before this project. This newer version introduced significant improvements aimed at reducing cognitive load, increasing the general intuitiveness of the system, and ensuring that the current system is scalable for future endeavours. The improvement of adding new motions and actions to complete the obstacle course proved an extreme success and was beneficial. Along with adding the bindings for those movements, they proved to be more important for the user 12 A PREPRINT experience with the screen UI than anticipated. Much of the time, it can be harder to navigate a screen if you are moving back and forth pressing different buttons, adding to the user’s overstimulation. The addition of all the new layout features also leads to a very comfortable user navigation experience. The logically grouped controls and common placement of major controls (such as page navigation being in the top corners of the screen or confirmations being at the bottom of a section) provide a very simplistic navigation experience that should be familiar to most users in some way or another. Navigation of the basic application, as well as controlling the robot’s mechanisms, is much easier because of this layout. Although the application of this system is for a particular event on this robot, it was developed with the goal in mind to change how other UIs of a similar nature and tasks are developed and displayed. I think the improvement shown in this project could help others who are creating UIs to look deeper into the human-friendly layout of their systems, as it may provide more efficient operation and output from an individual user. This is an idea that is reinforced multiple times in similar literature, showing its underlying importance in development. Although the absence of formal user testing and time limitations posed constraints on the scope of development, the resulting system represents a marked advancement over the old GUI for the event. Future research should focus on quantitative measurements and reviews with non-expert participants, in order to more directly develop the system. Overall, this project has led to an improved system for the robot’s competition-level event and may provide more results in the future with further development and study. It is important to develop the interactions that your user has with your system to be curated to the user, rather than the developer or the computer, as it may provide better results than one expects. 6.1 Acknowledgement We gratefully acknowledge the financial and institutional support provided by Laurentian University, particularly the Faculty of Science, Engineering and Architecture, and the School of Engineering and Computer Science. Their backing has been instrumental in the development and success of the Snobots team and the Laurentian Intelligent Mobile Robotics Lab (LIMRL). We also extend our sincere thanks to our industrial sponsors, whose generous contributions have supported our operational and travel expenses, enabling us to participate in", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "da81df84dc78acd08d7f22428b26ac4d18d6b652fc062662aa7e5cbe7d18e814"}
{"doc_id": "arxiv:2510.13594#conclusion:part-2", "url": "https://arxiv.org/abs/2510.13594", "anchor": "#conclusion:part-2", "type": "paper", "title": "", "section": "Conclusion", "text": "than one expects. 6.1 Acknowledgement We gratefully acknowledge the financial and institutional support provided by Laurentian University, particularly the Faculty of Science, Engineering and Architecture, and the School of Engineering and Computer Science. Their backing has been instrumental in the development and success of the Snobots team and the Laurentian Intelligent Mobile Robotics Lab (LIMRL). We also extend our sincere thanks to our industrial sponsors, whose generous contributions have supported our operational and travel expenses, enabling us to participate in national and international competitions. Their support has been vital in advancing our research and outreach activities in the field of humanoid robotics.", "source": "arxiv_pdf", "published": "", "tokens": 102, "sha256": "15047e1338e83cd7cdce6b7d8f77e667ff2220a188ad2762e90046f15d3b29e0"}
{"doc_id": "arxiv:2510.13794#introduction", "url": "https://arxiv.org/abs/2510.13794", "anchor": "#introduction", "type": "paper", "title": "", "section": "INTRODUCTION", "text": "Reinforcement-learning (RL) based motion imitation techniques have become a versatile and effective paradigm for constructing motion controllers that are able to produce agile, life-like behaviors for both simulated characters and robots in the real world. Although the many of the core ideas are conceptually simple, building effective motion imitation systems requires careful attention to numerous nuances and detailed design decisions that are often challenging to implement in practice. MimicKit is designed to lower the barrier for experimentation and reproducible research in this field by bringing together a suite of high-quality implementations of training methods and tools into a single unified and extensible framework. 2", "source": "arxiv_pdf", "published": "", "tokens": 104, "sha256": "fe55bf00c369dd2a2e5cb8f358feee2a25443a33b39be424d7cc1cb7c06d389b"}
{"doc_id": "arxiv:2510.13794#background:part-1", "url": "https://arxiv.org/abs/2510.13794", "anchor": "#background:part-1", "type": "paper", "title": "", "section": "BACKGROUND", "text": "In MimicKit, most models are trained using reinforcement learning, where an agent interacts with an environment according to a policy 𝜋in order to optimize a given objective [Sutton and Barto 2018]. At each time step 𝑡, the agent receives an observations o𝑡of the environment, which provides partial information of the state s𝑡of the underlying system. The agent responds by sampling an action from a policy a𝑡∼𝜋(a𝑡|o𝑡). The agent then Author’s address: Xue Bin Peng, xbpeng@sfu.ca, Simon Fraser University and NVIDIA. arXiv:2510.13794v2 [cs.GR] 16 Oct 2025 2 • Xue Bin Peng Fig. 2. Schematic overview of the MimicKit framework. The main components of the system are 1) the Agent, 2) the Model, 3) the Environment, and 4) the Engine. The learning algorithms are implemented primarily through the Agent and Model, while the Environment and Engine are responsible for simulating the desired task. executes the action, which leads to a new state s𝑡+1, sampled according to the dynamics of the environment s𝑡+1 ∼𝑝(s𝑡+1|s𝑡, a𝑡). The agent in turn receives a scalar reward 𝑟𝑡= 𝑟(s𝑡, a𝑡, s𝑡+1), and a new observation o𝑡+1 of the next state s𝑡+1. The agent’s objective is to learn a policy that maximizes its expected discounted return 𝐽(𝜋), 𝐽(𝜋) = E𝑝(𝜏|𝜋) \"𝑇−1 ∑︁ 𝑡=0 𝛾𝑡𝑟𝑡 # , (1) where 𝑝(𝜏|𝜋) represents the likelihood of a trajectory 𝜏= {o0, a0,𝑟0, o1, ..., o𝑇−1, a𝑇−1,𝑟𝑇−1, o𝑇} under 𝜋.𝑇denotes the time horizon of a trajectory, and 𝛾∈[0, 1] is a discount factor. Each trajectory corresponds to one episode of interactions between the agent and the environment. 3 SYSTEM OVERVIEW A schematic overview of the MimicKit framework is provided in Figure 2. The core components of MimicKit consist of: 1) the Agent, 2) the Model, 3) the Environment, and 4) the Engine. The learning algorithms are implemented primarily through the Agent and the Model, while the Environment and Engine are responsible for simulating the desired task. These components are designed to be modular and composable, enabling users to combine different learning algorithms, model architectures, characters, tasks, and simulators. The simulations are implemented using vectorized environments, which can be massively parallelized using GPU simulators for high-throughput data collection during training. The environments and learning algorithms are designed to be character-agnostic, enabling the overall system to be easily configured to support characters with different morphologies, including humanoid and non-humanoid characters, such as quadrupedal robots. 3.1 Agent The Agent class is responsible for implementing the learning algorithm and managing data recorded through inter- actions with the environment. Implementations for a suite of different agents are provided in mimickit/learning/ . MimicKit: A Reinforcement Learning Framework for Motion Imitation and Control • 3 At each timestep 𝑡, the Agent receives an observation o𝑡from the Environment. This observations is processed into an input x𝑡for the Model, which can include pre-processing steps such as observation normalization. The Model is then queried with the processed input x𝑡, which produces an output y𝑡. The Model outputs y𝑡may specify parameters of an action distribution, value function predictions, discriminator predictions, or other quantities required by the learning algorithm. The Agent then extracts an action a𝑡from the", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "56a544da41ec78c159e31a5a446ad5294b33710872a001c494f522cf680265bd"}
{"doc_id": "arxiv:2510.13794#background:part-2", "url": "https://arxiv.org/abs/2510.13794", "anchor": "#background:part-2", "type": "paper", "title": "", "section": "BACKGROUND", "text": "Control • 3 At each timestep 𝑡, the Agent receives an observation o𝑡from the Environment. This observations is processed into an input x𝑡for the Model, which can include pre-processing steps such as observation normalization. The Model is then queried with the processed input x𝑡, which produces an output y𝑡. The Model outputs y𝑡may specify parameters of an action distribution, value function predictions, discriminator predictions, or other quantities required by the learning algorithm. The Agent then extracts an action a𝑡from the model outputs, and applies a𝑡to the Environment. The Environment in turn transitions to a new state s𝑡+1 and provides the Agent with the next observations o𝑡+1, reward 𝑟𝑡, and a done flag 𝑑𝑡. The done flag 𝑑𝑡indicates if the current episode has been terminated. In each iteration, the Agent repeats this interaction loop with the Environment until a designated number of timesteps has been collected. The data collected through these interactions are stored in an experience buffer implemented in mimickit/learning/experience_buffer.py . Once a sufficiently large batch of data has been collected, the Agent then uses the data to update its Model. The agent configuration files, located in data/agents/ , are used to specify the type of Agent to use for training, as well as its associated hyperparameters. 3.2", "source": "arxiv_pdf", "published": "", "tokens": 208, "sha256": "02182b2b975ac5a511cc44625985b05a27ee800de970a5a76493d75c4516781c"}
{"doc_id": "arxiv:2510.13794#model:part-1", "url": "https://arxiv.org/abs/2510.13794", "anchor": "#model:part-1", "type": "paper", "title": "", "section": "Model", "text": "While the Agent implements the learning procedure, the Model is responsible for implementing the underlying neural network architecture used in the learning process. Each Agent is paired with a corresponding Model, located in mimickit/learning/ . For an actor-critic algorithm, such as PPO [Schulman et al. 2017], the model may contain multiple neural networks, one for the policy (i.e. actor) and the value function (i.e. critic). For methods such as AMP, an additional network might be constructed for the discriminator. The Agent can query the Model’s various networks with the appropriate input x𝑡, and the Model returns the corresponding output y𝑡. The Model’s network architectures are specified through the model field in the agent configuration file. 3.3 Environment The Environment implements the task-specific logic necessary to simulated a desired task. This class is used to define the interface through which the agent observes and interacts with its surrounding environment. At each timestep 𝑡, the Environment constructs the observation o𝑡based on the state s𝑡of the world determined by the Engine. The observation o𝑡can contain proprioceptive information on the character’s body, information on the configuration of surrounding objects, as well as task-specific information, such as the target locations and steering commands. Upon receiving the action a𝑡from the Agent, the Environment processes a𝑡into a command c𝑡. The command is then applied to the Engine to update the state of the underlying system, which can be modeled by a simulator or correspond to a real-world system. The environment update is performed through a step function: obs , r, done , info = self._env.step(action) The step function returns a new observation o𝑡+1 and a reward 𝑟𝑡for the state transition. Furthermore, an info dictionary can be used to store additional information from the Environment, such as auxiliary observations for a critic or discriminator. Finally, the done flag 𝑑𝑡indicates if the current episode has terminated. The done flag can assume 4 different values, as defined in mimickit/envs/base_env.py , depending on the conditions under which an episode was terminated. The different done flags include: • NULL : The episode has not been terminated. • FAIL : The episode terminated due to a failure, such as the character falling down. This flag can be used to apply a terminal penalty when calculating returns during training. 4 • Xue Bin Peng • SUCC : The episode terminated due to successfully completing a task, such as the character successfully reaching the target location. This flag can be used to apply a terminal bonus when calculating returns during training. • TIME : The episode is terminated due to a time limit, but should in principle continue after the last timestep of the episode. In the event that a trajectory is truncated due to time, bootstrapping with a value function can be used to estimate future returns, as if the trajectory had continued after the last timestep. This enables the learning algorithms to emulate infinite-horizon MDPs given finite-length trajectories. The configuration of the Environment is specified through environment configuration files located in data/envs/ . 3.4 Engine While the Environment implements the high-level logic for simulating a", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "e964bd8711e1990e92790878d34ed71f830e2d0d715858493004c54788601fb8"}
{"doc_id": "arxiv:2510.13794#model:part-2", "url": "https://arxiv.org/abs/2510.13794", "anchor": "#model:part-2", "type": "paper", "title": "", "section": "Model", "text": "continue after the last timestep of the episode. In the event that a trajectory is truncated due to time, bootstrapping with a value function can be used to estimate future returns, as if the trajectory had continued after the last timestep. This enables the learning algorithms to emulate infinite-horizon MDPs given finite-length trajectories. The configuration of the Environment is specified through environment configuration files located in data/envs/ . 3.4 Engine While the Environment implements the high-level logic for simulating a particular task, the low-level simulation of the world is delegated to an Engine. The Engine class, implemented in mimickit/engines/engine.py , provides a unified API that abstracts away the low-level details of how an Environment is simulated. Different Engines can be constructed for different physics engines and real-world robotic systems. This enables a specific task and environment to be instantiated through different underlying simulators and physical robots. MimicKit currently only supports IsaacGym [Makoviychuk et al. 2021], but additional Engines will be introduced in the future to support other physics simulators, as well as deployment on real robots. At each timestep, the Engine receives the command c𝑡from the Environment, and returns an updated state s𝑡+1. The representation of c𝑡depends on the control modes that are supported by a specific Engine. For example, the IsaacGym Engine, implemented in mimickit/engines/isaac_gym_engine.py supports the following control modes: • none : Commands have no effect on the simulation. This mode can be useful for visualization and debugging. • pos : Commands specify target rotations for PD controllers, which support both 1D revolute joints and 3D spherical joints. • vel : Commands specify target velocities for each joint. • torque : Commands directly specify torques for each joint. • pd_1d : Commands specify target rotations for 1D revolute joints. This control mode can only be applied to morphologies that solely consist of 1D revolute joints, and does not support 3D spherical joints. This control mode is best suited for simulating robots that only contain 1D revolute joints. The configuration of the engine can be specified through the engine field in the environment configuration file. The environment file can be used to specify the type of Engine to use for simulation, along with parameters such as the control model, control frequency, simulation frequency, etc. MimicKit: A Reinforcement Learning Framework for Motion Imitation and Control • 5 4", "source": "arxiv_pdf", "published": "", "tokens": 388, "sha256": "5aff8dc21f529927a1a49dda6b864f5080da3c1075c3db15ce30ba00a0480171"}
{"doc_id": "arxiv:2510.13794#methods:part-1", "url": "https://arxiv.org/abs/2510.13794", "anchor": "#methods:part-1", "type": "paper", "title": "", "section": "METHODS", "text": "MimicKit provides a suite of motion imitation methods for training controllers. These methods offer different characteristics and trade-offs, and an appropriate method should be selected based on the requirements of a target application. Example argument files are provided in the arguments directory args/ for using the various methods. 4.1 DeepMimic DeepMimic is a simple RL-based motion tracking method [Peng et al. 2018], which trains a tracking controller to follow target reference motions. This method is very general and reliable, and has been successfully applied to train controllers for a wide range of behaviors. DeepMimic is often a good starting point before considering more sophisticated techniques, and can be a highly effective method for applications that require precise replication of a target reference motion. However, a key limitation of DeepMimic is that the motion tracking objective used during training often leads to inflexible policies that are restricted to closely following a given reference motion. This can limit the agent’s ability to modify and adapt behaviors in the dataset as necessary to perform new tasks. Example arguments for running DeepMimic are provided in args/deepmimic_humanoid_ppo_args.txt . The reference motion data used for training can be specified using the motion_file in the environment configuration file data/envs/deepmimic_humanoid_env.yaml . 4.2 Adversarial Motion Priors (AMP) Unlike DeepMimic, which trains a controllers to closely track a given reference motion, AMP is an adversarial distribution-matching method that aims to imitate the overall behavioral distribution (i.e. style) depicted in a dataset of motion clips [Peng et al. 2021], without explicitly tracking any specific motion clip. AMP provides more versatility than tracking-based methods, providing the agent with more flexibility to compose and adapt behaviors in the dataset in order to perform new tasks. However, a key drawback of distribution-matching methods, such as AMP, is that they are more prone to converging to local optima, especially for challenging, highly dynamics motions. Therefore AMP may struggle more to closely replicate challenging behaviors, compared to tracking-based methods, such as DeepMimic. Example arguments for using AMP to imitate individual motion clips, without auxiliary tasks, are provided in args/amp_humanoid_args.txt . An example for training an AMP model with auxiliary tasks is provided in args/amp_location_humanoid_args.txt . 6 • Xue Bin Peng 4.3 Adversarial Skill Embeddings (ASE) ASE is an adversarial methods for training reusable generative controllers [Peng et al. 2022]. This method combines adversarial imitation learning with a mutual information-based skill discovery objective to learn latent skill embeddings. Points in the latent space can be mapped to diverse behaviors by the ASE controller. Once trained, the ASE controller can be reused to perform new tasks by training task-specific high-level controllers to select skills from the learned latent space. Example arguments for training ASE models are provided in args/ase_humanoid_args.txt . 4.4 Adversarial Differential Discriminator (ADD) ADD is an adversarial motion tracking method that uses a differential discriminator to automatically learn adaptive motion tracking objectives [Zhang et al. 2025]. This method can mitigate the manual effort required to design and tune tracking reward functions for different characters and motions. Example arguments for training ADD models are provided in args/add_humanoid_args.txt . Most", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "8635f8c44244a5850b656d90f8fe258793e56e045a075f52827727cf8b6fc49b"}
{"doc_id": "arxiv:2510.13794#methods:part-2", "url": "https://arxiv.org/abs/2510.13794", "anchor": "#methods:part-2", "type": "paper", "title": "", "section": "METHODS", "text": "controllers to select skills from the learned latent space. Example arguments for training ASE models are provided in args/ase_humanoid_args.txt . 4.4 Adversarial Differential Discriminator (ADD) ADD is an adversarial motion tracking method that uses a differential discriminator to automatically learn adaptive motion tracking objectives [Zhang et al. 2025]. This method can mitigate the manual effort required to design and tune tracking reward functions for different characters and motions. Example arguments for training ADD models are provided in args/add_humanoid_args.txt . Most of the methods in MimicKit are implemented using proximal policy optimization (PPO) as the underlying RL algorithm [Schulman et al. 2017]. PPO is currently the most commonly-used RL algorithm for motion control tasks, and can be effectively scaled with high-throughput GPU simulators. However, since PPO is an on-policy algorithm [Sutton and Barto 2018], it can be notoriously sample inefficient. Our framework provides an off-policy algorithm, Advantage-Weighted Regression (AWR) [Peng et al. 2019], as an alternative to PPO for settings that may require off-policy RL algorithms. 5 INSTRUCTIONS In this section, we provide starter instructions for installing MimicKit, training models, testing models, and an overview of basic tools to assist in common workflows. 5.1 Installation MimicKit can be installed by following the steps below: MimicKit: A Reinforcement Learning Framework for Motion Imitation and Control • 7 (1) MimicKit utilizes NVIDIA IsaacGym for high-performance physics simulation. IsaacGym installation instructions can be found at: https://developer.nvidia.com/isaac-gym. (2) Next install the dependencies from requirements.txt : pip install -r requirements.txt (3) Download assets and motion data from the data repository, then extract the contents into the data directory data/ . After completing these steps, MimicKit should be ready for use. 5.2 Training To train a model, a typical training command will be as follows: python mimickit/run.py --mode train --num_envs 4096 \\ --env_config data/envs/deepmimic_humanoid_env.yaml \\ --agent_config data/agents/deepmimic_humanoid_ppo_agent.yaml \\ --visualize true --log_file output/log.txt \\ --out_model_file output/model.pt The arguments consist of • --mode selects either train or test mode. • --num_envs specifies the number of parallel environments used for simulation. • --env_config specifies the configuration file for the environment. • --agent_config specifies configuration file for the agent. • --visualize enables visualization. Rendering should be disabled for faster training. • --log_file specifies the output log file, which will keep track of statistics during training. • --out_model_file specifies the output model file, which contains the model parameters. • --logger specifies the logger used to record training statistics. The options are TensorBoard tb or wandb . Instead of specifying all arguments through the command line, arguments can also be loaded from an argument file arg_file : python mimickit/run.py --arg_file args/deepmimic_humanoid_ppo_args.txt The arguments in arg_file are treated the same as command line arguments. When using an argument file, additional command line arguments can be included to override the arguments in the arg_file . A library of arguments are provided in the arguments directory args/ for training models and using various tools. 5.3 Distributed Training The standard training command will train a model using a single process. To accelerate training, distributed training with multi-CPU or multi-GPU can be used with the following command: python mimickit/run.py --arg_file", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "f94de77a58acb750ec98442b7795233b6bac5d18163237b3d30a5070ec522aa9"}
{"doc_id": "arxiv:2510.13794#methods:part-3", "url": "https://arxiv.org/abs/2510.13794", "anchor": "#methods:part-3", "type": "paper", "title": "", "section": "METHODS", "text": "are treated the same as command line arguments. When using an argument file, additional command line arguments can be included to override the arguments in the arg_file . A library of arguments are provided in the arguments directory args/ for training models and using various tools. 5.3 Distributed Training The standard training command will train a model using a single process. To accelerate training, distributed training with multi-CPU or multi-GPU can be used with the following command: python mimickit/run.py --arg_file args/deepmimic_humanoid_ppo_args.txt \\ --num_workers 2 --device cuda:0 where --num_workers specifies the number of worker processes used to parallelize training. --device specifies the device used for training, which can be either cpu or cuda:0 . When training with multiple GPUs, the 8 • Xue Bin Peng number of worker processes used to parallelize training must be less than or equal to the number of GPUs available on the system. 5.4 Testing During training, the latest model parameters will be saved to a checkpoint .pt file, specified by --out_model_file . A typical command to test a trained model will be as follows: python rl_forge/run.py --arg_file args/deepmimic_humanoid_ppo_args.txt \\ --num_envs 4 \\ --visualize true \\ --mode test \\ --model_file data/models/deepmimic_humanoid_spinkick_model.pt --mode test specifies that the code should be run in testing mode. --model_file specifies the .pt file that contains the parameters of the trained model. Pretrained models are provided in data/models/ , and the corresponding training log files are available in data/logs/ . 5.5 Visualizing Training Logs During training, When using the TensorBoard logger during training, a TensorBoard events file will be saved the same output directory as the log file. The log can be viewed with: tensorboard --logdir=output/ --port =6006 --bind_all In addition to visualizing training statistics with the runtime loggers, output log .txt file can also be visu- alized using the plotting script tools/plot_log/plot_log.py . Examples of learning curves generated by plot_log.py are shown in Figure 5. 6 MOTION DATA Most of the methods implemented in MimicKit utilize motion data to guide the training process. Example motion clips are provided in data/motions/ . The motion_file field in the environment configuration file can be used to specify the reference motion clip used for training and testing. In addition to imitating individual motion clips, motion_file can also specify a dataset file, located in data/datasets/ , which will train a model to imitate a dataset containing multiple motion clips. The view_motion environment can be used to visualize motion clips: python mimickit/run.py --mode test --arg_file args/view_motion_humanoid_args.txt \\ --visualize true Motion clips are represented by the Motion class implemented in mimickit/anim/motion.py . Each mo- tion clip is stored in a .pkl file. Each frame in a motion specifies the pose of the character according to [root position (3D), root rotation (3D), joint rotations] , where 3D rotations are specified using 3D exponential maps [Grassia 1998]. Joint rotations are recorded in the order that the joints are specified in the .xml file (i.e. depth-first traversal of the kinematic tree). For example, in the case of the Humanoid character data/assets/humanoid.xml , each frame is represented as: MimicKit: A Reinforcement Learning Framework for", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "00f236134220769fd56a6ada9fbaedae96a904f966db8f8ccfa4ec4874caaff1"}
{"doc_id": "arxiv:2510.13794#methods:part-4", "url": "https://arxiv.org/abs/2510.13794", "anchor": "#methods:part-4", "type": "paper", "title": "", "section": "METHODS", "text": ".pkl file. Each frame in a motion specifies the pose of the character according to [root position (3D), root rotation (3D), joint rotations] , where 3D rotations are specified using 3D exponential maps [Grassia 1998]. Joint rotations are recorded in the order that the joints are specified in the .xml file (i.e. depth-first traversal of the kinematic tree). For example, in the case of the Humanoid character data/assets/humanoid.xml , each frame is represented as: MimicKit: A Reinforcement Learning Framework for Motion Imitation and Control • 9 (1) root position (3D) (2) root rotation (3D) (3) abdomen (3D) (4) neck (3D) (5) right_shoulder (3D) (6) right_elbow (1D) (7) left_shoulder (3D) (8) left_elbow (1D) (9) right_hip (3D) (10) right_knee (1D) (11) right_ankle (3D) (12) left_hip (3D) (13) left_knee (1D) (14) left_ankle (3D) Fig. 3. Simulated Humanoid character. The rotations of 3D joints are represented using 3D exponential maps, and the rotations of 1D joints are represented using 1D rotation angles. 7", "source": "arxiv_pdf", "published": "", "tokens": 159, "sha256": "7d182652fe6612bce74e9da7b23d7b4a62d7983ab81b6c663f9fff481d7249d6"}
{"doc_id": "arxiv:2510.13794#experiments:part-1", "url": "https://arxiv.org/abs/2510.13794", "anchor": "#experiments:part-1", "type": "paper", "title": "", "section": "EXPERIMENTS", "text": "To evaluate the framework’s effectiveness to reproduce diverse naturalistic motions, we apply the methods implemented in MimicKit on motion imitation tasks with a diverse suite of motions, ranging from common everyday behaviors, such as walking and running, to highly dynamic and athletic behaviors, such as acrobatics and martial arts. Our experiments assess both the quantitative tracking performance of the learned controllers and the qualitative fidelity of the resulting motions. 7.1 Motion Imitation All experiments are conducted using the IsaacGym physics simulator. Each task involves a simulated humanoid character trained to imitate reference motion clips recorded via motion capture of live actors. We compare policies trained using three representative algorithms implemented within MimicKit: DeepMimic [Peng et al. 2018], AMP [Peng et al. 2021], and ADD [Zhang et al. 2025]. Separate policies are trained for each motion clip. In order to compare different methods under similar settings, we disable pose error termination used in Peng et al. [2018] during training, which terminates an episode if the character’s pose deviates significantly from the reference. Pose error termination is not applicable to distribution matching techniques such as AMP, where the policy is not synchronized with the reference motion. During training and evaluation, early termination is triggered only when the character makes undesired contact with the ground. Snapshots of the behaviors learned by policies trained using various methods implemented in MimicKit are shown in Figure 4. Our framework is able to effectively train policies for a wide range of challenging and highly dynamics behaviors with a diverse cast of simulated characters, including a humanoid character modeled after the SMPL body model [Loper et al. 2015], a Unitree G1 humanoid robot, and a Unitree Go2 quadrupedal robot. Despite the significant differences in morphology among these character, the same underlying training framework can be applied with minimal modifications, highlighting the modularity and generality of our system. 10 • Xue Bin Peng (a) Humanoid - Backflip (b) Humanoid - Roll (c) SMPL - Spin (d) SMPL - Getup Facedown (e) G1 - Run (f) G1 - Cartwheel (g) Go2 - Trot (h) Go2 - Canter Fig. 4. Snapshots of physically simulated characters performing skills learned by imitating motion data recorded from real-life actors. The methods implemented in MimicKit can be applied to train policies for a diverse cast of simulated characters and skills. MimicKit: A Reinforcement Learning Framework for Motion Imitation and Control • 11 Motion Length Position Tracking Error [m] DoF Velocity Tracking Error [rad/s] AMP DeepMimic ADD AMP DeepMimic ADD) Run 0.80s 0.163±0.008 0.013±0.002 0.165±0.017 2.811±0.048 0.584±0.054 0.478±0.007 Jog 0.83s 0.120±0.007 0.021±0.000 0.024±0.004 2.017±0.052 0.575±0.007 0.507±0.010 Sideflip 2.44s 0.387±0.011 0.138±0.004 0.145±0.006 2.276±0.014 1.118±0.034 1.350±0.049 Crawl 2.93s 0.050±0.006 0.027±0.000 0.028±0.002 0.646±0.089 0.430±0.006 0.283±0.002 Roll 2.00s 0.141±0.031 0.115±0.132 0.152±0.005 1.576±0.318 0.994±0.051 1.330±0.101 Getup Face- down 3.03s 0.096±0.018 0.023±0.001 0.022±0.001 0.838±0.029 0.433±0.008 0.325±0.005 Spinkick 1.28s 0.064±0.010 0.078±0.062 0.025±0.000 1.453±0.327 1.222±0.233 0.774±0.007 Cartwheel 2.71s 0.076±0.006 0.144±0.153 0.017±0.000 0.722±0.020 0.659±0.160 0.317±0.002 Backflip 1.75s 0.267±0.015 0.111±0.054 0.062±0.001 2.243±0.113 1.103±0.024 0.878±0.013 Dance A 1.62s 0.065±0.009 0.065±0.029 0.028±0.007 0.895±0.108 0.830±0.090 0.428±0.014 Walk 0.96s 0.132±0.021 0.009±0.001 0.009±0.001 1.394±0.123 0.286±0.005 0.213±0.003 Table 1. Motion tracking performance of the", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "dad8310440cef7f077fa42a07408c717d2ed61a3fa57f8858c7e1e0da2585b50"}
{"doc_id": "arxiv:2510.13794#experiments:part-2", "url": "https://arxiv.org/abs/2510.13794", "anchor": "#experiments:part-2", "type": "paper", "title": "", "section": "EXPERIMENTS", "text": "0.387±0.011 0.138±0.004 0.145±0.006 2.276±0.014 1.118±0.034 1.350±0.049 Crawl 2.93s 0.050±0.006 0.027±0.000 0.028±0.002 0.646±0.089 0.430±0.006 0.283±0.002 Roll 2.00s 0.141±0.031 0.115±0.132 0.152±0.005 1.576±0.318 0.994±0.051 1.330±0.101 Getup Face- down 3.03s 0.096±0.018 0.023±0.001 0.022±0.001 0.838±0.029 0.433±0.008 0.325±0.005 Spinkick 1.28s 0.064±0.010 0.078±0.062 0.025±0.000 1.453±0.327 1.222±0.233 0.774±0.007 Cartwheel 2.71s 0.076±0.006 0.144±0.153 0.017±0.000 0.722±0.020 0.659±0.160 0.317±0.002 Backflip 1.75s 0.267±0.015 0.111±0.054 0.062±0.001 2.243±0.113 1.103±0.024 0.878±0.013 Dance A 1.62s 0.065±0.009 0.065±0.029 0.028±0.007 0.895±0.108 0.830±0.090 0.428±0.014 Walk 0.96s 0.132±0.021 0.009±0.001 0.009±0.001 1.394±0.123 0.286±0.005 0.213±0.003 Table 1. Motion tracking performance of the Humanoid character trained using AMP, DeepMimic, and ADD. Position (Eq. 2) and DoF Velocity tracking errors are averaged across 5 models initialized with different random seeds. For each model, errors are calculated using 4096 test episodes. Motion tracking methods, such as DeepMimic and ADD, are able to more accurately reproduce a given reference motion compared to distribution-matching methods, such as AMP. To evaluate the performance of each policy, we measure the position tracking error 𝑒pos 𝑡 , and DoF velocity tracking error 𝑒vel 𝑡, which provides an indicator of motion smoothness. The position tracking error 𝑒pos 𝑡 measures the difference in the global root position and relative joint positions between the simulated character and the reference motion: 𝑒pos 𝑡 = 1 𝑁joint + 1 ©­ « ∑︁ 𝑗∈joints (ˆx𝑗 𝑡−ˆxroot 𝑡 ) −(x𝑗 𝑡−xroot 𝑡 ) 2 + ˆxroot 𝑡 −xroot 𝑡 2 ª® ¬ . (2) Here, x𝑗 𝑡and ˆx𝑗 𝑡represent the 3D Cartesian position of joint 𝑗from the simulated character and the reference motion, respectively. 𝑁joint denotes the number of joints in the character. The DoF velocity tracking error measures the differences in local angular velocities of each joint between the simulated character and the reference motion: 𝑒vel 𝑡 = 1 𝑁joint + 1 ∑︁ 𝑗∈joints ˆ¤q𝑗 𝑡−¤q𝑗 𝑡 2 , (3) where ¤q𝑗 𝑡and ˆ¤q𝑗 𝑡represent the local angular velocity of joint 𝑗from the simulated character and the reference motion. Table 1 summarizes performance of the various methods. Performance statistics for each method are calculated across 5 models initialized with different random seeds. AMP exhibits poor tracking performance, since the policies are trained using a general distribution-matching objective. However, qualitatively AMP can still be effective at reproducing the general behaviors of a reference motion, despite not precisely tracking the motion clip. Motion tracking methods, such as DeepMimic and ADD are able to accurately track a wide variety of reference motions. However, there are important distinctions in the consistency of the results across training runs. Since 12 • Xue Bin Peng Fig. 5. Learning curves comparing the tracking performance with the simulated humanoid character trained with DeepMimic, AMP, and ADD. Five training runs initialized with different random seeds are shown for each method. In order to better compare methods under similar settings, policies are trained without pose-error termination. The standard configuration for tracking-based methods, such as DeepMimic and ADD, utilizes pose-error termination, which tends to produce better performance and more consistent results across training runs. DeepMimic relies on a manually-designed reward function, it can be difficult to craft a general reward function that can effectively and consistently imitate a diverse", "source": "arxiv_pdf", "published": "", "tokens": 512, "sha256": "bfc4e0926db664064395625e6bef12cb6675befe03ba17c1b97ad29c1617100b"}
{"doc_id": "arxiv:2510.13794#experiments:part-3", "url": "https://arxiv.org/abs/2510.13794", "anchor": "#experiments:part-3", "type": "paper", "title": "", "section": "EXPERIMENTS", "text": "Five training runs initialized with different random seeds are shown for each method. In order to better compare methods under similar settings, policies are trained without pose-error termination. The standard configuration for tracking-based methods, such as DeepMimic and ADD, utilizes pose-error termination, which tends to produce better performance and more consistent results across training runs. DeepMimic relies on a manually-designed reward function, it can be difficult to craft a general reward function that can effectively and consistently imitate a diverse variety of behaviors, in the absence of additional heuristics such as pose error termination. In contrast, ADD leverages a differential discriminator to automatically learn an adaptive reward function, which can lead to more consistent performance across diverse motions. However, we would like to note that when pose error termination is enabled during training, tracking accuracy and consistency across training runs generally improve substantially. Therefore, the default configuration for tracking-based methods generally incorporate pose error termination during training. 8", "source": "arxiv_pdf", "published": "", "tokens": 158, "sha256": "0df580351de41ebff3a51002d74950b3acf4a8d3617f2276c9cc9a376fbe215f"}
{"doc_id": "arxiv:2510.13794#conclusion", "url": "https://arxiv.org/abs/2510.13794", "anchor": "#conclusion", "type": "paper", "title": "", "section": "CONCLUSION", "text": "In this work, we introduced MimicKit, an open-source reinforcement learning framework for motion imitation and control. MimicKit a unifies a suite of motion imitation methods for training motion controllers within a modular and extensible framework. We hope MimicKit will facilitate reproducible research in motor skill learning, and provide a convenient platform to accelerate progress in learning-based methods for motion control. ACKNOWLEDGMENTS We would like to thank all of the collaborators who have made valuable contributions to the development of MimicKit (ordered alphabetically): Yuxuan Mu, Yi Shi, Michael Xu, Dun Yang, and Ziyu Zhang. This work MimicKit: A Reinforcement Learning Framework for Motion Imitation and Control • 13 was supported by NSERC (RGPIN-2015-04843), the National Research Council Canada (AI4D-166), and Sony Interactive Entertainment.", "source": "arxiv_pdf", "published": "", "tokens": 122, "sha256": "f4a2bd28d085b4df2e0d0e72f155404dfd0b8be59b7cac471a205b890d005639"}
