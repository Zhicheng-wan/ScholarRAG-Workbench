{"doc_id": "arxiv:2404.10630", "chunk_id": "model", "text": "Sizes Sequence length OpenLLaMA1 7B, 13B 2048 OpenLLaMA2 7B 2048 LLaMA1 7B, 13B, 33B, 65B 2048 LLaMA2 7B, 13B, 70B 4096 HLAT 7B, 70B 4096 Evaluation Tasks: We evaluate HLAT against baselines on 7 groups of tasks including both zero-shot and few-shot tasks [36]. We use HumanEval [37] for coding tasks, and Language Model Evaluation Harness [38] for others. Massive Multitask Language Understanding (MMLU) [15], [39] contains 57 tasks, spanning STEM, social sciences, humanities, and other subjects. The difficulty ranges from elementary to professional levels. The breadth of the dataset tests model’s overall problem solving and knowledge ability. Commonsense Reasoning (CR) consists of 6 datasets: PIQA [40], HellaSwag [41], WinoGrande [42], ARC easy and challenge [43], and OpenBookQA [29]. These multi- choice tasks include carefully crafted riddles, puzzles, and scenarios designed to probe a model’s ability to leverage implicit knowledge, make logical inferences, and navigate the rules of physical and social worlds. World Knowledge (WK) includes NaturalQuestions [44] and TriviaQA [45]. Both tasks are designed to test model’s question-answering ability in closed book setting. The models are not provided documents that may contain information about the question, and it has to rely on information learnt or memorized in pre-training data. Reading Comprehension (RC) uses BoolQ [46] to test model’s open book comprehension ability. BoolQ is a question answering dataset for yes/no questions. Each example is a triplet of (question, passage, answer), with the title of the page as optional additional context. The model is required to answer the question based on the given context in passage. Math ability is evaluated with GSM8K (Grade School Math 8K) [47]. GSM8K contains 8,500 grade school math problems. Both problems and answers are provided in natural language. These problems take between 2 and 8 steps to solve, which is ideal for testing basic multi-step reasoning ability. Code evaluation uses HumanEval [37] dataset including 164 programming problems with a function signature, docstring, body, and several unit tests. They were handwritten to ensure not present in the training set of the models. A. Performance against open-source Models We compare the performance of HLAT with other open- source benchmarks in Table II. The numbers are reported in percentage and for HLAT results, we include both mean and TABLE II: Evaluation of HLAT against 4 open-source models on 6 groups of tasks described in Section V. Numbers in the parentheses represent standard deviation, if available.", "tokens": 516, "chunk_type": "original", "url": "https://arxiv.org/abs/2404.10630", "anchor": "#model", "type": "paper", "title": "", "section": "Model", "source": "arxiv_pdf", "published": "", "sha256": "1f7a04ae12cdbefc36365643cd475c081a310e246a966cd9d7d68b198939e1e1"}
{"doc_id": "arxiv:2404.10630", "chunk_id": "model:part-1", "text": "Size MMLU CR WK RC Math Code Average - - accuracy accuracy exact match accuracy accuracy pass@1 pass@10 - OpenLLaMA-1 7B 30.5 58.4 40.6 70.5 5.2 4.5 13.4 41.2 OpenLLaMA-2 7B 41.1 61.3 37.9 72.4 6.8 9.7 25 44.9 LLaMA-1 7B 35.1 63.5 43.6 76.5 11 10.5 21.3 47.4 LLaMA-2 7B 45.3 64 45.2 77.4 14.6 12.2 25.2 49.2 HLAT-7B 7B 41.3 (3.6) 59.5 (1.2) 38.8 (0.5) 72.5 (0.8) 9.4 (0.8) 7.6 19.8 44.6 OpenLLaMA-1 13B 43.5 62 45.9 72.3 8.3 7 17 47.1 LLaMA-1 13B 46.9 65.3 49.7 78.1 17.8 15.8 22.6 53.1 LLaMA-2 13B 45.3 66.3 50.5 81.7 28.7 18.3 30.5 54.6 LLaMA-1 33B 57.8 68.9 54.6 83.1 35.6 21.7 38.4 59.2 LLaMA-1 65B 63.4 69.8 57 85.3 50.9 23.7 - 62.1 LLaMA-2 70B 68.9 70.7 59 85 56.8 30.5 59.4 64.7 HLAT-70B 70B 65.1 (3.4) 67.3 (1.2) 54.5 (0.6) 82.6 (0.7) 48.5 (1.4) 21.4 57.9 60.8 standard deviation (in the parentheses, if available). We also report an average score over all tasks in the last column. HLAT-7B performs better than OpenLLaMA-1 and is on- par with OpenLLaMA-2. Both HLAT-7B and OpenLLaMA models have some gap with LLaMA-1 and LLaMA-2, which is likely due to the training data quality. Even though the data composition of RedPajama-1T is similar as those used in LLaMA-1, the data cleaning pipeline and final data quality are different, which therefore affects the model performance [48]. For HLAT-70B, we use the same training dataset as the 7B model for consistency. Although there is no OpenLLaMA baseline for a fair comparison, HLAT-70B performs better than LLaMA-1 and LLaMA-2 models of smaller sizes. The model performance gap with LLaMA-1 (65B) and LLaMA-2 (70B) is also smaller than those on 7B models. We acknowl- edge the lack of effort on data quality improvement, but our main goal is to showcase the effectiveness and efficiency of AWS TRAINIUM. On MMLU (5-shot), both HLAT models perform better than OpenLLaMA-1 and LLaMA-1 models of similar size. The performance is slightly worse than LLaMA-2 family of models, likely due to the difference in training dataset size and composition [8]. On Commonsense Reasoning (0-shot) and World Knowl- edge (5-shot), HLAT-7B performs similar to OpenLLaMA-1 and OpenLLaMA-2 models. By diving deep into performance on each individual task, HLAT-7B excels in 19/29 tasks as compared with OpenLLaMA-1, and 15/29 tasks compared with OpenLLaMA-2. Both HLAT and OpenLLaMA models have some gaps with LLaMA-1 and LLaMA-2 models, which may be due to the training set quality. Nevertheless, the gap (∼3%) is consistent on 7B and 70B models. On Math problems (GSM8K, 8-shot), HLAT-7B performs significantly better than OpenLLaMA-1 and OpenLLaMA-2. As will be discussed in the next section, HLAT-70B has a big improvement of Math ability in later training stage. HLAT- 70B performs similar as LLaMA1-65B, and we observed significant improvement in upsampling training stage. On Coding problems, both HLAT-7B and HLAT-70B perform comparable with LLaMA-1. HLAT-7B performs better than OpenLLaMA-1 and worse than OpenLLaMA- 2 and LLaMA-2. First, for OpenLLaMA-1, the tokenizer merges consecutive spaces which negatively affects the coding performance, as it eliminates important information", "tokens": 665, "chunk_type": "original-large", "url": "https://arxiv.org/abs/2404.10630", "anchor": "#model:part-1", "type": "paper", "title": "", "section": "Model", "source": "arxiv_pdf", "published": "", "sha256": "a7552a7c57daa5102b6e9cb04cf7a777ce27d4c763396756430de988a1e565d1"}
{"doc_id": "arxiv:2404.10630", "chunk_id": "model:part-1-o1", "text": "Size MMLU CR WK RC Math Code Average - - accuracy accuracy exact match accuracy accuracy pass@1 pass@10 - OpenLLaMA-1 7B 30.5 58.4 40.6 70.5 5.2 4.5 13.4 41.2 OpenLLaMA-2 7B 41.1 61.3 37.9 72.4 6.8 9.7 25 44.9 LLaMA-1 7B 35.1 63.5 43.6 76.5 11 10.5 21.3 47.4 LLaMA-2 7B 45.3 64 45.2 77.4 14.6 12.2 25.2 49.2 HLAT-7B 7B 41.3 (3.6) 59.5 (1.2) 38.8 (0.5) 72.5 (0.8) 9.4 (0.8) 7.6 19.8 44.6 OpenLLaMA-1 13B 43.5 62 45.9 72.3 8.3 7 17 47.1 LLaMA-1 13B 46.9 65.3 49.7 78.1 17.8 15.8 22.6 53.1 LLaMA-2 13B 45.3 66.3 50.5 81.7 28.7 18.3 30.5 54.6 LLaMA-1 33B 57.8 68.9 54.6 83.1 35.6 21.7 38.4 59.2 LLaMA-1 65B 63.4 69.8 57 85.3 50.9 23.7 - 62.1 LLaMA-2 70B 68.9 70.7 59 85 56.8 30.5 59.4 64.7 HLAT-70B 70B 65.1 (3.4) 67.3 (1.2) 54.5 (0.6) 82.6 (0.7) 48.5 (1.4) 21.4 57.9 60.8 standard deviation (in the parentheses, if available). We also report an average score over all tasks in the last column. HLAT-7B performs better than OpenLLaMA-1 and is on- par with OpenLLaMA-2. Both HLAT-7B and OpenLLaMA models have some gap with LLaMA-1 and LLaMA-2, which is likely due to the training data quality. Even though the data composition of RedPajama-1T is similar as those used in LLaMA-1, the data cleaning pipeline and final data quality are different, which therefore affects the model performance [48]. For HLAT-70B, we use the same training dataset as the 7B model for consistency. Although there is no OpenLLaMA baseline for a fair comparison, HLAT-70B performs better than LLaMA-1 and LLaMA-2 models of smaller sizes. The model performance gap with LLaMA-1 (65B) and LLaMA-2 (70B) is also smaller than those on 7B models. We acknowl- edge the lack of effort on data quality improvement, but our main goal is to showcase the effectiveness and efficiency of AWS TRAINIUM.", "tokens": 403, "chunk_type": "overlap-1", "url": "https://arxiv.org/abs/2404.10630", "anchor": "#model:part-1", "type": "paper", "title": "", "section": "Model", "source": "arxiv_pdf", "published": "", "sha256": "a7552a7c57daa5102b6e9cb04cf7a777ce27d4c763396756430de988a1e565d1"}
{"doc_id": "arxiv:2404.10630", "chunk_id": "model:part-1-o2", "text": "Although there is no OpenLLaMA baseline for a fair comparison, HLAT-70B performs better than LLaMA-1 and LLaMA-2 models of smaller sizes. The model performance gap with LLaMA-1 (65B) and LLaMA-2 (70B) is also smaller than those on 7B models. We acknowl- edge the lack of effort on data quality improvement, but our main goal is to showcase the effectiveness and efficiency of AWS TRAINIUM. On MMLU (5-shot), both HLAT models perform better than OpenLLaMA-1 and LLaMA-1 models of similar size. The performance is slightly worse than LLaMA-2 family of models, likely due to the difference in training dataset size and composition [8]. On Commonsense Reasoning (0-shot) and World Knowl- edge (5-shot), HLAT-7B performs similar to OpenLLaMA-1 and OpenLLaMA-2 models. By diving deep into performance on each individual task, HLAT-7B excels in 19/29 tasks as compared with OpenLLaMA-1, and 15/29 tasks compared with OpenLLaMA-2. Both HLAT and OpenLLaMA models have some gaps with LLaMA-1 and LLaMA-2 models, which may be due to the training set quality. Nevertheless, the gap (∼3%) is consistent on 7B and 70B models. On Math problems (GSM8K, 8-shot), HLAT-7B performs significantly better than OpenLLaMA-1 and OpenLLaMA-2. As will be discussed in the next section, HLAT-70B has a big improvement of Math ability in later training stage. HLAT- 70B performs similar as LLaMA1-65B, and we observed significant improvement in upsampling training stage. On Coding problems, both HLAT-7B and HLAT-70B perform comparable with LLaMA-1. HLAT-7B performs better than OpenLLaMA-1 and worse than OpenLLaMA- 2 and LLaMA-2. First, for OpenLLaMA-1, the tokenizer merges consecutive spaces which negatively affects the coding performance, as it eliminates important information", "tokens": 345, "chunk_type": "overlap-2", "url": "https://arxiv.org/abs/2404.10630", "anchor": "#model:part-1", "type": "paper", "title": "", "section": "Model", "source": "arxiv_pdf", "published": "", "sha256": "a7552a7c57daa5102b6e9cb04cf7a777ce27d4c763396756430de988a1e565d1"}
{"doc_id": "arxiv:2404.10630", "chunk_id": "model:part-2", "text": "OpenLLaMA-1 and OpenLLaMA-2. As will be discussed in the next section, HLAT-70B has a big improvement of Math ability in later training stage. HLAT- 70B performs similar as LLaMA1-65B, and we observed significant improvement in upsampling training stage. On Coding problems, both HLAT-7B and HLAT-70B perform comparable with LLaMA-1. HLAT-7B performs better than OpenLLaMA-1 and worse than OpenLLaMA- 2 and LLaMA-2. First, for OpenLLaMA-1, the tokenizer merges consecutive spaces which negatively affects the coding performance, as it eliminates important information such as indentation and line breaks. This issue is subsequently fixed in OpenLLaMA-2, which explains its better performance. Besides, OpenLLaMA-2 is trained with additional code data from StarCoder which also contributes to performance im- provement. B. Intermediate Model Performance During the model training, we also evaluate the intermediate checkpoints about every 200 billion tokens. Figure 3 and Figure 4 show the model performance of HLAT-7B and HLAT-70B with respect to number of seen training tokens (in billions), respectively. On most benchmarks, the performance improves steadily, and correlates with the training loss. We found that for different tasks, the model converges at different rates. For Commonsense Reasoning, the model accuracy improves quickly at beginning of training, and starts to saturate at later training stages. This is similar as the trends observed in other LLM model trainings [8], [49]. However, for Math task (GSM8K) shown in Figure 3e, the learning curve shows an exponentially increasing trend. It increase very gradually for the initial ∼1 trillion tokens and begins to improve significantly during the later stages of training. Intuitively, this seems to indicate that the model is able to grasp more logical abilities after entering a relatively stable training plateau. We defer further research into this behavior as a future work. For World Knowledge task shown in Figure 3c, the per- formance increases almost linearly with number of training tokens. Since this is a closed book test and mainly evaluates the model’s ability of memorizing facts in pre-training data, the model seems to consistently improve its ability on this domain with more training steps and epochs. In addition, we tested if the trending is related to number of shots used in evaluation. It turns out that the trends are very similar for zero-shot, 3-shot, and 5-shot tests. 250 500 750 1000 1250 1500 1750 Tokens (in Billions) 25 30 35 40 45 Accuracy (norm) MMLU (a) 250 500 750 1000 1250 1500 1750 Tokens (in Billions) 50 52 54 56 58 60 Accuracy (norm) Commonsense Reasoning (b) 250 500 750 1000 1250 1500 1750 Tokens (in Billions) 5 10 15 20 25 30 35 40 Exact match World Knowledge 0 shots 3 shots 5 shots (c) 250 500 750 1000 1250 1500 1750 Tokens (in Billions) 55.0 57.5 60.0 62.5 65.0 67.5 70.0 72.5 Accuracy BoolQ (d) 250 500 750 1000 1250 1500 1750 Tokens (in Billions) 2 4 6 8 10 Accuracy GSM8K (e) 250 500 750 1000 1250 1500 1750 Tokens (in Billions) 5 10 15 20 25 30 35 Percentage Code pass@1 pass@10 pass@100 (f) Fig. 3: Intermediate model performance with number", "tokens": 665, "chunk_type": "original-large", "url": "https://arxiv.org/abs/2404.10630", "anchor": "#model:part-2", "type": "paper", "title": "", "section": "Model", "source": "arxiv_pdf", "published": "", "sha256": "e73cb464adeaf1fc0a25a88e8ec0a68dbd5f59bd15a217af33fa8f0378a3727b"}
{"doc_id": "arxiv:2404.10630", "chunk_id": "model:part-2-o1", "text": "OpenLLaMA-1 and OpenLLaMA-2. As will be discussed in the next section, HLAT-70B has a big improvement of Math ability in later training stage. HLAT- 70B performs similar as LLaMA1-65B, and we observed significant improvement in upsampling training stage. On Coding problems, both HLAT-7B and HLAT-70B perform comparable with LLaMA-1. HLAT-7B performs better than OpenLLaMA-1 and worse than OpenLLaMA- 2 and LLaMA-2. First, for OpenLLaMA-1, the tokenizer merges consecutive spaces which negatively affects the coding performance, as it eliminates important information such as indentation and line breaks. This issue is subsequently fixed in OpenLLaMA-2, which explains its better performance. Besides, OpenLLaMA-2 is trained with additional code data from StarCoder which also contributes to performance im- provement. B. Intermediate Model Performance During the model training, we also evaluate the intermediate checkpoints about every 200 billion tokens. Figure 3 and Figure 4 show the model performance of HLAT-7B and HLAT-70B with respect to number of seen training tokens (in billions), respectively. On most benchmarks, the performance improves steadily, and correlates with the training loss. We found that for different tasks, the model converges at different rates. For Commonsense Reasoning, the model accuracy improves quickly at beginning of training, and starts to saturate at later training stages. This is similar as the trends observed in other LLM model trainings [8], [49]. However, for Math task (GSM8K) shown in Figure 3e, the learning curve shows an exponentially increasing trend. It increase very gradually for the initial ∼1 trillion tokens and begins to improve significantly during the later stages of training. Intuitively, this seems to indicate that the model is able to grasp more logical abilities after entering a relatively stable training plateau. We defer further research into this behavior as a future work. For World Knowledge task shown in Figure 3c, the per- formance increases almost linearly with number of training tokens.", "tokens": 399, "chunk_type": "overlap-1", "url": "https://arxiv.org/abs/2404.10630", "anchor": "#model:part-2", "type": "paper", "title": "", "section": "Model", "source": "arxiv_pdf", "published": "", "sha256": "e73cb464adeaf1fc0a25a88e8ec0a68dbd5f59bd15a217af33fa8f0378a3727b"}
{"doc_id": "arxiv:2404.10630", "chunk_id": "model:part-2-o2", "text": "It increase very gradually for the initial ∼1 trillion tokens and begins to improve significantly during the later stages of training. Intuitively, this seems to indicate that the model is able to grasp more logical abilities after entering a relatively stable training plateau. We defer further research into this behavior as a future work. For World Knowledge task shown in Figure 3c, the per- formance increases almost linearly with number of training tokens. Since this is a closed book test and mainly evaluates the model’s ability of memorizing facts in pre-training data, the model seems to consistently improve its ability on this domain with more training steps and epochs. In addition, we tested if the trending is related to number of shots used in evaluation. It turns out that the trends are very similar for zero-shot, 3-shot, and 5-shot tests. 250 500 750 1000 1250 1500 1750 Tokens (in Billions) 25 30 35 40 45 Accuracy (norm) MMLU (a) 250 500 750 1000 1250 1500 1750 Tokens (in Billions) 50 52 54 56 58 60 Accuracy (norm) Commonsense Reasoning (b) 250 500 750 1000 1250 1500 1750 Tokens (in Billions) 5 10 15 20 25 30 35 40 Exact match World Knowledge 0 shots 3 shots 5 shots (c) 250 500 750 1000 1250 1500 1750 Tokens (in Billions) 55.0 57.5 60.0 62.5 65.0 67.5 70.0 72.5 Accuracy BoolQ (d) 250 500 750 1000 1250 1500 1750 Tokens (in Billions) 2 4 6 8 10 Accuracy GSM8K (e) 250 500 750 1000 1250 1500 1750 Tokens (in Billions) 5 10 15 20 25 30 35 Percentage Code pass@1 pass@10 pass@100 (f) Fig. 3: Intermediate model performance with number", "tokens": 361, "chunk_type": "overlap-2", "url": "https://arxiv.org/abs/2404.10630", "anchor": "#model:part-2", "type": "paper", "title": "", "section": "Model", "source": "arxiv_pdf", "published": "", "sha256": "e73cb464adeaf1fc0a25a88e8ec0a68dbd5f59bd15a217af33fa8f0378a3727b"}
{"doc_id": "arxiv:2404.10630", "chunk_id": "model:part-3", "text": "Exact match World Knowledge 0 shots 3 shots 5 shots (c) 250 500 750 1000 1250 1500 1750 Tokens (in Billions) 55.0 57.5 60.0 62.5 65.0 67.5 70.0 72.5 Accuracy BoolQ (d) 250 500 750 1000 1250 1500 1750 Tokens (in Billions) 2 4 6 8 10 Accuracy GSM8K (e) 250 500 750 1000 1250 1500 1750 Tokens (in Billions) 5 10 15 20 25 30 35 Percentage Code pass@1 pass@10 pass@100 (f) Fig. 3: Intermediate model performance with number of seen tokens for HLAT-7B. 250 500 750 1000 1250 1500 1750 Tokens (in Billions) 30 40 50 60 70 Accuracy MMLU (a) 250 500 750 1000 1250 1500 1750 Tokens (in Billions) 54 56 58 60 62 64 66 68 Accuracy (norm) Commonsense Reasoning (b) 250 500 750 1000 1250 1500 1750 Tokens (in Billions) 35 40 45 50 55 Exact match World Knowledge (c) 250 500 750 1000 1250 1500 1750 Tokens (in Billions) 65 70 75 80 85 Accuracy BoolQ (d) 250 500 750 1000 1250 1500 1750 Tokens (in Billions) 0 10 20 30 40 50 Exact match GSM8K (e) 200 400 600 800 1000 1200 1400 1600 1800 Tokens (in Billions) 10 20 30 40 50 Percentage Code pass@1 pass@10 (f) Fig. 4: Intermediate model performance with number of seen tokens for HLAT-70B. Those observations indicate the necessity of a set of eval- uation tasks covering a wide range of domains for LLM pre-training. A single validation set or evaluation tasks from narrow domains may not fully reflect the actual over- or under- fitting of the model for general downstream performance. C. Upsampling During HLAT-70B training, we upsampled the training dataset in last 400B tokens. Specifically, we use 35.47% web data, 41.27% math data, and 23.26% coding data with more details listed in Table III. In Figure 4, we plot the evalua- tion performance of HLAT-70B with seen training tokens. In upsampling training stage, that is, after 1400B tokens, TABLE III: Upsampling dataset composition for HLAT-70B. Datasets Size Percentage (billions of tokens) Web Data Wikipedia [21] 90 35.47% C4 [21] Domain Specific StackExchange 104.7 41.27% Arxiv [21] Open-Web-Math [23] PeS2o [22] Code Github [21] 59 23.26% Total - 253.7 15.16% we observe significant model performance improvement over math, coding, and MMLU performance. It improved math by 10% and coding by 5%. This is consistent with the findings in LLaMA-3 [10], where the researchers found significant improvement of LLaMA-3 8B model on math problems. However, they mentioned such method did not help much for 405B models. Our experiment fills the model size gap, and shows that upsampling still helps for a 70B model. D. Checkpoint Averaging For HLAT-70B, we average the last two checkpoints used in pre-training to generate a checkpoint for final evaluation [10], [33]. Table IV compares the individual checkpoints with 1740B training tokens and 1800B training tokens, as well as the averaged checkpoints. The averaged checkpoints outperforms individual checkpoints on average performance, as well as some individual tasks. TABLE IV: Evaluation of HLAT-70B with individual and averaged checkpoints. Checkpoint MMLU RC WK CR Math Code", "tokens": 665, "chunk_type": "original-large", "url": "https://arxiv.org/abs/2404.10630", "anchor": "#model:part-3", "type": "paper", "title": "", "section": "Model", "source": "arxiv_pdf", "published": "", "sha256": "a8bb281e58c90887129de05b676493b851f3bec063df775da898963a0c5d5a78"}
{"doc_id": "arxiv:2404.10630", "chunk_id": "model:part-3-o1", "text": "Exact match World Knowledge 0 shots 3 shots 5 shots (c) 250 500 750 1000 1250 1500 1750 Tokens (in Billions) 55.0 57.5 60.0 62.5 65.0 67.5 70.0 72.5 Accuracy BoolQ (d) 250 500 750 1000 1250 1500 1750 Tokens (in Billions) 2 4 6 8 10 Accuracy GSM8K (e) 250 500 750 1000 1250 1500 1750 Tokens (in Billions) 5 10 15 20 25 30 35 Percentage Code pass@1 pass@10 pass@100 (f) Fig. 3: Intermediate model performance with number of seen tokens for HLAT-7B. 250 500 750 1000 1250 1500 1750 Tokens (in Billions) 30 40 50 60 70 Accuracy MMLU (a) 250 500 750 1000 1250 1500 1750 Tokens (in Billions) 54 56 58 60 62 64 66 68 Accuracy (norm) Commonsense Reasoning (b) 250 500 750 1000 1250 1500 1750 Tokens (in Billions) 35 40 45 50 55 Exact match World Knowledge (c) 250 500 750 1000 1250 1500 1750 Tokens (in Billions) 65 70 75 80 85 Accuracy BoolQ (d) 250 500 750 1000 1250 1500 1750 Tokens (in Billions) 0 10 20 30 40 50 Exact match GSM8K (e) 200 400 600 800 1000 1200 1400 1600 1800 Tokens (in Billions) 10 20 30 40 50 Percentage Code pass@1 pass@10 (f) Fig. 4: Intermediate model performance with number of seen tokens for HLAT-70B. Those observations indicate the necessity of a set of eval- uation tasks covering a wide range of domains for LLM pre-training. A single validation set or evaluation tasks from narrow domains may not fully reflect the actual over- or under- fitting of the model for general downstream performance. C. Upsampling During HLAT-70B training, we upsampled the training dataset in last 400B tokens. Specifically, we use 35.47% web data, 41.27% math data, and 23.26% coding data with more details listed in Table III.", "tokens": 390, "chunk_type": "overlap-1", "url": "https://arxiv.org/abs/2404.10630", "anchor": "#model:part-3", "type": "paper", "title": "", "section": "Model", "source": "arxiv_pdf", "published": "", "sha256": "a8bb281e58c90887129de05b676493b851f3bec063df775da898963a0c5d5a78"}
{"doc_id": "arxiv:2404.10630", "chunk_id": "model:part-3-o2", "text": "A single validation set or evaluation tasks from narrow domains may not fully reflect the actual over- or under- fitting of the model for general downstream performance. C. Upsampling During HLAT-70B training, we upsampled the training dataset in last 400B tokens. Specifically, we use 35.47% web data, 41.27% math data, and 23.26% coding data with more details listed in Table III. In Figure 4, we plot the evalua- tion performance of HLAT-70B with seen training tokens. In upsampling training stage, that is, after 1400B tokens, TABLE III: Upsampling dataset composition for HLAT-70B. Datasets Size Percentage (billions of tokens) Web Data Wikipedia [21] 90 35.47% C4 [21] Domain Specific StackExchange 104.7 41.27% Arxiv [21] Open-Web-Math [23] PeS2o [22] Code Github [21] 59 23.26% Total - 253.7 15.16% we observe significant model performance improvement over math, coding, and MMLU performance. It improved math by 10% and coding by 5%. This is consistent with the findings in LLaMA-3 [10], where the researchers found significant improvement of LLaMA-3 8B model on math problems. However, they mentioned such method did not help much for 405B models. Our experiment fills the model size gap, and shows that upsampling still helps for a 70B model. D. Checkpoint Averaging For HLAT-70B, we average the last two checkpoints used in pre-training to generate a checkpoint for final evaluation [10], [33]. Table IV compares the individual checkpoints with 1740B training tokens and 1800B training tokens, as well as the averaged checkpoints. The averaged checkpoints outperforms individual checkpoints on average performance, as well as some individual tasks. TABLE IV: Evaluation of HLAT-70B with individual and averaged checkpoints. Checkpoint MMLU RC WK CR Math Code", "tokens": 354, "chunk_type": "overlap-2", "url": "https://arxiv.org/abs/2404.10630", "anchor": "#model:part-3", "type": "paper", "title": "", "section": "Model", "source": "arxiv_pdf", "published": "", "sha256": "a8bb281e58c90887129de05b676493b851f3bec063df775da898963a0c5d5a78"}
{"doc_id": "arxiv:2404.10630", "chunk_id": "model:part-4", "text": "helps for a 70B model. D. Checkpoint Averaging For HLAT-70B, we average the last two checkpoints used in pre-training to generate a checkpoint for final evaluation [10], [33]. Table IV compares the individual checkpoints with 1740B training tokens and 1800B training tokens, as well as the averaged checkpoints. The averaged checkpoints outperforms individual checkpoints on average performance, as well as some individual tasks. TABLE IV: Evaluation of HLAT-70B with individual and averaged checkpoints. Checkpoint MMLU RC WK CR Math Code Avg. 1740B 64.5 67.5 54 83.1 47.3 18.3 60.3 1800B 64.2 66.9 54 82.1 47.2 21.8 60.2 Average 65.1 67.3 54.5 82.6 48.5 21.4 60.8 E. Truthfulness and Bias We report the model’s truthfulness and bias using Truth- fulQA [50] and CrowS-pairs [51]. TruthfulQA presents a collection of meticulously crafted questions spanning diverse domains such as health, law, finance, and even politics. These queries deliberately target areas where human intuition and personal biases can lead to incorrect responses, and measure an LLM’s resistance to misinformed or erroneous knowledge. CrowS-Pairs is a benchmark designed to probe LLMs for social biases across nine categories, including gender, reli- gion, race/color, sexual orientation, age, nationality, disability, physical appearance and socioeconomic status. Each example is composed of a stereotype and an anti-stereotype. TABLE V: Model Truthfulness and Bias evaluation. CrowS- pairs (CSP) uses percentage of stereotypes as metric and TruthfulQA (TQA) uses multiple choice accuracy as metric. Dataset Size CSP (↓) CSP TQA (↑) TQA Tasks - english french mc1 mc2 OpenLLaMA-1 7B 64.6 50.1 23.1 35.1 OpenLLaMA-2 7B 65.6 51.7 22.6 34.6 LLaMA-1 7B 53.7 47.5 22.0 34.1 LLaMA-2 7B 66.9 54.9 25.2 39.0 HLAT-7B 7B 65.2 54.5 23.6 37.2 LLaMA-1 65B 69.3 58.3 27.9 42.6 LLaMA-2 70B 69.8 63.5 30.6 44.8 HLAT-70B 70B 68.1 59.1 32.3 45.9 We present the results in Table V with 0 shot inference. For TruthfulQA, we measure the multiple-choice score, and higher score shows better truthfulness. For CrowS-Pairs, it measures the percentage of models choosing answers of stereotypes, so lower scores indicates smaller bias. Overall, HLAT performs similar to other open-source models. F. Efficiency and Scalability We describe the training efficiency in terms of Cost per 4-million tokens (CPT) and scalability reported in [52]. The CPT is defined as CPT = C T ×3600 × 4e6, where C is instance cost per hour ($21.50 for Trainium, and $32.77 for GPU), T is training throughput (tokens per second). CPT quantifies both the training speed and also hardware cost. We use this metric to compare training efficiency of Trainium and GPU. 4 8 16 32 64 Number of nodes 0 2 4 6 8 10 12 Normalized CPT trn1, 7B p4d, 7B trn1, 70B p4d, 70B Fig. 5: Normalized cost per 4 million tokens (CPT) for 7B and 70B models on AWS TRAINIUM with various number of nodes. CPT of GPU baseline (p4d, 7B) with 4 nodes is normalized to 100%. 70B models ran into out-of-memory on 4 nodes. For comparison, the GPU baseline is established using p4d.24xlarge instances and NeMo 23.08 [25] (available inside NeMo docker container with tag 23.08) software", "tokens": 665, "chunk_type": "original-large", "url": "https://arxiv.org/abs/2404.10630", "anchor": "#model:part-4", "type": "paper", "title": "", "section": "Model", "source": "arxiv_pdf", "published": "", "sha256": "16290b00b94467bce4ef9dc5eb01a8d0c4f47501d979f21cfe631896e70c702d"}
{"doc_id": "arxiv:2404.10630", "chunk_id": "model:part-4-o1", "text": "helps for a 70B model. D. Checkpoint Averaging For HLAT-70B, we average the last two checkpoints used in pre-training to generate a checkpoint for final evaluation [10], [33]. Table IV compares the individual checkpoints with 1740B training tokens and 1800B training tokens, as well as the averaged checkpoints. The averaged checkpoints outperforms individual checkpoints on average performance, as well as some individual tasks. TABLE IV: Evaluation of HLAT-70B with individual and averaged checkpoints. Checkpoint MMLU RC WK CR Math Code Avg. 1740B 64.5 67.5 54 83.1 47.3 18.3 60.3 1800B 64.2 66.9 54 82.1 47.2 21.8 60.2 Average 65.1 67.3 54.5 82.6 48.5 21.4 60.8 E. Truthfulness and Bias We report the model’s truthfulness and bias using Truth- fulQA [50] and CrowS-pairs [51]. TruthfulQA presents a collection of meticulously crafted questions spanning diverse domains such as health, law, finance, and even politics. These queries deliberately target areas where human intuition and personal biases can lead to incorrect responses, and measure an LLM’s resistance to misinformed or erroneous knowledge. CrowS-Pairs is a benchmark designed to probe LLMs for social biases across nine categories, including gender, reli- gion, race/color, sexual orientation, age, nationality, disability, physical appearance and socioeconomic status. Each example is composed of a stereotype and an anti-stereotype. TABLE V: Model Truthfulness and Bias evaluation. CrowS- pairs (CSP) uses percentage of stereotypes as metric and TruthfulQA (TQA) uses multiple choice accuracy as metric. Dataset Size CSP (↓) CSP TQA (↑) TQA Tasks - english french mc1 mc2 OpenLLaMA-1 7B 64.6 50.1 23.1 35.1 OpenLLaMA-2 7B 65.6 51.7 22.6 34.6 LLaMA-1 7B 53.7 47.5 22.0 34.1 LLaMA-2 7B 66.9 54.9 25.2 39.0 HLAT-7B 7B 65.2 54.5 23.6 37.2 LLaMA-1 65B 69.3 58.3 27.9 42.6 LLaMA-2 70B 69.8 63.5 30.6 44.8 HLAT-70B 70B 68.1 59.1 32.3 45.9 We present the results in Table V with 0 shot inference.", "tokens": 396, "chunk_type": "overlap-1", "url": "https://arxiv.org/abs/2404.10630", "anchor": "#model:part-4", "type": "paper", "title": "", "section": "Model", "source": "arxiv_pdf", "published": "", "sha256": "16290b00b94467bce4ef9dc5eb01a8d0c4f47501d979f21cfe631896e70c702d"}
{"doc_id": "arxiv:2404.10630", "chunk_id": "model:part-4-o2", "text": "Dataset Size CSP (↓) CSP TQA (↑) TQA Tasks - english french mc1 mc2 OpenLLaMA-1 7B 64.6 50.1 23.1 35.1 OpenLLaMA-2 7B 65.6 51.7 22.6 34.6 LLaMA-1 7B 53.7 47.5 22.0 34.1 LLaMA-2 7B 66.9 54.9 25.2 39.0 HLAT-7B 7B 65.2 54.5 23.6 37.2 LLaMA-1 65B 69.3 58.3 27.9 42.6 LLaMA-2 70B 69.8 63.5 30.6 44.8 HLAT-70B 70B 68.1 59.1 32.3 45.9 We present the results in Table V with 0 shot inference. For TruthfulQA, we measure the multiple-choice score, and higher score shows better truthfulness. For CrowS-Pairs, it measures the percentage of models choosing answers of stereotypes, so lower scores indicates smaller bias. Overall, HLAT performs similar to other open-source models. F. Efficiency and Scalability We describe the training efficiency in terms of Cost per 4-million tokens (CPT) and scalability reported in [52]. The CPT is defined as CPT = C T ×3600 × 4e6, where C is instance cost per hour ($21.50 for Trainium, and $32.77 for GPU), T is training throughput (tokens per second). CPT quantifies both the training speed and also hardware cost. We use this metric to compare training efficiency of Trainium and GPU. 4 8 16 32 64 Number of nodes 0 2 4 6 8 10 12 Normalized CPT trn1, 7B p4d, 7B trn1, 70B p4d, 70B Fig. 5: Normalized cost per 4 million tokens (CPT) for 7B and 70B models on AWS TRAINIUM with various number of nodes. CPT of GPU baseline (p4d, 7B) with 4 nodes is normalized to 100%. 70B models ran into out-of-memory on 4 nodes. For comparison, the GPU baseline is established using p4d.24xlarge instances and NeMo 23.08 [25] (available inside NeMo docker container with tag 23.08) software", "tokens": 364, "chunk_type": "overlap-2", "url": "https://arxiv.org/abs/2404.10630", "anchor": "#model:part-4", "type": "paper", "title": "", "section": "Model", "source": "arxiv_pdf", "published": "", "sha256": "16290b00b94467bce4ef9dc5eb01a8d0c4f47501d979f21cfe631896e70c702d"}
{"doc_id": "arxiv:2404.10630", "chunk_id": "model:part-5", "text": "6 8 10 12 Normalized CPT trn1, 7B p4d, 7B trn1, 70B p4d, 70B Fig. 5: Normalized cost per 4 million tokens (CPT) for 7B and 70B models on AWS TRAINIUM with various number of nodes. CPT of GPU baseline (p4d, 7B) with 4 nodes is normalized to 100%. 70B models ran into out-of-memory on 4 nodes. For comparison, the GPU baseline is established using p4d.24xlarge instances and NeMo 23.08 [25] (available inside NeMo docker container with tag 23.08) software stack. Fig- ure 5 plots the normalized CPT of training on TRAINIUM and scaling. The TRAINIUM CPT is normalized, such that the CPT of the GPU baseline (p4d, 7B) on 4 nodes is 100%. Overall, the training cost on trn1 is approximately 60% of GPU, and is consistent with the number of nodes. In addition, the CPTs on 70B models are roughly 10 times of those on 7B models. G. Model Limitation We note some limitations of HLAT in this section. Similar as other LLMs, HLAT suffers a set of limitations such as hallucinations, potential non-factual generations, biases, and toxicity [53]. For example, although comparable with other open-source pre-trained models, the bias of HLAT is still relative high on some subjects such as sexual orientation, physical appearance, religion, and socioeconomic (see Table V). This is partially due to the usage of publicly available datasets. More importantly, as a pre-trained model, HLAT has not gone through a supervised finetuning and human prefer- ence alignment. Those fine-tuning methods have been shown to be able to alleviate some limitations of pre-trained LLMs [9]. Another limitation is that our training is stopped after 1.8 trillion tokens. As is suggested by LLaMA-3 [10], HLAT may be able to further improve on certain tasks, such as math, world knowledge, MMLU, and coding, with more training tokens. VI. BEST PRACTICES & FUTURE DIRECTIONS In this section, we share some best practices we observed for training on AWS TRAINIUM, and raise open questions for future research. Parallelism: NxDT supports TP up to 32 degrees and pipeline parallelism. For a 7B model, we found that the combination of TP=8 and PP=1 provides the highest training throughput, but not for HLAT-70B. So the optimal parallelism configuration varies with model sizes and architectures. To achieve the highest training throughput, parallelism configu- ration needs to be jointly optimized with choice of activation checkpointing method, gradient accumulation steps, and train- ing precision, to balance memory and communication costs. Training Precision: NxDT supports various training pre- cision configurations, including full precision (FP32), BF16 with and without SR, standard mixed precision training, etc. Full precision training is often memory-wise infeasible for multi-billion LLMs. We compared multiple training strategies for HLAT: pure BF16, BF16 with SR and standard mixed precision training. Empirically, we found that training loss of pure BF16 diverges. BF16 with SR shows similar training loss as mixed precision on HLAT-7B model, but converges slower on HLAT-70B. We finally chose BF16 with SR for higher throughput on HLAT-7B, but standard mixed precision on HLAT-70B. For models of other sizes and architecture, warm-up study may be needed", "tokens": 665, "chunk_type": "original-large", "url": "https://arxiv.org/abs/2404.10630", "anchor": "#model:part-5", "type": "paper", "title": "", "section": "Model", "source": "arxiv_pdf", "published": "", "sha256": "2f5e409cfd33be307785bce29ac7af9f1d41871bbc242b54daeb78df32791f87"}
{"doc_id": "arxiv:2404.10630", "chunk_id": "model:part-5-o1", "text": "6 8 10 12 Normalized CPT trn1, 7B p4d, 7B trn1, 70B p4d, 70B Fig. 5: Normalized cost per 4 million tokens (CPT) for 7B and 70B models on AWS TRAINIUM with various number of nodes. CPT of GPU baseline (p4d, 7B) with 4 nodes is normalized to 100%. 70B models ran into out-of-memory on 4 nodes. For comparison, the GPU baseline is established using p4d.24xlarge instances and NeMo 23.08 [25] (available inside NeMo docker container with tag 23.08) software stack. Fig- ure 5 plots the normalized CPT of training on TRAINIUM and scaling. The TRAINIUM CPT is normalized, such that the CPT of the GPU baseline (p4d, 7B) on 4 nodes is 100%. Overall, the training cost on trn1 is approximately 60% of GPU, and is consistent with the number of nodes. In addition, the CPTs on 70B models are roughly 10 times of those on 7B models. G. Model Limitation We note some limitations of HLAT in this section. Similar as other LLMs, HLAT suffers a set of limitations such as hallucinations, potential non-factual generations, biases, and toxicity [53]. For example, although comparable with other open-source pre-trained models, the bias of HLAT is still relative high on some subjects such as sexual orientation, physical appearance, religion, and socioeconomic (see Table V). This is partially due to the usage of publicly available datasets. More importantly, as a pre-trained model, HLAT has not gone through a supervised finetuning and human prefer- ence alignment. Those fine-tuning methods have been shown to be able to alleviate some limitations of pre-trained LLMs [9]. Another limitation is that our training is stopped after 1.8 trillion tokens. As is suggested by LLaMA-3 [10], HLAT may be able to further improve on certain tasks, such as math, world knowledge, MMLU, and coding, with more training tokens. VI.", "tokens": 391, "chunk_type": "overlap-1", "url": "https://arxiv.org/abs/2404.10630", "anchor": "#model:part-5", "type": "paper", "title": "", "section": "Model", "source": "arxiv_pdf", "published": "", "sha256": "2f5e409cfd33be307785bce29ac7af9f1d41871bbc242b54daeb78df32791f87"}
{"doc_id": "arxiv:2404.10630", "chunk_id": "model:part-5-o2", "text": "More importantly, as a pre-trained model, HLAT has not gone through a supervised finetuning and human prefer- ence alignment. Those fine-tuning methods have been shown to be able to alleviate some limitations of pre-trained LLMs [9]. Another limitation is that our training is stopped after 1.8 trillion tokens. As is suggested by LLaMA-3 [10], HLAT may be able to further improve on certain tasks, such as math, world knowledge, MMLU, and coding, with more training tokens. VI. BEST PRACTICES & FUTURE DIRECTIONS In this section, we share some best practices we observed for training on AWS TRAINIUM, and raise open questions for future research. Parallelism: NxDT supports TP up to 32 degrees and pipeline parallelism. For a 7B model, we found that the combination of TP=8 and PP=1 provides the highest training throughput, but not for HLAT-70B. So the optimal parallelism configuration varies with model sizes and architectures. To achieve the highest training throughput, parallelism configu- ration needs to be jointly optimized with choice of activation checkpointing method, gradient accumulation steps, and train- ing precision, to balance memory and communication costs. Training Precision: NxDT supports various training pre- cision configurations, including full precision (FP32), BF16 with and without SR, standard mixed precision training, etc. Full precision training is often memory-wise infeasible for multi-billion LLMs. We compared multiple training strategies for HLAT: pure BF16, BF16 with SR and standard mixed precision training. Empirically, we found that training loss of pure BF16 diverges. BF16 with SR shows similar training loss as mixed precision on HLAT-7B model, but converges slower on HLAT-70B. We finally chose BF16 with SR for higher throughput on HLAT-7B, but standard mixed precision on HLAT-70B. For models of other sizes and architecture, warm-up study may be needed", "tokens": 374, "chunk_type": "overlap-2", "url": "https://arxiv.org/abs/2404.10630", "anchor": "#model:part-5", "type": "paper", "title": "", "section": "Model", "source": "arxiv_pdf", "published": "", "sha256": "2f5e409cfd33be307785bce29ac7af9f1d41871bbc242b54daeb78df32791f87"}
{"doc_id": "arxiv:2404.10630", "chunk_id": "model:part-6", "text": "often memory-wise infeasible for multi-billion LLMs. We compared multiple training strategies for HLAT: pure BF16, BF16 with SR and standard mixed precision training. Empirically, we found that training loss of pure BF16 diverges. BF16 with SR shows similar training loss as mixed precision on HLAT-7B model, but converges slower on HLAT-70B. We finally chose BF16 with SR for higher throughput on HLAT-7B, but standard mixed precision on HLAT-70B. For models of other sizes and architecture, warm-up study may be needed to decide the optimal training precision. Usually, the divergence can be observed in first few thousands of steps. Choice of β2: We observed that using β2 = 0.99 causes training instability and slower convergence. This is related to the choice of BF16 with SR training precision. A large β2 fails to capture the gradient explosion at current and recent steps, and hence does not effectively reduce the gradients in occurrence of gradient explosion. Switching to β2 = 0.95 addresses the above-mentioned problem. Weight decay: We applied weight decay to all layers. Empirically, weight decay is not applied to normalization and bias layers [54]. In our experiment, we did not found much performance-wise difference of those two methods. Pre-compilation: TRAINIUM requires pre-compiling the scripts to graphs. The compilation takes some time, especially for large models. Debugging on training scripts (e.g., printing out intermediate tensors) may require re-compilation. Instead of directly developing on a large model, we found it more efficient to develop and test on a smaller model and scale up afterwards. VII. RELATED WORK LLM pre-training: After the Transformer architecture [1] was introduced, BERT [54] was proposed to pre-train a language model on a large corpus of unlabeled data. Fol- lowing the success of BERT model on various NLP tasks, many pre-trained language models are later introduced with different architectures and training methods, such as GPT-2 [55], RoBERTa [56], BART [57], and so on [6]. Studies later observed significant performance improvement of language models by increasing model size and training data [58]. Such abilities are further demonstrated in LLMs such as GPT-3 [7], PaLM [59], LLaMA [8]–[10], Falcon [60], Gemini [61], Phi [48], etc. Pre-trained on trillions of tokens, LLMs with tens or hundreds of billions parameters show remarkable ability in generating creative text contents, as well as a variety of downstream tasks, such as question answering, summarization, machine translation, programming, etc. [6]. AI accelerators: Most models are trained on NVIDIA GPU accelerators, such as GPT [7], [62] and LLaMA [8], [9]. Falcon-180B [60] was trained on AWS SageMaker, with up to 4,096 A100 40GB GPUs using p4d instances. However, the landscape of hardware accelerators for deep learning train- ing has blossomed in recent years, with established players like NVIDIA GPUs facing fierce competition from custom offerings like Google’s TPU and AWS TRAINIUM. PaLM- 2 [59] and OpenLLaMA [29] have demonstreated successful LLM pre-training on Google TPU. Recently, OLMo [49] is an open-source model developed by AI2. It has two models trained on AMD and Nvidia GPUs, separately. The two models have nearly identical performance on their evaluation suite by 2T tokens.", "tokens": 665, "chunk_type": "original-large", "url": "https://arxiv.org/abs/2404.10630", "anchor": "#model:part-6", "type": "paper", "title": "", "section": "Model", "source": "arxiv_pdf", "published": "", "sha256": "c96b0fd22b26e5e58be5abed3349e5bb069d60164005e5b9aac25e1774330083"}
{"doc_id": "arxiv:2404.10630", "chunk_id": "model:part-6-o1", "text": "often memory-wise infeasible for multi-billion LLMs. We compared multiple training strategies for HLAT: pure BF16, BF16 with SR and standard mixed precision training. Empirically, we found that training loss of pure BF16 diverges. BF16 with SR shows similar training loss as mixed precision on HLAT-7B model, but converges slower on HLAT-70B. We finally chose BF16 with SR for higher throughput on HLAT-7B, but standard mixed precision on HLAT-70B. For models of other sizes and architecture, warm-up study may be needed to decide the optimal training precision. Usually, the divergence can be observed in first few thousands of steps. Choice of β2: We observed that using β2 = 0.99 causes training instability and slower convergence. This is related to the choice of BF16 with SR training precision. A large β2 fails to capture the gradient explosion at current and recent steps, and hence does not effectively reduce the gradients in occurrence of gradient explosion. Switching to β2 = 0.95 addresses the above-mentioned problem. Weight decay: We applied weight decay to all layers. Empirically, weight decay is not applied to normalization and bias layers [54]. In our experiment, we did not found much performance-wise difference of those two methods. Pre-compilation: TRAINIUM requires pre-compiling the scripts to graphs. The compilation takes some time, especially for large models. Debugging on training scripts (e.g., printing out intermediate tensors) may require re-compilation. Instead of directly developing on a large model, we found it more efficient to develop and test on a smaller model and scale up afterwards. VII. RELATED WORK LLM pre-training: After the Transformer architecture [1] was introduced, BERT [54] was proposed to pre-train a language model on a large corpus of unlabeled data. Fol- lowing the success of BERT model on various NLP tasks, many pre-trained language models are later introduced with different architectures and training methods, such as GPT-2 [55], RoBERTa [56], BART [57], and so on [6].", "tokens": 409, "chunk_type": "overlap-1", "url": "https://arxiv.org/abs/2404.10630", "anchor": "#model:part-6", "type": "paper", "title": "", "section": "Model", "source": "arxiv_pdf", "published": "", "sha256": "c96b0fd22b26e5e58be5abed3349e5bb069d60164005e5b9aac25e1774330083"}
{"doc_id": "arxiv:2404.10630", "chunk_id": "model:part-6-o2", "text": "RELATED WORK LLM pre-training: After the Transformer architecture [1] was introduced, BERT [54] was proposed to pre-train a language model on a large corpus of unlabeled data. Fol- lowing the success of BERT model on various NLP tasks, many pre-trained language models are later introduced with different architectures and training methods, such as GPT-2 [55], RoBERTa [56], BART [57], and so on [6]. Studies later observed significant performance improvement of language models by increasing model size and training data [58]. Such abilities are further demonstrated in LLMs such as GPT-3 [7], PaLM [59], LLaMA [8]–[10], Falcon [60], Gemini [61], Phi [48], etc. Pre-trained on trillions of tokens, LLMs with tens or hundreds of billions parameters show remarkable ability in generating creative text contents, as well as a variety of downstream tasks, such as question answering, summarization, machine translation, programming, etc. [6]. AI accelerators: Most models are trained on NVIDIA GPU accelerators, such as GPT [7], [62] and LLaMA [8], [9]. Falcon-180B [60] was trained on AWS SageMaker, with up to 4,096 A100 40GB GPUs using p4d instances. However, the landscape of hardware accelerators for deep learning train- ing has blossomed in recent years, with established players like NVIDIA GPUs facing fierce competition from custom offerings like Google’s TPU and AWS TRAINIUM. PaLM- 2 [59] and OpenLLaMA [29] have demonstreated successful LLM pre-training on Google TPU. Recently, OLMo [49] is an open-source model developed by AI2. It has two models trained on AMD and Nvidia GPUs, separately. The two models have nearly identical performance on their evaluation suite by 2T tokens.", "tokens": 338, "chunk_type": "overlap-2", "url": "https://arxiv.org/abs/2404.10630", "anchor": "#model:part-6", "type": "paper", "title": "", "section": "Model", "source": "arxiv_pdf", "published": "", "sha256": "c96b0fd22b26e5e58be5abed3349e5bb069d60164005e5b9aac25e1774330083"}
{"doc_id": "arxiv:2404.10630", "chunk_id": "model:part-7", "text": "of hardware accelerators for deep learning train- ing has blossomed in recent years, with established players like NVIDIA GPUs facing fierce competition from custom offerings like Google’s TPU and AWS TRAINIUM. PaLM- 2 [59] and OpenLLaMA [29] have demonstreated successful LLM pre-training on Google TPU. Recently, OLMo [49] is an open-source model developed by AI2. It has two models trained on AMD and Nvidia GPUs, separately. The two models have nearly identical performance on their evaluation suite by 2T tokens. AWS TRAINIUM is a machine learning accelerator developed for deep learning training with high performance and cost-competitiveness. Our work is the first demonstration of end-to-end multi-billion LLM pre-trained on AWS TRAINIUM. Ultimately, the optimal choice depends on the specific needs of the training task, with further research required to fully explore the potential of each accelerator and their possible convergence in future architectures. VIII. CONCLUSION In this paper, we pre-train HLAT, a family of 7 bil- lion and 70 billion parameter large language models, using AWS TRAINIUM over ∼1.8 trillion tokens. HLAT follows the decoder-only architecture and is trained with up to 256 Amazon EC2 trn1.32xlarge instances. We evaluate the per- formance of HLAT against popular open-source baseline models including LLaMA and OpenLLaMA on a variety of popular benchmarking tasks. We find that HLAT achieves model quality on par with these baseline models of similar sizes. This work demonstrates, for the first time, that AWS TRAINIUM with NxDT is able to successfully pre-train high- quality LLMs with high efficiency and low cost.", "tokens": 327, "chunk_type": "original", "url": "https://arxiv.org/abs/2404.10630", "anchor": "#model:part-7", "type": "paper", "title": "", "section": "Model", "source": "arxiv_pdf", "published": "", "sha256": "16d93448a27f17f55f281fffad1dc07f8c560e5ec9eb6a999f9690fc6ff65bab"}
{"doc_id": "arxiv:2412.10543", "chunk_id": "abstract", "text": "RAG (Retrieval Augmented Generation) allows LLMs (large language models) to generate better responses with exter- nal knowledge, but using more external knowledge causes higher response delay. Prior work focuses either on reducing the response delay (e.g., better scheduling of RAG queries) or on maximizing quality (e.g., tuning the RAG workflow), but they fall short in systematically balancing the tradeoff between the delay and quality of RAG responses. To bal- ance both quality and response delay, this paper presents METIS, the first RAG system that jointly schedules queries and adapts the key RAG configurations of each query, such as the number of retrieved text chunks and synthesis meth- ods. Using four popular RAG-QA datasets, we show that compared to the state-of-the-art RAG optimization schemes, METIS reduces the generation latency by 1.64 −2.54× with- out sacrificing generation quality. 1", "tokens": 178, "chunk_type": "original", "url": "https://arxiv.org/abs/2412.10543", "anchor": "#abstract", "type": "paper", "title": "", "section": "Abstract", "source": "arxiv_pdf", "published": "", "sha256": "b406a2a0b9db36394819f2e170b6319f504c0bec0ef9ea59bec676bf50e21d4f"}
{"doc_id": "arxiv:2412.10543", "chunk_id": "introduction:part-1", "text": "Retrieval-augmented generation (RAG) is a popular LLM inference technique that augments an LLM inference query with relevant text chunks, or “context”, retrieved from a large corpus.1 RAG systems, which include retrieval and LLM inference2, have found many use cases in QA tasks, personal assistants, chatbots, and LLM-powered search [10, 62]. While RAG can enhance the quality (accuracy and relevance) of LLM-generated responses [7, 53, 58, 91, 96], RAG queries are inherently slow as they need more compute and mem- ory resources to process the long input context to answer a query [6, 15, 42]. Thus, it is essential to balance high response quality and low response delays in RAG inference systems. 1RAG vs. long-context models is an active field of research, with the industry widely deploying RAG for its task-focused model inference quality and better resource-sharing capabilities [68]. 2Though RAG sometimes refers to the retrieval step, in this work, a RAG system includes both retrieval and LLM inference based on the retrieved texts, and we aim to optimize the whole pipeline. Past research efforts have optimized RAG, regarding ei- ther response quality or response delay, but they fall short in optimizing the quality-delay tradeoffs of RAG. RAG queries have an associated RAG configuration which de- scribes how and how much data to input for the query (more in §2) [72, 79, 83]. One line of prior work focuses on reducing response delay through better query schedul- ing (e.g., GPU allocation and inference batching) for RAG queries [2, 44, 45, 70, 76], without adapting the RAG con- figuration themselves. An alternate line of work focuses on maximizing generation quality by tuning the configurations of RAG queries [32, 77, 83], but this is often done at the cost of longer response delay. The RAG configuration simultaneously affects generation quality and response delay (e.g., retrieving too many chunks for a simple RAG query may unnecessarily inflate delay with- out increasing quality). Unlike traditional data queries (e.g., SQL) which specify the inputs and operators, RAG queries are inherently under-specified as they consist of a text query written in natural language [27, 32, 57, 64] and do not directly specify the exact RAG configuration of its execution. Moreover, multiple configuration knobs can influence the delay-quality tradeoffs. For instance, besides how many chunks to retrieve, how to use them in the LLM’s input involves two design choices—should the chunks be processed by the LLM jointly, or should the chunks be summarized first before being fed into the LLM together (and how long should a summary be). Recent works also attempt to tune RAG con- figuration [32, 77], but they focus on either tuning individual knobs or maximizing quality at the cost of higher delay. How- ever, tuning configurations across multiple knobs quickly hits a prohibitive combinatorial space (more in §3) and re- quires optimizations to reduce the search cost. What’s more, the RAG configuration should be tuned jointly with scheduling. Consider two configurations: 𝐴feeds all retrieved text chunks in one LLM input, and 𝐵summarizes first each chunk with an LLM and then feeds the summaries 1 arXiv:2412.10543v2 [cs.LG]", "tokens": 665, "chunk_type": "original-large", "url": "https://arxiv.org/abs/2412.10543", "anchor": "#introduction:part-1", "type": "paper", "title": "", "section": "Introduction", "source": "arxiv_pdf", "published": "", "sha256": "d9142e9627112b69af8cb5f22978e474158cd1ffcbf75d2f7635604ecdc58a0b"}
{"doc_id": "arxiv:2412.10543", "chunk_id": "introduction:part-1-o1", "text": "Retrieval-augmented generation (RAG) is a popular LLM inference technique that augments an LLM inference query with relevant text chunks, or “context”, retrieved from a large corpus.1 RAG systems, which include retrieval and LLM inference2, have found many use cases in QA tasks, personal assistants, chatbots, and LLM-powered search [10, 62]. While RAG can enhance the quality (accuracy and relevance) of LLM-generated responses [7, 53, 58, 91, 96], RAG queries are inherently slow as they need more compute and mem- ory resources to process the long input context to answer a query [6, 15, 42]. Thus, it is essential to balance high response quality and low response delays in RAG inference systems. 1RAG vs. long-context models is an active field of research, with the industry widely deploying RAG for its task-focused model inference quality and better resource-sharing capabilities [68]. 2Though RAG sometimes refers to the retrieval step, in this work, a RAG system includes both retrieval and LLM inference based on the retrieved texts, and we aim to optimize the whole pipeline. Past research efforts have optimized RAG, regarding ei- ther response quality or response delay, but they fall short in optimizing the quality-delay tradeoffs of RAG. RAG queries have an associated RAG configuration which de- scribes how and how much data to input for the query (more in §2) [72, 79, 83]. One line of prior work focuses on reducing response delay through better query schedul- ing (e.g., GPU allocation and inference batching) for RAG queries [2, 44, 45, 70, 76], without adapting the RAG con- figuration themselves. An alternate line of work focuses on maximizing generation quality by tuning the configurations of RAG queries [32, 77, 83], but this is often done at the cost of longer response delay.", "tokens": 377, "chunk_type": "overlap-1", "url": "https://arxiv.org/abs/2412.10543", "anchor": "#introduction:part-1", "type": "paper", "title": "", "section": "Introduction", "source": "arxiv_pdf", "published": "", "sha256": "d9142e9627112b69af8cb5f22978e474158cd1ffcbf75d2f7635604ecdc58a0b"}
{"doc_id": "arxiv:2412.10543", "chunk_id": "introduction:part-1-o2", "text": "One line of prior work focuses on reducing response delay through better query schedul- ing (e.g., GPU allocation and inference batching) for RAG queries [2, 44, 45, 70, 76], without adapting the RAG con- figuration themselves. An alternate line of work focuses on maximizing generation quality by tuning the configurations of RAG queries [32, 77, 83], but this is often done at the cost of longer response delay. The RAG configuration simultaneously affects generation quality and response delay (e.g., retrieving too many chunks for a simple RAG query may unnecessarily inflate delay with- out increasing quality). Unlike traditional data queries (e.g., SQL) which specify the inputs and operators, RAG queries are inherently under-specified as they consist of a text query written in natural language [27, 32, 57, 64] and do not directly specify the exact RAG configuration of its execution. Moreover, multiple configuration knobs can influence the delay-quality tradeoffs. For instance, besides how many chunks to retrieve, how to use them in the LLM’s input involves two design choices—should the chunks be processed by the LLM jointly, or should the chunks be summarized first before being fed into the LLM together (and how long should a summary be). Recent works also attempt to tune RAG con- figuration [32, 77], but they focus on either tuning individual knobs or maximizing quality at the cost of higher delay. How- ever, tuning configurations across multiple knobs quickly hits a prohibitive combinatorial space (more in §3) and re- quires optimizations to reduce the search cost. What’s more, the RAG configuration should be tuned jointly with scheduling. Consider two configurations: 𝐴feeds all retrieved text chunks in one LLM input, and 𝐵summarizes first each chunk with an LLM and then feeds the summaries 1 arXiv:2412.10543v2 [cs.LG]", "tokens": 377, "chunk_type": "overlap-2", "url": "https://arxiv.org/abs/2412.10543", "anchor": "#introduction:part-1", "type": "paper", "title": "", "section": "Introduction", "source": "arxiv_pdf", "published": "", "sha256": "d9142e9627112b69af8cb5f22978e474158cd1ffcbf75d2f7635604ecdc58a0b"}
{"doc_id": "arxiv:2412.10543", "chunk_id": "introduction:part-2", "text": "they focus on either tuning individual knobs or maximizing quality at the cost of higher delay. How- ever, tuning configurations across multiple knobs quickly hits a prohibitive combinatorial space (more in §3) and re- quires optimizations to reduce the search cost. What’s more, the RAG configuration should be tuned jointly with scheduling. Consider two configurations: 𝐴feeds all retrieved text chunks in one LLM input, and 𝐵summarizes first each chunk with an LLM and then feeds the summaries 1 arXiv:2412.10543v2 [cs.LG] 16 Jul 2025 to an LLM input for a final generation. While 𝐴(which calls the LLM once) is seemingly faster than 𝐵(which calls the LLM multiple times), 𝐴could be slower as it requires more GPU memory than 𝐵and thus could be delayed in the sched- uler queue. Without making batching and configuration se- lection jointly, it would be difficult to avoid such pitfalls. Finally, the impact of RAG configurations on quality-delay tradeoffs also varies significantly with queries. For example, to answer “In which country is the Kimbrough Memorial Sta- dium located?”, the RAG may retrieve and analyze one text chunk about the stadium. In contrast, to answer “Compare NVIDIA’s operating cost over the first three quarters of 2024 and identify the highest one”, the RAG may need multiple chunks, each containing the quarter’s operating cost, and process these chunks jointly, instead of reading them separately. The above examples illustrate queries differ in complexity (more in §4), leading to needing different configurations per-query for optimal quality-delay tradeoffs. Empirically, we show that picking RAG configuration per-query achieves 12 −15% higher quality and 2.5 −3× lower delay than using any fixed configuration across all queries in a dataset (§5). Thus, RAG configurations should be adapted on a per-query basis. Yet, existing RAG systems, which hand-pick a static config- uration offline based on a few example queries [1, 21, 39, 85], lose out on quality or response time. This paper presents METIS, the first RAG system that adapts multiple configuration knobs on a per-query basis and jointly makes configuration selections and scheduling decisions (i.e., which LLM inference in a batch) to optimize the delay-quality tradeoffs for RAG. As this would require solving a joint combinatorial prob- lem for every query, which can be prohibitively expensive (§3), METIS tackles the challenge with a two-step approach. First, METIS prunes the massive configuration space for each received query to a smaller yet promising one that con- tains configurations that likely yield high-quality output for the given query. Specifically, METIS uses a separate LLM to estimate the query’s profile, including how many pieces of information are required to answer the query and whether joint reasoning is likely required across these pieces of in- formation (more in §4.1). The intuition of the query profiles is that they can effectively filter out undesirable RAG config- urations. For the earlier query example “Compare NVIDIA’s operating cost over the first three quarters of 2024 and identify the highest one,” the estimated profile would suggest that it involves at least three separate pieces of information, so the number of chunks (one of the configuration knobs)", "tokens": 665, "chunk_type": "original-large", "url": "https://arxiv.org/abs/2412.10543", "anchor": "#introduction:part-2", "type": "paper", "title": "", "section": "Introduction", "source": "arxiv_pdf", "published": "", "sha256": "0a67a5bbdd0544d7e6604402f5d0f8df2c7cfcf1ca854e48afbfcffc7ad3a80c"}
{"doc_id": "arxiv:2412.10543", "chunk_id": "introduction:part-2-o1", "text": "they focus on either tuning individual knobs or maximizing quality at the cost of higher delay. How- ever, tuning configurations across multiple knobs quickly hits a prohibitive combinatorial space (more in §3) and re- quires optimizations to reduce the search cost. What’s more, the RAG configuration should be tuned jointly with scheduling. Consider two configurations: 𝐴feeds all retrieved text chunks in one LLM input, and 𝐵summarizes first each chunk with an LLM and then feeds the summaries 1 arXiv:2412.10543v2 [cs.LG] 16 Jul 2025 to an LLM input for a final generation. While 𝐴(which calls the LLM once) is seemingly faster than 𝐵(which calls the LLM multiple times), 𝐴could be slower as it requires more GPU memory than 𝐵and thus could be delayed in the sched- uler queue. Without making batching and configuration se- lection jointly, it would be difficult to avoid such pitfalls. Finally, the impact of RAG configurations on quality-delay tradeoffs also varies significantly with queries. For example, to answer “In which country is the Kimbrough Memorial Sta- dium located?”, the RAG may retrieve and analyze one text chunk about the stadium. In contrast, to answer “Compare NVIDIA’s operating cost over the first three quarters of 2024 and identify the highest one”, the RAG may need multiple chunks, each containing the quarter’s operating cost, and process these chunks jointly, instead of reading them separately. The above examples illustrate queries differ in complexity (more in §4), leading to needing different configurations per-query for optimal quality-delay tradeoffs. Empirically, we show that picking RAG configuration per-query achieves 12 −15% higher quality and 2.5 −3× lower delay than using any fixed configuration across all queries in a dataset (§5). Thus, RAG configurations should be adapted on a per-query basis.", "tokens": 371, "chunk_type": "overlap-1", "url": "https://arxiv.org/abs/2412.10543", "anchor": "#introduction:part-2", "type": "paper", "title": "", "section": "Introduction", "source": "arxiv_pdf", "published": "", "sha256": "0a67a5bbdd0544d7e6604402f5d0f8df2c7cfcf1ca854e48afbfcffc7ad3a80c"}
{"doc_id": "arxiv:2412.10543", "chunk_id": "introduction:part-2-o2", "text": "The above examples illustrate queries differ in complexity (more in §4), leading to needing different configurations per-query for optimal quality-delay tradeoffs. Empirically, we show that picking RAG configuration per-query achieves 12 −15% higher quality and 2.5 −3× lower delay than using any fixed configuration across all queries in a dataset (§5). Thus, RAG configurations should be adapted on a per-query basis. Yet, existing RAG systems, which hand-pick a static config- uration offline based on a few example queries [1, 21, 39, 85], lose out on quality or response time. This paper presents METIS, the first RAG system that adapts multiple configuration knobs on a per-query basis and jointly makes configuration selections and scheduling decisions (i.e., which LLM inference in a batch) to optimize the delay-quality tradeoffs for RAG. As this would require solving a joint combinatorial prob- lem for every query, which can be prohibitively expensive (§3), METIS tackles the challenge with a two-step approach. First, METIS prunes the massive configuration space for each received query to a smaller yet promising one that con- tains configurations that likely yield high-quality output for the given query. Specifically, METIS uses a separate LLM to estimate the query’s profile, including how many pieces of information are required to answer the query and whether joint reasoning is likely required across these pieces of in- formation (more in §4.1). The intuition of the query profiles is that they can effectively filter out undesirable RAG config- urations. For the earlier query example “Compare NVIDIA’s operating cost over the first three quarters of 2024 and identify the highest one,” the estimated profile would suggest that it involves at least three separate pieces of information, so the number of chunks (one of the configuration knobs)", "tokens": 373, "chunk_type": "overlap-2", "url": "https://arxiv.org/abs/2412.10543", "anchor": "#introduction:part-2", "type": "paper", "title": "", "section": "Introduction", "source": "arxiv_pdf", "published": "", "sha256": "0a67a5bbdd0544d7e6604402f5d0f8df2c7cfcf1ca854e48afbfcffc7ad3a80c"}
{"doc_id": "arxiv:2412.10543", "chunk_id": "introduction:part-3", "text": "query and whether joint reasoning is likely required across these pieces of in- formation (more in §4.1). The intuition of the query profiles is that they can effectively filter out undesirable RAG config- urations. For the earlier query example “Compare NVIDIA’s operating cost over the first three quarters of 2024 and identify the highest one,” the estimated profile would suggest that it involves at least three separate pieces of information, so the number of chunks (one of the configuration knobs) should be at least three. It should be noted that the LLM-based profiler is an extra overhead in METIS, but fortunately, its input only contains the RAG query itself and the metadata of the RAG database, which are orders of magnitude shorter than the long contexts in RAG, so the estimation can be relatively fast, about 1/10 of the delay of the execution of the RAG query. METIS (Ours) AdaptiveRAG (ACL 2024) Parrot (OSDI 2024) vLLM : SOTA LLM Engine Better METIS (Ours) AdaptiveRAG (ACL 2024) Parrot (OSDI 2024) vLLM : SOTA LLM Engine Better Figure 1. Performance of METIS on the KG RAG FinSec [50] dataset compared to the baselines. Full results shown in §7. Using the narrowed configuration space, METIS reduces the RAG response delays by jointly deciding the per-query configuration and query scheduling based on available re- sources (§4.3). The insight is that within the pruned configu- ration space, the scheduler can make optimal configuration decisions without exploring the original, large configuration space and the implications on quality. In short, METIS’s two-level design loosely decouples the problem into (1) pruning configuration space to a smaller yet promising range of configurations, which focuses solely on keeping the accuracy high, and (2) jointly optimizing configuration (within the narrowed range) and scheduling to optimize response delay by choosing configurations which best-fit into the GPU memory. We evaluate METIS across four RAG datasets with diverse query profiles (e.g., reasoning vs. domain-specific QA). Fig- ure 1 shows a preview of our results. Our key takeaways are as follows. When achieving the same or higher quality than the baselines, METIS reduces the response delay by 1.6−2.8× compared to the latest vLLM (a state-of-the-art serving en- gine), Parrot (the latest LLM query-scheduling method), as well as AdaptiveRAG (the latest RAG configuration-tuning method). METIS also achieves 1.8 −4.5× higher through- put compared to these baselines when achieving the same response delay and same/higher quality. The general concept of using LLMs to guide system tuning is not exactly new [60, 88], but our key contribution lies in applying the concept to RAG systems, through joint sched- uling with resource-aware configuration selection, leading to significantly better resource sharing (§4.2, §4.3). 2 RAG systems and configurations As an LLM often does not have domain-specific or up-to-date knowledge, LLM applications commonly employ RAG to sup- plement LLM inference with external knowledge to generate high-quality responses. Despite the growth of model context length, using RAG to pinpoint the relevant context is still significantly cheaper in terms of resource cost (GPU require- ment), latency, and memory consumption (KV Cache size). For 2 general-purpose", "tokens": 665, "chunk_type": "original-large", "url": "https://arxiv.org/abs/2412.10543", "anchor": "#introduction:part-3", "type": "paper", "title": "", "section": "Introduction", "source": "arxiv_pdf", "published": "", "sha256": "47a76b7d8d9debb103d1a73314c681eb320a0d3d23b207a942306207ed5d05f3"}
{"doc_id": "arxiv:2412.10543", "chunk_id": "introduction:part-3-o1", "text": "query and whether joint reasoning is likely required across these pieces of in- formation (more in §4.1). The intuition of the query profiles is that they can effectively filter out undesirable RAG config- urations. For the earlier query example “Compare NVIDIA’s operating cost over the first three quarters of 2024 and identify the highest one,” the estimated profile would suggest that it involves at least three separate pieces of information, so the number of chunks (one of the configuration knobs) should be at least three. It should be noted that the LLM-based profiler is an extra overhead in METIS, but fortunately, its input only contains the RAG query itself and the metadata of the RAG database, which are orders of magnitude shorter than the long contexts in RAG, so the estimation can be relatively fast, about 1/10 of the delay of the execution of the RAG query. METIS (Ours) AdaptiveRAG (ACL 2024) Parrot (OSDI 2024) vLLM : SOTA LLM Engine Better METIS (Ours) AdaptiveRAG (ACL 2024) Parrot (OSDI 2024) vLLM : SOTA LLM Engine Better Figure 1. Performance of METIS on the KG RAG FinSec [50] dataset compared to the baselines. Full results shown in §7. Using the narrowed configuration space, METIS reduces the RAG response delays by jointly deciding the per-query configuration and query scheduling based on available re- sources (§4.3). The insight is that within the pruned configu- ration space, the scheduler can make optimal configuration decisions without exploring the original, large configuration space and the implications on quality. In short, METIS’s two-level design loosely decouples the problem into (1) pruning configuration space to a smaller yet promising range of configurations, which focuses solely on keeping the accuracy high, and (2) jointly optimizing configuration (within the narrowed range) and scheduling to optimize response delay by choosing configurations which best-fit into the GPU memory.", "tokens": 396, "chunk_type": "overlap-1", "url": "https://arxiv.org/abs/2412.10543", "anchor": "#introduction:part-3", "type": "paper", "title": "", "section": "Introduction", "source": "arxiv_pdf", "published": "", "sha256": "47a76b7d8d9debb103d1a73314c681eb320a0d3d23b207a942306207ed5d05f3"}
{"doc_id": "arxiv:2412.10543", "chunk_id": "introduction:part-3-o2", "text": "The insight is that within the pruned configu- ration space, the scheduler can make optimal configuration decisions without exploring the original, large configuration space and the implications on quality. In short, METIS’s two-level design loosely decouples the problem into (1) pruning configuration space to a smaller yet promising range of configurations, which focuses solely on keeping the accuracy high, and (2) jointly optimizing configuration (within the narrowed range) and scheduling to optimize response delay by choosing configurations which best-fit into the GPU memory. We evaluate METIS across four RAG datasets with diverse query profiles (e.g., reasoning vs. domain-specific QA). Fig- ure 1 shows a preview of our results. Our key takeaways are as follows. When achieving the same or higher quality than the baselines, METIS reduces the response delay by 1.6−2.8× compared to the latest vLLM (a state-of-the-art serving en- gine), Parrot (the latest LLM query-scheduling method), as well as AdaptiveRAG (the latest RAG configuration-tuning method). METIS also achieves 1.8 −4.5× higher through- put compared to these baselines when achieving the same response delay and same/higher quality. The general concept of using LLMs to guide system tuning is not exactly new [60, 88], but our key contribution lies in applying the concept to RAG systems, through joint sched- uling with resource-aware configuration selection, leading to significantly better resource sharing (§4.2, §4.3). 2 RAG systems and configurations As an LLM often does not have domain-specific or up-to-date knowledge, LLM applications commonly employ RAG to sup- plement LLM inference with external knowledge to generate high-quality responses. Despite the growth of model context length, using RAG to pinpoint the relevant context is still significantly cheaper in terms of resource cost (GPU require- ment), latency, and memory consumption (KV Cache size). For 2 general-purpose", "tokens": 377, "chunk_type": "overlap-2", "url": "https://arxiv.org/abs/2412.10543", "anchor": "#introduction:part-3", "type": "paper", "title": "", "section": "Introduction", "source": "arxiv_pdf", "published": "", "sha256": "47a76b7d8d9debb103d1a73314c681eb320a0d3d23b207a942306207ed5d05f3"}
{"doc_id": "arxiv:2412.10543", "chunk_id": "introduction:part-4", "text": "resource-aware configuration selection, leading to significantly better resource sharing (§4.2, §4.3). 2 RAG systems and configurations As an LLM often does not have domain-specific or up-to-date knowledge, LLM applications commonly employ RAG to sup- plement LLM inference with external knowledge to generate high-quality responses. Despite the growth of model context length, using RAG to pinpoint the relevant context is still significantly cheaper in terms of resource cost (GPU require- ment), latency, and memory consumption (KV Cache size). For 2 general-purpose QA pipelines, RAG is cost-efficient with re- trieving targeted chunks based on semantic similarity to the query. Using LLMs with long-context documents in contrast has much higher GPU memory usage and delay [43, 45, 71]. Before processing queries, a RAG system organizes back- ground documents by splitting them into chunks (each with a fixed number of tokens), embedding each chunk using models like Bert [12, 19], and storing the embeddings with the chunks in a vector database. Processing a RAG query involves two main steps: • Retrieval: The RAG system retrieves one or more rele- vant context chunks from the database by comparing the query’s embedding, (using the same embedding model as for database indexing), with the stored embeddings. • Synthesis: After retrieving the relevant chunks, the RAG system combines these chunks and the RAG query to form a single/multiple LLM call(s) to generate the response. Retrieval is computationally lightweight and much faster than synthesis (> 100×), so the response delay is typically dominated by the synthesis step [90]. RAG configuration: This work focuses on optimizing three configuration knobs, illustrated in Figure 2, which are de- rived from key design questions that affect RAG performance in terms of response delay and quality: • How many chunks to retrieve (num_chunks): The number of context chunks directly affects the delay of the synthesis step, with more computation needed to process the longer sequences with more chunks. In the meantime, retrieving too few chunks risks low response quality if the retrieved chunks do not contain enough useful information. • How to synthesize (synthesis_method): If the LLM should read the chunks separately, RAG uses the LLM to generate one answer for the query using each chunk separately and picks the output with the highest confidence, which is called map_rerank. This often incurs the least compu- tation but can cause low quality if the useful information is scattered in different chunks, in which case the LLM should read the chunks jointly. The RAG system can feed these chunks in the LLM input directly by concatenating them within a single prompt (called stuff) or to create a shorter summary for each chunk first before feeding the summaries and the query into the LLM to generate the final response (called map_reduce). stuff needs less computa- tion than map_reduce, but risks degraded output quality for long inputs due to the lost-in-the-middle problem [47]. • How long is each summary (intermediate_length): Fi- nally, if the LLM produces the summary for each chunk based on the user query, the length of each summary greatly affects the quality and response of map_reduce— shorter", "tokens": 665, "chunk_type": "original-large", "url": "https://arxiv.org/abs/2412.10543", "anchor": "#introduction:part-4", "type": "paper", "title": "", "section": "Introduction", "source": "arxiv_pdf", "published": "", "sha256": "3b568ece8ef08daa463a1d90b44e98742e87a8d6b61869bc8b9e9d5ef7a30a9d"}
{"doc_id": "arxiv:2412.10543", "chunk_id": "introduction:part-4-o1", "text": "resource-aware configuration selection, leading to significantly better resource sharing (§4.2, §4.3). 2 RAG systems and configurations As an LLM often does not have domain-specific or up-to-date knowledge, LLM applications commonly employ RAG to sup- plement LLM inference with external knowledge to generate high-quality responses. Despite the growth of model context length, using RAG to pinpoint the relevant context is still significantly cheaper in terms of resource cost (GPU require- ment), latency, and memory consumption (KV Cache size). For 2 general-purpose QA pipelines, RAG is cost-efficient with re- trieving targeted chunks based on semantic similarity to the query. Using LLMs with long-context documents in contrast has much higher GPU memory usage and delay [43, 45, 71]. Before processing queries, a RAG system organizes back- ground documents by splitting them into chunks (each with a fixed number of tokens), embedding each chunk using models like Bert [12, 19], and storing the embeddings with the chunks in a vector database. Processing a RAG query involves two main steps: • Retrieval: The RAG system retrieves one or more rele- vant context chunks from the database by comparing the query’s embedding, (using the same embedding model as for database indexing), with the stored embeddings. • Synthesis: After retrieving the relevant chunks, the RAG system combines these chunks and the RAG query to form a single/multiple LLM call(s) to generate the response. Retrieval is computationally lightweight and much faster than synthesis (> 100×), so the response delay is typically dominated by the synthesis step [90].", "tokens": 323, "chunk_type": "overlap-1", "url": "https://arxiv.org/abs/2412.10543", "anchor": "#introduction:part-4", "type": "paper", "title": "", "section": "Introduction", "source": "arxiv_pdf", "published": "", "sha256": "3b568ece8ef08daa463a1d90b44e98742e87a8d6b61869bc8b9e9d5ef7a30a9d"}
{"doc_id": "arxiv:2412.10543", "chunk_id": "introduction:part-4-o2", "text": "Processing a RAG query involves two main steps: • Retrieval: The RAG system retrieves one or more rele- vant context chunks from the database by comparing the query’s embedding, (using the same embedding model as for database indexing), with the stored embeddings. • Synthesis: After retrieving the relevant chunks, the RAG system combines these chunks and the RAG query to form a single/multiple LLM call(s) to generate the response. Retrieval is computationally lightweight and much faster than synthesis (> 100×), so the response delay is typically dominated by the synthesis step [90]. RAG configuration: This work focuses on optimizing three configuration knobs, illustrated in Figure 2, which are de- rived from key design questions that affect RAG performance in terms of response delay and quality: • How many chunks to retrieve (num_chunks): The number of context chunks directly affects the delay of the synthesis step, with more computation needed to process the longer sequences with more chunks. In the meantime, retrieving too few chunks risks low response quality if the retrieved chunks do not contain enough useful information. • How to synthesize (synthesis_method): If the LLM should read the chunks separately, RAG uses the LLM to generate one answer for the query using each chunk separately and picks the output with the highest confidence, which is called map_rerank. This often incurs the least compu- tation but can cause low quality if the useful information is scattered in different chunks, in which case the LLM should read the chunks jointly.", "tokens": 323, "chunk_type": "overlap-2", "url": "https://arxiv.org/abs/2412.10543", "anchor": "#introduction:part-4", "type": "paper", "title": "", "section": "Introduction", "source": "arxiv_pdf", "published": "", "sha256": "3b568ece8ef08daa463a1d90b44e98742e87a8d6b61869bc8b9e9d5ef7a30a9d"}
{"doc_id": "arxiv:2412.10543", "chunk_id": "introduction:part-4-o3", "text": "In the meantime, retrieving too few chunks risks low response quality if the retrieved chunks do not contain enough useful information. • How to synthesize (synthesis_method): If the LLM should read the chunks separately, RAG uses the LLM to generate one answer for the query using each chunk separately and picks the output with the highest confidence, which is called map_rerank. This often incurs the least compu- tation but can cause low quality if the useful information is scattered in different chunks, in which case the LLM should read the chunks jointly. The RAG system can feed these chunks in the LLM input directly by concatenating them within a single prompt (called stuff) or to create a shorter summary for each chunk first before feeding the summaries and the query into the LLM to generate the final response (called map_reduce). stuff needs less computa- tion than map_reduce, but risks degraded output quality for long inputs due to the lost-in-the-middle problem [47]. • How long is each summary (intermediate_length): Fi- nally, if the LLM produces the summary for each chunk based on the user query, the length of each summary greatly affects the quality and response of map_reduce— shorter", "tokens": 257, "chunk_type": "overlap-3", "url": "https://arxiv.org/abs/2412.10543", "anchor": "#introduction:part-4", "type": "paper", "title": "", "section": "Introduction", "source": "arxiv_pdf", "published": "", "sha256": "3b568ece8ef08daa463a1d90b44e98742e87a8d6b61869bc8b9e9d5ef7a30a9d"}
{"doc_id": "arxiv:2412.10543", "chunk_id": "introduction:part-5", "text": "summary for each chunk first before feeding the summaries and the query into the LLM to generate the final response (called map_reduce). stuff needs less computa- tion than map_reduce, but risks degraded output quality for long inputs due to the lost-in-the-middle problem [47]. • How long is each summary (intermediate_length): Fi- nally, if the LLM produces the summary for each chunk based on the user query, the length of each summary greatly affects the quality and response of map_reduce— shorter summaries yield lower delay but also risk not feed- ing enough information to the final LLM inference. How many chunks to retrieve? If jointly, should the LLM summarize each chunk first? If so, how long should each summary be? If multiple chunks, should the LLM read them jointly? Knob 1: num_chunks Knob 2: synthesis_method Knob 3: intermediate_length Key design choices of RAG Figure 2. The configuration knobs adapted by METIS are derived from key design choices of RAG systems. Chunk 1 Chunk 2 Chunk 3 LLM Final Answer Chunk 1 Chunk 2 Chunk 3 Final Answer 1 Confidence : 80% Final Answer 2 Confidence : 99% Final Answer 3 Confidence : 90% Chunk 1 Chunk 2 Chunk 3 S1 S2 S3 Final Answer (a) Stuff (b) Map Rerank (c) Map Reduce LLM LLM LLM Figure 3. Illustration of different RAG synthesis methods, which have various LLM reasoning capabilities. In this work, while we focus on universal RAG knobs which affect quality and delay common to all RAG systems, METIS can be extended to other tunable knobs (e.g., some RAG system may dynamically choose the embedding model, retrieval index or serving LLM). METIS’ design is extensible to any RAG configuration knob based on the query profile. Performance metrics: We evaluate the performance of a RAG system using two metrics: • Response quality calculates the F1 score of the generated response against the ground truth. The F1 score is the harmonic mean of precision (# correctly generated words) and recall (# of correct words successfully generated). This metric is widely used in prior works [10, 69, 72]. • Response delay measures the time elapsed from when the RAG system receives a RAG request to when it completes generating the response. Next, we show that these knobs need to be properly tuned on a per-query basis to achieve optimal tradeoff between quality and delay in §3. 3 Towards better quality-delay tradeoffs Prior work on RAG either optimizes for lower delay or higher quality, i.e., the first picks static configurations and focuses on reducing the delay by smart scheduling and resource allo- cation [44, 70, 76] and the second picks RAG configurations to maximize quality without regard to resource usage or delay [32, 77, 83]. For the first time, we explore the potential of optimizing the quality-delay tradeoffs for RAG. 3 (a) Change: synthesis method from map_rerank (circle) , stuff (plus) and map_reduce (square) (b) Change: number of chunks from 1 to 35 with stuff (c) Change: intermediate length from 1 to 100 with map_reduce Q1 Q1 Q1 Q2 Q2 Q2 Q3 Q3 Q3 Figure 4.", "tokens": 665, "chunk_type": "original-large", "url": "https://arxiv.org/abs/2412.10543", "anchor": "#introduction:part-5", "type": "paper", "title": "", "section": "Introduction", "source": "arxiv_pdf", "published": "", "sha256": "aa2a62e6a2676c1bc20827d57ee1fe6cbb65e434a71ff2e9c1182f9a553a14f2"}
{"doc_id": "arxiv:2412.10543", "chunk_id": "introduction:part-5-o1", "text": "summary for each chunk first before feeding the summaries and the query into the LLM to generate the final response (called map_reduce). stuff needs less computa- tion than map_reduce, but risks degraded output quality for long inputs due to the lost-in-the-middle problem [47]. • How long is each summary (intermediate_length): Fi- nally, if the LLM produces the summary for each chunk based on the user query, the length of each summary greatly affects the quality and response of map_reduce— shorter summaries yield lower delay but also risk not feed- ing enough information to the final LLM inference. How many chunks to retrieve? If jointly, should the LLM summarize each chunk first? If so, how long should each summary be? If multiple chunks, should the LLM read them jointly? Knob 1: num_chunks Knob 2: synthesis_method Knob 3: intermediate_length Key design choices of RAG Figure 2. The configuration knobs adapted by METIS are derived from key design choices of RAG systems. Chunk 1 Chunk 2 Chunk 3 LLM Final Answer Chunk 1 Chunk 2 Chunk 3 Final Answer 1 Confidence : 80% Final Answer 2 Confidence : 99% Final Answer 3 Confidence : 90% Chunk 1 Chunk 2 Chunk 3 S1 S2 S3 Final Answer (a) Stuff (b) Map Rerank (c) Map Reduce LLM LLM LLM Figure 3. Illustration of different RAG synthesis methods, which have various LLM reasoning capabilities. In this work, while we focus on universal RAG knobs which affect quality and delay common to all RAG systems, METIS can be extended to other tunable knobs (e.g., some RAG system may dynamically choose the embedding model, retrieval index or serving LLM). METIS’ design is extensible to any RAG configuration knob based on the query profile.", "tokens": 370, "chunk_type": "overlap-1", "url": "https://arxiv.org/abs/2412.10543", "anchor": "#introduction:part-5", "type": "paper", "title": "", "section": "Introduction", "source": "arxiv_pdf", "published": "", "sha256": "aa2a62e6a2676c1bc20827d57ee1fe6cbb65e434a71ff2e9c1182f9a553a14f2"}
{"doc_id": "arxiv:2412.10543", "chunk_id": "introduction:part-5-o2", "text": "Illustration of different RAG synthesis methods, which have various LLM reasoning capabilities. In this work, while we focus on universal RAG knobs which affect quality and delay common to all RAG systems, METIS can be extended to other tunable knobs (e.g., some RAG system may dynamically choose the embedding model, retrieval index or serving LLM). METIS’ design is extensible to any RAG configuration knob based on the query profile. Performance metrics: We evaluate the performance of a RAG system using two metrics: • Response quality calculates the F1 score of the generated response against the ground truth. The F1 score is the harmonic mean of precision (# correctly generated words) and recall (# of correct words successfully generated). This metric is widely used in prior works [10, 69, 72]. • Response delay measures the time elapsed from when the RAG system receives a RAG request to when it completes generating the response. Next, we show that these knobs need to be properly tuned on a per-query basis to achieve optimal tradeoff between quality and delay in §3. 3 Towards better quality-delay tradeoffs Prior work on RAG either optimizes for lower delay or higher quality, i.e., the first picks static configurations and focuses on reducing the delay by smart scheduling and resource allo- cation [44, 70, 76] and the second picks RAG configurations to maximize quality without regard to resource usage or delay [32, 77, 83]. For the first time, we explore the potential of optimizing the quality-delay tradeoffs for RAG. 3 (a) Change: synthesis method from map_rerank (circle) , stuff (plus) and map_reduce (square) (b) Change: number of chunks from 1 to 35 with stuff (c) Change: intermediate length from 1 to 100 with map_reduce Q1 Q1 Q1 Q2 Q2 Q2 Q3 Q3 Q3 Figure 4.", "tokens": 384, "chunk_type": "overlap-2", "url": "https://arxiv.org/abs/2412.10543", "anchor": "#introduction:part-5", "type": "paper", "title": "", "section": "Introduction", "source": "arxiv_pdf", "published": "", "sha256": "aa2a62e6a2676c1bc20827d57ee1fe6cbb65e434a71ff2e9c1182f9a553a14f2"}
{"doc_id": "arxiv:2412.10543", "chunk_id": "introduction:part-6", "text": "and the second picks RAG configurations to maximize quality without regard to resource usage or delay [32, 77, 83]. For the first time, we explore the potential of optimizing the quality-delay tradeoffs for RAG. 3 (a) Change: synthesis method from map_rerank (circle) , stuff (plus) and map_reduce (square) (b) Change: number of chunks from 1 to 35 with stuff (c) Change: intermediate length from 1 to 100 with map_reduce Q1 Q1 Q1 Q2 Q2 Q2 Q3 Q3 Q3 Figure 4. Varying each RAG configuration knob leads to different quality-latency tradeoffs, and these tradeoffs differ across queries (Q1 in green, Q2 in blue, and Q3 in red). To improve the delay-quality tradeoff, our insight is that quality and delay should jointly be optimized in this large tradeoff space created by the choice of RAG configuration knobs. Importantly, the configurations with better quality- delay tradeoffs vary significantly across queries. To showcase this observation, we use three queries from Musique [78], a popular reasoning QA dataset (§7.1). • Q1: “In what county was William W. Blair’s born?” • Q2: “Are Alison Skipper, Diane Gilliam Fisher, and Rachel McAdams from the same country?” • Q3: “When and why did the Voyager 1, the spacecraft that detected storms on Neptune, leave our solar system?” We chose queries with different natural language complexity and reasoning, Q1 being relatively less complex than Q2 and Q3. Then, we adjust the value of each configuration knob in order to quantify each knob’s impact on the quality- delay tradeoffs in each of the queries. Impact of synthesis method: Figure 4 (a) changes the syn- thesis method and shows its effect on the quality-delay trade- off, while keeping the other RAG configuration knobs con- stant. We vary the synthesis method as map_rerank, stuff, and map_reduce from left to right. The insight is that the optimal synthesis method that strikes the best quality-delay tradeoff (closest to the top left corner) differs significantly across the different queries. For simple queries like Q1 (green), quality plateaus for more complex synthesis methods (stuff and map_reduce). Because it only needs a single piece of context, map_rerank which processes chunks in isolation suffices, whereas cross- chunk reasoning (stuff and map_reduce) adds undue delay (2×) without improving quality. For queries such as Q2 (blue) that require cross-chunk rea- soning, stuff and map_reduce provide significant quality improvements (35% increase) by processing chunks jointly. For more complex queries, such as Q3 (red), which require even more reasoning and information (why Voyager 1 left has multiple reasons), methods like map_reduce improve quality (30% increase) by removing unnecessary text in the mapper phase, to help the LLM focus on the relevant content. Impact of the number of retrieved chunks: Figure 4 (b) fixes the synthesis method (stuff) and shows the impact of the number of retrieved chunks (1-35) on quality and delay. Simple queries, like Q1 (green), can often be answered using just one or two chunks (needs only birth county). For more complex queries, Q2 (blue) and Q3 (red), increasing the number of chunks (1-15) improves the likelihood of retriev- ing all relevant", "tokens": 665, "chunk_type": "original-large", "url": "https://arxiv.org/abs/2412.10543", "anchor": "#introduction:part-6", "type": "paper", "title": "", "section": "Introduction", "source": "arxiv_pdf", "published": "", "sha256": "60112ce6bcfaf4e7f034686d17df1802d9f1667ef6a3a09f8d321c375c38fbed"}
{"doc_id": "arxiv:2412.10543", "chunk_id": "introduction:part-6-o1", "text": "and the second picks RAG configurations to maximize quality without regard to resource usage or delay [32, 77, 83]. For the first time, we explore the potential of optimizing the quality-delay tradeoffs for RAG. 3 (a) Change: synthesis method from map_rerank (circle) , stuff (plus) and map_reduce (square) (b) Change: number of chunks from 1 to 35 with stuff (c) Change: intermediate length from 1 to 100 with map_reduce Q1 Q1 Q1 Q2 Q2 Q2 Q3 Q3 Q3 Figure 4. Varying each RAG configuration knob leads to different quality-latency tradeoffs, and these tradeoffs differ across queries (Q1 in green, Q2 in blue, and Q3 in red). To improve the delay-quality tradeoff, our insight is that quality and delay should jointly be optimized in this large tradeoff space created by the choice of RAG configuration knobs. Importantly, the configurations with better quality- delay tradeoffs vary significantly across queries. To showcase this observation, we use three queries from Musique [78], a popular reasoning QA dataset (§7.1). • Q1: “In what county was William W. Blair’s born?” • Q2: “Are Alison Skipper, Diane Gilliam Fisher, and Rachel McAdams from the same country?” • Q3: “When and why did the Voyager 1, the spacecraft that detected storms on Neptune, leave our solar system?” We chose queries with different natural language complexity and reasoning, Q1 being relatively less complex than Q2 and Q3. Then, we adjust the value of each configuration knob in order to quantify each knob’s impact on the quality- delay tradeoffs in each of the queries. Impact of synthesis method: Figure 4 (a) changes the syn- thesis method and shows its effect on the quality-delay trade- off, while keeping the other RAG configuration knobs con- stant. We vary the synthesis method as map_rerank, stuff, and map_reduce from left to right.", "tokens": 387, "chunk_type": "overlap-1", "url": "https://arxiv.org/abs/2412.10543", "anchor": "#introduction:part-6", "type": "paper", "title": "", "section": "Introduction", "source": "arxiv_pdf", "published": "", "sha256": "60112ce6bcfaf4e7f034686d17df1802d9f1667ef6a3a09f8d321c375c38fbed"}
{"doc_id": "arxiv:2412.10543", "chunk_id": "introduction:part-6-o2", "text": "Then, we adjust the value of each configuration knob in order to quantify each knob’s impact on the quality- delay tradeoffs in each of the queries. Impact of synthesis method: Figure 4 (a) changes the syn- thesis method and shows its effect on the quality-delay trade- off, while keeping the other RAG configuration knobs con- stant. We vary the synthesis method as map_rerank, stuff, and map_reduce from left to right. The insight is that the optimal synthesis method that strikes the best quality-delay tradeoff (closest to the top left corner) differs significantly across the different queries. For simple queries like Q1 (green), quality plateaus for more complex synthesis methods (stuff and map_reduce). Because it only needs a single piece of context, map_rerank which processes chunks in isolation suffices, whereas cross- chunk reasoning (stuff and map_reduce) adds undue delay (2×) without improving quality. For queries such as Q2 (blue) that require cross-chunk rea- soning, stuff and map_reduce provide significant quality improvements (35% increase) by processing chunks jointly. For more complex queries, such as Q3 (red), which require even more reasoning and information (why Voyager 1 left has multiple reasons), methods like map_reduce improve quality (30% increase) by removing unnecessary text in the mapper phase, to help the LLM focus on the relevant content. Impact of the number of retrieved chunks: Figure 4 (b) fixes the synthesis method (stuff) and shows the impact of the number of retrieved chunks (1-35) on quality and delay. Simple queries, like Q1 (green), can often be answered using just one or two chunks (needs only birth county). For more complex queries, Q2 (blue) and Q3 (red), increasing the number of chunks (1-15) improves the likelihood of retriev- ing all relevant", "tokens": 369, "chunk_type": "overlap-2", "url": "https://arxiv.org/abs/2412.10543", "anchor": "#introduction:part-6", "type": "paper", "title": "", "section": "Introduction", "source": "arxiv_pdf", "published": "", "sha256": "60112ce6bcfaf4e7f034686d17df1802d9f1667ef6a3a09f8d321c375c38fbed"}
{"doc_id": "arxiv:2412.10543", "chunk_id": "introduction:part-7", "text": "help the LLM focus on the relevant content. Impact of the number of retrieved chunks: Figure 4 (b) fixes the synthesis method (stuff) and shows the impact of the number of retrieved chunks (1-35) on quality and delay. Simple queries, like Q1 (green), can often be answered using just one or two chunks (needs only birth county). For more complex queries, Q2 (blue) and Q3 (red), increasing the number of chunks (1-15) improves the likelihood of retriev- ing all relevant context and improves quality. Blindly retrieving more chunks than necessary risks di- luting the relevance of actual important information, due to commonly known problems such as “lost-in-the-middle” [28, 47]. In all three queries, retrieving more chunks beyond a point harms the quality (up to 20% drop) and unnecessar- ily inflates delay (up to 3×). Hence we have a quality-delay tradeoff where increasing chunks up to a point helps quality but beyond that it increases delay while degrading quality. Impact of the intermediate output length: Figure 4 (c) shows the impact of our third configuration knob, vary- ing the intermediate output length (1-100) for map_reduce synthesis methods on the quality-delay tradeoff. For simple queries like Q1 (green), short amounts of intermediate length are enough to answer the query (10-20 words). For more com- plex queries Q2 (blue) and Q3 (red), increasing the amount of intermediate length (70-100 words) provided helps the model with enough information to answer the query. Overall, we see that RAG queries naturally vary in com- plexity, requiring differing levels of inter-chunk reasoning and varying numbers of context chunks. More complex queries, which require more reasoning and context, ben- efit from increased LLM computation, which can come at the cost of increased delay. Adding more context chunks helps to a point beyond which it harms the output quality and delay. Thus, adapting RAG configuration on a per-query basis is crucial. Figures 2, 3, 4 illustrate tuning most popular RAG configuration knobs, however the tuning extends to more RAG configurations with richer tradeoff spaces (§4.2). 4 Pareto Boundary of fixed configuration with vLLM Pareto Boundary of fixed configuration with vLLM Per-Query Configuration Per-Query Configuration Figure 5. Per-query configuration can achieve significantly better quality-delay tradeoffs across queries compared to every fixed configuration choice. Figure 5 uses queries from two datasets (Musique and QM- SUM, see §7.1) and shows that picking the best configuration for each query (the best configuration is the one with the lowest delay that achieves less than 2% drop than the highest achievable quality) achieves superior quality-delay tradeoff than picking any static configuration for all queries. Choos- ing the configuration per-query allows up to 3× delay saving compared to static configurations which are the closest in quality. Every single static configuration choice that achieves comparable delay has at least a 10% quality drop. In spite of the potential benefits, per-query configuration adaptation faces challenges that hinder their real-world adop- tion. Each RAG query comes in plain text with practically no associated RAG configurations. Moreover, the space of configurations grows exponentially with multiple knobs. For example, for a map_reduce configuration,", "tokens": 665, "chunk_type": "original-large", "url": "https://arxiv.org/abs/2412.10543", "anchor": "#introduction:part-7", "type": "paper", "title": "", "section": "Introduction", "source": "arxiv_pdf", "published": "", "sha256": "943a60526132d3179fa53f81ee80181bbc3d2be0426a4ab4f1177a0a25cb94b3"}
{"doc_id": "arxiv:2412.10543", "chunk_id": "introduction:part-7-o1", "text": "help the LLM focus on the relevant content. Impact of the number of retrieved chunks: Figure 4 (b) fixes the synthesis method (stuff) and shows the impact of the number of retrieved chunks (1-35) on quality and delay. Simple queries, like Q1 (green), can often be answered using just one or two chunks (needs only birth county). For more complex queries, Q2 (blue) and Q3 (red), increasing the number of chunks (1-15) improves the likelihood of retriev- ing all relevant context and improves quality. Blindly retrieving more chunks than necessary risks di- luting the relevance of actual important information, due to commonly known problems such as “lost-in-the-middle” [28, 47]. In all three queries, retrieving more chunks beyond a point harms the quality (up to 20% drop) and unnecessar- ily inflates delay (up to 3×). Hence we have a quality-delay tradeoff where increasing chunks up to a point helps quality but beyond that it increases delay while degrading quality. Impact of the intermediate output length: Figure 4 (c) shows the impact of our third configuration knob, vary- ing the intermediate output length (1-100) for map_reduce synthesis methods on the quality-delay tradeoff. For simple queries like Q1 (green), short amounts of intermediate length are enough to answer the query (10-20 words). For more com- plex queries Q2 (blue) and Q3 (red), increasing the amount of intermediate length (70-100 words) provided helps the model with enough information to answer the query. Overall, we see that RAG queries naturally vary in com- plexity, requiring differing levels of inter-chunk reasoning and varying numbers of context chunks. More complex queries, which require more reasoning and context, ben- efit from increased LLM computation, which can come at the cost of increased delay. Adding more context chunks helps to a point beyond which it harms the output quality and delay. Thus, adapting RAG configuration on a per-query basis is crucial.", "tokens": 405, "chunk_type": "overlap-1", "url": "https://arxiv.org/abs/2412.10543", "anchor": "#introduction:part-7", "type": "paper", "title": "", "section": "Introduction", "source": "arxiv_pdf", "published": "", "sha256": "943a60526132d3179fa53f81ee80181bbc3d2be0426a4ab4f1177a0a25cb94b3"}
{"doc_id": "arxiv:2412.10543", "chunk_id": "introduction:part-7-o2", "text": "Overall, we see that RAG queries naturally vary in com- plexity, requiring differing levels of inter-chunk reasoning and varying numbers of context chunks. More complex queries, which require more reasoning and context, ben- efit from increased LLM computation, which can come at the cost of increased delay. Adding more context chunks helps to a point beyond which it harms the output quality and delay. Thus, adapting RAG configuration on a per-query basis is crucial. Figures 2, 3, 4 illustrate tuning most popular RAG configuration knobs, however the tuning extends to more RAG configurations with richer tradeoff spaces (§4.2). 4 Pareto Boundary of fixed configuration with vLLM Pareto Boundary of fixed configuration with vLLM Per-Query Configuration Per-Query Configuration Figure 5. Per-query configuration can achieve significantly better quality-delay tradeoffs across queries compared to every fixed configuration choice. Figure 5 uses queries from two datasets (Musique and QM- SUM, see §7.1) and shows that picking the best configuration for each query (the best configuration is the one with the lowest delay that achieves less than 2% drop than the highest achievable quality) achieves superior quality-delay tradeoff than picking any static configuration for all queries. Choos- ing the configuration per-query allows up to 3× delay saving compared to static configurations which are the closest in quality. Every single static configuration choice that achieves comparable delay has at least a 10% quality drop. In spite of the potential benefits, per-query configuration adaptation faces challenges that hinder their real-world adop- tion. Each RAG query comes in plain text with practically no associated RAG configurations. Moreover, the space of configurations grows exponentially with multiple knobs. For example, for a map_reduce configuration,", "tokens": 356, "chunk_type": "overlap-2", "url": "https://arxiv.org/abs/2412.10543", "anchor": "#introduction:part-7", "type": "paper", "title": "", "section": "Introduction", "source": "arxiv_pdf", "published": "", "sha256": "943a60526132d3179fa53f81ee80181bbc3d2be0426a4ab4f1177a0a25cb94b3"}
{"doc_id": "arxiv:2412.10543", "chunk_id": "introduction:part-8", "text": "configuration per-query allows up to 3× delay saving compared to static configurations which are the closest in quality. Every single static configuration choice that achieves comparable delay has at least a 10% quality drop. In spite of the potential benefits, per-query configuration adaptation faces challenges that hinder their real-world adop- tion. Each RAG query comes in plain text with practically no associated RAG configurations. Moreover, the space of configurations grows exponentially with multiple knobs. For example, for a map_reduce configuration, with 30 values for num_chunks and 50 values for intermediate_length leads to 1500 configurations for a query. Exhaustively profiling all configurations per-query and choosing the best is infeasible. Alternatively, if we profile periodically, we lose out on the potential configuration selection for each query, as variance in query profile leads to different quality-delay tradeoffs. Pro- filing cost is also prohibitively expensive as the LLM needs to be run with many synthesis methods, number of chunks etc., which require high GPU usage. Additionally, the delay of profiling can be ∼100× the inference delay due to multiple LLM calls during profiling. Online RAG queries have strin- gent requirements for GPU resource usage and end-to-end delay [70, 76]. This makes it hard to systematically decide what an optimal per-input configuration should be. To truly achieve the benefit of per-query configuration adaptation, we need a smart system to drastically reduce to a useful configuration space, in a fast and cheap manner. 4 METIS: Enabling per-query configuration adaptation for RAG We present METIS, a novel system for serving RAG queries focusing on high generation quality and minimal delay. METIS is a RAG controller (Figure 6) with two main components: Configuration Space Pruning (§ 4.1, 4.2 ) Joint scheduler (§ 4.3) RAG Queries Vector Database GPU Memory Serving LLM RAG Configs Text Chunks Check Resource Status Generated Output Retriever RAG Synthesis Chosen Config Figure 6. METIS consists of a RAG controller which per- forms configuration space pruning and joint scheduling. • Pruning configuration space: We estimate each query’s pro- file (§4.1) and reduce the RAG configuration space to a smaller yet promising one that still yields high generation quality (§4.2) (leading to a 50-100× reduction). • RAG scheduler: Within the pruned configuration space for the query, METIS’ scheduler chooses the best config- uration for the query to achieve the best quality-latency trade-off based on the available system resources (§4.3). Once the configuration is chosen, the METIS’ executes the query using the chosen configuration—retrieving the selected number of chunks and uses the selected synthesis method to feed into the LLM’s input. 4.1 Estimating a query’s profile Query profile: To choose the correct RAG configurations, the first step of METIS is to create the profile of the query (as we see in Figure 7) by querying an LLM (we call this LLM query profiler). We ask the query profiler to estimate four high-level dimensions for each query. • Query complexity refers to the intricacy of the query itself. Queries with less complexity are more like simple yes/no questions, while queries with high complexity are more like why questions, which require", "tokens": 665, "chunk_type": "original-large", "url": "https://arxiv.org/abs/2412.10543", "anchor": "#introduction:part-8", "type": "paper", "title": "", "section": "Introduction", "source": "arxiv_pdf", "published": "", "sha256": "04052c26574ccf1c6894d4cd522e43fbaa41c43502576fb7ac3a3e6cdeb53d0c"}
{"doc_id": "arxiv:2412.10543", "chunk_id": "introduction:part-8-o1", "text": "configuration per-query allows up to 3× delay saving compared to static configurations which are the closest in quality. Every single static configuration choice that achieves comparable delay has at least a 10% quality drop. In spite of the potential benefits, per-query configuration adaptation faces challenges that hinder their real-world adop- tion. Each RAG query comes in plain text with practically no associated RAG configurations. Moreover, the space of configurations grows exponentially with multiple knobs. For example, for a map_reduce configuration, with 30 values for num_chunks and 50 values for intermediate_length leads to 1500 configurations for a query. Exhaustively profiling all configurations per-query and choosing the best is infeasible. Alternatively, if we profile periodically, we lose out on the potential configuration selection for each query, as variance in query profile leads to different quality-delay tradeoffs. Pro- filing cost is also prohibitively expensive as the LLM needs to be run with many synthesis methods, number of chunks etc., which require high GPU usage. Additionally, the delay of profiling can be ∼100× the inference delay due to multiple LLM calls during profiling. Online RAG queries have strin- gent requirements for GPU resource usage and end-to-end delay [70, 76]. This makes it hard to systematically decide what an optimal per-input configuration should be. To truly achieve the benefit of per-query configuration adaptation, we need a smart system to drastically reduce to a useful configuration space, in a fast and cheap manner. 4 METIS: Enabling per-query configuration adaptation for RAG We present METIS, a novel system for serving RAG queries focusing on high generation quality and minimal delay. METIS is a RAG controller (Figure 6) with two main components: Configuration Space Pruning (§ 4.1, 4.2 ) Joint scheduler (§ 4.3) RAG Queries Vector Database GPU Memory Serving LLM RAG Configs Text Chunks Check Resource Status Generated Output Retriever RAG Synthesis Chosen Config Figure 6.", "tokens": 401, "chunk_type": "overlap-1", "url": "https://arxiv.org/abs/2412.10543", "anchor": "#introduction:part-8", "type": "paper", "title": "", "section": "Introduction", "source": "arxiv_pdf", "published": "", "sha256": "04052c26574ccf1c6894d4cd522e43fbaa41c43502576fb7ac3a3e6cdeb53d0c"}
{"doc_id": "arxiv:2412.10543", "chunk_id": "introduction:part-8-o2", "text": "To truly achieve the benefit of per-query configuration adaptation, we need a smart system to drastically reduce to a useful configuration space, in a fast and cheap manner. 4 METIS: Enabling per-query configuration adaptation for RAG We present METIS, a novel system for serving RAG queries focusing on high generation quality and minimal delay. METIS is a RAG controller (Figure 6) with two main components: Configuration Space Pruning (§ 4.1, 4.2 ) Joint scheduler (§ 4.3) RAG Queries Vector Database GPU Memory Serving LLM RAG Configs Text Chunks Check Resource Status Generated Output Retriever RAG Synthesis Chosen Config Figure 6. METIS consists of a RAG controller which per- forms configuration space pruning and joint scheduling. • Pruning configuration space: We estimate each query’s pro- file (§4.1) and reduce the RAG configuration space to a smaller yet promising one that still yields high generation quality (§4.2) (leading to a 50-100× reduction). • RAG scheduler: Within the pruned configuration space for the query, METIS’ scheduler chooses the best config- uration for the query to achieve the best quality-latency trade-off based on the available system resources (§4.3). Once the configuration is chosen, the METIS’ executes the query using the chosen configuration—retrieving the selected number of chunks and uses the selected synthesis method to feed into the LLM’s input. 4.1 Estimating a query’s profile Query profile: To choose the correct RAG configurations, the first step of METIS is to create the profile of the query (as we see in Figure 7) by querying an LLM (we call this LLM query profiler). We ask the query profiler to estimate four high-level dimensions for each query. • Query complexity refers to the intricacy of the query itself. Queries with less complexity are more like simple yes/no questions, while queries with high complexity are more like why questions, which require", "tokens": 393, "chunk_type": "overlap-2", "url": "https://arxiv.org/abs/2412.10543", "anchor": "#introduction:part-8", "type": "paper", "title": "", "section": "Introduction", "source": "arxiv_pdf", "published": "", "sha256": "04052c26574ccf1c6894d4cd522e43fbaa41c43502576fb7ac3a3e6cdeb53d0c"}
{"doc_id": "arxiv:2412.10543", "chunk_id": "introduction:part-9", "text": "choose the correct RAG configurations, the first step of METIS is to create the profile of the query (as we see in Figure 7) by querying an LLM (we call this LLM query profiler). We ask the query profiler to estimate four high-level dimensions for each query. • Query complexity refers to the intricacy of the query itself. Queries with less complexity are more like simple yes/no questions, while queries with high complexity are more like why questions, which require deeper reasoning than yes/no questions. As a result, it requires more LLM com- putation to correctly answer complex queries. The output for this dimension is binary “High/Low” • Joint reasoning requirement describes whether multiple pieces of information are needed to answer the query. Even relatively simple queries may require joint reasoning (e.g., checking whether the annual income from two years is the same). The output for this dimension is binary “Yes/No” • Pieces of information required refers to the distinct, stan- dalone pieces of information required to fully answer the query (e.g., the annual income from how many years is required to draw the trend of annual income). The output for this dimension is a number from 1-10. 5 Query Profiler ( LLM ) § 4.1 Estimate the query complexity How many pieces of information? How much can we summarize? Input Prompt Content Query complexity: High/ Low Needs X pieces of information Summary length: X to Y Rule-based Mapping § 4.2 Query Synthesis", "tokens": 315, "chunk_type": "original", "url": "https://arxiv.org/abs/2412.10543", "anchor": "#introduction:part-9", "type": "paper", "title": "", "section": "Introduction", "source": "arxiv_pdf", "published": "", "sha256": "ced0960f22e0f95a13cb8c636578d5a4be831abe66773826495af351833b97cf"}
{"doc_id": "arxiv:2412.10543", "chunk_id": "method:part-1", "text": "Number of chunks (value) Intermediat e Length (value) Do we need joint reasoning? Joint reasoning: Yes/No Figure 7. METIS RAG configuration selection workflow. • The length of the summarization: If the query is complex and needs a lot of different information, it is often neces- sary to first summarize the relevant information chunks first (to reduce the noise inside these chunks) and then gen- erate the final answer from these summaries. The output for this dimension is a number from 30-200. METIS is not the first to use query profile as a metric for deciding RAG configurations, it extends upon methods like AdaptiveRAG [32] which have used LLM’s to estimate query profile but they only focus on one dimension (the number of chunks to retrieve). In Section 7, we show the impact of each dimension on the overall improvement. Why the query profile could be estimated: Estimating the aforementioned query profile is feasible, not only be- cause of the reasoning power of LLMs3 in analyzing natural language queries, but also because we provide sufficient in- formation to the LLM-based profiler. METIS feeds the profile estimator with not only the query, but also a metadata of the database that contains the background document. The metadata is a short description about the type of con- tent in the database and its data size (chunk_size). Specif- ically, we use a single-line summaries already attached to the original source datasets as the metadata of the dataset. For example, the metadata for the KG RAG Finsec’s database [50] contains quarterly financial reports and questions of Fortune 500 companies with a chunk_size of 1000. It de- scribes the content topics of the chunks with information such as revenue growth indicators, product release informa- tion, sales etc.,. When presented with a query on financials of such a company, the LLM can use the metadata to decide questions like how much to summarize/how much reasoning is required. We give details on the prompt and the intuition to generate metadata for new datasets in Appendix §A. It is important to acknowledge that for highly under- specified queries, it is hard for any model (even human) to reasonably estimate the query’s profile. For an example 3We have tested both GPT and Llama models as the profile query-profiler, and they yield similarly impressive results (§7). query “Compare current US Stock Market trends,” the query profile here does not provide enough information (e.g., how many years should the trend be derived from). To answer such highly under-specified queries, more information about the dataset will unlikely help.4 Moreover, we observed that extra information does not significantly improve the profiler’s estimates. For instance, in theory, it helps to know the embedding algorithm used by RAG. Yet, the embedding models perform similarly overall across queries and datasets under our consideration. This ex- plains their limited contribution to the profiler, though more future work is needed to understand the wider implications. 4.2 Mapping query profiles to RAG configurations After METIS obtains the query profile using the LLM, it per- forms rule-based mapping to generate values for RAG con-", "tokens": 665, "chunk_type": "original-large", "url": "https://arxiv.org/abs/2412.10543", "anchor": "#method:part-1", "type": "paper", "title": "", "section": "Method", "source": "arxiv_pdf", "published": "", "sha256": "43cbd0071ab281e42dc44526049de45b52c2f8e97c3b2d15842d061cdddffb4d"}
{"doc_id": "arxiv:2412.10543", "chunk_id": "method:part-1-o1", "text": "Number of chunks (value) Intermediat e Length (value) Do we need joint reasoning? Joint reasoning: Yes/No Figure 7. METIS RAG configuration selection workflow. • The length of the summarization: If the query is complex and needs a lot of different information, it is often neces- sary to first summarize the relevant information chunks first (to reduce the noise inside these chunks) and then gen- erate the final answer from these summaries. The output for this dimension is a number from 30-200. METIS is not the first to use query profile as a metric for deciding RAG configurations, it extends upon methods like AdaptiveRAG [32] which have used LLM’s to estimate query profile but they only focus on one dimension (the number of chunks to retrieve). In Section 7, we show the impact of each dimension on the overall improvement. Why the query profile could be estimated: Estimating the aforementioned query profile is feasible, not only be- cause of the reasoning power of LLMs3 in analyzing natural language queries, but also because we provide sufficient in- formation to the LLM-based profiler. METIS feeds the profile estimator with not only the query, but also a metadata of the database that contains the background document. The metadata is a short description about the type of con- tent in the database and its data size (chunk_size). Specif- ically, we use a single-line summaries already attached to the original source datasets as the metadata of the dataset. For example, the metadata for the KG RAG Finsec’s database [50] contains quarterly financial reports and questions of Fortune 500 companies with a chunk_size of 1000. It de- scribes the content topics of the chunks with information such as revenue growth indicators, product release informa- tion, sales etc.,.", "tokens": 377, "chunk_type": "overlap-1", "url": "https://arxiv.org/abs/2412.10543", "anchor": "#method:part-1", "type": "paper", "title": "", "section": "Method", "source": "arxiv_pdf", "published": "", "sha256": "43cbd0071ab281e42dc44526049de45b52c2f8e97c3b2d15842d061cdddffb4d"}
{"doc_id": "arxiv:2412.10543", "chunk_id": "method:part-1-o2", "text": "Specif- ically, we use a single-line summaries already attached to the original source datasets as the metadata of the dataset. For example, the metadata for the KG RAG Finsec’s database [50] contains quarterly financial reports and questions of Fortune 500 companies with a chunk_size of 1000. It de- scribes the content topics of the chunks with information such as revenue growth indicators, product release informa- tion, sales etc.,. When presented with a query on financials of such a company, the LLM can use the metadata to decide questions like how much to summarize/how much reasoning is required. We give details on the prompt and the intuition to generate metadata for new datasets in Appendix §A. It is important to acknowledge that for highly under- specified queries, it is hard for any model (even human) to reasonably estimate the query’s profile. For an example 3We have tested both GPT and Llama models as the profile query-profiler, and they yield similarly impressive results (§7). query “Compare current US Stock Market trends,” the query profile here does not provide enough information (e.g., how many years should the trend be derived from). To answer such highly under-specified queries, more information about the dataset will unlikely help.4 Moreover, we observed that extra information does not significantly improve the profiler’s estimates. For instance, in theory, it helps to know the embedding algorithm used by RAG. Yet, the embedding models perform similarly overall across queries and datasets under our consideration. This ex- plains their limited contribution to the profiler, though more future work is needed to understand the wider implications. 4.2 Mapping query profiles to RAG configurations After METIS obtains the query profile using the LLM, it per- forms rule-based mapping to generate values for RAG con-", "tokens": 377, "chunk_type": "overlap-2", "url": "https://arxiv.org/abs/2412.10543", "anchor": "#method:part-1", "type": "paper", "title": "", "section": "Method", "source": "arxiv_pdf", "published": "", "sha256": "43cbd0071ab281e42dc44526049de45b52c2f8e97c3b2d15842d061cdddffb4d"}
{"doc_id": "arxiv:2412.10543", "chunk_id": "method:part-2", "text": "significantly improve the profiler’s estimates. For instance, in theory, it helps to know the embedding algorithm used by RAG. Yet, the embedding models perform similarly overall across queries and datasets under our consideration. This ex- plains their limited contribution to the profiler, though more future work is needed to understand the wider implications. 4.2 Mapping query profiles to RAG configurations After METIS obtains the query profile using the LLM, it per- forms rule-based mapping to generate values for RAG con- figuration knobs (e.g., synthesis_method etc. introduced in §2). based on the query profiler’s outputs. How we map and why the profile helps: To understand the role of query profiles, consider the following examples: • “Who is the current CEO of NVIDIA?” This query is not complex and does not require joint reasoning. Due to the query being simple with no reasoning required and one piece of information (name of CEO). • “Which month had the highest NVIDIA’s stock price the six months from January to June 2024?” This query is simple but still needs to read information jointly, specifically six pieces of information (stock price for every month) • “What are the reasons for NVIDIA’s month-on-month stock price change from January to June 2024” This query is complex and needs to read multiple pieces of information jointly (stock prices, reasons for change etc.) As multiple reasons need to be analyzed here, summarizing all of the in- formation first helps narrow down to relevant information and perform clearer reasoning (why the prices changed). 4Maybe some chat history from the same user will help, but that is beyond the scope of this work. 6 Algorithm 1: Rule based mapping algorithm Input: Query complexity, Joint reasoning required Input: Pieces of information , Summarization length range Result: synthesis_method, num_chunks, intermediate_length 1 if Joint reasoning required == “no” then 2 synthesis_method = map_rerank 3 else 4 if Query complexity == “low” then 5 synthesis_method = stuff 6 else 7 synthesis_method = stuff, map_reduce 8 num_chunks = [Pieces of information , 3× Pieces of information] 9 intermediate_length_range = Summarization length range Algorithm 1 outlines the rule-based mapping process. This mapping is significantly helpful, it improves upon raw pro- filer outputs and converts them to usable RAG configurations. It reduces the cost of the profiler LLM by restricting it to provide short binary decisions only. We decide the range of synthesis_method selections based on two of the profile dimensions estimated in §4.1, i.e., the “Query complexity” and the “Joint reasoning require- ment”. Simple queries that don’t need any reasoning can an- swered with map_rerank while queries that require joint rea- soning need stuff or map_reduce. We then decide the range of values for num_chunks based on the profile dimension of the “Pieces of information required”, i.e., 𝑛—specifically, we set the range of num_chunks to be 1−3 times of 𝑛. We do not directly set num_chunks at𝑛, because it (1) gives some leeway for the retrieval logic (e.g., typically Bert-embedding-based)5 to find necessary information, and (2) provides the room for the scheduler to select the configuration that fits in available", "tokens": 665, "chunk_type": "original-large", "url": "https://arxiv.org/abs/2412.10543", "anchor": "#method:part-2", "type": "paper", "title": "", "section": "Method", "source": "arxiv_pdf", "published": "", "sha256": "f265b10e1abcb4c9a7843379ed75bc814f64bf8bb91b83c17d0688ba17448d1d"}
{"doc_id": "arxiv:2412.10543", "chunk_id": "method:part-2-o1", "text": "significantly improve the profiler’s estimates. For instance, in theory, it helps to know the embedding algorithm used by RAG. Yet, the embedding models perform similarly overall across queries and datasets under our consideration. This ex- plains their limited contribution to the profiler, though more future work is needed to understand the wider implications. 4.2 Mapping query profiles to RAG configurations After METIS obtains the query profile using the LLM, it per- forms rule-based mapping to generate values for RAG con- figuration knobs (e.g., synthesis_method etc. introduced in §2). based on the query profiler’s outputs. How we map and why the profile helps: To understand the role of query profiles, consider the following examples: • “Who is the current CEO of NVIDIA?” This query is not complex and does not require joint reasoning.", "tokens": 171, "chunk_type": "overlap-1", "url": "https://arxiv.org/abs/2412.10543", "anchor": "#method:part-2", "type": "paper", "title": "", "section": "Method", "source": "arxiv_pdf", "published": "", "sha256": "f265b10e1abcb4c9a7843379ed75bc814f64bf8bb91b83c17d0688ba17448d1d"}
{"doc_id": "arxiv:2412.10543", "chunk_id": "method:part-2-o2", "text": "This ex- plains their limited contribution to the profiler, though more future work is needed to understand the wider implications. 4.2 Mapping query profiles to RAG configurations After METIS obtains the query profile using the LLM, it per- forms rule-based mapping to generate values for RAG con- figuration knobs (e.g., synthesis_method etc. introduced in §2). based on the query profiler’s outputs. How we map and why the profile helps: To understand the role of query profiles, consider the following examples: • “Who is the current CEO of NVIDIA?” This query is not complex and does not require joint reasoning.", "tokens": 128, "chunk_type": "overlap-2", "url": "https://arxiv.org/abs/2412.10543", "anchor": "#method:part-2", "type": "paper", "title": "", "section": "Method", "source": "arxiv_pdf", "published": "", "sha256": "f265b10e1abcb4c9a7843379ed75bc814f64bf8bb91b83c17d0688ba17448d1d"}
{"doc_id": "arxiv:2412.10543", "chunk_id": "method:part-2-o3", "text": "Due to the query being simple with no reasoning required and one piece of information (name of CEO). • “Which month had the highest NVIDIA’s stock price the six months from January to June 2024?” This query is simple but still needs to read information jointly, specifically six pieces of information (stock price for every month) • “What are the reasons for NVIDIA’s month-on-month stock price change from January to June 2024” This query is complex and needs to read multiple pieces of information jointly (stock prices, reasons for change etc.) As multiple reasons need to be analyzed here, summarizing all of the in- formation first helps narrow down to relevant information and perform clearer reasoning (why the prices changed). 4Maybe some chat history from the same user will help, but that is beyond the scope of this work. 6 Algorithm 1: Rule based mapping algorithm Input: Query complexity, Joint reasoning required Input: Pieces of information , Summarization length range Result: synthesis_method, num_chunks, intermediate_length 1 if Joint reasoning required == “no” then 2 synthesis_method = map_rerank 3 else 4 if Query complexity == “low” then 5 synthesis_method = stuff 6 else 7 synthesis_method = stuff, map_reduce 8 num_chunks = [Pieces of information , 3× Pieces of information] 9 intermediate_length_range = Summarization length range Algorithm 1 outlines the rule-based mapping process. This mapping is significantly helpful, it improves upon raw pro- filer outputs and converts them to usable RAG configurations. It reduces the cost of the profiler LLM by restricting it to provide short binary decisions only. We decide the range of synthesis_method selections based on two of the profile dimensions estimated in §4.1, i.e., the “Query complexity” and the “Joint reasoning require- ment”. Simple queries that don’t need any reasoning can an- swered with map_rerank while queries that require joint rea- soning need stuff or map_reduce.", "tokens": 397, "chunk_type": "overlap-3", "url": "https://arxiv.org/abs/2412.10543", "anchor": "#method:part-2", "type": "paper", "title": "", "section": "Method", "source": "arxiv_pdf", "published": "", "sha256": "f265b10e1abcb4c9a7843379ed75bc814f64bf8bb91b83c17d0688ba17448d1d"}
{"doc_id": "arxiv:2412.10543", "chunk_id": "method:part-2-o4", "text": "It reduces the cost of the profiler LLM by restricting it to provide short binary decisions only. We decide the range of synthesis_method selections based on two of the profile dimensions estimated in §4.1, i.e., the “Query complexity” and the “Joint reasoning require- ment”. Simple queries that don’t need any reasoning can an- swered with map_rerank while queries that require joint rea- soning need stuff or map_reduce. We then decide the range of values for num_chunks based on the profile dimension of the “Pieces of information required”, i.e., 𝑛—specifically, we set the range of num_chunks to be 1−3 times of 𝑛. We do not directly set num_chunks at𝑛, because it (1) gives some leeway for the retrieval logic (e.g., typically Bert-embedding-based)5 to find necessary information, and (2) provides the room for the scheduler to select the configuration that fits in available", "tokens": 183, "chunk_type": "overlap-4", "url": "https://arxiv.org/abs/2412.10543", "anchor": "#method:part-2", "type": "paper", "title": "", "section": "Method", "source": "arxiv_pdf", "published": "", "sha256": "f265b10e1abcb4c9a7843379ed75bc814f64bf8bb91b83c17d0688ba17448d1d"}
{"doc_id": "arxiv:2412.10543", "chunk_id": "method:part-3", "text": "rea- soning need stuff or map_reduce. We then decide the range of values for num_chunks based on the profile dimension of the “Pieces of information required”, i.e., 𝑛—specifically, we set the range of num_chunks to be 1−3 times of 𝑛. We do not directly set num_chunks at𝑛, because it (1) gives some leeway for the retrieval logic (e.g., typically Bert-embedding-based)5 to find necessary information, and (2) provides the room for the scheduler to select the configuration that fits in available memory. Finally, we get the intermediate_length range from the “summary length” estimate, which is already a value range (derived from the query, metadata and chunk size). Algorithm 1 is central to METIS’ design to reduce to the space to our useful RAG configurations and this is extendable to other RAG configurations. For instance, a particular RAG pipeline might use an external re-ranker [23, 52], query re- writer [36, 51] or perform an external web-search [73] along with database retrieval. The mapping algorithm can map the profiling LLM’s output (e.g., of Query complexity) and be used to guide such decisions for these newer RAG configurations. Additionally, such mapping algorithms greatly reduce the overall inference cost of RAG inference. Attempting to use 5A typical RAG retriever these days will have to retrieve 2-3× more chunks than minimally required to provide sufficient information for the LLM inference [24, 55]. Used GPU Mem (6GB) Used GPU mem (6GB) time time Map 1 (6GB) Map 2 (6GB) Reduce (6GB) Chunk 1, Query Chunk 2, Query Chunk 1, Chunk 2, Query Stuff (12GB) (a) Baseline Separates configuration selection and scheduling In general, \"Stuff\" is faster than \"MapReduce\" as a RAG config Yet, \"Stuff\" is memory-intensive and thus is slower when available GPU RAM is limited Free mem (6GB) (b) Ours performs configuration selection and scheduling jointly Delay saved We select MapReduce as it can readily fits in the current batch Figure 8. METIS joint schedules RAG configurations with available GPU memory (chosen example - map_reduce) the LLM profiler to directly provide the exact RAG configu- ration values does not work. For this, the LLM needs to be regularly retrained for this task to adapt to new configura- tions and will require significantly greater system resources (e.g., GPUs blocked for this). In contrast, METIS uses the LLM to only analyze natural language properties and provide bi- nary decisions, which the mapping algorithm translates to useful configurations with a significantly lower cost. It is important to note that the concept of METIS belongs to an active research trend in the ML and systems community that leverages LLM outputs and mapping functions to guide real system decisions and optimizations, an example of which is LLM routing [13, 31, 56, 59]. While current LLM routers use trained LLMs to map decisions from query complexity to only choose from families of inference models (outside the realm of RAG), we differ by mapping the output to the configuration knob we run for the RAG queries. Like these prior efforts, METIS is a heuristic to best utilize the LLM-generated information to guide system optimiza- tions.", "tokens": 665, "chunk_type": "original-large", "url": "https://arxiv.org/abs/2412.10543", "anchor": "#method:part-3", "type": "paper", "title": "", "section": "Method", "source": "arxiv_pdf", "published": "", "sha256": "c9ffdbe285143ef221b3746bf25e04ff45fb406a69526c91cfc8cdcedcec9d9d"}
{"doc_id": "arxiv:2412.10543", "chunk_id": "method:part-3-o1", "text": "rea- soning need stuff or map_reduce. We then decide the range of values for num_chunks based on the profile dimension of the “Pieces of information required”, i.e., 𝑛—specifically, we set the range of num_chunks to be 1−3 times of 𝑛. We do not directly set num_chunks at𝑛, because it (1) gives some leeway for the retrieval logic (e.g., typically Bert-embedding-based)5 to find necessary information, and (2) provides the room for the scheduler to select the configuration that fits in available memory. Finally, we get the intermediate_length range from the “summary length” estimate, which is already a value range (derived from the query, metadata and chunk size). Algorithm 1 is central to METIS’ design to reduce to the space to our useful RAG configurations and this is extendable to other RAG configurations. For instance, a particular RAG pipeline might use an external re-ranker [23, 52], query re- writer [36, 51] or perform an external web-search [73] along with database retrieval. The mapping algorithm can map the profiling LLM’s output (e.g., of Query complexity) and be used to guide such decisions for these newer RAG configurations. Additionally, such mapping algorithms greatly reduce the overall inference cost of RAG inference. Attempting to use 5A typical RAG retriever these days will have to retrieve 2-3× more chunks than minimally required to provide sufficient information for the LLM inference [24, 55].", "tokens": 293, "chunk_type": "overlap-1", "url": "https://arxiv.org/abs/2412.10543", "anchor": "#method:part-3", "type": "paper", "title": "", "section": "Method", "source": "arxiv_pdf", "published": "", "sha256": "c9ffdbe285143ef221b3746bf25e04ff45fb406a69526c91cfc8cdcedcec9d9d"}
{"doc_id": "arxiv:2412.10543", "chunk_id": "method:part-3-o2", "text": "The mapping algorithm can map the profiling LLM’s output (e.g., of Query complexity) and be used to guide such decisions for these newer RAG configurations. Additionally, such mapping algorithms greatly reduce the overall inference cost of RAG inference. Attempting to use 5A typical RAG retriever these days will have to retrieve 2-3× more chunks than minimally required to provide sufficient information for the LLM inference [24, 55]. Used GPU Mem (6GB) Used GPU mem (6GB) time time Map 1 (6GB) Map 2 (6GB) Reduce (6GB) Chunk 1, Query Chunk 2, Query Chunk 1, Chunk 2, Query Stuff (12GB) (a) Baseline Separates configuration selection and scheduling In general, \"Stuff\" is faster than \"MapReduce\" as a RAG config Yet, \"Stuff\" is memory-intensive and thus is slower when available GPU RAM is limited Free mem (6GB) (b) Ours performs configuration selection and scheduling jointly Delay saved We select MapReduce as it can readily fits in the current batch Figure 8. METIS joint schedules RAG configurations with available GPU memory (chosen example - map_reduce) the LLM profiler to directly provide the exact RAG configu- ration values does not work. For this, the LLM needs to be regularly retrained for this task to adapt to new configura- tions and will require significantly greater system resources (e.g., GPUs blocked for this). In contrast, METIS uses the LLM to only analyze natural language properties and provide bi- nary decisions, which the mapping algorithm translates to useful configurations with a significantly lower cost. It is important to note that the concept of METIS belongs to an active research trend in the ML and systems community that leverages LLM outputs and mapping functions to guide real system decisions and optimizations, an example of which is LLM routing [13, 31, 56, 59].", "tokens": 379, "chunk_type": "overlap-2", "url": "https://arxiv.org/abs/2412.10543", "anchor": "#method:part-3", "type": "paper", "title": "", "section": "Method", "source": "arxiv_pdf", "published": "", "sha256": "c9ffdbe285143ef221b3746bf25e04ff45fb406a69526c91cfc8cdcedcec9d9d"}
{"doc_id": "arxiv:2412.10543", "chunk_id": "method:part-3-o3", "text": "In contrast, METIS uses the LLM to only analyze natural language properties and provide bi- nary decisions, which the mapping algorithm translates to useful configurations with a significantly lower cost. It is important to note that the concept of METIS belongs to an active research trend in the ML and systems community that leverages LLM outputs and mapping functions to guide real system decisions and optimizations, an example of which is LLM routing [13, 31, 56, 59]. While current LLM routers use trained LLMs to map decisions from query complexity to only choose from families of inference models (outside the realm of RAG), we differ by mapping the output to the configuration knob we run for the RAG queries. Like these prior efforts, METIS is a heuristic to best utilize the LLM-generated information to guide system optimiza- tions.", "tokens": 179, "chunk_type": "overlap-3", "url": "https://arxiv.org/abs/2412.10543", "anchor": "#method:part-3", "type": "paper", "title": "", "section": "Method", "source": "arxiv_pdf", "published": "", "sha256": "c9ffdbe285143ef221b3746bf25e04ff45fb406a69526c91cfc8cdcedcec9d9d"}
{"doc_id": "arxiv:2412.10543", "chunk_id": "method:part-4", "text": "functions to guide real system decisions and optimizations, an example of which is LLM routing [13, 31, 56, 59]. While current LLM routers use trained LLMs to map decisions from query complexity to only choose from families of inference models (outside the realm of RAG), we differ by mapping the output to the configuration knob we run for the RAG queries. Like these prior efforts, METIS is a heuristic to best utilize the LLM-generated information to guide system optimiza- tions. While it demonstrates remarkable improvement in practice, more work will be needed to complement it for better interpretability and robustness. 4.3 Joint configuration-scheduling adaptation Once provided with the narrowed range of each RAG con- figuration knob (synthesis_method, num_chunks and intermediate_length), we need to choose a RAG configu- ration, which is aware of the current system resource (GPU memory). If we pick configurations which do not fit in cur- rent memory, it will lead to additional queuing delay waiting for the GPU memory to free up. We have METIS’s pruned configuration space where the quality is high, we now focus on choosing the best configu- ration which fits in memory, without focusing on quality. 7 Why we need to choose the scheduling jointly: We motivate the need for joint scheduling along with the RAG configuration choice in Figure 8. Consider a setup where we tune only one RAG configura- tion knob of synthesis_method. Other knobs num_chunks and intermediate_length are fixed at 20 and 100 respec- tively. Let’s assume both stuff and map_reduce are present in the pruned space. For the scheduling knob, we consider the amount of GPU memory available for the current batch. Consider a baseline system which separates the joint de- cision from the scheduling and picks only the RAG con- figuration knob (synthesis_method). It chooses the stuff configuration knob as it has lower compute requirement, so given enough memory it should be fast. The baseline system in Figure 8 (a) does not consider other jobs in the system and does not evaluate the amount of available resource to make its scheduling decision. Due to its long input length with 20 chunks, stuff turns out to be memory-intensive. If the available GPU memory is low, stuff doesn’t fit in memory and needs to be queued. This ends up with stuff being slow. Jointly considering the available GPU memory with choos- ing the RAG configuration knob avoids this pitfall. For exam- ple, in Figure 8 (b), if the original configuration was stuff, METIS can choose to use map_reduce (based on the current GPU memory available). By doing so, METIS can start putting the mappers which fit in memory, into the current running_batch of requests which fits in the GPU. While map_reduce requires more compute, in this case, it benefits from being able to start execution much faster, as some of the mappers fit in memory. METIS does not need to wait for the GPU memory to free up and changes the configuration aware of system resource, to save delay and achieve a better quality-delay tradeoff. Jointly choosing the configuration knobs: METIS first", "tokens": 665, "chunk_type": "original-large", "url": "https://arxiv.org/abs/2412.10543", "anchor": "#method:part-4", "type": "paper", "title": "", "section": "Method", "source": "arxiv_pdf", "published": "", "sha256": "b418ac55abf8a09f57ef90b9486cc62ef341adac4f029e987777f934d8ea9148"}
{"doc_id": "arxiv:2412.10543", "chunk_id": "method:part-4-o1", "text": "functions to guide real system decisions and optimizations, an example of which is LLM routing [13, 31, 56, 59]. While current LLM routers use trained LLMs to map decisions from query complexity to only choose from families of inference models (outside the realm of RAG), we differ by mapping the output to the configuration knob we run for the RAG queries. Like these prior efforts, METIS is a heuristic to best utilize the LLM-generated information to guide system optimiza- tions. While it demonstrates remarkable improvement in practice, more work will be needed to complement it for better interpretability and robustness. 4.3 Joint configuration-scheduling adaptation Once provided with the narrowed range of each RAG con- figuration knob (synthesis_method, num_chunks and intermediate_length), we need to choose a RAG configu- ration, which is aware of the current system resource (GPU memory). If we pick configurations which do not fit in cur- rent memory, it will lead to additional queuing delay waiting for the GPU memory to free up. We have METIS’s pruned configuration space where the quality is high, we now focus on choosing the best configu- ration which fits in memory, without focusing on quality. 7 Why we need to choose the scheduling jointly: We motivate the need for joint scheduling along with the RAG configuration choice in Figure 8. Consider a setup where we tune only one RAG configura- tion knob of synthesis_method. Other knobs num_chunks and intermediate_length are fixed at 20 and 100 respec- tively. Let’s assume both stuff and map_reduce are present in the pruned space. For the scheduling knob, we consider the amount of GPU memory available for the current batch. Consider a baseline system which separates the joint de- cision from the scheduling and picks only the RAG con- figuration knob (synthesis_method).", "tokens": 383, "chunk_type": "overlap-1", "url": "https://arxiv.org/abs/2412.10543", "anchor": "#method:part-4", "type": "paper", "title": "", "section": "Method", "source": "arxiv_pdf", "published": "", "sha256": "b418ac55abf8a09f57ef90b9486cc62ef341adac4f029e987777f934d8ea9148"}
{"doc_id": "arxiv:2412.10543", "chunk_id": "method:part-4-o2", "text": "Other knobs num_chunks and intermediate_length are fixed at 20 and 100 respec- tively. Let’s assume both stuff and map_reduce are present in the pruned space. For the scheduling knob, we consider the amount of GPU memory available for the current batch. Consider a baseline system which separates the joint de- cision from the scheduling and picks only the RAG con- figuration knob (synthesis_method). It chooses the stuff configuration knob as it has lower compute requirement, so given enough memory it should be fast. The baseline system in Figure 8 (a) does not consider other jobs in the system and does not evaluate the amount of available resource to make its scheduling decision. Due to its long input length with 20 chunks, stuff turns out to be memory-intensive. If the available GPU memory is low, stuff doesn’t fit in memory and needs to be queued. This ends up with stuff being slow. Jointly considering the available GPU memory with choos- ing the RAG configuration knob avoids this pitfall. For exam- ple, in Figure 8 (b), if the original configuration was stuff, METIS can choose to use map_reduce (based on the current GPU memory available). By doing so, METIS can start putting the mappers which fit in memory, into the current running_batch of requests which fits in the GPU. While map_reduce requires more compute, in this case, it benefits from being able to start execution much faster, as some of the mappers fit in memory. METIS does not need to wait for the GPU memory to free up and changes the configuration aware of system resource, to save delay and achieve a better quality-delay tradeoff. Jointly choosing the configuration knobs: METIS first", "tokens": 364, "chunk_type": "overlap-2", "url": "https://arxiv.org/abs/2412.10543", "anchor": "#method:part-4", "type": "paper", "title": "", "section": "Method", "source": "arxiv_pdf", "published": "", "sha256": "b418ac55abf8a09f57ef90b9486cc62ef341adac4f029e987777f934d8ea9148"}
{"doc_id": "arxiv:2412.10543", "chunk_id": "method:part-5", "text": "the mappers which fit in memory, into the current running_batch of requests which fits in the GPU. While map_reduce requires more compute, in this case, it benefits from being able to start execution much faster, as some of the mappers fit in memory. METIS does not need to wait for the GPU memory to free up and changes the configuration aware of system resource, to save delay and achieve a better quality-delay tradeoff. Jointly choosing the configuration knobs: METIS first provides us with a pruned range of configurations. A straw- man solution is to pick a constant value from the across queries. (e.g., the median value of the num_chunks). While this is better than using one static configuration for all queries, it is still sub-optimal as it does not look at the current system resource availability. This prevents us from exploiting the best quality-delay tradeoff across RAG queries. We use a best-fit algorithm to allow for variation in config- urations across queries. We first compute the GPU memory requirement for the RAG query from the RAG configura- tion knobs (e.g., num_chunks) for every configuration in the pruned space. Then, we measure the current available mem- ory on the GPU to see what can fit into the current batch. We then pick the best configuration from the pruned space that fits into the GPU. METIS defines the best configuration as the one with overall highest memory requirement, from all which fit in memory. The insight here is that within the reduced range of good quality configurations, higher mem- ory configurations correspond to expensive configurations (e.g. more number of chunks, higher intermediate length). In general, these configurations should lead to slightly higher quality in the reduced space. For example, if the pruned space says num_chunks is 5-10 and the synthesis_method is stuff and both 5 or 6 chunks can fit in memory, we choose 6 chunks. We don’t pick a configuration that doesn’t fit in GPU, so we would never choose more than 6 chunks. If we do that, the system will queue the request inflating the delay. After choosing the configuration that fits into the current running_batch, the vLLM engine is optimized to perform chunked_prefill. However, even with chunked_prefill, it can only offload parts of long prefill of stuff requests which do not fit in the current batch and still inflates the queuing de- lay. Jointly scheduling RAG configurations enables efficient resource usage, which cannot be obtained by only relying on the output of the LLM profiler. What if none of the configurations fit in the GPU? A main insight for METIS’s design comes from the observation that in general, the RAG-specific focused configurations can be loosely-decoupled from the scheduling-specific configura- tions. METIS tries to fit the best possible configurations into GPU memory after it gets the profiler’s reduced configura- tion space. It can sometimes happen that the current GPU memory availability is too low and none of the profiler’s configurations fit in the currently available GPU. One way we handle this is by falling back to a cheaper fixed configuration and choosing", "tokens": 665, "chunk_type": "original-large", "url": "https://arxiv.org/abs/2412.10543", "anchor": "#method:part-5", "type": "paper", "title": "", "section": "Method", "source": "arxiv_pdf", "published": "", "sha256": "0467499c95c579e49bf99a02f274eeb8ef1eae536adf30d35ef5ee4cdab06f26"}
{"doc_id": "arxiv:2412.10543", "chunk_id": "method:part-5-o1", "text": "the mappers which fit in memory, into the current running_batch of requests which fits in the GPU. While map_reduce requires more compute, in this case, it benefits from being able to start execution much faster, as some of the mappers fit in memory. METIS does not need to wait for the GPU memory to free up and changes the configuration aware of system resource, to save delay and achieve a better quality-delay tradeoff. Jointly choosing the configuration knobs: METIS first provides us with a pruned range of configurations. A straw- man solution is to pick a constant value from the across queries. (e.g., the median value of the num_chunks). While this is better than using one static configuration for all queries, it is still sub-optimal as it does not look at the current system resource availability. This prevents us from exploiting the best quality-delay tradeoff across RAG queries. We use a best-fit algorithm to allow for variation in config- urations across queries. We first compute the GPU memory requirement for the RAG query from the RAG configura- tion knobs (e.g., num_chunks) for every configuration in the pruned space. Then, we measure the current available mem- ory on the GPU to see what can fit into the current batch. We then pick the best configuration from the pruned space that fits into the GPU. METIS defines the best configuration as the one with overall highest memory requirement, from all which fit in memory. The insight here is that within the reduced range of good quality configurations, higher mem- ory configurations correspond to expensive configurations (e.g. more number of chunks, higher intermediate length). In general, these configurations should lead to slightly higher quality in the reduced space.", "tokens": 370, "chunk_type": "overlap-1", "url": "https://arxiv.org/abs/2412.10543", "anchor": "#method:part-5", "type": "paper", "title": "", "section": "Method", "source": "arxiv_pdf", "published": "", "sha256": "0467499c95c579e49bf99a02f274eeb8ef1eae536adf30d35ef5ee4cdab06f26"}
{"doc_id": "arxiv:2412.10543", "chunk_id": "method:part-5-o2", "text": "METIS defines the best configuration as the one with overall highest memory requirement, from all which fit in memory. The insight here is that within the reduced range of good quality configurations, higher mem- ory configurations correspond to expensive configurations (e.g. more number of chunks, higher intermediate length). In general, these configurations should lead to slightly higher quality in the reduced space. For example, if the pruned space says num_chunks is 5-10 and the synthesis_method is stuff and both 5 or 6 chunks can fit in memory, we choose 6 chunks. We don’t pick a configuration that doesn’t fit in GPU, so we would never choose more than 6 chunks. If we do that, the system will queue the request inflating the delay. After choosing the configuration that fits into the current running_batch, the vLLM engine is optimized to perform chunked_prefill. However, even with chunked_prefill, it can only offload parts of long prefill of stuff requests which do not fit in the current batch and still inflates the queuing de- lay. Jointly scheduling RAG configurations enables efficient resource usage, which cannot be obtained by only relying on the output of the LLM profiler. What if none of the configurations fit in the GPU? A main insight for METIS’s design comes from the observation that in general, the RAG-specific focused configurations can be loosely-decoupled from the scheduling-specific configura- tions. METIS tries to fit the best possible configurations into GPU memory after it gets the profiler’s reduced configura- tion space. It can sometimes happen that the current GPU memory availability is too low and none of the profiler’s configurations fit in the currently available GPU. One way we handle this is by falling back to a cheaper fixed configuration and choosing", "tokens": 375, "chunk_type": "overlap-2", "url": "https://arxiv.org/abs/2412.10543", "anchor": "#method:part-5", "type": "paper", "title": "", "section": "Method", "source": "arxiv_pdf", "published": "", "sha256": "0467499c95c579e49bf99a02f274eeb8ef1eae536adf30d35ef5ee4cdab06f26"}
{"doc_id": "arxiv:2412.10543", "chunk_id": "method:part-6", "text": "comes from the observation that in general, the RAG-specific focused configurations can be loosely-decoupled from the scheduling-specific configura- tions. METIS tries to fit the best possible configurations into GPU memory after it gets the profiler’s reduced configura- tion space. It can sometimes happen that the current GPU memory availability is too low and none of the profiler’s configurations fit in the currently available GPU. One way we handle this is by falling back to a cheaper fixed configuration and choosing to ignore the output of the pruning. As we already have access to the query complexity profile and we can pick cheaper configurations, which would meet the requirement for the current query. For instance, if the query doesn’t require joint reason- ing, we can pick a map_rerank configuration with as many chunks that fit into the current GPU memory, ignoring the remaining pruned spaces. If joint reasoning is required, we pick a stuff or map_reduce configurations with the few chunks that fit into memory. We can choose which synthesis method to use once based on the exact memory availability. This allows loose-decoupling of the RAG configurations into a smaller space and then choosing configurations based on system resource availability. This also allows SLO-based constraints on RAG queries if certain queries have strict budgets on their generation latency. 5 Refinements to METIS In spite of it all, it is possible for the profiler to (sometimes) fail and in such cases, it is important to detect if METIS’s profiler fails on a query in a fast manner to prevent it from leading to bad RAG configurations. Also it is useful to decide how to provide feedback to METIS to improve. When is the quality profile reliable? METIS uses LLM to generate the quality profile. Inspired by recent work in use 8 Above threshold - 98% good profiles Above threshold - 96% good profiles 7% below threshold - 90% bad profiles 7% below threshold - 85% bad profiles 90% Threshold Figure 9. Confidence score threshold for different profiler outputs is used to decide when not to use the profiler output. of model confidence [20, 25, 84] as a quality metric, we use confidence scores for METIS’s LLM profiler as to measure the reliability of the profile provided. We obtain the confidence scores from the LLM’s log-probs values on the output (the logarithm of the confidence score, which is directly provided with the output with no extra overhead). We then threshold the confidence score using a confidence score threshold (90% across different datasets) to predict whether the quality profile derived from the quality profiler LLM is actually good (defined as whether the profile can lead to 10% increase in F1-score or 1.5 −2× reduction in delay or both) or not. Such 90% threshold can be tuned for better performance, and we leave it to future work. From Figure 9, we draw two conclusions. First, over 93% of the quality profiles derived from LLM are of high confidence (i.e., over 90%). Further, for those high-confidence profile, over 96% of them are good profiles, meaning that they can", "tokens": 665, "chunk_type": "original-large", "url": "https://arxiv.org/abs/2412.10543", "anchor": "#method:part-6", "type": "paper", "title": "", "section": "Method", "source": "arxiv_pdf", "published": "", "sha256": "a4c4c75ae3c41f03a22c0d711c7eae462cd3dc58039ef3019f9c174db36140e9"}
{"doc_id": "arxiv:2412.10543", "chunk_id": "method:part-6-o1", "text": "comes from the observation that in general, the RAG-specific focused configurations can be loosely-decoupled from the scheduling-specific configura- tions. METIS tries to fit the best possible configurations into GPU memory after it gets the profiler’s reduced configura- tion space. It can sometimes happen that the current GPU memory availability is too low and none of the profiler’s configurations fit in the currently available GPU. One way we handle this is by falling back to a cheaper fixed configuration and choosing to ignore the output of the pruning. As we already have access to the query complexity profile and we can pick cheaper configurations, which would meet the requirement for the current query. For instance, if the query doesn’t require joint reason- ing, we can pick a map_rerank configuration with as many chunks that fit into the current GPU memory, ignoring the remaining pruned spaces. If joint reasoning is required, we pick a stuff or map_reduce configurations with the few chunks that fit into memory. We can choose which synthesis method to use once based on the exact memory availability. This allows loose-decoupling of the RAG configurations into a smaller space and then choosing configurations based on system resource availability. This also allows SLO-based constraints on RAG queries if certain queries have strict budgets on their generation latency. 5 Refinements to METIS In spite of it all, it is possible for the profiler to (sometimes) fail and in such cases, it is important to detect if METIS’s profiler fails on a query in a fast manner to prevent it from leading to bad RAG configurations. Also it is useful to decide how to provide feedback to METIS to improve. When is the quality profile reliable? METIS uses LLM to generate the quality profile.", "tokens": 379, "chunk_type": "overlap-1", "url": "https://arxiv.org/abs/2412.10543", "anchor": "#method:part-6", "type": "paper", "title": "", "section": "Method", "source": "arxiv_pdf", "published": "", "sha256": "a4c4c75ae3c41f03a22c0d711c7eae462cd3dc58039ef3019f9c174db36140e9"}
{"doc_id": "arxiv:2412.10543", "chunk_id": "method:part-6-o2", "text": "This also allows SLO-based constraints on RAG queries if certain queries have strict budgets on their generation latency. 5 Refinements to METIS In spite of it all, it is possible for the profiler to (sometimes) fail and in such cases, it is important to detect if METIS’s profiler fails on a query in a fast manner to prevent it from leading to bad RAG configurations. Also it is useful to decide how to provide feedback to METIS to improve. When is the quality profile reliable? METIS uses LLM to generate the quality profile. Inspired by recent work in use 8 Above threshold - 98% good profiles Above threshold - 96% good profiles 7% below threshold - 90% bad profiles 7% below threshold - 85% bad profiles 90% Threshold Figure 9. Confidence score threshold for different profiler outputs is used to decide when not to use the profiler output. of model confidence [20, 25, 84] as a quality metric, we use confidence scores for METIS’s LLM profiler as to measure the reliability of the profile provided. We obtain the confidence scores from the LLM’s log-probs values on the output (the logarithm of the confidence score, which is directly provided with the output with no extra overhead). We then threshold the confidence score using a confidence score threshold (90% across different datasets) to predict whether the quality profile derived from the quality profiler LLM is actually good (defined as whether the profile can lead to 10% increase in F1-score or 1.5 −2× reduction in delay or both) or not. Such 90% threshold can be tuned for better performance, and we leave it to future work. From Figure 9, we draw two conclusions. First, over 93% of the quality profiles derived from LLM are of high confidence (i.e., over 90%).", "tokens": 386, "chunk_type": "overlap-2", "url": "https://arxiv.org/abs/2412.10543", "anchor": "#method:part-6", "type": "paper", "title": "", "section": "Method", "source": "arxiv_pdf", "published": "", "sha256": "a4c4c75ae3c41f03a22c0d711c7eae462cd3dc58039ef3019f9c174db36140e9"}
{"doc_id": "arxiv:2412.10543", "chunk_id": "method:part-6-o3", "text": "We then threshold the confidence score using a confidence score threshold (90% across different datasets) to predict whether the quality profile derived from the quality profiler LLM is actually good (defined as whether the profile can lead to 10% increase in F1-score or 1.5 −2× reduction in delay or both) or not. Such 90% threshold can be tuned for better performance, and we leave it to future work. From Figure 9, we draw two conclusions. First, over 93% of the quality profiles derived from LLM are of high confidence (i.e., over 90%). Further, for those high-confidence profile, over 96% of them are good profiles, meaning that they can", "tokens": 140, "chunk_type": "overlap-3", "url": "https://arxiv.org/abs/2412.10543", "anchor": "#method:part-6", "type": "paper", "title": "", "section": "Method", "source": "arxiv_pdf", "published": "", "sha256": "a4c4c75ae3c41f03a22c0d711c7eae462cd3dc58039ef3019f9c174db36140e9"}
{"doc_id": "arxiv:2412.10543", "chunk_id": "method:part-7", "text": "actually good (defined as whether the profile can lead to 10% increase in F1-score or 1.5 −2× reduction in delay or both) or not. Such 90% threshold can be tuned for better performance, and we leave it to future work. From Figure 9, we draw two conclusions. First, over 93% of the quality profiles derived from LLM are of high confidence (i.e., over 90%). Further, for those high-confidence profile, over 96% of them are good profiles, meaning that they can be used to improve quality, or reduce latency, or both. To handle those cases where the quality profile is of con- fidence score lower than 90% , METIS will fall back to the pruned configuration space of recent 10 queries. How to improve the profiler over time? METIS improves the query profiler LLM by profiling extra feedback prompt to this LLM. We generate this feedback prompt by generating the most accurate output, which is obtained by performing inference on the most resource-demanding configuration (the map_reduce configuration with a large number of input chunks (30) and a high value of intermediate length (300 tokens)) and then ask the quality profiler LLM what config- uration it should choose based on the query and the most accurate answer to that query. The key insight is that, the most accurate answer to the query provides the quality profiler LLM extra knowledge and thus can be used to further improve its decision. To control the cost of generating feedback prompts, METIS only generates the feedback prompt once every 30 queries and we only keep the last four feedback prompts. The cost of METIS’ LLM quality profiler: For the profiler LLM, we use a larger LLM as compared to the serving LLM Dataset Task Type Input Output Squad Single hop QA 0.4K - 2K 5-10 Musique Multihop QA 1K - 5K 5-20 KG RAG FinSec Doc Level QA 4K - 10K 20-40 QMSUM Summarization QA 4K - 12K 20-60 Table 1. Input and output length (# of tokens) distributions of the RAG datasets used in our evaluation. (7B parameters). Using this has minimal cost, as METIS only runs it on the query itself and in METIS as the query is at least 100× shorter than the context. Using this approach, METIS still saves cost as opposed to using a large LLM for inference (as shown in Section 7). We also show that METIS can use different closed and open-source LLMs as the profiler LLM for pruning and can still provide impressive delay reduction without hurting the accuracy in Section 7. 6 Implementation We implement METIS in about 2K lines of code in Python on top of the state-of-the-art popular LLM serving engine vLLM [41]. For the profiler used for configuration space pruning, we define a class LLMProfiler inheriting OpenAI’s Chat Completion API [61] interface (to invoke GPT-4o) and HuggingaceAPI [81] (to invoke LLama-3.1-70B) as models to profile the queries. We use Cohere-embed-v3.0 [4] as a state-of-the-art em- bedding method. We construct a FAISS [16] index using the IndexFlatL2 interface and perform L2-distance similarity search with index.search(query_embedding, top_k) on", "tokens": 665, "chunk_type": "original-large", "url": "https://arxiv.org/abs/2412.10543", "anchor": "#method:part-7", "type": "paper", "title": "", "section": "Method", "source": "arxiv_pdf", "published": "", "sha256": "54d68378c9c3a0744ece7a9ce8eefe998f03513d2303f3b8431c99e59ecc5090"}
{"doc_id": "arxiv:2412.10543", "chunk_id": "method:part-7-o1", "text": "actually good (defined as whether the profile can lead to 10% increase in F1-score or 1.5 −2× reduction in delay or both) or not. Such 90% threshold can be tuned for better performance, and we leave it to future work. From Figure 9, we draw two conclusions. First, over 93% of the quality profiles derived from LLM are of high confidence (i.e., over 90%). Further, for those high-confidence profile, over 96% of them are good profiles, meaning that they can be used to improve quality, or reduce latency, or both. To handle those cases where the quality profile is of con- fidence score lower than 90% , METIS will fall back to the pruned configuration space of recent 10 queries. How to improve the profiler over time? METIS improves the query profiler LLM by profiling extra feedback prompt to this LLM. We generate this feedback prompt by generating the most accurate output, which is obtained by performing inference on the most resource-demanding configuration (the map_reduce configuration with a large number of input chunks (30) and a high value of intermediate length (300 tokens)) and then ask the quality profiler LLM what config- uration it should choose based on the query and the most accurate answer to that query. The key insight is that, the most accurate answer to the query provides the quality profiler LLM extra knowledge and thus can be used to further improve its decision. To control the cost of generating feedback prompts, METIS only generates the feedback prompt once every 30 queries and we only keep the last four feedback prompts.", "tokens": 343, "chunk_type": "overlap-1", "url": "https://arxiv.org/abs/2412.10543", "anchor": "#method:part-7", "type": "paper", "title": "", "section": "Method", "source": "arxiv_pdf", "published": "", "sha256": "54d68378c9c3a0744ece7a9ce8eefe998f03513d2303f3b8431c99e59ecc5090"}
{"doc_id": "arxiv:2412.10543", "chunk_id": "method:part-7-o2", "text": "We generate this feedback prompt by generating the most accurate output, which is obtained by performing inference on the most resource-demanding configuration (the map_reduce configuration with a large number of input chunks (30) and a high value of intermediate length (300 tokens)) and then ask the quality profiler LLM what config- uration it should choose based on the query and the most accurate answer to that query. The key insight is that, the most accurate answer to the query provides the quality profiler LLM extra knowledge and thus can be used to further improve its decision. To control the cost of generating feedback prompts, METIS only generates the feedback prompt once every 30 queries and we only keep the last four feedback prompts. The cost of METIS’ LLM quality profiler: For the profiler LLM, we use a larger LLM as compared to the serving LLM Dataset Task Type Input Output Squad Single hop QA 0.4K - 2K 5-10 Musique Multihop QA 1K - 5K 5-20 KG RAG FinSec Doc Level QA 4K - 10K 20-40 QMSUM Summarization QA 4K - 12K 20-60 Table 1. Input and output length (# of tokens) distributions of the RAG datasets used in our evaluation. (7B parameters). Using this has minimal cost, as METIS only runs it on the query itself and in METIS as the query is at least 100× shorter than the context. Using this approach, METIS still saves cost as opposed to using a large LLM for inference (as shown in Section 7). We also show that METIS can use different closed and open-source LLMs as the profiler LLM for pruning and can still provide impressive delay reduction without hurting the accuracy in Section 7. 6 Implementation We implement METIS in about 2K lines of code in Python on top of the state-of-the-art popular LLM serving engine vLLM [41].", "tokens": 399, "chunk_type": "overlap-2", "url": "https://arxiv.org/abs/2412.10543", "anchor": "#method:part-7", "type": "paper", "title": "", "section": "Method", "source": "arxiv_pdf", "published": "", "sha256": "54d68378c9c3a0744ece7a9ce8eefe998f03513d2303f3b8431c99e59ecc5090"}
{"doc_id": "arxiv:2412.10543", "chunk_id": "method:part-7-o3", "text": "Using this approach, METIS still saves cost as opposed to using a large LLM for inference (as shown in Section 7). We also show that METIS can use different closed and open-source LLMs as the profiler LLM for pruning and can still provide impressive delay reduction without hurting the accuracy in Section 7. 6 Implementation We implement METIS in about 2K lines of code in Python on top of the state-of-the-art popular LLM serving engine vLLM [41]. For the profiler used for configuration space pruning, we define a class LLMProfiler inheriting OpenAI’s Chat Completion API [61] interface (to invoke GPT-4o) and HuggingaceAPI [81] (to invoke LLama-3.1-70B) as models to profile the queries. We use Cohere-embed-v3.0 [4] as a state-of-the-art em- bedding method. We construct a FAISS [16] index using the IndexFlatL2 interface and perform L2-distance similarity search with index.search(query_embedding, top_k) on", "tokens": 183, "chunk_type": "overlap-3", "url": "https://arxiv.org/abs/2412.10543", "anchor": "#method:part-7", "type": "paper", "title": "", "section": "Method", "source": "arxiv_pdf", "published": "", "sha256": "54d68378c9c3a0744ece7a9ce8eefe998f03513d2303f3b8431c99e59ecc5090"}
{"doc_id": "arxiv:2412.10543", "chunk_id": "method:part-8", "text": "lines of code in Python on top of the state-of-the-art popular LLM serving engine vLLM [41]. For the profiler used for configuration space pruning, we define a class LLMProfiler inheriting OpenAI’s Chat Completion API [61] interface (to invoke GPT-4o) and HuggingaceAPI [81] (to invoke LLama-3.1-70B) as models to profile the queries. We use Cohere-embed-v3.0 [4] as a state-of-the-art em- bedding method. We construct a FAISS [16] index using the IndexFlatL2 interface and perform L2-distance similarity search with index.search(query_embedding, top_k) on the chunk embeddings to retrieve for RAG inference. We use the LLMChain interface from Langchain [8] in order to build efficient implementations of multiple synthesis methods. Finally, we use PyTorch’s [5] library modules support to perform query-level memory profiling and measurement to implement the best-fit scheduling logic and request batching. Particularly, we use pynvml to construct get_free_memory() with its interfaces of nvmlDeviceGetHandleByIndex and nvmlDeviceGetMemoryInfo to measure the amount of GPU memory available. We measure the current num-seqs and num-batched-tokens within vLLM to calculate which con- figuration can be fit into the current batch, based on the GPU availability and the request’s memory requirement. 7", "tokens": 239, "chunk_type": "original", "url": "https://arxiv.org/abs/2412.10543", "anchor": "#method:part-8", "type": "paper", "title": "", "section": "Method", "source": "arxiv_pdf", "published": "", "sha256": "4e33ce4c2d31ea7919623137436843b03789184715abb20c523c90f2af1be49b"}
{"doc_id": "arxiv:2412.10543", "chunk_id": "evaluation:part-1", "text": "The key takeaways from the evaluation are • Lower delay : Across 4 task representative datasets for RAG QA, METIS achieves 1.64 −2.54× lower response delay compared to fixed configurations of comparable quality. • Higher throughput : METIS achieves 1.8 −4.5× higher throughput than RAG serving systems which use fixed configurations reaching similar quality. 9 • Negligible overhead : METIS’ profiler’s delay is negligible compared to the overall delay of the LLM’s RAG inference. 7.1 Setup Models and hardware: : We evaluate METIS on a popular model for LLM inference, specifically the fine-tuned version of Mistral-7B-v3. We also use Llama3.1-70B for additional experiments. All models are fine-tuned such that they can take long contexts (up to 32K and 128K respectively). We apply AWQ-model quantization both models. We use an NVIDIA A40 GPU server with 2 GPUs to benchmark our results. The server is equipped with 384GB of memory and two Intel(R) Xeon(R) Gold 6130 CPUs with Hyper-threading and Turbo Boost enabled by default. We use 1 GPU to serve Mistral-7B-v3 and 2 GPUs to serve Llama3.1-70B. Datasets: We use multiple RAG QA datasets with various query profiles, in order to have task-representative work- loads. Table 1 summarizes their input-output statistics. • Squad [66]: Squad is a single-hop reading comprehension dataset, consisting of questions on articles, where the an- swer to every question is a segment from the correspond- ing reading passage. • Musique [78]: Musique is a multihop QA dataset with reasoning-based questions. It is designated to test LLM’s reasoning ability where one reasoning step critically relies on information from another. • KG RAG FinSec [50]: KG RAG Finsec is part of a Knowledge Graph family of RAG datasets and focuses on financial do- main questions from Fortune 500 companies. This dataset contains quarterly financial reports and queries need to read information for multiple chunks for answering. • QMSUM [93]: QMSUM is a human-annotated query-based multi-domain meeting summarization benchmark designed to test LLM’s reasoning-based summarization capabilities. This dataset contains multiple meeting transcripts and queries to summarize relevant spans of meetings. We build a retrieval database database by splitting the queries’ contexts into fixed-sized chunks using Langchain [8] for the database, with Cohere embed-v3.0 [4] embeddings and FAISS [16] L2-distance similarity search in order to re- trieve relevant chunks for RAG inference. To simulate a real RAG workload, we create a mix of queries from each dataset, and send them to METIS using arrival rates that follow a Poisson distribution. We report the results per dataset. Quality Metric: We adopt the following standard metric to measure the generation quality. • F1-score is used to evaluate the METIS’s serving model’s generated response (defined in §2) It is the most widely adopted metric for evaluating RAG QA tasks [10, 69, 72] System Metrics: We adopt the following system metrics: • Delay is used to measure the generation response delay of the model for every RAG query. We choose this system metric similar to other RAG serving papers [44, 70, 76] • Dollar Cost is used to measure the lower cost of using METIS’s profiler as", "tokens": 665, "chunk_type": "original-large", "url": "https://arxiv.org/abs/2412.10543", "anchor": "#evaluation:part-1", "type": "paper", "title": "", "section": "Evaluation", "source": "arxiv_pdf", "published": "", "sha256": "13da1df748fe2324a50c0df6fca43f3bcb54efc5f5b589682246a768cacdf843"}
{"doc_id": "arxiv:2412.10543", "chunk_id": "evaluation:part-1-o1", "text": "The key takeaways from the evaluation are • Lower delay : Across 4 task representative datasets for RAG QA, METIS achieves 1.64 −2.54× lower response delay compared to fixed configurations of comparable quality. • Higher throughput : METIS achieves 1.8 −4.5× higher throughput than RAG serving systems which use fixed configurations reaching similar quality. 9 • Negligible overhead : METIS’ profiler’s delay is negligible compared to the overall delay of the LLM’s RAG inference. 7.1 Setup Models and hardware: : We evaluate METIS on a popular model for LLM inference, specifically the fine-tuned version of Mistral-7B-v3. We also use Llama3.1-70B for additional experiments. All models are fine-tuned such that they can take long contexts (up to 32K and 128K respectively). We apply AWQ-model quantization both models. We use an NVIDIA A40 GPU server with 2 GPUs to benchmark our results. The server is equipped with 384GB of memory and two Intel(R) Xeon(R) Gold 6130 CPUs with Hyper-threading and Turbo Boost enabled by default. We use 1 GPU to serve Mistral-7B-v3 and 2 GPUs to serve Llama3.1-70B. Datasets: We use multiple RAG QA datasets with various query profiles, in order to have task-representative work- loads. Table 1 summarizes their input-output statistics. • Squad [66]: Squad is a single-hop reading comprehension dataset, consisting of questions on articles, where the an- swer to every question is a segment from the correspond- ing reading passage. • Musique [78]: Musique is a multihop QA dataset with reasoning-based questions. It is designated to test LLM’s reasoning ability where one reasoning step critically relies on information from another. • KG RAG FinSec [50]: KG RAG Finsec is part of a Knowledge Graph family of RAG datasets and focuses on financial do- main questions from Fortune 500 companies.", "tokens": 377, "chunk_type": "overlap-1", "url": "https://arxiv.org/abs/2412.10543", "anchor": "#evaluation:part-1", "type": "paper", "title": "", "section": "Evaluation", "source": "arxiv_pdf", "published": "", "sha256": "13da1df748fe2324a50c0df6fca43f3bcb54efc5f5b589682246a768cacdf843"}
{"doc_id": "arxiv:2412.10543", "chunk_id": "evaluation:part-1-o2", "text": "Table 1 summarizes their input-output statistics. • Squad [66]: Squad is a single-hop reading comprehension dataset, consisting of questions on articles, where the an- swer to every question is a segment from the correspond- ing reading passage. • Musique [78]: Musique is a multihop QA dataset with reasoning-based questions. It is designated to test LLM’s reasoning ability where one reasoning step critically relies on information from another. • KG RAG FinSec [50]: KG RAG Finsec is part of a Knowledge Graph family of RAG datasets and focuses on financial do- main questions from Fortune 500 companies. This dataset contains quarterly financial reports and queries need to read information for multiple chunks for answering. • QMSUM [93]: QMSUM is a human-annotated query-based multi-domain meeting summarization benchmark designed to test LLM’s reasoning-based summarization capabilities. This dataset contains multiple meeting transcripts and queries to summarize relevant spans of meetings. We build a retrieval database database by splitting the queries’ contexts into fixed-sized chunks using Langchain [8] for the database, with Cohere embed-v3.0 [4] embeddings and FAISS [16] L2-distance similarity search in order to re- trieve relevant chunks for RAG inference. To simulate a real RAG workload, we create a mix of queries from each dataset, and send them to METIS using arrival rates that follow a Poisson distribution. We report the results per dataset. Quality Metric: We adopt the following standard metric to measure the generation quality. • F1-score is used to evaluate the METIS’s serving model’s generated response (defined in §2) It is the most widely adopted metric for evaluating RAG QA tasks [10, 69, 72] System Metrics: We adopt the following system metrics: • Delay is used to measure the generation response delay of the model for every RAG query.", "tokens": 375, "chunk_type": "overlap-2", "url": "https://arxiv.org/abs/2412.10543", "anchor": "#evaluation:part-1", "type": "paper", "title": "", "section": "Evaluation", "source": "arxiv_pdf", "published": "", "sha256": "13da1df748fe2324a50c0df6fca43f3bcb54efc5f5b589682246a768cacdf843"}
{"doc_id": "arxiv:2412.10543", "chunk_id": "evaluation:part-1-o3", "text": "Quality Metric: We adopt the following standard metric to measure the generation quality. • F1-score is used to evaluate the METIS’s serving model’s generated response (defined in §2) It is the most widely adopted metric for evaluating RAG QA tasks [10, 69, 72] System Metrics: We adopt the following system metrics: • Delay is used to measure the generation response delay of the model for every RAG query. We choose this system metric similar to other RAG serving papers [44, 70, 76] • Dollar Cost is used to measure the lower cost of using METIS’s profiler as", "tokens": 126, "chunk_type": "overlap-3", "url": "https://arxiv.org/abs/2412.10543", "anchor": "#evaluation:part-1", "type": "paper", "title": "", "section": "Evaluation", "source": "arxiv_pdf", "published": "", "sha256": "13da1df748fe2324a50c0df6fca43f3bcb54efc5f5b589682246a768cacdf843"}
{"doc_id": "arxiv:2412.10543", "chunk_id": "evaluation:part-2", "text": "to evaluate the METIS’s serving model’s generated response (defined in §2) It is the most widely adopted metric for evaluating RAG QA tasks [10, 69, 72] System Metrics: We adopt the following system metrics: • Delay is used to measure the generation response delay of the model for every RAG query. We choose this system metric similar to other RAG serving papers [44, 70, 76] • Dollar Cost is used to measure the lower cost of using METIS’s profiler as compared to using larger serving mod- els with fixed configurations having the closest accuracy. Baselines: We compare METIS with the following baselines. • vLLM: We serve RAG with vLLM with multiple static con- figurations across different queries. • Parrot*: We implement Parrot’s [44] configuration-based batching. Parrot* does not adapt the configuration per query. We compare with Parrot* using fixed RAG configu- rations which achieve the closest quality to us. • AdaptiveRAG*: We implement AdaptiveRAG’s [32], query complexity-based RAG-configuration selection and choose the configuration which maximizes the F1-score, without considering the system resource cost. 7.2 Overall improvement Lower delay without sacrificing generation quality: Fig- ure 10 shows METIS achieves delay reduction 1.64 −2.54× over AdaptiveRAG* with no reduction in F1-score. Over us- ing fixed configurations of similar delay, served with both Parrot* and vLLM, METIS achieves 12 −18% higher F1-score. Higher throughput at lower delay: Figure 11 shows METIS achieves higher throughput compared to fixed config- uration baselines when they choose the fixed-config which achieves the closest quality. Compared to Parrot* and vLLM, METIS achieves 1.8 −4.5× times higher throughput. Understanding METIS’ improvement: METIS’s gains come from jointly selecting the configuration based on the available resource, along with performing scheduling. METIS achieves higher quality than the fixed-config baselines as it is adapts the RAG-configuration per query. It reduces delay by resource-aware scheduling, making it better than fixed configurations which achieve closest quality. METIS achieves higher throughput due to being able to adapt configurations based on resource availability as com- pared to the baselines. Both Parrot* and vLLM schedule fixed RAG-configurations and cannot benefit from delay achieved by adapting the configuration like METIS. Parrot* can im- prove the delay over using fixed configurations with vLLM by 1.4 −1.8× but cannot improve the quality. 7.3 Analyzing the gains from METIS Delay saving: Figure 12 shows the contribution of every component of METIS. We compare with vLLM’s fixed config- uration, which achieves the highest quality (blue bar). Using the profiler’s outputs and choosing the median value every time (orange bar), we achieve 1.4 −1.68× reduction in delay. Next, we see the effect of batching (like Parrot*), by choosing the median value configuration and batching, we achieve 1.1 −1.2× reduction in delay. Finally, METIS achieves even greater delay reduction by 1.45 −1.75× by adapting the con- figuration based on available GPU memory with batching. 10 2.41X faster 16% higher 2.24X faster 12% higher 1.64X faster 15% higher 2.54X faster 18% higher Figure 10. METIS achieves 1.64−2.54× lower delay compared to both best fixed configuration baselines and quality-optimized RAG configuration without sacrificing generation quality. 0 2 4 6", "tokens": 665, "chunk_type": "original-large", "url": "https://arxiv.org/abs/2412.10543", "anchor": "#evaluation:part-2", "type": "paper", "title": "", "section": "Evaluation", "source": "arxiv_pdf", "published": "", "sha256": "a3341a95dece57c34b5e9fe8212acbbcbaab9c8292d87c88aad6389428db4b44"}
{"doc_id": "arxiv:2412.10543", "chunk_id": "evaluation:part-2-o1", "text": "to evaluate the METIS’s serving model’s generated response (defined in §2) It is the most widely adopted metric for evaluating RAG QA tasks [10, 69, 72] System Metrics: We adopt the following system metrics: • Delay is used to measure the generation response delay of the model for every RAG query. We choose this system metric similar to other RAG serving papers [44, 70, 76] • Dollar Cost is used to measure the lower cost of using METIS’s profiler as compared to using larger serving mod- els with fixed configurations having the closest accuracy. Baselines: We compare METIS with the following baselines. • vLLM: We serve RAG with vLLM with multiple static con- figurations across different queries. • Parrot*: We implement Parrot’s [44] configuration-based batching. Parrot* does not adapt the configuration per query. We compare with Parrot* using fixed RAG configu- rations which achieve the closest quality to us. • AdaptiveRAG*: We implement AdaptiveRAG’s [32], query complexity-based RAG-configuration selection and choose the configuration which maximizes the F1-score, without considering the system resource cost. 7.2 Overall improvement Lower delay without sacrificing generation quality: Fig- ure 10 shows METIS achieves delay reduction 1.64 −2.54× over AdaptiveRAG* with no reduction in F1-score. Over us- ing fixed configurations of similar delay, served with both Parrot* and vLLM, METIS achieves 12 −18% higher F1-score. Higher throughput at lower delay: Figure 11 shows METIS achieves higher throughput compared to fixed config- uration baselines when they choose the fixed-config which achieves the closest quality. Compared to Parrot* and vLLM, METIS achieves 1.8 −4.5× times higher throughput. Understanding METIS’ improvement: METIS’s gains come from jointly selecting the configuration based on the available resource, along with performing scheduling. METIS achieves higher quality than the fixed-config baselines as it is adapts the RAG-configuration per query. It reduces delay by resource-aware scheduling, making it better than fixed configurations which achieve closest quality.", "tokens": 404, "chunk_type": "overlap-1", "url": "https://arxiv.org/abs/2412.10543", "anchor": "#evaluation:part-2", "type": "paper", "title": "", "section": "Evaluation", "source": "arxiv_pdf", "published": "", "sha256": "a3341a95dece57c34b5e9fe8212acbbcbaab9c8292d87c88aad6389428db4b44"}
{"doc_id": "arxiv:2412.10543", "chunk_id": "evaluation:part-2-o2", "text": "Compared to Parrot* and vLLM, METIS achieves 1.8 −4.5× times higher throughput. Understanding METIS’ improvement: METIS’s gains come from jointly selecting the configuration based on the available resource, along with performing scheduling. METIS achieves higher quality than the fixed-config baselines as it is adapts the RAG-configuration per query. It reduces delay by resource-aware scheduling, making it better than fixed configurations which achieve closest quality. METIS achieves higher throughput due to being able to adapt configurations based on resource availability as com- pared to the baselines. Both Parrot* and vLLM schedule fixed RAG-configurations and cannot benefit from delay achieved by adapting the configuration like METIS. Parrot* can im- prove the delay over using fixed configurations with vLLM by 1.4 −1.8× but cannot improve the quality. 7.3 Analyzing the gains from METIS Delay saving: Figure 12 shows the contribution of every component of METIS. We compare with vLLM’s fixed config- uration, which achieves the highest quality (blue bar). Using the profiler’s outputs and choosing the median value every time (orange bar), we achieve 1.4 −1.68× reduction in delay. Next, we see the effect of batching (like Parrot*), by choosing the median value configuration and batching, we achieve 1.1 −1.2× reduction in delay. Finally, METIS achieves even greater delay reduction by 1.45 −1.75× by adapting the con- figuration based on available GPU memory with batching. 10 2.41X faster 16% higher 2.24X faster 12% higher 1.64X faster 15% higher 2.54X faster 18% higher Figure 10. METIS achieves 1.64−2.54× lower delay compared to both best fixed configuration baselines and quality-optimized RAG configuration without sacrificing generation quality. 0 2 4 6", "tokens": 344, "chunk_type": "overlap-2", "url": "https://arxiv.org/abs/2412.10543", "anchor": "#evaluation:part-2", "type": "paper", "title": "", "section": "Evaluation", "source": "arxiv_pdf", "published": "", "sha256": "a3341a95dece57c34b5e9fe8212acbbcbaab9c8292d87c88aad6389428db4b44"}
{"doc_id": "arxiv:2412.10543", "chunk_id": "evaluation:part-3", "text": "by choosing the median value configuration and batching, we achieve 1.1 −1.2× reduction in delay. Finally, METIS achieves even greater delay reduction by 1.45 −1.75× by adapting the con- figuration based on available GPU memory with batching. 10 2.41X faster 16% higher 2.24X faster 12% higher 1.64X faster 15% higher 2.54X faster 18% higher Figure 10. METIS achieves 1.64−2.54× lower delay compared to both best fixed configuration baselines and quality-optimized RAG configuration without sacrificing generation quality. 0 2 4 6 8 2 4 6 Average Delay (s) Dataset : KG RAG FinSec 0 2 4 6 8 2 4 6 Dataset: Musique 0 2 4 6 8 1 2 3 Dataset: Squad 0 2 4 6 8 5 10 Datset: QMSUM Average Queries per Second METIS (w/ adapted RAG config and batching) Parrot * (w/ fixed RAG config) vLLM (w/ fixed RAG config) Figure 11. METIS achieves 1.8 −4.5× higher throughput (at 1.8 seconds) than baselines which use fixed configurations of closest (not higher) quality. 1.68x 1.2x 1.75x 1.4x 1.1x 1.45x Figure 12. Understanding the delay improvement in METIS Better Better Figure 13. Even with increasing the inference model size, fixed configurations have 2.38 −6.8× higher cost and lower quality compared to METIS. Cost saving: Figure 13 shows METIS (including its pro- filer) has significant lower dollar cost and higher F1-score, compared to choosing the best fixed configuration, with increasing model complexity. The cost of using a (LLama3- 70B) inference model with vLLM and a fixed configuration 6% increase 4% increase Figure 14. Improvement for METIS using feedback from the output helps improve the F1-score by 4 −6%. 0 2 4 6 0.3 0.4 0.5 F1 Score Dataset: Musique 0 3 6 9 0.3 0.4 0.5 Dataset: QMSUM Average Delay (s) METIS Parrot* AdaptiveRAG* vLLM Figure 15. METIS achieves lower delay by 2.1 −2.4× at the same quality even with a larger inference LLM. is higher by 2.38× times while also having a lower F1-score of 6.5% times across datasets. Even more powerful inference models like GPT-4o fail to achieve the same F1-score with fixed configurations but have a much higher cost of 6.8×. Profiler feedback-based improvement: In Figure 14 we show the effect of the golden-configuration-based feedback to the profiler in order to improve its output. We use a 350 11 vLLM (fixed config) vLLM (change num_chunks) vLLM (change num_chunks + synthesis_method) vLLM (change num_chunks + synthesis_method + intermediate_length) METIS (change num_chunks + synthesis_method + intermediate_length + scheduling) Figure 16. Breakdown analysis: By tuning more knobs in METIS, we can see better quality-delay tradeoffs. 0 1 2 3 0.4 0.6 F1 Score Dataset: KG RAG FinSec 0 0.3 0.6 0.9 0.42 0.50 0.58 Dataset: Squad Average Delay (s) METIS Parrot* AdaptiveRAG* vLLM Figure 17. METIS’ performance gains remain substantial even with a smaller, open-source LLM profiler. Dataset: QMSUM Dataset: KG RAG FinSec Dataset: Musique Dataset: Squad Figure 18. METIS’ profiler delay is at most 1/10th of end-to- end response delay across queries from all datasets. query sample for the QMSUM and KG RAG FinSec dataset as the workload. We see that", "tokens": 665, "chunk_type": "original-large", "url": "https://arxiv.org/abs/2412.10543", "anchor": "#evaluation:part-3", "type": "paper", "title": "", "section": "Evaluation", "source": "arxiv_pdf", "published": "", "sha256": "ca33962a19de3919fb9d9cffa1f61eafc15cfa62ce7dd0cd7b083a1226578b80"}
{"doc_id": "arxiv:2412.10543", "chunk_id": "evaluation:part-3-o1", "text": "by choosing the median value configuration and batching, we achieve 1.1 −1.2× reduction in delay. Finally, METIS achieves even greater delay reduction by 1.45 −1.75× by adapting the con- figuration based on available GPU memory with batching. 10 2.41X faster 16% higher 2.24X faster 12% higher 1.64X faster 15% higher 2.54X faster 18% higher Figure 10. METIS achieves 1.64−2.54× lower delay compared to both best fixed configuration baselines and quality-optimized RAG configuration without sacrificing generation quality. 0 2 4 6 8 2 4 6 Average Delay (s) Dataset : KG RAG FinSec 0 2 4 6 8 2 4 6 Dataset: Musique 0 2 4 6 8 1 2 3 Dataset: Squad 0 2 4 6 8 5 10 Datset: QMSUM Average Queries per Second METIS (w/ adapted RAG config and batching) Parrot * (w/ fixed RAG config) vLLM (w/ fixed RAG config) Figure 11. METIS achieves 1.8 −4.5× higher throughput (at 1.8 seconds) than baselines which use fixed configurations of closest (not higher) quality. 1.68x 1.2x 1.75x 1.4x 1.1x 1.45x Figure 12. Understanding the delay improvement in METIS Better Better Figure 13. Even with increasing the inference model size, fixed configurations have 2.38 −6.8× higher cost and lower quality compared to METIS. Cost saving: Figure 13 shows METIS (including its pro- filer) has significant lower dollar cost and higher F1-score, compared to choosing the best fixed configuration, with increasing model complexity. The cost of using a (LLama3- 70B) inference model with vLLM and a fixed configuration 6% increase 4% increase Figure 14. Improvement for METIS using feedback from the output helps improve the F1-score by 4 −6%. 0 2 4 6 0.3 0.4 0.5 F1 Score Dataset: Musique 0 3 6 9 0.3 0.4 0.5 Dataset: QMSUM Average Delay (s) METIS Parrot* AdaptiveRAG* vLLM Figure 15.", "tokens": 386, "chunk_type": "overlap-1", "url": "https://arxiv.org/abs/2412.10543", "anchor": "#evaluation:part-3", "type": "paper", "title": "", "section": "Evaluation", "source": "arxiv_pdf", "published": "", "sha256": "ca33962a19de3919fb9d9cffa1f61eafc15cfa62ce7dd0cd7b083a1226578b80"}
{"doc_id": "arxiv:2412.10543", "chunk_id": "evaluation:part-3-o2", "text": "The cost of using a (LLama3- 70B) inference model with vLLM and a fixed configuration 6% increase 4% increase Figure 14. Improvement for METIS using feedback from the output helps improve the F1-score by 4 −6%. 0 2 4 6 0.3 0.4 0.5 F1 Score Dataset: Musique 0 3 6 9 0.3 0.4 0.5 Dataset: QMSUM Average Delay (s) METIS Parrot* AdaptiveRAG* vLLM Figure 15. METIS achieves lower delay by 2.1 −2.4× at the same quality even with a larger inference LLM. is higher by 2.38× times while also having a lower F1-score of 6.5% times across datasets. Even more powerful inference models like GPT-4o fail to achieve the same F1-score with fixed configurations but have a much higher cost of 6.8×. Profiler feedback-based improvement: In Figure 14 we show the effect of the golden-configuration-based feedback to the profiler in order to improve its output. We use a 350 11 vLLM (fixed config) vLLM (change num_chunks) vLLM (change num_chunks + synthesis_method) vLLM (change num_chunks + synthesis_method + intermediate_length) METIS (change num_chunks + synthesis_method + intermediate_length + scheduling) Figure 16. Breakdown analysis: By tuning more knobs in METIS, we can see better quality-delay tradeoffs. 0 1 2 3 0.4 0.6 F1 Score Dataset: KG RAG FinSec 0 0.3 0.6 0.9 0.42 0.50 0.58 Dataset: Squad Average Delay (s) METIS Parrot* AdaptiveRAG* vLLM Figure 17. METIS’ performance gains remain substantial even with a smaller, open-source LLM profiler. Dataset: QMSUM Dataset: KG RAG FinSec Dataset: Musique Dataset: Squad Figure 18. METIS’ profiler delay is at most 1/10th of end-to- end response delay across queries from all datasets. query sample for the QMSUM and KG RAG FinSec dataset as the workload. We see that", "tokens": 364, "chunk_type": "overlap-2", "url": "https://arxiv.org/abs/2412.10543", "anchor": "#evaluation:part-3", "type": "paper", "title": "", "section": "Evaluation", "source": "arxiv_pdf", "published": "", "sha256": "ca33962a19de3919fb9d9cffa1f61eafc15cfa62ce7dd0cd7b083a1226578b80"}
{"doc_id": "arxiv:2412.10543", "chunk_id": "evaluation:part-4", "text": "Score Dataset: KG RAG FinSec 0 0.3 0.6 0.9 0.42 0.50 0.58 Dataset: Squad Average Delay (s) METIS Parrot* AdaptiveRAG* vLLM Figure 17. METIS’ performance gains remain substantial even with a smaller, open-source LLM profiler. Dataset: QMSUM Dataset: KG RAG FinSec Dataset: Musique Dataset: Squad Figure 18. METIS’ profiler delay is at most 1/10th of end-to- end response delay across queries from all datasets. query sample for the QMSUM and KG RAG FinSec dataset as the workload. We see that with the feedback mechanism (blue line), the F1-score improves by 4 −6% as compared to not having feedback (red line) from the outputs of the golden configuration. We ensure that the feedback mechanism can- not result in the output of very expensive configurations, as METIS’ joint scheduler will not pick increasingly expensive configurations based on the GPU resource constraint. 7.4 Sensitivity analysis Changing the inference LLM: Figure 15 shows the out- come of changing the inference LLM to a larger LLM (Llama3.1- 70B) on the Musique and QMSUM datasets. Even with a more powerful LLM, METIS achieves 2.1 −2.4× lower delay than AdaptiveRAG* at a similar F1-score. The best fixed- configuration baselines such as Parrot* and vLLM have a lower F1-score of 7 −10%. In RAG, models mainly rely on the external context to answer the question instead of the model weights and we only get a 2% improvement in F1-score compared to the smaller inference models. Incrementally tuning knobs in METIS: In Figure 16, we show the benefit we the improvement we get by incremen- tally adding more knobs to METIS. We measure this for the QMSUM dataset with the original Mistral-7B-v3 model. We first only tune the num_chunks (red point). Progressively we tune the RAG-configuration knobs of synthesis_method and intermediate_length and scheduling. We achieve 5, 4, 3% higher F1-Score compared to vLLM. Finally, by adding the scheduling, 2.8× lower delay reduction in delay. Changing the profiler LLM: Figure 17 shows the effect of changing the LLM profiler from GPT-4o to a smaller Llama3.1-70B model. METIS with the new profiler, still achieves 1.4 −2.1× over AdaptiveRAG* with a similar F1-score. Static configurations of Parrot* and vLLM which achieve similar delay, METIS achieves 10 −14% higher F1-score. Changing the embedding algorithm: METIS picks a state- of-art retrieval algorithm Cohere-embed-v3.0 [4]. Using two other popular retrieval algorithms All-mpnet-base-v2 [67] and text-embedding-3-large-256 [18], the F1-score change is within 1%. The delay has no measurable difference as the retrieval is > 100× faster than LLM synthesis [6]. Delay overhead of METIS’s per-query profiling: We show the negligible delay overhead of using an LLM profiler within METIS. Figure 18 shows the fraction of METIS’ pro- filer of the total end-to-end delay. Using the profiler at most adds 0.1 fraction and in the average case only adds 0.03−0.06 fraction to the total delay across queries from all datasets. 8", "tokens": 616, "chunk_type": "original-large", "url": "https://arxiv.org/abs/2412.10543", "anchor": "#evaluation:part-4", "type": "paper", "title": "", "section": "Evaluation", "source": "arxiv_pdf", "published": "", "sha256": "a939e2012d34d87ac309466557f752ee1be1e304b90b7147d78fb64cc0f6addc"}
{"doc_id": "arxiv:2412.10543", "chunk_id": "evaluation:part-4-o1", "text": "Score Dataset: KG RAG FinSec 0 0.3 0.6 0.9 0.42 0.50 0.58 Dataset: Squad Average Delay (s) METIS Parrot* AdaptiveRAG* vLLM Figure 17. METIS’ performance gains remain substantial even with a smaller, open-source LLM profiler. Dataset: QMSUM Dataset: KG RAG FinSec Dataset: Musique Dataset: Squad Figure 18. METIS’ profiler delay is at most 1/10th of end-to- end response delay across queries from all datasets. query sample for the QMSUM and KG RAG FinSec dataset as the workload. We see that with the feedback mechanism (blue line), the F1-score improves by 4 −6% as compared to not having feedback (red line) from the outputs of the golden configuration. We ensure that the feedback mechanism can- not result in the output of very expensive configurations, as METIS’ joint scheduler will not pick increasingly expensive configurations based on the GPU resource constraint. 7.4 Sensitivity analysis Changing the inference LLM: Figure 15 shows the out- come of changing the inference LLM to a larger LLM (Llama3.1- 70B) on the Musique and QMSUM datasets. Even with a more powerful LLM, METIS achieves 2.1 −2.4× lower delay than AdaptiveRAG* at a similar F1-score. The best fixed- configuration baselines such as Parrot* and vLLM have a lower F1-score of 7 −10%. In RAG, models mainly rely on the external context to answer the question instead of the model weights and we only get a 2% improvement in F1-score compared to the smaller inference models. Incrementally tuning knobs in METIS: In Figure 16, we show the benefit we the improvement we get by incremen- tally adding more knobs to METIS. We measure this for the QMSUM dataset with the original Mistral-7B-v3 model. We first only tune the num_chunks (red point). Progressively we tune the RAG-configuration knobs of synthesis_method and intermediate_length and scheduling. We achieve 5, 4, 3% higher F1-Score compared to vLLM.", "tokens": 395, "chunk_type": "overlap-1", "url": "https://arxiv.org/abs/2412.10543", "anchor": "#evaluation:part-4", "type": "paper", "title": "", "section": "Evaluation", "source": "arxiv_pdf", "published": "", "sha256": "a939e2012d34d87ac309466557f752ee1be1e304b90b7147d78fb64cc0f6addc"}
{"doc_id": "arxiv:2412.10543", "chunk_id": "evaluation:part-4-o2", "text": "Incrementally tuning knobs in METIS: In Figure 16, we show the benefit we the improvement we get by incremen- tally adding more knobs to METIS. We measure this for the QMSUM dataset with the original Mistral-7B-v3 model. We first only tune the num_chunks (red point). Progressively we tune the RAG-configuration knobs of synthesis_method and intermediate_length and scheduling. We achieve 5, 4, 3% higher F1-Score compared to vLLM. Finally, by adding the scheduling, 2.8× lower delay reduction in delay. Changing the profiler LLM: Figure 17 shows the effect of changing the LLM profiler from GPT-4o to a smaller Llama3.1-70B model. METIS with the new profiler, still achieves 1.4 −2.1× over AdaptiveRAG* with a similar F1-score. Static configurations of Parrot* and vLLM which achieve similar delay, METIS achieves 10 −14% higher F1-score. Changing the embedding algorithm: METIS picks a state- of-art retrieval algorithm Cohere-embed-v3.0 [4]. Using two other popular retrieval algorithms All-mpnet-base-v2 [67] and text-embedding-3-large-256 [18], the F1-score change is within 1%. The delay has no measurable difference as the retrieval is > 100× faster than LLM synthesis [6]. Delay overhead of METIS’s per-query profiling: We show the negligible delay overhead of using an LLM profiler within METIS. Figure 18 shows the fraction of METIS’ pro- filer of the total end-to-end delay. Using the profiler at most adds 0.1 fraction and in the average case only adds 0.03−0.06 fraction to the total delay across queries from all datasets. 8", "tokens": 308, "chunk_type": "overlap-2", "url": "https://arxiv.org/abs/2412.10543", "anchor": "#evaluation:part-4", "type": "paper", "title": "", "section": "Evaluation", "source": "arxiv_pdf", "published": "", "sha256": "a939e2012d34d87ac309466557f752ee1be1e304b90b7147d78fb64cc0f6addc"}
{"doc_id": "arxiv:2412.10543", "chunk_id": "related-work", "text": "Systems for serving RAG: Several systems have been proposed for RAG [2, 17, 32, 34, 37, 40, 44, 54, 76, 87, 90] which focus on improving retrieval using complex, iterative retrieval algorithms or on serving model selection. METIS can work in conjunction with such systems as METIS focuses on optimizing quality and serving latency, independent of how the retrieval algorithm identifies chunks for retrieval. KV cache storage and retrieval: Storing and reusing KV cache across different requests have been commonly studied in recent work [2, 14, 22, 29, 33, 41, 46, 48, 49, 63, 75, 86, 92]. METIS can work alongside these systems, where instead of retrieving chunks, it can retrieve the KV Caches for generat- ing the output. In RAG, some additional optimizations are needed to combine KV Caches of different chunks that don’t share a common prefix. This is important as the trivial con- catenation of KV Caches loses important cross-attention and reasoning between chunks. These optimizations are enabled by KV Cache blending-based approaches [9, 26, 30, 38, 80, 85]. However RAG workloads have a large number of related contexts across queries and storing all the KV Cache is ex- tremely expensive. We do not measure the KV Cache reuse ratio across queries and leave it for future work. 12 Prefill-Decode Optimizations: Several systems have pro- posed optimizations to speed-up prefill and decode for LLMs by leveraging unique properties of each phase [3, 11, 35, 65, 74, 82, 94, 95]. Notable techniques include chunked-prefill which allows interleaving prefill and decode requests and dis- aggregated prefill which separates compute nodes for prefill and decode. All of these optimizations enable faster genera- tion speed but don’t focus on generation quality. METIS can be applied with such LLM serving systems optimizations. 9 Limitations METIS is currently designed to work with commonly de- ployed RAG pipelines. New research directions in RAG [17, 89] have developed further complex pipelines with more agents and stages for deep chain-of-thought RAG workloads. These pipelines improve on complex workloads but achieve similar performance on all the commonly used RAG QA workloads we consider [1]. We leave METIS’ design exten- sion to chain-of-thought pipelines to future work. 10", "tokens": 468, "chunk_type": "original", "url": "https://arxiv.org/abs/2412.10543", "anchor": "#related-work", "type": "paper", "title": "", "section": "Related work", "source": "arxiv_pdf", "published": "", "sha256": "c380f8272437143b26e7b663f2125f95b79c14c412b1af0073a83a58bddab78a"}
{"doc_id": "arxiv:2412.10543", "chunk_id": "conclusion", "text": "This paper introduces METIS, the first system that focuses on optimizing the tradeoffs between response delay and generation quality in RAG, by by jointly scheduling RAG queries and adapting key configurations on a per-query basis. Evaluation on four datasets shows that METIS outperforms the state-of-the-art, reducing generation latency by 1.64 − 2.54× without compromising response quality.", "tokens": 72, "chunk_type": "original", "url": "https://arxiv.org/abs/2412.10543", "anchor": "#conclusion", "type": "paper", "title": "", "section": "Conclusion", "source": "arxiv_pdf", "published": "", "sha256": "bdc2d24ec1b0123b44e3245a244cf152936dee29fdb2ccddfe2687d6adb2b70f"}
{"doc_id": "blog:news.microsoft.com", "chunk_id": "body:part-1", "text": "--- title: AI agents — what they are, and how they'll change the way we work - Source author: Wp-Block-Co-Authors-Plus-Coauthors Is-Layout-Flow url: https://news.microsoft.com/source/features/ai/ai-agents-what-they-are-and-how-theyll-change-the-way-we-work/ hostname: microsoft.com description: AI agents take the power of generative AI a step further by working alongside you or even on your behalf, and they can be built and used by anyone. sitename: Source date: 2024-11-21 --- AI agents — what they are, and how they’ll change the way we work It’s Monday morning, the caffeine hasn’t kicked in yet, and you have a busy day ahead: Maybe you have piles of returns or new shipping invoices to review, or you need to get the latest updates out to your field technicians or help employees get more efficient IT support. Now you can get help with all of this and more by simply asking an AI agent to take care of it — while you drink a second cup of coffee and focus on your team’s long-term strategy. An agent can tackle certain tasks with you or for you, from acting as a virtual project manager to handling more complex assignments like reconciling financial statements to close the books. Microsoft 365 Copilot is already a personal assistant that helps with everything from tedious daily duties to jumpstarting creative projects. Using it to interact with various agents brings a new world of possibilities for organizations to empower their employees, drive business and accomplish even more. Agents can operate around the clock to review and approve customer returns or go over shipping invoices to help businesses avoid costly supply-chain errors. They can reason over reams of product information to give field technicians step-by-step instructions or use context and memory to open and close tickets for an IT help desk. “Think of agents as the new apps for an AI-powered world,” says Jared Spataro, Microsoft’s chief marketing officer for AI at Work. “We’re rapidly adding new capabilities to tackle individuals’ biggest pain points at work and drive real business results.” What are agents, anyway? An agent takes the power of generative AI a step further, because instead of just assisting you, agents can work alongside you or even on your behalf. Agents can do a range of things, from responding to questions to more complicated or multistep assignments. What sets them apart from a personal assistant is that they can be tailored to have a particular expertise. For example, you could create an agent to know everything about your company’s product catalog so it can draft detailed responses to customer questions or automatically compile product details for an upcoming presentation. Other agents can do even more, acting on your behalf, like one that helps fulfill sales orders — freeing you up to focus on building new customer relationships. Having agents handle some of these routine needs can boost productivity across industries, from manufacturing and research to finance and retail, helping businesses save time and money. You can use ready-made agents in Microsoft 365 and Dynamics 365, or build custom agents to help with more specific needs in Copilot Studio. Imagine you’re a", "tokens": 665, "chunk_type": "original-large", "url": "https://news.microsoft.com/source/features/ai/ai-agents-what-they-are-and-how-theyll-change-the-way-we-work/", "anchor": "#body:part-1", "type": "blog", "title": "", "section": "Body", "source": "blog:news.microsoft.com", "published": "", "sha256": "30cab09af14e484587feb10a255be4320cd8752e60dcfc4816dc64ecdbcae086"}
{"doc_id": "blog:news.microsoft.com", "chunk_id": "body:part-1-o1", "text": "--- title: AI agents — what they are, and how they'll change the way we work - Source author: Wp-Block-Co-Authors-Plus-Coauthors Is-Layout-Flow url: https://news.microsoft.com/source/features/ai/ai-agents-what-they-are-and-how-theyll-change-the-way-we-work/ hostname: microsoft.com description: AI agents take the power of generative AI a step further by working alongside you or even on your behalf, and they can be built and used by anyone. sitename: Source date: 2024-11-21 --- AI agents — what they are, and how they’ll change the way we work It’s Monday morning, the caffeine hasn’t kicked in yet, and you have a busy day ahead: Maybe you have piles of returns or new shipping invoices to review, or you need to get the latest updates out to your field technicians or help employees get more efficient IT support. Now you can get help with all of this and more by simply asking an AI agent to take care of it — while you drink a second cup of coffee and focus on your team’s long-term strategy. An agent can tackle certain tasks with you or for you, from acting as a virtual project manager to handling more complex assignments like reconciling financial statements to close the books. Microsoft 365 Copilot is already a personal assistant that helps with everything from tedious daily duties to jumpstarting creative projects. Using it to interact with various agents brings a new world of possibilities for organizations to empower their employees, drive business and accomplish even more. Agents can operate around the clock to review and approve customer returns or go over shipping invoices to help businesses avoid costly supply-chain errors.", "tokens": 339, "chunk_type": "overlap-1", "url": "https://news.microsoft.com/source/features/ai/ai-agents-what-they-are-and-how-theyll-change-the-way-we-work/", "anchor": "#body:part-1", "type": "blog", "title": "", "section": "Body", "source": "blog:news.microsoft.com", "published": "", "sha256": "30cab09af14e484587feb10a255be4320cd8752e60dcfc4816dc64ecdbcae086"}
{"doc_id": "blog:news.microsoft.com", "chunk_id": "body:part-1-o2", "text": "Microsoft 365 Copilot is already a personal assistant that helps with everything from tedious daily duties to jumpstarting creative projects. Using it to interact with various agents brings a new world of possibilities for organizations to empower their employees, drive business and accomplish even more. Agents can operate around the clock to review and approve customer returns or go over shipping invoices to help businesses avoid costly supply-chain errors. They can reason over reams of product information to give field technicians step-by-step instructions or use context and memory to open and close tickets for an IT help desk. “Think of agents as the new apps for an AI-powered world,” says Jared Spataro, Microsoft’s chief marketing officer for AI at Work. “We’re rapidly adding new capabilities to tackle individuals’ biggest pain points at work and drive real business results.” What are agents, anyway? An agent takes the power of generative AI a step further, because instead of just assisting you, agents can work alongside you or even on your behalf. Agents can do a range of things, from responding to questions to more complicated or multistep assignments. What sets them apart from a personal assistant is that they can be tailored to have a particular expertise. For example, you could create an agent to know everything about your company’s product catalog so it can draft detailed responses to customer questions or automatically compile product details for an upcoming presentation. Other agents can do even more, acting on your behalf, like one that helps fulfill sales orders — freeing you up to focus on building new customer relationships. Having agents handle some of these routine needs can boost productivity across industries, from manufacturing and research to finance and retail, helping businesses save time and money.", "tokens": 380, "chunk_type": "overlap-2", "url": "https://news.microsoft.com/source/features/ai/ai-agents-what-they-are-and-how-theyll-change-the-way-we-work/", "anchor": "#body:part-1", "type": "blog", "title": "", "section": "Body", "source": "blog:news.microsoft.com", "published": "", "sha256": "30cab09af14e484587feb10a255be4320cd8752e60dcfc4816dc64ecdbcae086"}
{"doc_id": "blog:news.microsoft.com", "chunk_id": "body:part-1-o3", "text": "For example, you could create an agent to know everything about your company’s product catalog so it can draft detailed responses to customer questions or automatically compile product details for an upcoming presentation. Other agents can do even more, acting on your behalf, like one that helps fulfill sales orders — freeing you up to focus on building new customer relationships. Having agents handle some of these routine needs can boost productivity across industries, from manufacturing and research to finance and retail, helping businesses save time and money. You can use ready-made agents in Microsoft 365 and Dynamics 365, or build custom agents to help with more specific needs in Copilot Studio. Imagine you’re a", "tokens": 149, "chunk_type": "overlap-3", "url": "https://news.microsoft.com/source/features/ai/ai-agents-what-they-are-and-how-theyll-change-the-way-we-work/", "anchor": "#body:part-1", "type": "blog", "title": "", "section": "Body", "source": "blog:news.microsoft.com", "published": "", "sha256": "30cab09af14e484587feb10a255be4320cd8752e60dcfc4816dc64ecdbcae086"}
{"doc_id": "blog:news.microsoft.com", "chunk_id": "body:part-2", "text": "can do even more, acting on your behalf, like one that helps fulfill sales orders — freeing you up to focus on building new customer relationships. Having agents handle some of these routine needs can boost productivity across industries, from manufacturing and research to finance and retail, helping businesses save time and money. You can use ready-made agents in Microsoft 365 and Dynamics 365, or build custom agents to help with more specific needs in Copilot Studio. Imagine you’re a salesperson with big quarterly goals to meet. Copilot acts as your personal assistant, drafting emails, recapping a meeting you missed and helping you design a polished sales presentation. Meanwhile, an agent specialized in sales lead generation works autonomously in the background to find new prospects you can follow up with later in the week. Copilot partners on daily tasks, and your purpose-built agent uses its customized skills to help you meet your end-of-quarter goals. Agents are not new. Microsoft has done extensive research in the area and even created a multi-agent library last year for developers around the world, work that helped shape what agents can do today. They’re getting more attention now because recent advances in large language models (LLMs) help anyone — even outside the developer community — communicate with AI. That agent-LLM duo makes AI tools more tangibly useful. “People expect AI to do things for them,” not to just generate language, says Ece Kamar, the managing director of Microsoft’s AI Frontiers Lab. “If you want to have a system that can really solve real world problems and help people, that system has to have a good understanding of the world we live in, and when something happens, that system has to perceive that change and take action accordingly.” Agents are like layers on top of the language models that observe and collect information, provide input to the model and together generate an action plan and communicate that to the user — or even act on their own, if permitted. So both agents and models are equally important pieces of the puzzle, as far as generative AI tools go. Agents will become more useful and able to have more autonomy with innovations in their three necessary elements: memory, entitlements and tools. Memory helps provide continuity so that each time you ask for something, it isn’t like starting from scratch. “To be autonomous you have to carry context through a bunch of actions, but the models are very disconnected and don’t have continuity the way we do, so every prompt is in a vacuum and it might pull the wrong memory out,” says Sam Schillace, Microsoft’s deputy chief technology officer. “It’s like you’re watching a stop-motion animation, one isolated frame after another, and your mind puts it into motion. The clay model doesn’t move on its own.” To build up the memory infrastructure to address this, Schillace and his team are working on a process of chunking and chaining. That’s essentially what it sounds like: They’re experimenting with dividing up interactions in bits that can be stored and linked together", "tokens": 665, "chunk_type": "original-large", "url": "https://news.microsoft.com/source/features/ai/ai-agents-what-they-are-and-how-theyll-change-the-way-we-work/", "anchor": "#body:part-2", "type": "blog", "title": "", "section": "Body", "source": "blog:news.microsoft.com", "published": "", "sha256": "55af94d66f2d25283176fd0d8107ba56bb6bba4854c50349f48393badd196e78"}
{"doc_id": "blog:news.microsoft.com", "chunk_id": "body:part-2-o1", "text": "can do even more, acting on your behalf, like one that helps fulfill sales orders — freeing you up to focus on building new customer relationships. Having agents handle some of these routine needs can boost productivity across industries, from manufacturing and research to finance and retail, helping businesses save time and money. You can use ready-made agents in Microsoft 365 and Dynamics 365, or build custom agents to help with more specific needs in Copilot Studio. Imagine you’re a salesperson with big quarterly goals to meet. Copilot acts as your personal assistant, drafting emails, recapping a meeting you missed and helping you design a polished sales presentation. Meanwhile, an agent specialized in sales lead generation works autonomously in the background to find new prospects you can follow up with later in the week. Copilot partners on daily tasks, and your purpose-built agent uses its customized skills to help you meet your end-of-quarter goals. Agents are not new. Microsoft has done extensive research in the area and even created a multi-agent library last year for developers around the world, work that helped shape what agents can do today. They’re getting more attention now because recent advances in large language models (LLMs) help anyone — even outside the developer community — communicate with AI.", "tokens": 276, "chunk_type": "overlap-1", "url": "https://news.microsoft.com/source/features/ai/ai-agents-what-they-are-and-how-theyll-change-the-way-we-work/", "anchor": "#body:part-2", "type": "blog", "title": "", "section": "Body", "source": "blog:news.microsoft.com", "published": "", "sha256": "55af94d66f2d25283176fd0d8107ba56bb6bba4854c50349f48393badd196e78"}
{"doc_id": "blog:news.microsoft.com", "chunk_id": "body:part-2-o2", "text": "Agents are not new. Microsoft has done extensive research in the area and even created a multi-agent library last year for developers around the world, work that helped shape what agents can do today. They’re getting more attention now because recent advances in large language models (LLMs) help anyone — even outside the developer community — communicate with AI. That agent-LLM duo makes AI tools more tangibly useful. “People expect AI to do things for them,” not to just generate language, says Ece Kamar, the managing director of Microsoft’s AI Frontiers Lab. “If you want to have a system that can really solve real world problems and help people, that system has to have a good understanding of the world we live in, and when something happens, that system has to perceive that change and take action accordingly.” Agents are like layers on top of the language models that observe and collect information, provide input to the model and together generate an action plan and communicate that to the user — or even act on their own, if permitted. So both agents and models are equally important pieces of the puzzle, as far as generative AI tools go. Agents will become more useful and able to have more autonomy with innovations in their three necessary elements: memory, entitlements and tools. Memory helps provide continuity so that each time you ask for something, it isn’t like starting from scratch. “To be autonomous you have to carry context through a bunch of actions, but the models are very disconnected and don’t have continuity the way we do, so every prompt is in a vacuum and it might pull the wrong memory out,” says Sam Schillace, Microsoft’s deputy chief technology officer. “It’s like you’re watching a stop-motion animation, one isolated frame after another, and your mind puts it into motion.", "tokens": 399, "chunk_type": "overlap-2", "url": "https://news.microsoft.com/source/features/ai/ai-agents-what-they-are-and-how-theyll-change-the-way-we-work/", "anchor": "#body:part-2", "type": "blog", "title": "", "section": "Body", "source": "blog:news.microsoft.com", "published": "", "sha256": "55af94d66f2d25283176fd0d8107ba56bb6bba4854c50349f48393badd196e78"}
{"doc_id": "blog:news.microsoft.com", "chunk_id": "body:part-2-o3", "text": "Memory helps provide continuity so that each time you ask for something, it isn’t like starting from scratch. “To be autonomous you have to carry context through a bunch of actions, but the models are very disconnected and don’t have continuity the way we do, so every prompt is in a vacuum and it might pull the wrong memory out,” says Sam Schillace, Microsoft’s deputy chief technology officer. “It’s like you’re watching a stop-motion animation, one isolated frame after another, and your mind puts it into motion. The clay model doesn’t move on its own.” To build up the memory infrastructure to address this, Schillace and his team are working on a process of chunking and chaining. That’s essentially what it sounds like: They’re experimenting with dividing up interactions in bits that can be stored and linked together", "tokens": 179, "chunk_type": "overlap-3", "url": "https://news.microsoft.com/source/features/ai/ai-agents-what-they-are-and-how-theyll-change-the-way-we-work/", "anchor": "#body:part-2", "type": "blog", "title": "", "section": "Body", "source": "blog:news.microsoft.com", "published": "", "sha256": "55af94d66f2d25283176fd0d8107ba56bb6bba4854c50349f48393badd196e78"}
{"doc_id": "blog:news.microsoft.com", "chunk_id": "body:part-3", "text": "memory out,” says Sam Schillace, Microsoft’s deputy chief technology officer. “It’s like you’re watching a stop-motion animation, one isolated frame after another, and your mind puts it into motion. The clay model doesn’t move on its own.” To build up the memory infrastructure to address this, Schillace and his team are working on a process of chunking and chaining. That’s essentially what it sounds like: They’re experimenting with dividing up interactions in bits that can be stored and linked together by relevance for faster access, akin to a memory — like grouping conversations about a certain project so an agent can recall those details when you ask for a status update and not have to search through its entire database. The work with entitlements and tools is making sure agents have secure access to, or are entitled to, information they need in order to accomplish things for you, with your permission — like who your boss is, for example — and to the computer programs they need to take action on your behalf, like Teams and PowerPoint. How to use and build agents for work You can already create and publish agents in Microsoft 365 Copilot that can help you in your daily work as easily as you’d create a spreadsheet or presentation — no coding skills required. You don’t need to be a developer to build agents using Copilot Studio, either. Anyone can connect them to relevant business data such as emails, reports and customer management systems so they can perform tasks and provide insights. And you’ll soon be able to enlist new agents in Microsoft 365 to help with common workflows and tasks. Interpreter in Teams will provide real-time speech-to-speech translation during meetings, for example, and you can opt to have it simulate your voice. The Employee Self-Service Agent will simplify human resource and IT help desk-related tasks like helping workers resolve a laptop issue or find out if they’ve maxed out certain benefits, and it can connect to company systems for further customization in Copilot Studio. Microsoft Dynamics 365 will have agents as well for a range of common business workflows across sales, supply chain, finance and customer service functions. And every SharePoint site will soon come equipped with an agent tailored to your organization’s content that allows employees to quickly tap into these vast knowledge bases and find exactly what they need in seconds, whether it’s project details buried in a workback schedule or a summary of a recent product memo. Developers have even more options. With the new Azure AI Agent Service, you’ll be able to choose from small or large language models to orchestrate, develop and scale agent-powered apps to streamline and automate complex workflows like order processing and customer data synchronization. It provides a software development kit with tools for developing agents, allowing you to efficiently integrate agent capabilities using Visual Studio Code and GitHub. One type of model, OpenAI’s recently announced o1 series, can bring more advanced reasoning capabilities to agents, allowing them to take on more complicated tasks by breaking them down into", "tokens": 665, "chunk_type": "original-large", "url": "https://news.microsoft.com/source/features/ai/ai-agents-what-they-are-and-how-theyll-change-the-way-we-work/", "anchor": "#body:part-3", "type": "blog", "title": "", "section": "Body", "source": "blog:news.microsoft.com", "published": "", "sha256": "47cf1e24f3279396150288276637e6812a40f10d772da2d02ae0470bc033254f"}
{"doc_id": "blog:news.microsoft.com", "chunk_id": "body:part-3-o1", "text": "memory out,” says Sam Schillace, Microsoft’s deputy chief technology officer. “It’s like you’re watching a stop-motion animation, one isolated frame after another, and your mind puts it into motion. The clay model doesn’t move on its own.” To build up the memory infrastructure to address this, Schillace and his team are working on a process of chunking and chaining. That’s essentially what it sounds like: They’re experimenting with dividing up interactions in bits that can be stored and linked together by relevance for faster access, akin to a memory — like grouping conversations about a certain project so an agent can recall those details when you ask for a status update and not have to search through its entire database. The work with entitlements and tools is making sure agents have secure access to, or are entitled to, information they need in order to accomplish things for you, with your permission — like who your boss is, for example — and to the computer programs they need to take action on your behalf, like Teams and PowerPoint. How to use and build agents for work You can already create and publish agents in Microsoft 365 Copilot that can help you in your daily work as easily as you’d create a spreadsheet or presentation — no coding skills required. You don’t need to be a developer to build agents using Copilot Studio, either. Anyone can connect them to relevant business data such as emails, reports and customer management systems so they can perform tasks and provide insights. And you’ll soon be able to enlist new agents in Microsoft 365 to help with common workflows and tasks. Interpreter in Teams will provide real-time speech-to-speech translation during meetings, for example, and you can opt to have it simulate your voice.", "tokens": 386, "chunk_type": "overlap-1", "url": "https://news.microsoft.com/source/features/ai/ai-agents-what-they-are-and-how-theyll-change-the-way-we-work/", "anchor": "#body:part-3", "type": "blog", "title": "", "section": "Body", "source": "blog:news.microsoft.com", "published": "", "sha256": "47cf1e24f3279396150288276637e6812a40f10d772da2d02ae0470bc033254f"}
{"doc_id": "blog:news.microsoft.com", "chunk_id": "body:part-3-o2", "text": "Anyone can connect them to relevant business data such as emails, reports and customer management systems so they can perform tasks and provide insights. And you’ll soon be able to enlist new agents in Microsoft 365 to help with common workflows and tasks. Interpreter in Teams will provide real-time speech-to-speech translation during meetings, for example, and you can opt to have it simulate your voice. The Employee Self-Service Agent will simplify human resource and IT help desk-related tasks like helping workers resolve a laptop issue or find out if they’ve maxed out certain benefits, and it can connect to company systems for further customization in Copilot Studio. Microsoft Dynamics 365 will have agents as well for a range of common business workflows across sales, supply chain, finance and customer service functions. And every SharePoint site will soon come equipped with an agent tailored to your organization’s content that allows employees to quickly tap into these vast knowledge bases and find exactly what they need in seconds, whether it’s project details buried in a workback schedule or a summary of a recent product memo. Developers have even more options. With the new Azure AI Agent Service, you’ll be able to choose from small or large language models to orchestrate, develop and scale agent-powered apps to streamline and automate complex workflows like order processing and customer data synchronization. It provides a software development kit with tools for developing agents, allowing you to efficiently integrate agent capabilities using Visual Studio Code and GitHub. One type of model, OpenAI’s recently announced o1 series, can bring more advanced reasoning capabilities to agents, allowing them to take on more complicated tasks by breaking them down into", "tokens": 364, "chunk_type": "overlap-2", "url": "https://news.microsoft.com/source/features/ai/ai-agents-what-they-are-and-how-theyll-change-the-way-we-work/", "anchor": "#body:part-3", "type": "blog", "title": "", "section": "Body", "source": "blog:news.microsoft.com", "published": "", "sha256": "47cf1e24f3279396150288276637e6812a40f10d772da2d02ae0470bc033254f"}
{"doc_id": "blog:news.microsoft.com", "chunk_id": "body:part-4", "text": "from small or large language models to orchestrate, develop and scale agent-powered apps to streamline and automate complex workflows like order processing and customer data synchronization. It provides a software development kit with tools for developing agents, allowing you to efficiently integrate agent capabilities using Visual Studio Code and GitHub. One type of model, OpenAI’s recently announced o1 series, can bring more advanced reasoning capabilities to agents, allowing them to take on more complicated tasks by breaking them down into steps — like getting the information someone on an IT help desk would need to solve a problem, factoring in solutions they’ve tried and coming up with a plan. You can also use the power of agents in LinkedIn; the platform’s first agent can help recruiters with hiring. Assessing risk for autonomous action There are extra safety considerations with agents that can act autonomously, and Microsoft is focused on making sure agents only access what you want them to, says Sarah Bird, the company’s chief product officer of Responsible AI. “Agents certainly up the stakes from a responsible AI point of view,” Bird says. “So we have to have much, much lower error rates. And there’s many more nuanced ways in which something could be an error. This is the big challenge with agents.” But the same responsible AI foundational playbook for other AI applications can be used to assess and mitigate risk with agents, she says. The new Copilot Control System helps IT departments manage Copilot and agents with data access and governance, management and security controls, as well as measurement reports and tools to track adoption and business value. Many agents, like those created for Microsoft 365 and Dynamics 365, include “human in the loop” approvals, where people are required to take the final step of reviewing and sending an email the Sales Order Agent wrote, for example. And for agents developed in Copilot Studio, authors can review the records to see which actions the agent took and why. The key is to focus on testing and moderating to ensure accuracy, Bird says, and for organizations to choose the right starting point for their needs. “We will of course make progress by building on the foundation we already have, so we’re starting the journey from a strong place,” Bird says. Looking back — and into the future Technologists have long been excited by the idea of autonomous systems working side-by-side with people to help them, says Kamar, who has been working on AI agents since 2005 and even wrote her Ph.D. thesis on the topic in 2010. The hurdle was that “we lacked that general problem-solving power” on the back end, she says. With LLMs, “we finally have this missing component,” she says. “Now we can bring back a lot of the ideas from our decades of research.” Going forward, Kamar envisions a new ecosystem or marketplace of agents, sort of like how apps empower people to do more with their smartphones. Agents already have “the basic building blocks of what it takes to complete a task,” she says. “Like", "tokens": 665, "chunk_type": "original-large", "url": "https://news.microsoft.com/source/features/ai/ai-agents-what-they-are-and-how-theyll-change-the-way-we-work/", "anchor": "#body:part-4", "type": "blog", "title": "", "section": "Body", "source": "blog:news.microsoft.com", "published": "", "sha256": "edd8c4a310eb39ea330003ad833ea2cdb9b4e5ba1fd775a6c0a4c3d1d751e61f"}
{"doc_id": "blog:news.microsoft.com", "chunk_id": "body:part-4-o1", "text": "from small or large language models to orchestrate, develop and scale agent-powered apps to streamline and automate complex workflows like order processing and customer data synchronization. It provides a software development kit with tools for developing agents, allowing you to efficiently integrate agent capabilities using Visual Studio Code and GitHub. One type of model, OpenAI’s recently announced o1 series, can bring more advanced reasoning capabilities to agents, allowing them to take on more complicated tasks by breaking them down into steps — like getting the information someone on an IT help desk would need to solve a problem, factoring in solutions they’ve tried and coming up with a plan. You can also use the power of agents in LinkedIn; the platform’s first agent can help recruiters with hiring. Assessing risk for autonomous action There are extra safety considerations with agents that can act autonomously, and Microsoft is focused on making sure agents only access what you want them to, says Sarah Bird, the company’s chief product officer of Responsible AI. “Agents certainly up the stakes from a responsible AI point of view,” Bird says. “So we have to have much, much lower error rates. And there’s many more nuanced ways in which something could be an error. This is the big challenge with agents.” But the same responsible AI foundational playbook for other AI applications can be used to assess and mitigate risk with agents, she says. The new Copilot Control System helps IT departments manage Copilot and agents with data access and governance, management and security controls, as well as measurement reports and tools to track adoption and business value. Many agents, like those created for Microsoft 365 and Dynamics 365, include “human in the loop” approvals, where people are required to take the final step of reviewing and sending an email the Sales Order Agent wrote, for example.", "tokens": 403, "chunk_type": "overlap-1", "url": "https://news.microsoft.com/source/features/ai/ai-agents-what-they-are-and-how-theyll-change-the-way-we-work/", "anchor": "#body:part-4", "type": "blog", "title": "", "section": "Body", "source": "blog:news.microsoft.com", "published": "", "sha256": "edd8c4a310eb39ea330003ad833ea2cdb9b4e5ba1fd775a6c0a4c3d1d751e61f"}
{"doc_id": "blog:news.microsoft.com", "chunk_id": "body:part-4-o2", "text": "The new Copilot Control System helps IT departments manage Copilot and agents with data access and governance, management and security controls, as well as measurement reports and tools to track adoption and business value. Many agents, like those created for Microsoft 365 and Dynamics 365, include “human in the loop” approvals, where people are required to take the final step of reviewing and sending an email the Sales Order Agent wrote, for example. And for agents developed in Copilot Studio, authors can review the records to see which actions the agent took and why. The key is to focus on testing and moderating to ensure accuracy, Bird says, and for organizations to choose the right starting point for their needs. “We will of course make progress by building on the foundation we already have, so we’re starting the journey from a strong place,” Bird says. Looking back — and into the future Technologists have long been excited by the idea of autonomous systems working side-by-side with people to help them, says Kamar, who has been working on AI agents since 2005 and even wrote her Ph.D. thesis on the topic in 2010. The hurdle was that “we lacked that general problem-solving power” on the back end, she says. With LLMs, “we finally have this missing component,” she says. “Now we can bring back a lot of the ideas from our decades of research.” Going forward, Kamar envisions a new ecosystem or marketplace of agents, sort of like how apps empower people to do more with their smartphones. Agents already have “the basic building blocks of what it takes to complete a task,” she says. “Like", "tokens": 357, "chunk_type": "overlap-2", "url": "https://news.microsoft.com/source/features/ai/ai-agents-what-they-are-and-how-theyll-change-the-way-we-work/", "anchor": "#body:part-4", "type": "blog", "title": "", "section": "Body", "source": "blog:news.microsoft.com", "published": "", "sha256": "edd8c4a310eb39ea330003ad833ea2cdb9b4e5ba1fd775a6c0a4c3d1d751e61f"}
{"doc_id": "blog:news.microsoft.com", "chunk_id": "body:part-5", "text": "that “we lacked that general problem-solving power” on the back end, she says. With LLMs, “we finally have this missing component,” she says. “Now we can bring back a lot of the ideas from our decades of research.” Going forward, Kamar envisions a new ecosystem or marketplace of agents, sort of like how apps empower people to do more with their smartphones. Agents already have “the basic building blocks of what it takes to complete a task,” she says. “Like observing, ‘I can see your meeting is taking longer; I should delay the next meeting.’” They’re getting more helpful as they gain autonomy through the innovations in memory and entitlements. They’re relieving pain points for employees by helping with things like expense reporting, project management and meeting facilitation. And they’re driving exponential impact for businesses by taking on duties like alerting supply chain managers to low inventory and then automatically reordering to help drive sales and keep customers satisfied. Agents matter because they “open up a whole set of opportunities for working with people for getting tasks done, and that’s what we expect from AI systems,” Kamar says. “AI agents are not only a way to get more value for people but are going to be a paradigm shift in terms of how work gets done.” And this is just the beginning. Copilot is set to evolve with new capabilities like Copilot Actions, designed to handle routine tasks that can bog down employees like summarizing emails missed during time off, compiling agenda items and generating monthly reports. More capabilities like these are coming over the next year to lift the weight of work for employees and teams. “Copilot will empower every employee to do their best work in less time, and focus on more meaningful tasks,” Spataro says. “And agents created in Copilot Studio will transform every business process, helping companies streamline operations, enhance collaboration and drive innovation at scale.” Illustrations by Michał Bednarski / Makeshift Studios Story published on November 19, 2024", "tokens": 432, "chunk_type": "original", "url": "https://news.microsoft.com/source/features/ai/ai-agents-what-they-are-and-how-theyll-change-the-way-we-work/", "anchor": "#body:part-5", "type": "blog", "title": "", "section": "Body", "source": "blog:news.microsoft.com", "published": "", "sha256": "2007988b18e359b4183d1751542115ddfba19712caae0a2355729eea906ad7ec"}
{"doc_id": "blog:www.databricks.com", "chunk_id": "body:part-1", "text": "--- title: LLM Inference Performance Engineering: Best Practices url: https://www.databricks.com/blog/llm-inference-performance-engineering-best-practices hostname: databricks.com description: Learn best practices for optimizing LLM inference performance on Databricks, enhancing the efficiency of your machine learning models. sitename: Databricks date: 2023-12-10 --- In this blog post, the MosaicML engineering team shares best practices for how to capitalize on popular open source large language models (LLMs) for production usage. We also provide guidelines for deploying inference services built around these models to help users in their selection of models and deployment hardware. We have worked with multiple PyTorch-based backends in production; these guidelines are drawn from our experience with FasterTransformers, vLLM, NVIDIA's soon-to-be-released TensorRT-LLM, and others. Understanding LLM Text Generation Large Language Models (LLMs) generate text in a two-step process: \"prefill\", where the tokens in the input prompt are processed in parallel, and \"decoding\", where text is generated one 'token' at a time in an autoregressive manner. Each generated token is appended to the input and fed back into the model to generate the next token. Generation stops when the LLM outputs a special stop token or when a user-defined condition is met (e.g., some maximum number of tokens has been generated). If you'd like more background on how LLMs use decoder blocks, check out this blog post. Tokens can be words or sub-words; the exact rules for splitting text into tokens vary from model to model. For instance, you can compare how Llama models tokenize text to how OpenAI models tokenize text. Although LLM inference providers often talk about performance in token-based metrics (e.g., tokens/second), these numbers are not always comparable across model types given these variations. For a concrete example, the team at Anyscale found that Llama 2 tokenization is 19% longer than ChatGPT tokenization (but still has a much lower overall cost). And researchers at HuggingFace also found that Llama 2 required ~20% more tokens to train over the same amount of text as GPT-4. Important Metrics for LLM Serving So, how exactly should we think about inference speed? Our team uses four key metrics for LLM serving: - Time To First Token (TTFT): How quickly users start seeing the model's output after entering their query. Low waiting times for a response are essential in real-time interactions, but less important in offline workloads. This metric is driven by the time required to process the prompt and then generate the first output token. - Time Per Output Token (TPOT): Time to generate an output token for each user that is querying our system. This metric corresponds with how each user will perceive the \"speed\" of the model. For example, a TPOT of 100 milliseconds/tok would be 10 tokens per second per user, or ~450 words per minute, which is faster than a typical person can read. - Latency: The overall time it takes for the model to generate the full response for a user. Overall response latency can be calculated using the previous two metrics: latency = (TTFT) + (TPOT) * (the number of tokens to be generated). - Throughput: The number of output tokens per second", "tokens": 665, "chunk_type": "original-large", "url": "https://www.databricks.com/blog/llm-inference-performance-engineering-best-practices", "anchor": "#body:part-1", "type": "blog", "title": "", "section": "Body", "source": "blog:www.databricks.com", "published": "", "sha256": "4d36c51acb2f7b3440e4c10381c4004c8d72f9c91f21400d6ada1f6f409d44c2"}
{"doc_id": "blog:www.databricks.com", "chunk_id": "body:part-1-o1", "text": "--- title: LLM Inference Performance Engineering: Best Practices url: https://www.databricks.com/blog/llm-inference-performance-engineering-best-practices hostname: databricks.com description: Learn best practices for optimizing LLM inference performance on Databricks, enhancing the efficiency of your machine learning models. sitename: Databricks date: 2023-12-10 --- In this blog post, the MosaicML engineering team shares best practices for how to capitalize on popular open source large language models (LLMs) for production usage. We also provide guidelines for deploying inference services built around these models to help users in their selection of models and deployment hardware. We have worked with multiple PyTorch-based backends in production; these guidelines are drawn from our experience with FasterTransformers, vLLM, NVIDIA's soon-to-be-released TensorRT-LLM, and others. Understanding LLM Text Generation Large Language Models (LLMs) generate text in a two-step process: \"prefill\", where the tokens in the input prompt are processed in parallel, and \"decoding\", where text is generated one 'token' at a time in an autoregressive manner. Each generated token is appended to the input and fed back into the model to generate the next token. Generation stops when the LLM outputs a special stop token or when a user-defined condition is met (e.g., some maximum number of tokens has been generated). If you'd like more background on how LLMs use decoder blocks, check out this blog post. Tokens can be words or sub-words; the exact rules for splitting text into tokens vary from model to model. For instance, you can compare how Llama models tokenize text to how OpenAI models tokenize text. Although LLM inference providers often talk about performance in token-based metrics (e.g., tokens/second), these numbers are not always comparable across model types given these variations. For a concrete example, the team at Anyscale found that Llama 2 tokenization is 19% longer than ChatGPT tokenization (but still has a much lower overall cost).", "tokens": 387, "chunk_type": "overlap-1", "url": "https://www.databricks.com/blog/llm-inference-performance-engineering-best-practices", "anchor": "#body:part-1", "type": "blog", "title": "", "section": "Body", "source": "blog:www.databricks.com", "published": "", "sha256": "4d36c51acb2f7b3440e4c10381c4004c8d72f9c91f21400d6ada1f6f409d44c2"}
{"doc_id": "blog:www.databricks.com", "chunk_id": "body:part-1-o2", "text": "For instance, you can compare how Llama models tokenize text to how OpenAI models tokenize text. Although LLM inference providers often talk about performance in token-based metrics (e.g., tokens/second), these numbers are not always comparable across model types given these variations. For a concrete example, the team at Anyscale found that Llama 2 tokenization is 19% longer than ChatGPT tokenization (but still has a much lower overall cost). And researchers at HuggingFace also found that Llama 2 required ~20% more tokens to train over the same amount of text as GPT-4. Important Metrics for LLM Serving So, how exactly should we think about inference speed? Our team uses four key metrics for LLM serving: - Time To First Token (TTFT): How quickly users start seeing the model's output after entering their query. Low waiting times for a response are essential in real-time interactions, but less important in offline workloads. This metric is driven by the time required to process the prompt and then generate the first output token. - Time Per Output Token (TPOT): Time to generate an output token for each user that is querying our system. This metric corresponds with how each user will perceive the \"speed\" of the model. For example, a TPOT of 100 milliseconds/tok would be 10 tokens per second per user, or ~450 words per minute, which is faster than a typical person can read. - Latency: The overall time it takes for the model to generate the full response for a user. Overall response latency can be calculated using the previous two metrics: latency = (TTFT) + (TPOT) * (the number of tokens to be generated). - Throughput: The number of output tokens per second", "tokens": 366, "chunk_type": "overlap-2", "url": "https://www.databricks.com/blog/llm-inference-performance-engineering-best-practices", "anchor": "#body:part-1", "type": "blog", "title": "", "section": "Body", "source": "blog:www.databricks.com", "published": "", "sha256": "4d36c51acb2f7b3440e4c10381c4004c8d72f9c91f21400d6ada1f6f409d44c2"}
{"doc_id": "blog:www.databricks.com", "chunk_id": "body:part-2", "text": "For example, a TPOT of 100 milliseconds/tok would be 10 tokens per second per user, or ~450 words per minute, which is faster than a typical person can read. - Latency: The overall time it takes for the model to generate the full response for a user. Overall response latency can be calculated using the previous two metrics: latency = (TTFT) + (TPOT) * (the number of tokens to be generated). - Throughput: The number of output tokens per second an inference server can generate across all users and requests. Our goal? The fastest time to first token, the highest throughput, and the quickest time per output token. In other words, we want our models to generate text as fast as possible for as many users as we can support. Notably, there is a tradeoff between throughput and time per output token: if we process 16 user queries concurrently, we'll have higher throughput compared to running the queries sequentially, but we'll take longer to generate output tokens for each user. If you have overall inference latency targets, here are some useful heuristics for evaluating models: - Output length dominates overall response latency: For average latency, you can usually just take your expected/max output token length and multiply it by an overall average time per output token for the model. - Input length is not significant for performance but important for hardware requirements: The addition of 512 input tokens increases latency less than the production of 8 additional output tokens in the MPT models. However, the need to support long inputs can make models harder to serve. For example, we recommend using the A100-80GB (or newer) to serve MPT-7B with its maximum context length of 2048 tokens. - Overall latency scales sub-linearly with model size: On the same hardware, larger models are slower, but the speed ratio won't necessarily match the parameter count ratio. MPT-30B latency is ~2.5x that of MPT-7B latency. Llama2-70B latency is ~2x that of Llama2-13B latency. We are often asked by prospective customers to provide an average inference latency. We recommend that before you anchor yourself to specific latency targets (\"we need less than 20 ms per token\"), you should spend some time characterizing your expected input and desired output lengths. Challenges in LLM Inference Optimizing LLM inference benefits from general techniques such as: - Operator Fusion: Combining different adjacent operators together often results in better latency. - Quantization: Activations and weights are compressed to use a smaller number of bits. - Compression: Sparsity or Distillation. - Parallelization: Tensor parallelism across multiple devices or pipeline parallelism for larger models. Beyond these methods, there are many important Transformer-specific optimizations. A prime example of this is KV (key-value) caching. The Attention mechanism in decoder-only Transformer-based models is computationally inefficient. Each token attends to all previously seen tokens, and thus recomputes many of the same values as each new token is generated. For example, while generating the Nth token, the (N-1)th token attends to (N-2)th, (N-3)th … 1st tokens. Similarly, while generating (N+1)th token, attention for the Nth token again needs to", "tokens": 665, "chunk_type": "original-large", "url": "https://www.databricks.com/blog/llm-inference-performance-engineering-best-practices", "anchor": "#body:part-2", "type": "blog", "title": "", "section": "Body", "source": "blog:www.databricks.com", "published": "", "sha256": "29629928a931ffb3e82519c274466351c37cfe4aeaf0e0ace8b65a978657b0d3"}
{"doc_id": "blog:www.databricks.com", "chunk_id": "body:part-2-o1", "text": "For example, a TPOT of 100 milliseconds/tok would be 10 tokens per second per user, or ~450 words per minute, which is faster than a typical person can read. - Latency: The overall time it takes for the model to generate the full response for a user. Overall response latency can be calculated using the previous two metrics: latency = (TTFT) + (TPOT) * (the number of tokens to be generated). - Throughput: The number of output tokens per second an inference server can generate across all users and requests. Our goal? The fastest time to first token, the highest throughput, and the quickest time per output token. In other words, we want our models to generate text as fast as possible for as many users as we can support. Notably, there is a tradeoff between throughput and time per output token: if we process 16 user queries concurrently, we'll have higher throughput compared to running the queries sequentially, but we'll take longer to generate output tokens for each user. If you have overall inference latency targets, here are some useful heuristics for evaluating models: - Output length dominates overall response latency: For average latency, you can usually just take your expected/max output token length and multiply it by an overall average time per output token for the model. - Input length is not significant for performance but important for hardware requirements: The addition of 512 input tokens increases latency less than the production of 8 additional output tokens in the MPT models. However, the need to support long inputs can make models harder to serve.", "tokens": 345, "chunk_type": "overlap-1", "url": "https://www.databricks.com/blog/llm-inference-performance-engineering-best-practices", "anchor": "#body:part-2", "type": "blog", "title": "", "section": "Body", "source": "blog:www.databricks.com", "published": "", "sha256": "29629928a931ffb3e82519c274466351c37cfe4aeaf0e0ace8b65a978657b0d3"}
{"doc_id": "blog:www.databricks.com", "chunk_id": "body:part-2-o2", "text": "If you have overall inference latency targets, here are some useful heuristics for evaluating models: - Output length dominates overall response latency: For average latency, you can usually just take your expected/max output token length and multiply it by an overall average time per output token for the model. - Input length is not significant for performance but important for hardware requirements: The addition of 512 input tokens increases latency less than the production of 8 additional output tokens in the MPT models. However, the need to support long inputs can make models harder to serve. For example, we recommend using the A100-80GB (or newer) to serve MPT-7B with its maximum context length of 2048 tokens. - Overall latency scales sub-linearly with model size: On the same hardware, larger models are slower, but the speed ratio won't necessarily match the parameter count ratio. MPT-30B latency is ~2.5x that of MPT-7B latency. Llama2-70B latency is ~2x that of Llama2-13B latency. We are often asked by prospective customers to provide an average inference latency. We recommend that before you anchor yourself to specific latency targets (\"we need less than 20 ms per token\"), you should spend some time characterizing your expected input and desired output lengths. Challenges in LLM Inference Optimizing LLM inference benefits from general techniques such as: - Operator Fusion: Combining different adjacent operators together often results in better latency. - Quantization: Activations and weights are compressed to use a smaller number of bits. - Compression: Sparsity or Distillation. - Parallelization: Tensor parallelism across multiple devices or pipeline parallelism for larger models. Beyond these methods, there are many important Transformer-specific optimizations. A prime example of this is KV (key-value) caching. The Attention mechanism in decoder-only Transformer-based models is computationally inefficient. Each token attends to all previously seen tokens, and thus recomputes many of the same values as each new token is generated.", "tokens": 405, "chunk_type": "overlap-2", "url": "https://www.databricks.com/blog/llm-inference-performance-engineering-best-practices", "anchor": "#body:part-2", "type": "blog", "title": "", "section": "Body", "source": "blog:www.databricks.com", "published": "", "sha256": "29629928a931ffb3e82519c274466351c37cfe4aeaf0e0ace8b65a978657b0d3"}
{"doc_id": "blog:www.databricks.com", "chunk_id": "body:part-2-o3", "text": "Challenges in LLM Inference Optimizing LLM inference benefits from general techniques such as: - Operator Fusion: Combining different adjacent operators together often results in better latency. - Quantization: Activations and weights are compressed to use a smaller number of bits. - Compression: Sparsity or Distillation. - Parallelization: Tensor parallelism across multiple devices or pipeline parallelism for larger models. Beyond these methods, there are many important Transformer-specific optimizations. A prime example of this is KV (key-value) caching. The Attention mechanism in decoder-only Transformer-based models is computationally inefficient. Each token attends to all previously seen tokens, and thus recomputes many of the same values as each new token is generated. For example, while generating the Nth token, the (N-1)th token attends to (N-2)th, (N-3)th … 1st tokens. Similarly, while generating (N+1)th token, attention for the Nth token again needs to", "tokens": 179, "chunk_type": "overlap-3", "url": "https://www.databricks.com/blog/llm-inference-performance-engineering-best-practices", "anchor": "#body:part-2", "type": "blog", "title": "", "section": "Body", "source": "blog:www.databricks.com", "published": "", "sha256": "29629928a931ffb3e82519c274466351c37cfe4aeaf0e0ace8b65a978657b0d3"}
{"doc_id": "blog:www.databricks.com", "chunk_id": "body:part-3", "text": "Beyond these methods, there are many important Transformer-specific optimizations. A prime example of this is KV (key-value) caching. The Attention mechanism in decoder-only Transformer-based models is computationally inefficient. Each token attends to all previously seen tokens, and thus recomputes many of the same values as each new token is generated. For example, while generating the Nth token, the (N-1)th token attends to (N-2)th, (N-3)th … 1st tokens. Similarly, while generating (N+1)th token, attention for the Nth token again needs to look at the (N-1)th, (N-2)th, (N-3)th, … 1st tokens. KV caching, i.e., saving of intermediate keys/values for the attention layers, is used to preserve those results for later reuse, avoiding repeated computation. Memory Bandwidth is Key Computations in LLMs are mainly dominated by matrix-matrix multiplication operations; these operations with small dimensions are typically memory-bandwidth-bound on most hardware. When generating tokens in an autoregressive manner, one of the activation matrix dimensions (defined by batch size and number of tokens in the sequence) is small at small batch sizes. Therefore, the speed is dependent on how quickly we can load model parameters from GPU memory to local caches/registers, rather than how quickly we can compute on loaded data. Available and achieved memory bandwidth in inference hardware is a better predictor of speed of token generation than their peak compute performance. Inference hardware utilization is very important in terms of serving costs. GPUs are expensive and we need them to do as much work as possible. Shared inference services promise to keep costs low by combining workloads from many users, filling in individual gaps and batching together overlapping requests. For large models like Llama2-70B, we only achieve good cost/performance at large batch sizes. Having an inference serving system that can operate at large batch sizes is critical for cost efficiency. However, a large batch means larger KV cache size, and that in turn increases the number of GPUs required to serve the model. There's a tug-of-war here and shared service operators need to make some cost trade-offs and implement systems optimizations. Model Bandwidth Utilization (MBU) How optimized is an LLM inference server? As briefly explained earlier, inference for LLMs at smaller batch sizes—especially at decode time—is bottlenecked on how quickly we can load model parameters from the device memory to the compute units. Memory bandwidth dictates how quickly the data movement happens. To measure the underlying hardware's utilization, we introduce a new metric called Model Bandwidth Utilization (MBU). MBU is defined as (achieved memory bandwidth) / (peak memory bandwidth) where achieved memory bandwidth is ((total model parameter size + KV cache size) / TPOT). For example, if a 7B parameter running with 16-bit precision has TPOT equal to 14ms, then it's moving 14GB of parameters in 14ms translating to 1TB/sec bandwidth usage. If the peak bandwidth of the machine is 2TB/sec, we are running at an MBU of 50%. For simplicity, this example ignores KV cache size, which is small for smaller batch sizes and shorter sequence lengths. MBU values close to 100% imply that the inference system is effectively utilizing the available memory bandwidth.", "tokens": 665, "chunk_type": "original-large", "url": "https://www.databricks.com/blog/llm-inference-performance-engineering-best-practices", "anchor": "#body:part-3", "type": "blog", "title": "", "section": "Body", "source": "blog:www.databricks.com", "published": "", "sha256": "3fb17a287f431181d3ab682cf1bf162131fc7c5f81f1d3f159c9582e6bdbfcd4"}
{"doc_id": "blog:www.databricks.com", "chunk_id": "body:part-3-o1", "text": "Beyond these methods, there are many important Transformer-specific optimizations. A prime example of this is KV (key-value) caching. The Attention mechanism in decoder-only Transformer-based models is computationally inefficient. Each token attends to all previously seen tokens, and thus recomputes many of the same values as each new token is generated. For example, while generating the Nth token, the (N-1)th token attends to (N-2)th, (N-3)th … 1st tokens. Similarly, while generating (N+1)th token, attention for the Nth token again needs to look at the (N-1)th, (N-2)th, (N-3)th, … 1st tokens. KV caching, i.e., saving of intermediate keys/values for the attention layers, is used to preserve those results for later reuse, avoiding repeated computation. Memory Bandwidth is Key Computations in LLMs are mainly dominated by matrix-matrix multiplication operations; these operations with small dimensions are typically memory-bandwidth-bound on most hardware. When generating tokens in an autoregressive manner, one of the activation matrix dimensions (defined by batch size and number of tokens in the sequence) is small at small batch sizes. Therefore, the speed is dependent on how quickly we can load model parameters from GPU memory to local caches/registers, rather than how quickly we can compute on loaded data. Available and achieved memory bandwidth in inference hardware is a better predictor of speed of token generation than their peak compute performance. Inference hardware utilization is very important in terms of serving costs. GPUs are expensive and we need them to do as much work as possible. Shared inference services promise to keep costs low by combining workloads from many users, filling in individual gaps and batching together overlapping requests. For large models like Llama2-70B, we only achieve good cost/performance at large batch sizes. Having an inference serving system that can operate at large batch sizes is critical for cost efficiency.", "tokens": 386, "chunk_type": "overlap-1", "url": "https://www.databricks.com/blog/llm-inference-performance-engineering-best-practices", "anchor": "#body:part-3", "type": "blog", "title": "", "section": "Body", "source": "blog:www.databricks.com", "published": "", "sha256": "3fb17a287f431181d3ab682cf1bf162131fc7c5f81f1d3f159c9582e6bdbfcd4"}
{"doc_id": "blog:www.databricks.com", "chunk_id": "body:part-3-o2", "text": "GPUs are expensive and we need them to do as much work as possible. Shared inference services promise to keep costs low by combining workloads from many users, filling in individual gaps and batching together overlapping requests. For large models like Llama2-70B, we only achieve good cost/performance at large batch sizes. Having an inference serving system that can operate at large batch sizes is critical for cost efficiency. However, a large batch means larger KV cache size, and that in turn increases the number of GPUs required to serve the model. There's a tug-of-war here and shared service operators need to make some cost trade-offs and implement systems optimizations. Model Bandwidth Utilization (MBU) How optimized is an LLM inference server? As briefly explained earlier, inference for LLMs at smaller batch sizes—especially at decode time—is bottlenecked on how quickly we can load model parameters from the device memory to the compute units. Memory bandwidth dictates how quickly the data movement happens. To measure the underlying hardware's utilization, we introduce a new metric called Model Bandwidth Utilization (MBU). MBU is defined as (achieved memory bandwidth) / (peak memory bandwidth) where achieved memory bandwidth is ((total model parameter size + KV cache size) / TPOT). For example, if a 7B parameter running with 16-bit precision has TPOT equal to 14ms, then it's moving 14GB of parameters in 14ms translating to 1TB/sec bandwidth usage. If the peak bandwidth of the machine is 2TB/sec, we are running at an MBU of 50%. For simplicity, this example ignores KV cache size, which is small for smaller batch sizes and shorter sequence lengths. MBU values close to 100% imply that the inference system is effectively utilizing the available memory bandwidth.", "tokens": 367, "chunk_type": "overlap-2", "url": "https://www.databricks.com/blog/llm-inference-performance-engineering-best-practices", "anchor": "#body:part-3", "type": "blog", "title": "", "section": "Body", "source": "blog:www.databricks.com", "published": "", "sha256": "3fb17a287f431181d3ab682cf1bf162131fc7c5f81f1d3f159c9582e6bdbfcd4"}
{"doc_id": "blog:www.databricks.com", "chunk_id": "body:part-4", "text": "example, if a 7B parameter running with 16-bit precision has TPOT equal to 14ms, then it's moving 14GB of parameters in 14ms translating to 1TB/sec bandwidth usage. If the peak bandwidth of the machine is 2TB/sec, we are running at an MBU of 50%. For simplicity, this example ignores KV cache size, which is small for smaller batch sizes and shorter sequence lengths. MBU values close to 100% imply that the inference system is effectively utilizing the available memory bandwidth. MBU is also useful to compare different inference systems (hardware + software) in a normalized manner. MBU is complementary to the Model Flops Utilization (MFU; introduced in the PaLM paper) metric which is important in compute-bound settings. Figure 1 shows a pictorial representation of MBU in a plot similar to a roofline plot. The solid sloped line of the orange-shaded region shows the maximum possible throughput if memory bandwidth is fully saturated at 100%. However, in reality for low batch sizes (white dot), the observed performance is lower than maximum – how much lower is a measure of the MBU. For large batch sizes (yellow region), the system is compute bound, and the achieved throughput as a fraction of the peak possible throughput is measured as the Model Flops Utilization (MFU). MBU and MFU determine how much more room is available to push the inference speed further on a given hardware setup. Figure 2 shows measured MBU for different degrees of tensor parallelism with our TensorRT-LLM-based inference server. Peak memory bandwidth utilization is attained when transferring large contiguous memory chunks. When smaller models like MPT-7B are distributed across multiple GPUs, we observe lower MBU as we are moving smaller memory chunks in each GPU. Figure 3 shows empirically observed MBU for different degrees of tensor parallelism and batch sizes on the NVIDIA H100 GPUs. MBU decreases as batch size increases. However, as we scale GPUs, the relative decrease in MBU is less significant. It is also worthy to note that picking hardware with greater memory bandwidth can boost performance with fewer GPUs. At batch size 1, we can achieve a higher MBU of 60% on 2xH100-80GBs as compared to 55% on 4xA100-40GB GPUs (Figure 2). Benchmarking Results Latency We have measured time to first token (TTFT) and time per output token (TPOT) across different degrees of tensor parallelism for MPT-7B and Llama2-70B models. As input prompts lengthen, time to generate the first token starts to consume a substantial portion of total latency. Tensor parallelizing across multiple GPUs helps reduce this latency. Unlike model training, scaling to more GPUs offers significant diminishing returns for inference latency. Eg. for Llama2-70B going from 4x to 8x GPUs only decreases latency by 0.7x at small batch sizes. One reason for this is that higher parallelism has lower MBU (as discussed earlier). Another reason is that tensor parallelism introduces communication overhead across a GPU node. | Time to first token (ms) | |||| |---|---|---|---|---| | Model | 1xA100-40GB | 2xA100-40GB | 4xA100-40GB | 8xA100-40GB | | MPT-7B | 46 (1x) | 34 (0.73x) | 26 (0.56x) |", "tokens": 665, "chunk_type": "original-large", "url": "https://www.databricks.com/blog/llm-inference-performance-engineering-best-practices", "anchor": "#body:part-4", "type": "blog", "title": "", "section": "Body", "source": "blog:www.databricks.com", "published": "", "sha256": "594db46a3ae310e38c22fb057df6dfc94c9902b20d6164522e94e88b148c413f"}
{"doc_id": "blog:www.databricks.com", "chunk_id": "body:part-4-o1", "text": "example, if a 7B parameter running with 16-bit precision has TPOT equal to 14ms, then it's moving 14GB of parameters in 14ms translating to 1TB/sec bandwidth usage. If the peak bandwidth of the machine is 2TB/sec, we are running at an MBU of 50%. For simplicity, this example ignores KV cache size, which is small for smaller batch sizes and shorter sequence lengths. MBU values close to 100% imply that the inference system is effectively utilizing the available memory bandwidth. MBU is also useful to compare different inference systems (hardware + software) in a normalized manner. MBU is complementary to the Model Flops Utilization (MFU; introduced in the PaLM paper) metric which is important in compute-bound settings. Figure 1 shows a pictorial representation of MBU in a plot similar to a roofline plot. The solid sloped line of the orange-shaded region shows the maximum possible throughput if memory bandwidth is fully saturated at 100%. However, in reality for low batch sizes (white dot), the observed performance is lower than maximum – how much lower is a measure of the MBU. For large batch sizes (yellow region), the system is compute bound, and the achieved throughput as a fraction of the peak possible throughput is measured as the Model Flops Utilization (MFU). MBU and MFU determine how much more room is available to push the inference speed further on a given hardware setup. Figure 2 shows measured MBU for different degrees of tensor parallelism with our TensorRT-LLM-based inference server. Peak memory bandwidth utilization is attained when transferring large contiguous memory chunks. When smaller models like MPT-7B are distributed across multiple GPUs, we observe lower MBU as we are moving smaller memory chunks in each GPU. Figure 3 shows empirically observed MBU for different degrees of tensor parallelism and batch sizes on the NVIDIA H100 GPUs. MBU decreases as batch size increases.", "tokens": 403, "chunk_type": "overlap-1", "url": "https://www.databricks.com/blog/llm-inference-performance-engineering-best-practices", "anchor": "#body:part-4", "type": "blog", "title": "", "section": "Body", "source": "blog:www.databricks.com", "published": "", "sha256": "594db46a3ae310e38c22fb057df6dfc94c9902b20d6164522e94e88b148c413f"}
{"doc_id": "blog:www.databricks.com", "chunk_id": "body:part-4-o2", "text": "Peak memory bandwidth utilization is attained when transferring large contiguous memory chunks. When smaller models like MPT-7B are distributed across multiple GPUs, we observe lower MBU as we are moving smaller memory chunks in each GPU. Figure 3 shows empirically observed MBU for different degrees of tensor parallelism and batch sizes on the NVIDIA H100 GPUs. MBU decreases as batch size increases. However, as we scale GPUs, the relative decrease in MBU is less significant. It is also worthy to note that picking hardware with greater memory bandwidth can boost performance with fewer GPUs. At batch size 1, we can achieve a higher MBU of 60% on 2xH100-80GBs as compared to 55% on 4xA100-40GB GPUs (Figure 2). Benchmarking Results Latency We have measured time to first token (TTFT) and time per output token (TPOT) across different degrees of tensor parallelism for MPT-7B and Llama2-70B models. As input prompts lengthen, time to generate the first token starts to consume a substantial portion of total latency. Tensor parallelizing across multiple GPUs helps reduce this latency. Unlike model training, scaling to more GPUs offers significant diminishing returns for inference latency. Eg. for Llama2-70B going from 4x to 8x GPUs only decreases latency by 0.7x at small batch sizes. One reason for this is that higher parallelism has lower MBU (as discussed earlier). Another reason is that tensor parallelism introduces communication overhead across a GPU node. | Time to first token (ms) | |||| |---|---|---|---|---| | Model | 1xA100-40GB | 2xA100-40GB | 4xA100-40GB | 8xA100-40GB | | MPT-7B | 46 (1x) | 34 (0.73x) | 26 (0.56x) |", "tokens": 343, "chunk_type": "overlap-2", "url": "https://www.databricks.com/blog/llm-inference-performance-engineering-best-practices", "anchor": "#body:part-4", "type": "blog", "title": "", "section": "Body", "source": "blog:www.databricks.com", "published": "", "sha256": "594db46a3ae310e38c22fb057df6dfc94c9902b20d6164522e94e88b148c413f"}
{"doc_id": "blog:www.databricks.com", "chunk_id": "body:part-5", "text": "for inference latency. Eg. for Llama2-70B going from 4x to 8x GPUs only decreases latency by 0.7x at small batch sizes. One reason for this is that higher parallelism has lower MBU (as discussed earlier). Another reason is that tensor parallelism introduces communication overhead across a GPU node. | Time to first token (ms) | |||| |---|---|---|---|---| | Model | 1xA100-40GB | 2xA100-40GB | 4xA100-40GB | 8xA100-40GB | | MPT-7B | 46 (1x) | 34 (0.73x) | 26 (0.56x) | - | | Llama2-70B | Doesn't fit | 154 (1x) | 114 (0.74x) | Table 1: Time to first token given input requests are 512 tokens length with batch size of 1. Larger models like Llama2 70B needs at least 4xA100-40B GPUs to fit in memory At larger batch sizes, higher tensor parallelism leads to a more significant relative decrease in token latency. Figure 4 shows how time per output token varies for MPT-7B. At batch size 1, going from 2x to 4x only reduces token latency by ~12%. At batch size 16, latency with 4x is 33% lower than with 2x. This goes in line with our earlier observation that the relative decrease in MBU is smaller at higher degrees of tensor parallelism for batch size 16 as compared to batch size 1. Figure 5 shows similar results for Llama2-70B, except the relative improvement between 4x and 8x is less pronounced. We also compare GPU scaling across two different hardware. Because H100-80GB has 2.15x GPU memory bandwidth as compared to A100-40GB, we can see that latency is 36% lower at batch size 1 and 52% lower at batch size 16 for 4x systems. Throughput We can trade off throughput and time per token by batching requests together. Grouping queries during GPU evaluation increases throughput compared to processing queries sequentially, but each query will take longer to complete (ignoring queueing effects). There are a few common techniques for batching inference requests: - Static batching: Client packs multiple prompts into requests and a response is returned after all sequences in the batch have been completed. Our inference servers support this but do not require it. - Dynamic batching: Prompts are batched together on the fly inside the server. Typically, this method performs worse than static batching but can get close to optimal if responses are short or of uniform length. Does not work well when requests have different parameters. - Continuous batching: The idea of batching requests together as they arrive was introduced in this excellent paper and is currently the SOTA method. Instead of waiting for all sequences in a batch to finish, it groups sequences together at the iteration level. It can achieve 10x-20x better throughput than dynamic batching. Continuous batching is usually the best approach for shared services, but there are situations where the other two might be better. In low-QPS environments, dynamic batching can outperform continuous batching. It is sometimes easier to implement low-level GPU optimizations in a simpler batching framework. For offline batch inference workloads, static batching can avoid significant overhead and achieve better throughput. Batch Size How", "tokens": 665, "chunk_type": "original-large", "url": "https://www.databricks.com/blog/llm-inference-performance-engineering-best-practices", "anchor": "#body:part-5", "type": "blog", "title": "", "section": "Body", "source": "blog:www.databricks.com", "published": "", "sha256": "e16c710e4342f47d4d10657bfbe59717d69eb20e8eed87cd39beca06abe6ab6e"}
{"doc_id": "blog:www.databricks.com", "chunk_id": "body:part-5-o1", "text": "for inference latency. Eg. for Llama2-70B going from 4x to 8x GPUs only decreases latency by 0.7x at small batch sizes. One reason for this is that higher parallelism has lower MBU (as discussed earlier). Another reason is that tensor parallelism introduces communication overhead across a GPU node. | Time to first token (ms) | |||| |---|---|---|---|---| | Model | 1xA100-40GB | 2xA100-40GB | 4xA100-40GB | 8xA100-40GB | | MPT-7B | 46 (1x) | 34 (0.73x) | 26 (0.56x) | - | | Llama2-70B | Doesn't fit | 154 (1x) | 114 (0.74x) | Table 1: Time to first token given input requests are 512 tokens length with batch size of 1. Larger models like Llama2 70B needs at least 4xA100-40B GPUs to fit in memory At larger batch sizes, higher tensor parallelism leads to a more significant relative decrease in token latency. Figure 4 shows how time per output token varies for MPT-7B. At batch size 1, going from 2x to 4x only reduces token latency by ~12%. At batch size 16, latency with 4x is 33% lower than with 2x. This goes in line with our earlier observation that the relative decrease in MBU is smaller at higher degrees of tensor parallelism for batch size 16 as compared to batch size 1. Figure 5 shows similar results for Llama2-70B, except the relative improvement between 4x and 8x is less pronounced. We also compare GPU scaling across two different hardware. Because H100-80GB has 2.15x GPU memory bandwidth as compared to A100-40GB, we can see that latency is 36% lower at batch size 1 and 52% lower at batch size 16 for 4x systems. Throughput We can trade off throughput and time per token by batching requests together. Grouping queries during GPU evaluation increases throughput compared to processing queries sequentially, but each query will take longer to complete (ignoring queueing effects).", "tokens": 404, "chunk_type": "overlap-1", "url": "https://www.databricks.com/blog/llm-inference-performance-engineering-best-practices", "anchor": "#body:part-5", "type": "blog", "title": "", "section": "Body", "source": "blog:www.databricks.com", "published": "", "sha256": "e16c710e4342f47d4d10657bfbe59717d69eb20e8eed87cd39beca06abe6ab6e"}
{"doc_id": "blog:www.databricks.com", "chunk_id": "body:part-5-o2", "text": "Because H100-80GB has 2.15x GPU memory bandwidth as compared to A100-40GB, we can see that latency is 36% lower at batch size 1 and 52% lower at batch size 16 for 4x systems. Throughput We can trade off throughput and time per token by batching requests together. Grouping queries during GPU evaluation increases throughput compared to processing queries sequentially, but each query will take longer to complete (ignoring queueing effects). There are a few common techniques for batching inference requests: - Static batching: Client packs multiple prompts into requests and a response is returned after all sequences in the batch have been completed. Our inference servers support this but do not require it. - Dynamic batching: Prompts are batched together on the fly inside the server. Typically, this method performs worse than static batching but can get close to optimal if responses are short or of uniform length. Does not work well when requests have different parameters. - Continuous batching: The idea of batching requests together as they arrive was introduced in this excellent paper and is currently the SOTA method. Instead of waiting for all sequences in a batch to finish, it groups sequences together at the iteration level. It can achieve 10x-20x better throughput than dynamic batching. Continuous batching is usually the best approach for shared services, but there are situations where the other two might be better. In low-QPS environments, dynamic batching can outperform continuous batching. It is sometimes easier to implement low-level GPU optimizations in a simpler batching framework. For offline batch inference workloads, static batching can avoid significant overhead and achieve better throughput. Batch Size How", "tokens": 352, "chunk_type": "overlap-2", "url": "https://www.databricks.com/blog/llm-inference-performance-engineering-best-practices", "anchor": "#body:part-5", "type": "blog", "title": "", "section": "Body", "source": "blog:www.databricks.com", "published": "", "sha256": "e16c710e4342f47d4d10657bfbe59717d69eb20e8eed87cd39beca06abe6ab6e"}
{"doc_id": "blog:www.databricks.com", "chunk_id": "body:part-6", "text": "finish, it groups sequences together at the iteration level. It can achieve 10x-20x better throughput than dynamic batching. Continuous batching is usually the best approach for shared services, but there are situations where the other two might be better. In low-QPS environments, dynamic batching can outperform continuous batching. It is sometimes easier to implement low-level GPU optimizations in a simpler batching framework. For offline batch inference workloads, static batching can avoid significant overhead and achieve better throughput. Batch Size How well batching works is highly dependent on the request stream. But we can get an upper bound on its performance by benchmarking static batching with uniform requests. | Batch size | ||||||| |---|---|---|---|---|---|---|---| | Hardware | 1 | 4 | 8 | 16 | 32 | 64 | 128 | | 1 x A10 | 0.4 (1x) | 1.4 (3.5x) | 2.3 (6x) | 3.5 (9x) | OOM (Out of Memory) error | || | 2 x A10 | 0.8 | 2.5 | 4.0 | 7.0 | 8.0 | || | 1 x A100 | 0.9 (1x) | 3.2 (3.5x) | 5.3 (6x) | 8.0 (9x) | 10.5 (12x) | 12.5 (14x) | | | 2 x A100 | 1.3 | 3.0 | 5.5 | 9.5 | 14.5 | 17.0 | 22.0 | | 4 x A100 | 1.7 | 6.2 | 11.5 | 18.0 | 25.0 | 33.0 | 36.5 | Table 2: Peak MPT-7B throughput (req/sec) with static batching and a FasterTransformers-based backend. Requests: 512 input and 64 output tokens. For larger inputs, the OOM boundary will be at smaller batch sizes. Latency Trade-Off Request latency increases with batch size. With one NVIDIA A100 GPU, for example, if we maximize throughput with a batch size of 64, latency increases by 4x while throughput increases by 14x. Shared inference services typically pick a balanced batch size. Users hosting their own models should decide the appropriate latency/throughput trade-off for their applications. In some applications, like chatbots, low latency for fast responses is the top priority. In other applications, like batched processing of unstructured PDFs, we might want to sacrifice the latency to process an individual document to process all of them fast in parallel. Figure 7 shows the throughput vs latency curve for the 7B model. Each line on this curve is obtained by increasing the batch size from 1 to 256. This is useful in determining how large we can make the batch size, subject to different latency constraints. Recalling our roofline plot above, we find that these measurements are consistent with what we would expect. After a certain batch size, i.e., when we cross to the compute bound regime, every doubling of batch size just increases the latency without increasing throughput. When using parallelism, it's important to understand low-level hardware details. For instance, not all 8xA100 instances are the same across different clouds. Some servers have high bandwidth connections between all GPUs, others pair GPUs and have lower bandwidth connections between pairs. This could introduce bottlenecks, causing real-world performance to deviate significantly from the curves above. Optimization Case Study: Quantization", "tokens": 665, "chunk_type": "original-large", "url": "https://www.databricks.com/blog/llm-inference-performance-engineering-best-practices", "anchor": "#body:part-6", "type": "blog", "title": "", "section": "Body", "source": "blog:www.databricks.com", "published": "", "sha256": "dee0375f7b7419532d3a283e07390da86e87f854d6d832b6f264697768521c56"}
{"doc_id": "blog:www.databricks.com", "chunk_id": "body:part-6-o1", "text": "finish, it groups sequences together at the iteration level. It can achieve 10x-20x better throughput than dynamic batching. Continuous batching is usually the best approach for shared services, but there are situations where the other two might be better. In low-QPS environments, dynamic batching can outperform continuous batching. It is sometimes easier to implement low-level GPU optimizations in a simpler batching framework. For offline batch inference workloads, static batching can avoid significant overhead and achieve better throughput. Batch Size How well batching works is highly dependent on the request stream. But we can get an upper bound on its performance by benchmarking static batching with uniform requests. | Batch size | ||||||| |---|---|---|---|---|---|---|---| | Hardware | 1 | 4 | 8 | 16 | 32 | 64 | 128 | | 1 x A10 | 0.4 (1x) | 1.4 (3.5x) | 2.3 (6x) | 3.5 (9x) | OOM (Out of Memory) error | || | 2 x A10 | 0.8 | 2.5 | 4.0 | 7.0 | 8.0 | || | 1 x A100 | 0.9 (1x) | 3.2 (3.5x) | 5.3 (6x) | 8.0 (9x) | 10.5 (12x) | 12.5 (14x) | | | 2 x A100 | 1.3 | 3.0 | 5.5 | 9.5 | 14.5 | 17.0 | 22.0 | | 4 x A100 | 1.7 | 6.2 | 11.5 | 18.0 | 25.0 | 33.0 | 36.5 | Table 2: Peak MPT-7B throughput (req/sec) with static batching and a FasterTransformers-based backend. Requests: 512 input and 64 output tokens. For larger inputs, the OOM boundary will be at smaller batch sizes. Latency Trade-Off Request latency increases with batch size. With one NVIDIA A100 GPU, for example, if we maximize throughput with a batch size of 64, latency increases by 4x while throughput increases by 14x. Shared inference services typically pick a balanced batch size.", "tokens": 399, "chunk_type": "overlap-1", "url": "https://www.databricks.com/blog/llm-inference-performance-engineering-best-practices", "anchor": "#body:part-6", "type": "blog", "title": "", "section": "Body", "source": "blog:www.databricks.com", "published": "", "sha256": "dee0375f7b7419532d3a283e07390da86e87f854d6d832b6f264697768521c56"}
{"doc_id": "blog:www.databricks.com", "chunk_id": "body:part-6-o2", "text": "Requests: 512 input and 64 output tokens. For larger inputs, the OOM boundary will be at smaller batch sizes. Latency Trade-Off Request latency increases with batch size. With one NVIDIA A100 GPU, for example, if we maximize throughput with a batch size of 64, latency increases by 4x while throughput increases by 14x. Shared inference services typically pick a balanced batch size. Users hosting their own models should decide the appropriate latency/throughput trade-off for their applications. In some applications, like chatbots, low latency for fast responses is the top priority. In other applications, like batched processing of unstructured PDFs, we might want to sacrifice the latency to process an individual document to process all of them fast in parallel. Figure 7 shows the throughput vs latency curve for the 7B model. Each line on this curve is obtained by increasing the batch size from 1 to 256. This is useful in determining how large we can make the batch size, subject to different latency constraints. Recalling our roofline plot above, we find that these measurements are consistent with what we would expect. After a certain batch size, i.e., when we cross to the compute bound regime, every doubling of batch size just increases the latency without increasing throughput. When using parallelism, it's important to understand low-level hardware details. For instance, not all 8xA100 instances are the same across different clouds. Some servers have high bandwidth connections between all GPUs, others pair GPUs and have lower bandwidth connections between pairs. This could introduce bottlenecks, causing real-world performance to deviate significantly from the curves above. Optimization Case Study: Quantization", "tokens": 347, "chunk_type": "overlap-2", "url": "https://www.databricks.com/blog/llm-inference-performance-engineering-best-practices", "anchor": "#body:part-6", "type": "blog", "title": "", "section": "Body", "source": "blog:www.databricks.com", "published": "", "sha256": "dee0375f7b7419532d3a283e07390da86e87f854d6d832b6f264697768521c56"}
{"doc_id": "blog:www.databricks.com", "chunk_id": "body:part-7", "text": "i.e., when we cross to the compute bound regime, every doubling of batch size just increases the latency without increasing throughput. When using parallelism, it's important to understand low-level hardware details. For instance, not all 8xA100 instances are the same across different clouds. Some servers have high bandwidth connections between all GPUs, others pair GPUs and have lower bandwidth connections between pairs. This could introduce bottlenecks, causing real-world performance to deviate significantly from the curves above. Optimization Case Study: Quantization Quantization is a common technique used to reduce the hardware requirements for LLM inference. Reducing the precision of model weights and activations during inference can dramatically reduce hardware requirements. For instance, switching from 16-bit weights to 8-bit weights can halve the number of required GPUs in memory constrained environments (eg. Llama2-70B on A100s). Dropping down to 4-bit weights makes it possible to run inference on consumer hardware (eg. Llama2-70B on Macbooks). In our experience, quantization should be implemented with caution. Naive quantization techniques can lead to a substantial degradation in model quality. The impact of quantization also varies across model architectures (eg. MPT vs Llama) and sizes. We will explore this in more detail in a future blog post. When experimenting with techniques like quantization, we recommend using an LLM quality benchmark like the Mosaic Eval Gauntlet to evaluate the quality of the inference system, not just the quality of the model in isolation. Additionally, it's important to explore deeper systems optimizations. In particular, quantization can make KV caches much more efficient. As mentioned previously, in autoregressive token generation, past Key/Values (KV) from the attention layers are cached instead of recomputing them at every step. The size of the KV cache varies based on the number of sequences processed at a time and the length of these sequences. Moreover, during each iteration of the next token generation, new KV items are added to the existing cache making it bigger as new tokens are generated. Therefore, effective KV cache memory management when adding these new values is critical for good inference performance. Llama2 models use a variant of attention called Grouped Query Attention (GQA). Please note that when the number of KV heads is 1, GQA is the same as Multi-Query-Attention (MQA). GQA helps with keeping the KV cache size down by sharing Keys/Values. The formula to calculate KV cache size is batch_size * seqlen * (d_model/n_heads) * n_layers * 2 (K and V) * 2 (bytes per Float16) * n_kv_heads Table 3 shows GQA KV cache size calculated at different batch sizes at a sequence length of 1024 tokens. The parameter size for Llama2 models, in comparison, is 140 GB (Float16) for the 70B model. Quantization of KV cache is another technique (in addition to GQA/MQA) to reduce the size of KV cache, and we are actively evaluating its impact on the generation quality. | Batch Size | GQA KV cache memory (FP16) | GQA KV cache memory (Int8) | |---|---|---| | 1 | .312 GiB | .156 GiB | | 16 | 5 GiB | 2.5 GiB | | 32 |", "tokens": 665, "chunk_type": "original-large", "url": "https://www.databricks.com/blog/llm-inference-performance-engineering-best-practices", "anchor": "#body:part-7", "type": "blog", "title": "", "section": "Body", "source": "blog:www.databricks.com", "published": "", "sha256": "0e423ac1739c45f2c4799aec6224fa0e086cd28a66e9816b0b06582df22137cd"}
{"doc_id": "blog:www.databricks.com", "chunk_id": "body:part-7-o1", "text": "i.e., when we cross to the compute bound regime, every doubling of batch size just increases the latency without increasing throughput. When using parallelism, it's important to understand low-level hardware details. For instance, not all 8xA100 instances are the same across different clouds. Some servers have high bandwidth connections between all GPUs, others pair GPUs and have lower bandwidth connections between pairs. This could introduce bottlenecks, causing real-world performance to deviate significantly from the curves above. Optimization Case Study: Quantization Quantization is a common technique used to reduce the hardware requirements for LLM inference. Reducing the precision of model weights and activations during inference can dramatically reduce hardware requirements. For instance, switching from 16-bit weights to 8-bit weights can halve the number of required GPUs in memory constrained environments (eg. Llama2-70B on A100s). Dropping down to 4-bit weights makes it possible to run inference on consumer hardware (eg. Llama2-70B on Macbooks). In our experience, quantization should be implemented with caution. Naive quantization techniques can lead to a substantial degradation in model quality. The impact of quantization also varies across model architectures (eg. MPT vs Llama) and sizes. We will explore this in more detail in a future blog post. When experimenting with techniques like quantization, we recommend using an LLM quality benchmark like the Mosaic Eval Gauntlet to evaluate the quality of the inference system, not just the quality of the model in isolation. Additionally, it's important to explore deeper systems optimizations. In particular, quantization can make KV caches much more efficient. As mentioned previously, in autoregressive token generation, past Key/Values (KV) from the attention layers are cached instead of recomputing them at every step. The size of the KV cache varies based on the number of sequences processed at a time and the length of these sequences.", "tokens": 387, "chunk_type": "overlap-1", "url": "https://www.databricks.com/blog/llm-inference-performance-engineering-best-practices", "anchor": "#body:part-7", "type": "blog", "title": "", "section": "Body", "source": "blog:www.databricks.com", "published": "", "sha256": "0e423ac1739c45f2c4799aec6224fa0e086cd28a66e9816b0b06582df22137cd"}
{"doc_id": "blog:www.databricks.com", "chunk_id": "body:part-7-o2", "text": "Additionally, it's important to explore deeper systems optimizations. In particular, quantization can make KV caches much more efficient. As mentioned previously, in autoregressive token generation, past Key/Values (KV) from the attention layers are cached instead of recomputing them at every step. The size of the KV cache varies based on the number of sequences processed at a time and the length of these sequences. Moreover, during each iteration of the next token generation, new KV items are added to the existing cache making it bigger as new tokens are generated. Therefore, effective KV cache memory management when adding these new values is critical for good inference performance. Llama2 models use a variant of attention called Grouped Query Attention (GQA). Please note that when the number of KV heads is 1, GQA is the same as Multi-Query-Attention (MQA). GQA helps with keeping the KV cache size down by sharing Keys/Values. The formula to calculate KV cache size is batch_size * seqlen * (d_model/n_heads) * n_layers * 2 (K and V) * 2 (bytes per Float16) * n_kv_heads Table 3 shows GQA KV cache size calculated at different batch sizes at a sequence length of 1024 tokens. The parameter size for Llama2 models, in comparison, is 140 GB (Float16) for the 70B model. Quantization of KV cache is another technique (in addition to GQA/MQA) to reduce the size of KV cache, and we are actively evaluating its impact on the generation quality. | Batch Size | GQA KV cache memory (FP16) | GQA KV cache memory (Int8) | |---|---|---| | 1 | .312 GiB | .156 GiB | | 16 | 5 GiB | 2.5 GiB | | 32 |", "tokens": 361, "chunk_type": "overlap-2", "url": "https://www.databricks.com/blog/llm-inference-performance-engineering-best-practices", "anchor": "#body:part-7", "type": "blog", "title": "", "section": "Body", "source": "blog:www.databricks.com", "published": "", "sha256": "0e423ac1739c45f2c4799aec6224fa0e086cd28a66e9816b0b06582df22137cd"}
{"doc_id": "blog:www.databricks.com", "chunk_id": "body:part-8", "text": "for Llama2 models, in comparison, is 140 GB (Float16) for the 70B model. Quantization of KV cache is another technique (in addition to GQA/MQA) to reduce the size of KV cache, and we are actively evaluating its impact on the generation quality. | Batch Size | GQA KV cache memory (FP16) | GQA KV cache memory (Int8) | |---|---|---| | 1 | .312 GiB | .156 GiB | | 16 | 5 GiB | 2.5 GiB | | 32 | 10 GiB | 5 GiB | | 64 | 20 GiB | 10 GiB | Table 3: KV cache size for Llama-2-70B at a sequence length of 1024 As mentioned previously, token generation with LLMs at low batch sizes is a GPU memory bandwidth-bound problem, i.e. the speed of generation depends on how quickly model parameters can be moved from the GPU memory to on-chip caches. Converting model weights from FP16 (2 bytes) to INT8 (1 byte) or INT4 (0.5 byte) requires moving less data and thus speeds up token generation. However, quantization may negatively impact the model generation quality. We are currently evaluating the impact on model quality using Model Gauntlet and plan to publish a followup blog post on it soon. Conclusions and Key Results Each of the factors we've outlined above influences the way we build and deploy models. We use these results to make data-driven decisions that take into consideration the hardware type, the software stack, the model architecture, and typical usage patterns. Here are some recommendations drawn from our experience. Identify your optimization target: Do you care about interactive performance? Maximizing throughput? Minimizing cost? There are predictable trade-offs here. Pay attention to the components of latency: For interactive applications time-to-first-token drives how responsive your service will feel and time-per-output-token determines how fast it will feel. Memory bandwidth is key: Generating the first token is typically compute-bound, while subsequent decoding is memory-bound operation. Because LLM inference often operates in memory-bound settings, MBU is a useful metric to optimize for and can be used to compare the efficiency of inference systems. Batching is critical: Processing multiple requests concurrently is critical for achieving high throughput and for effectively utilizing expensive GPUs. For shared online services continuous batching is indispensable, whereas offline batch inference workloads can achieve high throughput with simpler batching techniques. In depth optimizations: Standard inference optimization techniques are important (eg. operator fusion, weight quantization) for LLMs but it's important to explore deeper systems optimizations, especially those which improve memory utilization. One example is KV cache quantization. Hardware configurations: The model type and expected workload should be used to decide deployment hardware. For instance, when scaling to multiple GPUs MBU falls much more rapidly for smaller models, such as MPT-7B, than it does for larger models, such as Llama2-70B. Performance also tends to scale sub-linearly with higher degrees of tensor parallelism. That said, a high degree of tensor parallelism might still make sense for smaller models if traffic is high or if users are willing to pay a premium for extra low latency. Data Driven Decisions: Understanding the theory", "tokens": 665, "chunk_type": "original-large", "url": "https://www.databricks.com/blog/llm-inference-performance-engineering-best-practices", "anchor": "#body:part-8", "type": "blog", "title": "", "section": "Body", "source": "blog:www.databricks.com", "published": "", "sha256": "980eda31f106d849fd8da6eb60da2e2bfe69456a55882de1440e1024f1e1a017"}
{"doc_id": "blog:www.databricks.com", "chunk_id": "body:part-8-o1", "text": "for Llama2 models, in comparison, is 140 GB (Float16) for the 70B model. Quantization of KV cache is another technique (in addition to GQA/MQA) to reduce the size of KV cache, and we are actively evaluating its impact on the generation quality. | Batch Size | GQA KV cache memory (FP16) | GQA KV cache memory (Int8) | |---|---|---| | 1 | .312 GiB | .156 GiB | | 16 | 5 GiB | 2.5 GiB | | 32 | 10 GiB | 5 GiB | | 64 | 20 GiB | 10 GiB | Table 3: KV cache size for Llama-2-70B at a sequence length of 1024 As mentioned previously, token generation with LLMs at low batch sizes is a GPU memory bandwidth-bound problem, i.e. the speed of generation depends on how quickly model parameters can be moved from the GPU memory to on-chip caches. Converting model weights from FP16 (2 bytes) to INT8 (1 byte) or INT4 (0.5 byte) requires moving less data and thus speeds up token generation. However, quantization may negatively impact the model generation quality. We are currently evaluating the impact on model quality using Model Gauntlet and plan to publish a followup blog post on it soon. Conclusions and Key Results Each of the factors we've outlined above influences the way we build and deploy models. We use these results to make data-driven decisions that take into consideration the hardware type, the software stack, the model architecture, and typical usage patterns. Here are some recommendations drawn from our experience. Identify your optimization target: Do you care about interactive performance? Maximizing throughput? Minimizing cost? There are predictable trade-offs here. Pay attention to the components of latency: For interactive applications time-to-first-token drives how responsive your service will feel and time-per-output-token determines how fast it will feel.", "tokens": 390, "chunk_type": "overlap-1", "url": "https://www.databricks.com/blog/llm-inference-performance-engineering-best-practices", "anchor": "#body:part-8", "type": "blog", "title": "", "section": "Body", "source": "blog:www.databricks.com", "published": "", "sha256": "980eda31f106d849fd8da6eb60da2e2bfe69456a55882de1440e1024f1e1a017"}
{"doc_id": "blog:www.databricks.com", "chunk_id": "body:part-8-o2", "text": "We use these results to make data-driven decisions that take into consideration the hardware type, the software stack, the model architecture, and typical usage patterns. Here are some recommendations drawn from our experience. Identify your optimization target: Do you care about interactive performance? Maximizing throughput? Minimizing cost? There are predictable trade-offs here. Pay attention to the components of latency: For interactive applications time-to-first-token drives how responsive your service will feel and time-per-output-token determines how fast it will feel. Memory bandwidth is key: Generating the first token is typically compute-bound, while subsequent decoding is memory-bound operation. Because LLM inference often operates in memory-bound settings, MBU is a useful metric to optimize for and can be used to compare the efficiency of inference systems. Batching is critical: Processing multiple requests concurrently is critical for achieving high throughput and for effectively utilizing expensive GPUs. For shared online services continuous batching is indispensable, whereas offline batch inference workloads can achieve high throughput with simpler batching techniques. In depth optimizations: Standard inference optimization techniques are important (eg. operator fusion, weight quantization) for LLMs but it's important to explore deeper systems optimizations, especially those which improve memory utilization. One example is KV cache quantization. Hardware configurations: The model type and expected workload should be used to decide deployment hardware. For instance, when scaling to multiple GPUs MBU falls much more rapidly for smaller models, such as MPT-7B, than it does for larger models, such as Llama2-70B. Performance also tends to scale sub-linearly with higher degrees of tensor parallelism. That said, a high degree of tensor parallelism might still make sense for smaller models if traffic is high or if users are willing to pay a premium for extra low latency. Data Driven Decisions: Understanding the theory", "tokens": 377, "chunk_type": "overlap-2", "url": "https://www.databricks.com/blog/llm-inference-performance-engineering-best-practices", "anchor": "#body:part-8", "type": "blog", "title": "", "section": "Body", "source": "blog:www.databricks.com", "published": "", "sha256": "980eda31f106d849fd8da6eb60da2e2bfe69456a55882de1440e1024f1e1a017"}
{"doc_id": "blog:www.databricks.com", "chunk_id": "body:part-9", "text": "decide deployment hardware. For instance, when scaling to multiple GPUs MBU falls much more rapidly for smaller models, such as MPT-7B, than it does for larger models, such as Llama2-70B. Performance also tends to scale sub-linearly with higher degrees of tensor parallelism. That said, a high degree of tensor parallelism might still make sense for smaller models if traffic is high or if users are willing to pay a premium for extra low latency. Data Driven Decisions: Understanding the theory is important, but we recommend always measuring end-to-end server performance. There are many reasons an inference deployment can perform worse than expected. MBU could be unexpectedly low because of software inefficiencies. Or differences in hardware between cloud providers could lead to surprises (we have observed a 2x latency difference between 8xA100 servers from two cloud providers). To get started with LLM inference, try out Databricks Model Serving. Check out the documentation to learn more.", "tokens": 200, "chunk_type": "original", "url": "https://www.databricks.com/blog/llm-inference-performance-engineering-best-practices", "anchor": "#body:part-9", "type": "blog", "title": "", "section": "Body", "source": "blog:www.databricks.com", "published": "", "sha256": "a458875070ded0e014d53d10a324231a08db255b72ef16428d010c6c10426683"}
{"doc_id": "blog:medium.com", "chunk_id": "body:part-1", "text": "--- title: Understanding AI Agents: How They Work, Types, and Practical Applications author: Warley's CatOps url: https://medium.com/@williamwarley/understanding-ai-agents-how-they-work-types-and-practical-applications-bd261845f7c3 hostname: medium.com description: Introduction to AI Agents sitename: Medium date: 2024-06-11 --- Understanding AI Agents: How They Work, Types, and Practical Applications Introduction to AI Agents Definition and Importance AI Agents are autonomous entities that use artificial intelligence (AI) to perceive their environment, make decisions, and perform actions to achieve specific goals. These agents can operate independently or interact with other agents and systems to accomplish tasks. AI agents are designed to simulate human-like intelligence, enabling them to solve complex problems, adapt to changing conditions, and learn from experiences. Key Characteristics of AI Agents: - Autonomy: Operate without human intervention, making decisions and taking actions based on their programming and learned experiences. - Perception: Use sensors or input mechanisms to perceive their environment, gather data, and understand the context in which they operate. - Decision-Making: Apply reasoning and decision-making processes to choose the best course of action based on their goals and current state. - Learning: Improve their performance over time by learning from past experiences, adapting to new situations, and optimizing their strategies. Historical Background and Evolution The concept of AI agents has evolved significantly since its inception, influenced by advancements in computer science, robotics, and cognitive science. Here’s a brief overview of the historical development: 1950s-1960s: The early days of AI research focused on creating machines that could mimic human thought processes. Pioneering work by researchers like Alan Turing and John McCarthy laid the foundation for AI, introducing concepts such as the Turing Test and symbolic AI. 1970s-1980s: The development of expert systems marked a significant milestone in AI. These systems used rule-based logic to emulate the decision-making abilities of human experts in specific domains. However, their lack of learning capabilities and rigidity limited their adaptability. 1990s: The emergence of machine learning (ML) and neural networks revolutionized AI. Agents could now learn from data and experiences, improving their performance over time. Reinforcement learning (RL) also gained prominence, enabling agents to learn optimal strategies through trial and error. 2000s: The advent of big data and increased computational power further accelerated AI development. AI agents became more sophisticated, capable of handling complex tasks such as natural language processing (NLP), computer vision, and autonomous navigation. 2010s-Present: Deep learning, a subset of ML, has driven significant advancements in AI agents. Techniques such as convolutional neural networks (CNNs) and recurrent neural networks (RNNs) have enabled agents to achieve state-of-the-art performance in various domains. Additionally, the integration of AI agents with the Internet of Things (IoT) and cloud computing has expanded their capabilities and applications even further. Importance of AI Agents AI agents are crucial in the modern technological landscape due to their ability to automate complex tasks, make informed decisions, and continuously improve through learning. Their importance can be seen in several key areas: 1. Automation: AI agents can automate repetitive and time-consuming tasks, freeing up human resources for more strategic and creative work. This increases efficiency and productivity across various industries. 2. Scalability: AI agents can handle", "tokens": 665, "chunk_type": "original-large", "url": "https://medium.com/@williamwarley/understanding-ai-agents-how-they-work-types-and-practical-applications-bd261845f7c3", "anchor": "#body:part-1", "type": "blog", "title": "", "section": "Body", "source": "blog:medium.com", "published": "", "sha256": "fc39282c59b0e70684197dc155633a75b8c5ee2084f795f95fb31a4a96d9befa"}
{"doc_id": "blog:medium.com", "chunk_id": "body:part-1-o1", "text": "--- title: Understanding AI Agents: How They Work, Types, and Practical Applications author: Warley's CatOps url: https://medium.com/@williamwarley/understanding-ai-agents-how-they-work-types-and-practical-applications-bd261845f7c3 hostname: medium.com description: Introduction to AI Agents sitename: Medium date: 2024-06-11 --- Understanding AI Agents: How They Work, Types, and Practical Applications Introduction to AI Agents Definition and Importance AI Agents are autonomous entities that use artificial intelligence (AI) to perceive their environment, make decisions, and perform actions to achieve specific goals. These agents can operate independently or interact with other agents and systems to accomplish tasks. AI agents are designed to simulate human-like intelligence, enabling them to solve complex problems, adapt to changing conditions, and learn from experiences. Key Characteristics of AI Agents: - Autonomy: Operate without human intervention, making decisions and taking actions based on their programming and learned experiences. - Perception: Use sensors or input mechanisms to perceive their environment, gather data, and understand the context in which they operate. - Decision-Making: Apply reasoning and decision-making processes to choose the best course of action based on their goals and current state. - Learning: Improve their performance over time by learning from past experiences, adapting to new situations, and optimizing their strategies. Historical Background and Evolution The concept of AI agents has evolved significantly since its inception, influenced by advancements in computer science, robotics, and cognitive science. Here’s a brief overview of the historical development: 1950s-1960s: The early days of AI research focused on creating machines that could mimic human thought processes. Pioneering work by researchers like Alan Turing and John McCarthy laid the foundation for AI, introducing concepts such as the Turing Test and symbolic AI. 1970s-1980s: The development of expert systems marked a significant milestone in AI. These systems used rule-based logic to emulate the decision-making abilities of human experts in specific domains.", "tokens": 382, "chunk_type": "overlap-1", "url": "https://medium.com/@williamwarley/understanding-ai-agents-how-they-work-types-and-practical-applications-bd261845f7c3", "anchor": "#body:part-1", "type": "blog", "title": "", "section": "Body", "source": "blog:medium.com", "published": "", "sha256": "fc39282c59b0e70684197dc155633a75b8c5ee2084f795f95fb31a4a96d9befa"}
{"doc_id": "blog:medium.com", "chunk_id": "body:part-1-o2", "text": "Here’s a brief overview of the historical development: 1950s-1960s: The early days of AI research focused on creating machines that could mimic human thought processes. Pioneering work by researchers like Alan Turing and John McCarthy laid the foundation for AI, introducing concepts such as the Turing Test and symbolic AI. 1970s-1980s: The development of expert systems marked a significant milestone in AI. These systems used rule-based logic to emulate the decision-making abilities of human experts in specific domains. However, their lack of learning capabilities and rigidity limited their adaptability. 1990s: The emergence of machine learning (ML) and neural networks revolutionized AI. Agents could now learn from data and experiences, improving their performance over time. Reinforcement learning (RL) also gained prominence, enabling agents to learn optimal strategies through trial and error. 2000s: The advent of big data and increased computational power further accelerated AI development. AI agents became more sophisticated, capable of handling complex tasks such as natural language processing (NLP), computer vision, and autonomous navigation. 2010s-Present: Deep learning, a subset of ML, has driven significant advancements in AI agents. Techniques such as convolutional neural networks (CNNs) and recurrent neural networks (RNNs) have enabled agents to achieve state-of-the-art performance in various domains. Additionally, the integration of AI agents with the Internet of Things (IoT) and cloud computing has expanded their capabilities and applications even further. Importance of AI Agents AI agents are crucial in the modern technological landscape due to their ability to automate complex tasks, make informed decisions, and continuously improve through learning. Their importance can be seen in several key areas: 1. Automation: AI agents can automate repetitive and time-consuming tasks, freeing up human resources for more strategic and creative work. This increases efficiency and productivity across various industries. 2. Scalability: AI agents can handle", "tokens": 384, "chunk_type": "overlap-2", "url": "https://medium.com/@williamwarley/understanding-ai-agents-how-they-work-types-and-practical-applications-bd261845f7c3", "anchor": "#body:part-1", "type": "blog", "title": "", "section": "Body", "source": "blog:medium.com", "published": "", "sha256": "fc39282c59b0e70684197dc155633a75b8c5ee2084f795f95fb31a4a96d9befa"}
{"doc_id": "blog:medium.com", "chunk_id": "body:part-2", "text": "has expanded their capabilities and applications even further. Importance of AI Agents AI agents are crucial in the modern technological landscape due to their ability to automate complex tasks, make informed decisions, and continuously improve through learning. Their importance can be seen in several key areas: 1. Automation: AI agents can automate repetitive and time-consuming tasks, freeing up human resources for more strategic and creative work. This increases efficiency and productivity across various industries. 2. Scalability: AI agents can handle large-scale operations and processes, making them ideal for applications that require processing vast amounts of data and performing tasks simultaneously. 3. Real-Time Decision Making: AI agents can process information and make decisions in real-time, which is crucial for applications like autonomous driving, financial trading, and real-time customer support. 4. Adaptability: Through machine learning and reinforcement learning, AI agents can adapt to new environments and situations, improving their performance and decision-making capabilities over time. 5. Personalization: AI agents can analyze individual user behavior and preferences to provide personalized experiences in applications such as recommendation systems, personal assistants, and targeted marketing. Evolution of AI Agents The evolution of AI agents can be traced through several key developments and milestones: 1. Early AI and Expert Systems: Initial AI research focused on rule-based systems and symbolic reasoning. Expert systems, which were designed to mimic the decision-making abilities of human experts, were among the first AI agents. However, their lack of learning capabilities and flexibility limited their effectiveness. 2. Machine Learning and Neural Networks: The introduction of machine learning algorithms allowed AI agents to learn from data rather than relying solely on predefined rules. Neural networks, inspired by the human brain, enabled agents to recognize patterns and make predictions, leading to significant improvements in tasks such as image and speech recognition. 3. Reinforcement Learning: Reinforcement learning (RL) provided a framework for AI agents to learn optimal behaviors through trial and error. Agents receive feedback in the form of rewards or penalties, allowing them to refine their strategies and actions. This approach has been particularly successful in applications like game playing and robotics. 4. Deep Learning: Deep learning, a subset of machine learning, involves training large neural networks with many layers. This has led to breakthroughs in natural language processing, computer vision, and other complex tasks. AI agents powered by deep learning can achieve state-of-the-art performance in various domains. 5. Integration with IoT and Cloud Computing: The integration of AI agents with the Internet of Things (IoT) and cloud computing has expanded their capabilities. AI agents can now leverage vast amounts of data collected from IoT devices and process it in real-time using cloud-based infrastructure. This integration has enabled applications such as smart homes, industrial automation, and predictive maintenance. Conclusion AI agents represent a significant advancement in artificial intelligence, offering the ability to automate complex tasks, make real-time decisions, and continuously improve through learning. Their evolution from early rule-based systems to sophisticated deep learning models has expanded their capabilities and applications across various industries. As we continue to innovate and integrate AI agents with emerging technologies, their impact", "tokens": 665, "chunk_type": "original-large", "url": "https://medium.com/@williamwarley/understanding-ai-agents-how-they-work-types-and-practical-applications-bd261845f7c3", "anchor": "#body:part-2", "type": "blog", "title": "", "section": "Body", "source": "blog:medium.com", "published": "", "sha256": "18fc581f7317d47f7beaa2d7f5e99bfdf7c5019a38c2d4f9c4c38315d7ec8585"}
{"doc_id": "blog:medium.com", "chunk_id": "body:part-2-o1", "text": "has expanded their capabilities and applications even further. Importance of AI Agents AI agents are crucial in the modern technological landscape due to their ability to automate complex tasks, make informed decisions, and continuously improve through learning. Their importance can be seen in several key areas: 1. Automation: AI agents can automate repetitive and time-consuming tasks, freeing up human resources for more strategic and creative work. This increases efficiency and productivity across various industries. 2. Scalability: AI agents can handle large-scale operations and processes, making them ideal for applications that require processing vast amounts of data and performing tasks simultaneously. 3. Real-Time Decision Making: AI agents can process information and make decisions in real-time, which is crucial for applications like autonomous driving, financial trading, and real-time customer support. 4. Adaptability: Through machine learning and reinforcement learning, AI agents can adapt to new environments and situations, improving their performance and decision-making capabilities over time. 5. Personalization: AI agents can analyze individual user behavior and preferences to provide personalized experiences in applications such as recommendation systems, personal assistants, and targeted marketing. Evolution of AI Agents The evolution of AI agents can be traced through several key developments and milestones: 1. Early AI and Expert Systems: Initial AI research focused on rule-based systems and symbolic reasoning. Expert systems, which were designed to mimic the decision-making abilities of human experts, were among the first AI agents. However, their lack of learning capabilities and flexibility limited their effectiveness. 2. Machine Learning and Neural Networks: The introduction of machine learning algorithms allowed AI agents to learn from data rather than relying solely on predefined rules. Neural networks, inspired by the human brain, enabled agents to recognize patterns and make predictions, leading to significant improvements in tasks such as image and speech recognition. 3.", "tokens": 386, "chunk_type": "overlap-1", "url": "https://medium.com/@williamwarley/understanding-ai-agents-how-they-work-types-and-practical-applications-bd261845f7c3", "anchor": "#body:part-2", "type": "blog", "title": "", "section": "Body", "source": "blog:medium.com", "published": "", "sha256": "18fc581f7317d47f7beaa2d7f5e99bfdf7c5019a38c2d4f9c4c38315d7ec8585"}
{"doc_id": "blog:medium.com", "chunk_id": "body:part-2-o2", "text": "However, their lack of learning capabilities and flexibility limited their effectiveness. 2. Machine Learning and Neural Networks: The introduction of machine learning algorithms allowed AI agents to learn from data rather than relying solely on predefined rules. Neural networks, inspired by the human brain, enabled agents to recognize patterns and make predictions, leading to significant improvements in tasks such as image and speech recognition. 3. Reinforcement Learning: Reinforcement learning (RL) provided a framework for AI agents to learn optimal behaviors through trial and error. Agents receive feedback in the form of rewards or penalties, allowing them to refine their strategies and actions. This approach has been particularly successful in applications like game playing and robotics. 4. Deep Learning: Deep learning, a subset of machine learning, involves training large neural networks with many layers. This has led to breakthroughs in natural language processing, computer vision, and other complex tasks. AI agents powered by deep learning can achieve state-of-the-art performance in various domains. 5. Integration with IoT and Cloud Computing: The integration of AI agents with the Internet of Things (IoT) and cloud computing has expanded their capabilities. AI agents can now leverage vast amounts of data collected from IoT devices and process it in real-time using cloud-based infrastructure. This integration has enabled applications such as smart homes, industrial automation, and predictive maintenance. Conclusion AI agents represent a significant advancement in artificial intelligence, offering the ability to automate complex tasks, make real-time decisions, and continuously improve through learning. Their evolution from early rule-based systems to sophisticated deep learning models has expanded their capabilities and applications across various industries. As we continue to innovate and integrate AI agents with emerging technologies, their impact", "tokens": 364, "chunk_type": "overlap-2", "url": "https://medium.com/@williamwarley/understanding-ai-agents-how-they-work-types-and-practical-applications-bd261845f7c3", "anchor": "#body:part-2", "type": "blog", "title": "", "section": "Body", "source": "blog:medium.com", "published": "", "sha256": "18fc581f7317d47f7beaa2d7f5e99bfdf7c5019a38c2d4f9c4c38315d7ec8585"}
{"doc_id": "blog:medium.com", "chunk_id": "body:part-3", "text": "process it in real-time using cloud-based infrastructure. This integration has enabled applications such as smart homes, industrial automation, and predictive maintenance. Conclusion AI agents represent a significant advancement in artificial intelligence, offering the ability to automate complex tasks, make real-time decisions, and continuously improve through learning. Their evolution from early rule-based systems to sophisticated deep learning models has expanded their capabilities and applications across various industries. As we continue to innovate and integrate AI agents with emerging technologies, their impact on our daily lives and the broader technological landscape will only grow. This introduction provides a comprehensive overview of AI agents, highlighting their definition, importance, historical evolution, and key advancements. How AI Agents Work To understand how AI agents work, it is essential to delve into their core concepts, components, and learning mechanisms. This chapter provides a detailed explanation of these elements to illustrate the functioning of AI agents. Core Concepts and Components 1. Perception: — AI agents use sensors or input mechanisms to perceive their environment. This can involve collecting data from various sources such as cameras, microphones, or other sensors. — Example: In autonomous vehicles, sensors like LIDAR, cameras, and radar gather information about the vehicle’s surroundings. 2. Reasoning: — After perceiving the environment, the agent processes the information to make informed decisions. This involves reasoning and applying logical rules or learned knowledge to interpret the data. — Example: A recommendation system analyzes user preferences and behaviors to suggest relevant products. 3. Action: — Based on its reasoning, the AI agent takes appropriate actions to achieve its goals. This can involve physical actions (e.g., a robot moving objects) or digital actions (e.g., sending an email). — Example: A robotic vacuum cleaner navigates a room to clean it efficiently. 4. Learning: — AI agents improve their performance over time by learning from experiences. This can involve supervised learning, unsupervised learning, or reinforcement learning, depending on the task and data available. — Example: A chatbot learns to provide better responses by analyzing previous interactions with users. Types of AI Agents 1. Simple Reflex Agents: — Operate based on a set of predefined rules and respond directly to specific stimuli from the environment. — Example: A thermostat that adjusts the temperature based on the current room temperature. 2. Model-Based Reflex Agents: — Maintain an internal model of the world to keep track of unobservable aspects of the environment, allowing for more informed decision-making. — Example: A navigation system that uses a map to plan routes and update the user’s location. 3. Goal-Based Agents: — Use goals to guide their actions, making decisions based on the desirability of outcomes and the likelihood of achieving their objectives. — Example: An AI planning system that schedules tasks to maximize efficiency and meet deadlines. 4. Utility-Based Agents: — Evaluate the utility of different actions to choose the one that maximizes overall satisfaction or performance, considering both goals and preferences. — Example: An autonomous trading system that selects trades to maximize profit while minimizing risk. 5. Learning Agents: — Continuously learn from their environment and experiences to improve", "tokens": 665, "chunk_type": "original-large", "url": "https://medium.com/@williamwarley/understanding-ai-agents-how-they-work-types-and-practical-applications-bd261845f7c3", "anchor": "#body:part-3", "type": "blog", "title": "", "section": "Body", "source": "blog:medium.com", "published": "", "sha256": "f16b20c950fcacafa5c48a7ea7d75b2babc3cdf87709c4e4eefd2384f0e16e24"}
{"doc_id": "blog:medium.com", "chunk_id": "body:part-3-o1", "text": "process it in real-time using cloud-based infrastructure. This integration has enabled applications such as smart homes, industrial automation, and predictive maintenance. Conclusion AI agents represent a significant advancement in artificial intelligence, offering the ability to automate complex tasks, make real-time decisions, and continuously improve through learning. Their evolution from early rule-based systems to sophisticated deep learning models has expanded their capabilities and applications across various industries. As we continue to innovate and integrate AI agents with emerging technologies, their impact on our daily lives and the broader technological landscape will only grow. This introduction provides a comprehensive overview of AI agents, highlighting their definition, importance, historical evolution, and key advancements. How AI Agents Work To understand how AI agents work, it is essential to delve into their core concepts, components, and learning mechanisms. This chapter provides a detailed explanation of these elements to illustrate the functioning of AI agents. Core Concepts and Components 1. Perception: — AI agents use sensors or input mechanisms to perceive their environment. This can involve collecting data from various sources such as cameras, microphones, or other sensors. — Example: In autonomous vehicles, sensors like LIDAR, cameras, and radar gather information about the vehicle’s surroundings. 2. Reasoning: — After perceiving the environment, the agent processes the information to make informed decisions. This involves reasoning and applying logical rules or learned knowledge to interpret the data. — Example: A recommendation system analyzes user preferences and behaviors to suggest relevant products. 3. Action: — Based on its reasoning, the AI agent takes appropriate actions to achieve its goals. This can involve physical actions (e.g., a robot moving objects) or digital actions (e.g., sending an email). — Example: A robotic vacuum cleaner navigates a room to clean it efficiently. 4. Learning: — AI agents improve their performance over time by learning from experiences.", "tokens": 395, "chunk_type": "overlap-1", "url": "https://medium.com/@williamwarley/understanding-ai-agents-how-they-work-types-and-practical-applications-bd261845f7c3", "anchor": "#body:part-3", "type": "blog", "title": "", "section": "Body", "source": "blog:medium.com", "published": "", "sha256": "f16b20c950fcacafa5c48a7ea7d75b2babc3cdf87709c4e4eefd2384f0e16e24"}
{"doc_id": "blog:medium.com", "chunk_id": "body:part-3-o2", "text": "Action: — Based on its reasoning, the AI agent takes appropriate actions to achieve its goals. This can involve physical actions (e.g., a robot moving objects) or digital actions (e.g., sending an email). — Example: A robotic vacuum cleaner navigates a room to clean it efficiently. 4. Learning: — AI agents improve their performance over time by learning from experiences. This can involve supervised learning, unsupervised learning, or reinforcement learning, depending on the task and data available. — Example: A chatbot learns to provide better responses by analyzing previous interactions with users. Types of AI Agents 1. Simple Reflex Agents: — Operate based on a set of predefined rules and respond directly to specific stimuli from the environment. — Example: A thermostat that adjusts the temperature based on the current room temperature. 2. Model-Based Reflex Agents: — Maintain an internal model of the world to keep track of unobservable aspects of the environment, allowing for more informed decision-making. — Example: A navigation system that uses a map to plan routes and update the user’s location. 3. Goal-Based Agents: — Use goals to guide their actions, making decisions based on the desirability of outcomes and the likelihood of achieving their objectives. — Example: An AI planning system that schedules tasks to maximize efficiency and meet deadlines. 4. Utility-Based Agents: — Evaluate the utility of different actions to choose the one that maximizes overall satisfaction or performance, considering both goals and preferences. — Example: An autonomous trading system that selects trades to maximize profit while minimizing risk. 5. Learning Agents: — Continuously learn from their environment and experiences to improve", "tokens": 348, "chunk_type": "overlap-2", "url": "https://medium.com/@williamwarley/understanding-ai-agents-how-they-work-types-and-practical-applications-bd261845f7c3", "anchor": "#body:part-3", "type": "blog", "title": "", "section": "Body", "source": "blog:medium.com", "published": "", "sha256": "f16b20c950fcacafa5c48a7ea7d75b2babc3cdf87709c4e4eefd2384f0e16e24"}
{"doc_id": "blog:medium.com", "chunk_id": "body:part-4", "text": "on the desirability of outcomes and the likelihood of achieving their objectives. — Example: An AI planning system that schedules tasks to maximize efficiency and meet deadlines. 4. Utility-Based Agents: — Evaluate the utility of different actions to choose the one that maximizes overall satisfaction or performance, considering both goals and preferences. — Example: An autonomous trading system that selects trades to maximize profit while minimizing risk. 5. Learning Agents: — Continuously learn from their environment and experiences to improve their performance over time, adapting to new situations and optimizing their behavior. — Example: A recommendation engine that refines its suggestions based on user feedback and interactions. Learning Mechanisms 1. Supervised Learning: — Involves training an agent using labeled data, where the correct output is provided for each input example. The agent learns to map inputs to outputs by minimizing prediction errors. — Example: Training an image recognition model to classify images of cats and dogs using labeled datasets. 2. Unsupervised Learning: — Involves training an agent using unlabeled data, where the agent identifies patterns and structures in the data without explicit instructions. Techniques like clustering and dimensionality reduction are common. — Example: Grouping similar customer profiles for targeted marketing campaigns. 3. Reinforcement Learning (RL): — Involves training an agent to make sequences of decisions by rewarding desirable behaviors and penalizing undesirable ones. The agent learns to maximize cumulative rewards over time. — Example: Training a game-playing AI to learn optimal strategies by receiving points for winning and penalties for losing. Practical Implementation Implementing AI agents involves several practical steps, including data collection, model training, and deployment. Here’s a high-level overview: 1. Data Collection and Preprocessing: — Gather relevant data from sensors or databases, preprocess it to remove noise, and structure it for analysis. — Example: Collecting and cleaning data from sensors for an autonomous robot. 2. Model Training: — Train the agent using appropriate learning algorithms and techniques, such as neural networks, decision trees, or RL algorithms. — Example: Training a neural network to recognize objects in images. 3. Deployment: — Deploy the trained agent in the target environment, ensuring it can interact with other systems and perform its tasks effectively. — Example: Deploying a chatbot on a company’s customer service platform. 4. Monitoring and Maintenance: — Continuously monitor the agent’s performance, update it with new data, and retrain as necessary to maintain its effectiveness. — Example: Regularly updating a recommendation engine with new user data to improve suggestions. This chapter provides a detailed explanation of how AI agents work, covering their core concepts, types, learning mechanisms, and practical implementation. Types of AI Agents AI agents come in various types, each designed to handle specific tasks and environments. Understanding these types helps in selecting the right kind of agent for a particular application. This chapter explores the different types of AI agents, their characteristics, and use cases. 1. Simple Reflex Agents Overview: - Simple reflex agents act solely based on the current percept and predefined rules, without considering the history of past perceptions. - They are the simplest form of agents", "tokens": 665, "chunk_type": "original-large", "url": "https://medium.com/@williamwarley/understanding-ai-agents-how-they-work-types-and-practical-applications-bd261845f7c3", "anchor": "#body:part-4", "type": "blog", "title": "", "section": "Body", "source": "blog:medium.com", "published": "", "sha256": "1291049a131f623687635c519aabfe7de0974ee1fc340caee5f8cfd08a6d0715"}
{"doc_id": "blog:medium.com", "chunk_id": "body:part-4-o1", "text": "on the desirability of outcomes and the likelihood of achieving their objectives. — Example: An AI planning system that schedules tasks to maximize efficiency and meet deadlines. 4. Utility-Based Agents: — Evaluate the utility of different actions to choose the one that maximizes overall satisfaction or performance, considering both goals and preferences. — Example: An autonomous trading system that selects trades to maximize profit while minimizing risk. 5. Learning Agents: — Continuously learn from their environment and experiences to improve their performance over time, adapting to new situations and optimizing their behavior. — Example: A recommendation engine that refines its suggestions based on user feedback and interactions. Learning Mechanisms 1. Supervised Learning: — Involves training an agent using labeled data, where the correct output is provided for each input example. The agent learns to map inputs to outputs by minimizing prediction errors. — Example: Training an image recognition model to classify images of cats and dogs using labeled datasets. 2. Unsupervised Learning: — Involves training an agent using unlabeled data, where the agent identifies patterns and structures in the data without explicit instructions. Techniques like clustering and dimensionality reduction are common. — Example: Grouping similar customer profiles for targeted marketing campaigns. 3. Reinforcement Learning (RL): — Involves training an agent to make sequences of decisions by rewarding desirable behaviors and penalizing undesirable ones. The agent learns to maximize cumulative rewards over time. — Example: Training a game-playing AI to learn optimal strategies by receiving points for winning and penalties for losing. Practical Implementation Implementing AI agents involves several practical steps, including data collection, model training, and deployment. Here’s a high-level overview: 1. Data Collection and Preprocessing: — Gather relevant data from sensors or databases, preprocess it to remove noise, and structure it for analysis. — Example: Collecting and cleaning data from sensors for an autonomous robot. 2.", "tokens": 399, "chunk_type": "overlap-1", "url": "https://medium.com/@williamwarley/understanding-ai-agents-how-they-work-types-and-practical-applications-bd261845f7c3", "anchor": "#body:part-4", "type": "blog", "title": "", "section": "Body", "source": "blog:medium.com", "published": "", "sha256": "1291049a131f623687635c519aabfe7de0974ee1fc340caee5f8cfd08a6d0715"}
{"doc_id": "blog:medium.com", "chunk_id": "body:part-4-o2", "text": "The agent learns to maximize cumulative rewards over time. — Example: Training a game-playing AI to learn optimal strategies by receiving points for winning and penalties for losing. Practical Implementation Implementing AI agents involves several practical steps, including data collection, model training, and deployment. Here’s a high-level overview: 1. Data Collection and Preprocessing: — Gather relevant data from sensors or databases, preprocess it to remove noise, and structure it for analysis. — Example: Collecting and cleaning data from sensors for an autonomous robot. 2. Model Training: — Train the agent using appropriate learning algorithms and techniques, such as neural networks, decision trees, or RL algorithms. — Example: Training a neural network to recognize objects in images. 3. Deployment: — Deploy the trained agent in the target environment, ensuring it can interact with other systems and perform its tasks effectively. — Example: Deploying a chatbot on a company’s customer service platform. 4. Monitoring and Maintenance: — Continuously monitor the agent’s performance, update it with new data, and retrain as necessary to maintain its effectiveness. — Example: Regularly updating a recommendation engine with new user data to improve suggestions. This chapter provides a detailed explanation of how AI agents work, covering their core concepts, types, learning mechanisms, and practical implementation. Types of AI Agents AI agents come in various types, each designed to handle specific tasks and environments. Understanding these types helps in selecting the right kind of agent for a particular application. This chapter explores the different types of AI agents, their characteristics, and use cases. 1. Simple Reflex Agents Overview: - Simple reflex agents act solely based on the current percept and predefined rules, without considering the history of past perceptions. - They are the simplest form of agents", "tokens": 375, "chunk_type": "overlap-2", "url": "https://medium.com/@williamwarley/understanding-ai-agents-how-they-work-types-and-practical-applications-bd261845f7c3", "anchor": "#body:part-4", "type": "blog", "title": "", "section": "Body", "source": "blog:medium.com", "published": "", "sha256": "1291049a131f623687635c519aabfe7de0974ee1fc340caee5f8cfd08a6d0715"}
{"doc_id": "blog:medium.com", "chunk_id": "body:part-5", "text": "of AI Agents AI agents come in various types, each designed to handle specific tasks and environments. Understanding these types helps in selecting the right kind of agent for a particular application. This chapter explores the different types of AI agents, their characteristics, and use cases. 1. Simple Reflex Agents Overview: - Simple reflex agents act solely based on the current percept and predefined rules, without considering the history of past perceptions. - They are the simplest form of agents and work well in environments that are fully observable and deterministic. How They Work: - These agents use condition-action rules (if-then statements) to decide on actions. - They do not maintain any internal state or model of the environment. Example: - A thermostat that turns the heating on or off based on the current temperature reading. - Use Case: Simple household appliances and basic automated systems. Advantages: - Easy to design and implement. - Efficient in predictable environments. Disadvantages: - Limited functionality in complex or partially observable environments. - Cannot learn or adapt to changes in the environment. 2. Model-Based Reflex Agents Overview: - Model-based reflex agents maintain an internal model of the environment, allowing them to handle partially observable environments better than simple reflex agents. - They can consider the history of past perceptions to make more informed decisions. How They Work: - These agents update their internal model based on incoming percepts and use this model to infer unseen aspects of the environment. - They use condition-action rules, but these rules can reference the internal model. Example: - A navigation system that uses a map to plan routes and update the user’s location. - Use Case: GPS navigation, industrial automation systems. Advantages: - Can handle partially observable environments. - More flexible and capable than simple reflex agents. Disadvantages: - More complex to design and implement. - Requires more computational resources to maintain and update the internal model. 3. Goal-Based Agents Overview: - Goal-based agents operate based on predefined goals. They make decisions by evaluating how well different actions achieve these goals. - They can plan sequences of actions to achieve their objectives. How They Work: - These agents use search and planning algorithms to determine the best course of action to reach a goal. - They consider both the current state and the desired goal state. Example: - An AI planning system that schedules tasks to maximize efficiency and meet deadlines. - Use Case: Automated scheduling, robotic path planning. Advantages: - Capable of complex decision-making and planning. - Can adapt to changes in goals and environment. Disadvantages: - Requires complex algorithms for search and planning. - May be computationally intensive, especially in dynamic environments. 4. Utility-Based Agents Overview: - Utility-based agents aim to maximize their overall utility or satisfaction by evaluating the desirability of different outcomes. - They make decisions based on a utility function that assigns a value to each possible state. How They Work: - These agents use optimization techniques to select actions that maximize their expected utility. - They consider multiple factors and trade-offs to achieve the", "tokens": 665, "chunk_type": "original-large", "url": "https://medium.com/@williamwarley/understanding-ai-agents-how-they-work-types-and-practical-applications-bd261845f7c3", "anchor": "#body:part-5", "type": "blog", "title": "", "section": "Body", "source": "blog:medium.com", "published": "", "sha256": "c2761f0ddb77fd6880e59aae306372d001ad87e661ad327d48ac9780dfcdd821"}
{"doc_id": "blog:medium.com", "chunk_id": "body:part-5-o1", "text": "of AI Agents AI agents come in various types, each designed to handle specific tasks and environments. Understanding these types helps in selecting the right kind of agent for a particular application. This chapter explores the different types of AI agents, their characteristics, and use cases. 1. Simple Reflex Agents Overview: - Simple reflex agents act solely based on the current percept and predefined rules, without considering the history of past perceptions. - They are the simplest form of agents and work well in environments that are fully observable and deterministic. How They Work: - These agents use condition-action rules (if-then statements) to decide on actions. - They do not maintain any internal state or model of the environment. Example: - A thermostat that turns the heating on or off based on the current temperature reading. - Use Case: Simple household appliances and basic automated systems. Advantages: - Easy to design and implement. - Efficient in predictable environments. Disadvantages: - Limited functionality in complex or partially observable environments. - Cannot learn or adapt to changes in the environment. 2. Model-Based Reflex Agents Overview: - Model-based reflex agents maintain an internal model of the environment, allowing them to handle partially observable environments better than simple reflex agents. - They can consider the history of past perceptions to make more informed decisions. How They Work: - These agents update their internal model based on incoming percepts and use this model to infer unseen aspects of the environment. - They use condition-action rules, but these rules can reference the internal model. Example: - A navigation system that uses a map to plan routes and update the user’s location. - Use Case: GPS navigation, industrial automation systems. Advantages: - Can handle partially observable environments. - More flexible and capable than simple reflex agents.", "tokens": 388, "chunk_type": "overlap-1", "url": "https://medium.com/@williamwarley/understanding-ai-agents-how-they-work-types-and-practical-applications-bd261845f7c3", "anchor": "#body:part-5", "type": "blog", "title": "", "section": "Body", "source": "blog:medium.com", "published": "", "sha256": "c2761f0ddb77fd6880e59aae306372d001ad87e661ad327d48ac9780dfcdd821"}
{"doc_id": "blog:medium.com", "chunk_id": "body:part-5-o2", "text": "How They Work: - These agents update their internal model based on incoming percepts and use this model to infer unseen aspects of the environment. - They use condition-action rules, but these rules can reference the internal model. Example: - A navigation system that uses a map to plan routes and update the user’s location. - Use Case: GPS navigation, industrial automation systems. Advantages: - Can handle partially observable environments. - More flexible and capable than simple reflex agents. Disadvantages: - More complex to design and implement. - Requires more computational resources to maintain and update the internal model. 3. Goal-Based Agents Overview: - Goal-based agents operate based on predefined goals. They make decisions by evaluating how well different actions achieve these goals. - They can plan sequences of actions to achieve their objectives. How They Work: - These agents use search and planning algorithms to determine the best course of action to reach a goal. - They consider both the current state and the desired goal state. Example: - An AI planning system that schedules tasks to maximize efficiency and meet deadlines. - Use Case: Automated scheduling, robotic path planning. Advantages: - Capable of complex decision-making and planning. - Can adapt to changes in goals and environment. Disadvantages: - Requires complex algorithms for search and planning. - May be computationally intensive, especially in dynamic environments. 4. Utility-Based Agents Overview: - Utility-based agents aim to maximize their overall utility or satisfaction by evaluating the desirability of different outcomes. - They make decisions based on a utility function that assigns a value to each possible state. How They Work: - These agents use optimization techniques to select actions that maximize their expected utility. - They consider multiple factors and trade-offs to achieve the", "tokens": 379, "chunk_type": "overlap-2", "url": "https://medium.com/@williamwarley/understanding-ai-agents-how-they-work-types-and-practical-applications-bd261845f7c3", "anchor": "#body:part-5", "type": "blog", "title": "", "section": "Body", "source": "blog:medium.com", "published": "", "sha256": "c2761f0ddb77fd6880e59aae306372d001ad87e661ad327d48ac9780dfcdd821"}
{"doc_id": "blog:medium.com", "chunk_id": "body:part-6", "text": "algorithms for search and planning. - May be computationally intensive, especially in dynamic environments. 4. Utility-Based Agents Overview: - Utility-based agents aim to maximize their overall utility or satisfaction by evaluating the desirability of different outcomes. - They make decisions based on a utility function that assigns a value to each possible state. How They Work: - These agents use optimization techniques to select actions that maximize their expected utility. - They consider multiple factors and trade-offs to achieve the best overall outcome. Example: - An autonomous trading system that selects trades to maximize profit while minimizing risk. - Use Case: Financial trading, resource management. Advantages: - Can handle complex decision-making scenarios involving trade-offs. - Capable of balancing multiple objectives and preferences. Disadvantages: - Designing an appropriate utility function can be challenging. - May require significant computational resources for optimization. 5. Learning Agents Overview: - Learning agents improve their performance over time by learning from experiences and adapting to new situations. - They can operate in dynamic and uncertain environments by continuously updating their knowledge and strategies. How They Work: - These agents use various learning algorithms, such as supervised learning, unsupervised learning, and reinforcement learning, to acquire new knowledge and skills. - They have four main components: a learning element, a performance element, a critic, and a problem generator. Example: - A recommendation engine that refines its suggestions based on user feedback and interactions. - Use Case: Personalized recommendations, autonomous systems, adaptive control. Advantages: - Capable of continuous improvement and adaptation. - Can handle complex and changing environments. Disadvantages: - Requires significant amounts of data for effective learning. - The learning process can be computationally intensive and time-consuming. This chapter explores the various types of AI agents, highlighting their characteristics, how they work, and their respective advantages and disadvantages. Applications of AI Agents AI agents are deployed across various industries to automate tasks, enhance decision-making, and improve overall efficiency. This chapter explores several practical applications of AI agents, highlighting their impact and benefits in different domains. 1. Autonomous Vehicles Overview: - AI agents play a crucial role in the development of autonomous vehicles, enabling them to perceive their environment, make driving decisions, and navigate safely. How They Work: - Autonomous vehicles use sensors like cameras, LIDAR, and radar to gather data about the surroundings. - AI agents process this data to identify objects, predict their movements, and make real-time driving decisions. - The agents use machine learning algorithms to improve their performance over time, adapting to different driving conditions. Example: - Waymo’s self-driving cars use AI agents to navigate complex urban environments and handle various driving scenarios. - Benefits: Enhanced safety, reduced traffic congestion, and increased mobility for individuals who cannot drive. 2. Robotics Overview: - AI agents are integral to robotics, enabling robots to perform tasks autonomously in various settings, from manufacturing to healthcare. How They Work: - Robots equipped with AI agents use sensors to perceive their environment and execute tasks with precision. - AI agents in robots can learn from interactions and adapt to new tasks, improving efficiency", "tokens": 665, "chunk_type": "original-large", "url": "https://medium.com/@williamwarley/understanding-ai-agents-how-they-work-types-and-practical-applications-bd261845f7c3", "anchor": "#body:part-6", "type": "blog", "title": "", "section": "Body", "source": "blog:medium.com", "published": "", "sha256": "0edafbfdfeee96e12231dd9f7c2d4476f5eb9cb7af71cbf55d475997e7ce5136"}
{"doc_id": "blog:medium.com", "chunk_id": "body:part-6-o1", "text": "algorithms for search and planning. - May be computationally intensive, especially in dynamic environments. 4. Utility-Based Agents Overview: - Utility-based agents aim to maximize their overall utility or satisfaction by evaluating the desirability of different outcomes. - They make decisions based on a utility function that assigns a value to each possible state. How They Work: - These agents use optimization techniques to select actions that maximize their expected utility. - They consider multiple factors and trade-offs to achieve the best overall outcome. Example: - An autonomous trading system that selects trades to maximize profit while minimizing risk. - Use Case: Financial trading, resource management. Advantages: - Can handle complex decision-making scenarios involving trade-offs. - Capable of balancing multiple objectives and preferences. Disadvantages: - Designing an appropriate utility function can be challenging. - May require significant computational resources for optimization. 5. Learning Agents Overview: - Learning agents improve their performance over time by learning from experiences and adapting to new situations. - They can operate in dynamic and uncertain environments by continuously updating their knowledge and strategies. How They Work: - These agents use various learning algorithms, such as supervised learning, unsupervised learning, and reinforcement learning, to acquire new knowledge and skills. - They have four main components: a learning element, a performance element, a critic, and a problem generator. Example: - A recommendation engine that refines its suggestions based on user feedback and interactions. - Use Case: Personalized recommendations, autonomous systems, adaptive control. Advantages: - Capable of continuous improvement and adaptation. - Can handle complex and changing environments. Disadvantages: - Requires significant amounts of data for effective learning. - The learning process can be computationally intensive and time-consuming. This chapter explores the various types of AI agents, highlighting their characteristics, how they work, and their respective advantages and disadvantages.", "tokens": 390, "chunk_type": "overlap-1", "url": "https://medium.com/@williamwarley/understanding-ai-agents-how-they-work-types-and-practical-applications-bd261845f7c3", "anchor": "#body:part-6", "type": "blog", "title": "", "section": "Body", "source": "blog:medium.com", "published": "", "sha256": "0edafbfdfeee96e12231dd9f7c2d4476f5eb9cb7af71cbf55d475997e7ce5136"}
{"doc_id": "blog:medium.com", "chunk_id": "body:part-6-o2", "text": "Example: - A recommendation engine that refines its suggestions based on user feedback and interactions. - Use Case: Personalized recommendations, autonomous systems, adaptive control. Advantages: - Capable of continuous improvement and adaptation. - Can handle complex and changing environments. Disadvantages: - Requires significant amounts of data for effective learning. - The learning process can be computationally intensive and time-consuming. This chapter explores the various types of AI agents, highlighting their characteristics, how they work, and their respective advantages and disadvantages. Applications of AI Agents AI agents are deployed across various industries to automate tasks, enhance decision-making, and improve overall efficiency. This chapter explores several practical applications of AI agents, highlighting their impact and benefits in different domains. 1. Autonomous Vehicles Overview: - AI agents play a crucial role in the development of autonomous vehicles, enabling them to perceive their environment, make driving decisions, and navigate safely. How They Work: - Autonomous vehicles use sensors like cameras, LIDAR, and radar to gather data about the surroundings. - AI agents process this data to identify objects, predict their movements, and make real-time driving decisions. - The agents use machine learning algorithms to improve their performance over time, adapting to different driving conditions. Example: - Waymo’s self-driving cars use AI agents to navigate complex urban environments and handle various driving scenarios. - Benefits: Enhanced safety, reduced traffic congestion, and increased mobility for individuals who cannot drive. 2. Robotics Overview: - AI agents are integral to robotics, enabling robots to perform tasks autonomously in various settings, from manufacturing to healthcare. How They Work: - Robots equipped with AI agents use sensors to perceive their environment and execute tasks with precision. - AI agents in robots can learn from interactions and adapt to new tasks, improving efficiency", "tokens": 379, "chunk_type": "overlap-2", "url": "https://medium.com/@williamwarley/understanding-ai-agents-how-they-work-types-and-practical-applications-bd261845f7c3", "anchor": "#body:part-6", "type": "blog", "title": "", "section": "Body", "source": "blog:medium.com", "published": "", "sha256": "0edafbfdfeee96e12231dd9f7c2d4476f5eb9cb7af71cbf55d475997e7ce5136"}
{"doc_id": "blog:medium.com", "chunk_id": "body:part-7", "text": "environments and handle various driving scenarios. - Benefits: Enhanced safety, reduced traffic congestion, and increased mobility for individuals who cannot drive. 2. Robotics Overview: - AI agents are integral to robotics, enabling robots to perform tasks autonomously in various settings, from manufacturing to healthcare. How They Work: - Robots equipped with AI agents use sensors to perceive their environment and execute tasks with precision. - AI agents in robots can learn from interactions and adapt to new tasks, improving efficiency and accuracy. - Robots use goal-based and utility-based agents to optimize their actions and achieve specific objectives. Example: - Collaborative robots (cobots) in manufacturing work alongside human workers, performing repetitive and precise tasks. - Benefits: Increased productivity, enhanced precision, and improved workplace safety. 3. Personal Assistants Overview: - AI agents power personal assistants like Siri, Alexa, and Google Assistant, enabling them to understand and respond to user queries. How They Work: - Personal assistants use natural language processing (NLP) to understand spoken or written commands. - AI agents process the input, retrieve relevant information, and generate appropriate responses. - These agents continuously learn from interactions to improve their understanding and accuracy. Example: - Amazon Alexa uses AI agents to control smart home devices, provide weather updates, and play music based on user preferences. - Benefits: Convenience, hands-free control, and personalized user experiences. 4. Game AI Overview: - AI agents are widely used in video games to create intelligent and adaptive non-player characters (NPCs) that enhance gameplay. How They Work: - Game AI agents use rule-based and learning algorithms to control NPC behavior, making them respond dynamically to player actions. - Agents can adapt their strategies based on player performance, providing a challenging and engaging experience. - Reinforcement learning is often used to train game AI agents, allowing them to optimize their behavior through trial and error. Example: - In games like “The Sims,” AI agents control the behavior of virtual characters, making decisions based on their needs and environment. - Benefits: Improved player engagement, realistic NPC behavior, and dynamic gameplay experiences. 5. Financial Trading Overview: - AI agents are employed in financial trading to analyze market data, make trading decisions, and execute trades autonomously. How They Work: - AI agents use machine learning algorithms to analyze historical and real-time market data, identifying patterns and trends. - These agents make trading decisions based on predefined strategies and continuously learn to improve their performance. - Utility-based agents optimize trading strategies to maximize profits while minimizing risks. Example: - AI-powered trading platforms like QuantConnect use AI agents to develop and execute automated trading strategies. - Benefits: Enhanced trading efficiency, reduced human error, and the ability to operate 24/7. 6. Healthcare Overview: - AI agents are transforming healthcare by assisting in diagnostics, treatment planning, and patient management. How They Work: - AI agents analyze medical data, such as patient records and imaging, to assist in diagnosing diseases and recommending treatments. - They use supervised learning to learn from labeled medical data and improve their diagnostic accuracy. - Agents can also monitor patient health in real-time, providing", "tokens": 665, "chunk_type": "original-large", "url": "https://medium.com/@williamwarley/understanding-ai-agents-how-they-work-types-and-practical-applications-bd261845f7c3", "anchor": "#body:part-7", "type": "blog", "title": "", "section": "Body", "source": "blog:medium.com", "published": "", "sha256": "17f0967d569f50feed0d1b75fff4db4d59fe8ee3388027b2ec37e414f50229a6"}
{"doc_id": "blog:medium.com", "chunk_id": "body:part-7-o1", "text": "environments and handle various driving scenarios. - Benefits: Enhanced safety, reduced traffic congestion, and increased mobility for individuals who cannot drive. 2. Robotics Overview: - AI agents are integral to robotics, enabling robots to perform tasks autonomously in various settings, from manufacturing to healthcare. How They Work: - Robots equipped with AI agents use sensors to perceive their environment and execute tasks with precision. - AI agents in robots can learn from interactions and adapt to new tasks, improving efficiency and accuracy. - Robots use goal-based and utility-based agents to optimize their actions and achieve specific objectives. Example: - Collaborative robots (cobots) in manufacturing work alongside human workers, performing repetitive and precise tasks. - Benefits: Increased productivity, enhanced precision, and improved workplace safety. 3. Personal Assistants Overview: - AI agents power personal assistants like Siri, Alexa, and Google Assistant, enabling them to understand and respond to user queries. How They Work: - Personal assistants use natural language processing (NLP) to understand spoken or written commands. - AI agents process the input, retrieve relevant information, and generate appropriate responses. - These agents continuously learn from interactions to improve their understanding and accuracy. Example: - Amazon Alexa uses AI agents to control smart home devices, provide weather updates, and play music based on user preferences. - Benefits: Convenience, hands-free control, and personalized user experiences. 4. Game AI Overview: - AI agents are widely used in video games to create intelligent and adaptive non-player characters (NPCs) that enhance gameplay. How They Work: - Game AI agents use rule-based and learning algorithms to control NPC behavior, making them respond dynamically to player actions. - Agents can adapt their strategies based on player performance, providing a challenging and engaging experience. - Reinforcement learning is often used to train game AI agents, allowing them to optimize their behavior through trial and error.", "tokens": 397, "chunk_type": "overlap-1", "url": "https://medium.com/@williamwarley/understanding-ai-agents-how-they-work-types-and-practical-applications-bd261845f7c3", "anchor": "#body:part-7", "type": "blog", "title": "", "section": "Body", "source": "blog:medium.com", "published": "", "sha256": "17f0967d569f50feed0d1b75fff4db4d59fe8ee3388027b2ec37e414f50229a6"}
{"doc_id": "blog:medium.com", "chunk_id": "body:part-7-o2", "text": "How They Work: - Game AI agents use rule-based and learning algorithms to control NPC behavior, making them respond dynamically to player actions. - Agents can adapt their strategies based on player performance, providing a challenging and engaging experience. - Reinforcement learning is often used to train game AI agents, allowing them to optimize their behavior through trial and error. Example: - In games like “The Sims,” AI agents control the behavior of virtual characters, making decisions based on their needs and environment. - Benefits: Improved player engagement, realistic NPC behavior, and dynamic gameplay experiences. 5. Financial Trading Overview: - AI agents are employed in financial trading to analyze market data, make trading decisions, and execute trades autonomously. How They Work: - AI agents use machine learning algorithms to analyze historical and real-time market data, identifying patterns and trends. - These agents make trading decisions based on predefined strategies and continuously learn to improve their performance. - Utility-based agents optimize trading strategies to maximize profits while minimizing risks. Example: - AI-powered trading platforms like QuantConnect use AI agents to develop and execute automated trading strategies. - Benefits: Enhanced trading efficiency, reduced human error, and the ability to operate 24/7. 6. Healthcare Overview: - AI agents are transforming healthcare by assisting in diagnostics, treatment planning, and patient management. How They Work: - AI agents analyze medical data, such as patient records and imaging, to assist in diagnosing diseases and recommending treatments. - They use supervised learning to learn from labeled medical data and improve their diagnostic accuracy. - Agents can also monitor patient health in real-time, providing", "tokens": 345, "chunk_type": "overlap-2", "url": "https://medium.com/@williamwarley/understanding-ai-agents-how-they-work-types-and-practical-applications-bd261845f7c3", "anchor": "#body:part-7", "type": "blog", "title": "", "section": "Body", "source": "blog:medium.com", "published": "", "sha256": "17f0967d569f50feed0d1b75fff4db4d59fe8ee3388027b2ec37e414f50229a6"}
{"doc_id": "blog:medium.com", "chunk_id": "body:part-8", "text": "Benefits: Enhanced trading efficiency, reduced human error, and the ability to operate 24/7. 6. Healthcare Overview: - AI agents are transforming healthcare by assisting in diagnostics, treatment planning, and patient management. How They Work: - AI agents analyze medical data, such as patient records and imaging, to assist in diagnosing diseases and recommending treatments. - They use supervised learning to learn from labeled medical data and improve their diagnostic accuracy. - Agents can also monitor patient health in real-time, providing alerts and recommendations based on the collected data. Example: - IBM Watson for Oncology uses AI agents to analyze medical literature and patient data, helping oncologists develop personalized cancer treatment plans. - Benefits: Improved diagnostic accuracy, personalized treatment, and enhanced patient care. This chapter explores the diverse applications of AI agents across various industries, demonstrating their impact and benefits. Advantages of AI Agents AI agents offer numerous advantages that make them indispensable in various applications. This chapter discusses the key benefits of AI agents, highlighting how they contribute to efficiency, scalability, real-time decision-making, and adaptability. 1. Efficiency and Automation Task Automation: - AI agents excel at automating repetitive and time-consuming tasks, freeing up human resources for more complex and creative work. - Example: In customer service, AI agents can handle common inquiries, process transactions, and provide instant support, allowing human agents to focus on more complex issues. Increased Productivity: - By performing tasks continuously without fatigue, AI agents significantly increase productivity and operational efficiency. - Example: In manufacturing, robotic AI agents can work 24/7, assembling products with precision and speed. Error Reduction: - AI agents reduce the likelihood of human error by performing tasks consistently and accurately. - Example: In data entry and processing, AI agents ensure accuracy and consistency, reducing errors that can occur with manual handling. 2. Scalability Handling Large Volumes: - AI agents can process vast amounts of data and manage large-scale operations, making them ideal for applications that require scalability. - Example: In financial trading, AI agents can analyze and act on market data from multiple sources in real-time, scaling to handle increased trading volumes. Flexible Resource Allocation: - AI agents can dynamically allocate resources based on demand, ensuring optimal performance and cost-efficiency. - Example: Cloud-based AI agents can scale computing resources up or down based on application needs, optimizing performance and costs. Global Reach: - AI agents can operate across different time zones and geographies, providing services and support around the clock. - Example: AI-driven customer support agents can assist customers worldwide, ensuring continuous service availability. 3. Real-Time Decision Making Immediate Responses: - AI agents can process information and make decisions in real-time, which is critical for applications requiring instant responses. - Example: In autonomous driving, AI agents process sensor data and make split-second decisions to navigate safely. Adaptive Strategies: - AI agents can adapt their strategies based on real-time data and changing conditions, optimizing outcomes dynamically. - Example: In online advertising, AI agents adjust bidding strategies in real-time to maximize ad performance and budget efficiency. Proactive Monitoring: - AI agents continuously monitor systems and environments, detecting", "tokens": 665, "chunk_type": "original-large", "url": "https://medium.com/@williamwarley/understanding-ai-agents-how-they-work-types-and-practical-applications-bd261845f7c3", "anchor": "#body:part-8", "type": "blog", "title": "", "section": "Body", "source": "blog:medium.com", "published": "", "sha256": "7637014467b449533fabba6363ef8c3a856ee1b2acaa88cfba0cb07b074d16fd"}
{"doc_id": "blog:medium.com", "chunk_id": "body:part-8-o1", "text": "Benefits: Enhanced trading efficiency, reduced human error, and the ability to operate 24/7. 6. Healthcare Overview: - AI agents are transforming healthcare by assisting in diagnostics, treatment planning, and patient management. How They Work: - AI agents analyze medical data, such as patient records and imaging, to assist in diagnosing diseases and recommending treatments. - They use supervised learning to learn from labeled medical data and improve their diagnostic accuracy. - Agents can also monitor patient health in real-time, providing alerts and recommendations based on the collected data. Example: - IBM Watson for Oncology uses AI agents to analyze medical literature and patient data, helping oncologists develop personalized cancer treatment plans. - Benefits: Improved diagnostic accuracy, personalized treatment, and enhanced patient care. This chapter explores the diverse applications of AI agents across various industries, demonstrating their impact and benefits. Advantages of AI Agents AI agents offer numerous advantages that make them indispensable in various applications. This chapter discusses the key benefits of AI agents, highlighting how they contribute to efficiency, scalability, real-time decision-making, and adaptability. 1. Efficiency and Automation Task Automation: - AI agents excel at automating repetitive and time-consuming tasks, freeing up human resources for more complex and creative work. - Example: In customer service, AI agents can handle common inquiries, process transactions, and provide instant support, allowing human agents to focus on more complex issues. Increased Productivity: - By performing tasks continuously without fatigue, AI agents significantly increase productivity and operational efficiency. - Example: In manufacturing, robotic AI agents can work 24/7, assembling products with precision and speed. Error Reduction: - AI agents reduce the likelihood of human error by performing tasks consistently and accurately. - Example: In data entry and processing, AI agents ensure accuracy and consistency, reducing errors that can occur with manual handling. 2.", "tokens": 388, "chunk_type": "overlap-1", "url": "https://medium.com/@williamwarley/understanding-ai-agents-how-they-work-types-and-practical-applications-bd261845f7c3", "anchor": "#body:part-8", "type": "blog", "title": "", "section": "Body", "source": "blog:medium.com", "published": "", "sha256": "7637014467b449533fabba6363ef8c3a856ee1b2acaa88cfba0cb07b074d16fd"}
{"doc_id": "blog:medium.com", "chunk_id": "body:part-8-o2", "text": "Increased Productivity: - By performing tasks continuously without fatigue, AI agents significantly increase productivity and operational efficiency. - Example: In manufacturing, robotic AI agents can work 24/7, assembling products with precision and speed. Error Reduction: - AI agents reduce the likelihood of human error by performing tasks consistently and accurately. - Example: In data entry and processing, AI agents ensure accuracy and consistency, reducing errors that can occur with manual handling. 2. Scalability Handling Large Volumes: - AI agents can process vast amounts of data and manage large-scale operations, making them ideal for applications that require scalability. - Example: In financial trading, AI agents can analyze and act on market data from multiple sources in real-time, scaling to handle increased trading volumes. Flexible Resource Allocation: - AI agents can dynamically allocate resources based on demand, ensuring optimal performance and cost-efficiency. - Example: Cloud-based AI agents can scale computing resources up or down based on application needs, optimizing performance and costs. Global Reach: - AI agents can operate across different time zones and geographies, providing services and support around the clock. - Example: AI-driven customer support agents can assist customers worldwide, ensuring continuous service availability. 3. Real-Time Decision Making Immediate Responses: - AI agents can process information and make decisions in real-time, which is critical for applications requiring instant responses. - Example: In autonomous driving, AI agents process sensor data and make split-second decisions to navigate safely. Adaptive Strategies: - AI agents can adapt their strategies based on real-time data and changing conditions, optimizing outcomes dynamically. - Example: In online advertising, AI agents adjust bidding strategies in real-time to maximize ad performance and budget efficiency. Proactive Monitoring: - AI agents continuously monitor systems and environments, detecting", "tokens": 370, "chunk_type": "overlap-2", "url": "https://medium.com/@williamwarley/understanding-ai-agents-how-they-work-types-and-practical-applications-bd261845f7c3", "anchor": "#body:part-8", "type": "blog", "title": "", "section": "Body", "source": "blog:medium.com", "published": "", "sha256": "7637014467b449533fabba6363ef8c3a856ee1b2acaa88cfba0cb07b074d16fd"}
{"doc_id": "blog:medium.com", "chunk_id": "body:part-9", "text": "information and make decisions in real-time, which is critical for applications requiring instant responses. - Example: In autonomous driving, AI agents process sensor data and make split-second decisions to navigate safely. Adaptive Strategies: - AI agents can adapt their strategies based on real-time data and changing conditions, optimizing outcomes dynamically. - Example: In online advertising, AI agents adjust bidding strategies in real-time to maximize ad performance and budget efficiency. Proactive Monitoring: - AI agents continuously monitor systems and environments, detecting anomalies and triggering appropriate actions immediately. - Example: In cybersecurity, AI agents detect and respond to threats in real-time, protecting systems from potential breaches. 4. Adaptability and Learning Continuous Improvement: - AI agents improve their performance over time by learning from experiences and feedback, adapting to new situations and tasks. - Example: Personalized recommendation systems learn from user interactions to provide increasingly relevant suggestions. Handling Uncertainty: - AI agents can operate effectively in uncertain and dynamic environments by adapting their behavior based on learned patterns and real-time data. - Example: In robotics, AI agents adapt to changes in their environment, such as obstacles or varying conditions, to complete tasks efficiently. Customization and Personalization: - AI agents can tailor their actions and responses to individual user preferences and needs, providing personalized experiences. - Example: Virtual personal assistants learn user preferences over time, offering personalized recommendations and assistance. 5. Cost Efficiency Reduced Operational Costs: - Automating tasks with AI agents reduces the need for manual labor, lowering operational costs and increasing profitability. - Example: Automated warehouses use AI agents to manage inventory and logistics, reducing labor costs and increasing efficiency. Optimized Resource Utilization: - AI agents optimize the use of resources, such as energy and materials, leading to cost savings and sustainability. - Example: Smart energy management systems use AI agents to optimize energy usage in buildings, reducing costs and environmental impact. Investment in Innovation: - The efficiency gains and cost savings from AI agents allow organizations to invest more in innovation and strategic initiatives. - Example: Companies can allocate resources saved from automation to research and development, driving future growth and competitiveness. This chapter highlights the numerous advantages of AI agents, emphasizing their role in enhancing efficiency, scalability, real-time decision-making, adaptability, and cost-efficiency. Implementing AI Agents Implementing AI agents involves a series of practical steps and considerations, from selecting the right tools and frameworks to addressing common challenges. This chapter provides a comprehensive guide to implementing AI agents effectively. Steps for Implementing AI Agents 1. Define Objectives and Requirements: — Clearly outline the goals you aim to achieve with the AI agent, including specific tasks and performance metrics. — Example: An e-commerce platform might aim to implement an AI agent for personalized product recommendations, with the objective of increasing sales and customer satisfaction. 2. Select Appropriate Tools and Frameworks: — Choose the tools and frameworks that best suit your needs based on the complexity of the task, available resources, and technical expertise. — Example: For natural language processing tasks, frameworks like Hugging Face Transformers or spaCy might be appropriate. 3. Data Collection and", "tokens": 665, "chunk_type": "original-large", "url": "https://medium.com/@williamwarley/understanding-ai-agents-how-they-work-types-and-practical-applications-bd261845f7c3", "anchor": "#body:part-9", "type": "blog", "title": "", "section": "Body", "source": "blog:medium.com", "published": "", "sha256": "7da69a6cc7838be688bb29867a8476b46bbbffdcf007c4b8d7a8696d63c19ffb"}
{"doc_id": "blog:medium.com", "chunk_id": "body:part-9-o1", "text": "information and make decisions in real-time, which is critical for applications requiring instant responses. - Example: In autonomous driving, AI agents process sensor data and make split-second decisions to navigate safely. Adaptive Strategies: - AI agents can adapt their strategies based on real-time data and changing conditions, optimizing outcomes dynamically. - Example: In online advertising, AI agents adjust bidding strategies in real-time to maximize ad performance and budget efficiency. Proactive Monitoring: - AI agents continuously monitor systems and environments, detecting anomalies and triggering appropriate actions immediately. - Example: In cybersecurity, AI agents detect and respond to threats in real-time, protecting systems from potential breaches. 4. Adaptability and Learning Continuous Improvement: - AI agents improve their performance over time by learning from experiences and feedback, adapting to new situations and tasks. - Example: Personalized recommendation systems learn from user interactions to provide increasingly relevant suggestions. Handling Uncertainty: - AI agents can operate effectively in uncertain and dynamic environments by adapting their behavior based on learned patterns and real-time data. - Example: In robotics, AI agents adapt to changes in their environment, such as obstacles or varying conditions, to complete tasks efficiently. Customization and Personalization: - AI agents can tailor their actions and responses to individual user preferences and needs, providing personalized experiences. - Example: Virtual personal assistants learn user preferences over time, offering personalized recommendations and assistance. 5. Cost Efficiency Reduced Operational Costs: - Automating tasks with AI agents reduces the need for manual labor, lowering operational costs and increasing profitability. - Example: Automated warehouses use AI agents to manage inventory and logistics, reducing labor costs and increasing efficiency. Optimized Resource Utilization: - AI agents optimize the use of resources, such as energy and materials, leading to cost savings and sustainability. - Example: Smart energy management systems use AI agents to optimize energy usage in buildings, reducing costs and environmental impact.", "tokens": 403, "chunk_type": "overlap-1", "url": "https://medium.com/@williamwarley/understanding-ai-agents-how-they-work-types-and-practical-applications-bd261845f7c3", "anchor": "#body:part-9", "type": "blog", "title": "", "section": "Body", "source": "blog:medium.com", "published": "", "sha256": "7da69a6cc7838be688bb29867a8476b46bbbffdcf007c4b8d7a8696d63c19ffb"}
{"doc_id": "blog:medium.com", "chunk_id": "body:part-9-o2", "text": "Cost Efficiency Reduced Operational Costs: - Automating tasks with AI agents reduces the need for manual labor, lowering operational costs and increasing profitability. - Example: Automated warehouses use AI agents to manage inventory and logistics, reducing labor costs and increasing efficiency. Optimized Resource Utilization: - AI agents optimize the use of resources, such as energy and materials, leading to cost savings and sustainability. - Example: Smart energy management systems use AI agents to optimize energy usage in buildings, reducing costs and environmental impact. Investment in Innovation: - The efficiency gains and cost savings from AI agents allow organizations to invest more in innovation and strategic initiatives. - Example: Companies can allocate resources saved from automation to research and development, driving future growth and competitiveness. This chapter highlights the numerous advantages of AI agents, emphasizing their role in enhancing efficiency, scalability, real-time decision-making, adaptability, and cost-efficiency. Implementing AI Agents Implementing AI agents involves a series of practical steps and considerations, from selecting the right tools and frameworks to addressing common challenges. This chapter provides a comprehensive guide to implementing AI agents effectively. Steps for Implementing AI Agents 1. Define Objectives and Requirements: — Clearly outline the goals you aim to achieve with the AI agent, including specific tasks and performance metrics. — Example: An e-commerce platform might aim to implement an AI agent for personalized product recommendations, with the objective of increasing sales and customer satisfaction. 2. Select Appropriate Tools and Frameworks: — Choose the tools and frameworks that best suit your needs based on the complexity of the task, available resources, and technical expertise. — Example: For natural language processing tasks, frameworks like Hugging Face Transformers or spaCy might be appropriate. 3. Data Collection and", "tokens": 370, "chunk_type": "overlap-2", "url": "https://medium.com/@williamwarley/understanding-ai-agents-how-they-work-types-and-practical-applications-bd261845f7c3", "anchor": "#body:part-9", "type": "blog", "title": "", "section": "Body", "source": "blog:medium.com", "published": "", "sha256": "7da69a6cc7838be688bb29867a8476b46bbbffdcf007c4b8d7a8696d63c19ffb"}
{"doc_id": "blog:medium.com", "chunk_id": "body:part-10", "text": "specific tasks and performance metrics. — Example: An e-commerce platform might aim to implement an AI agent for personalized product recommendations, with the objective of increasing sales and customer satisfaction. 2. Select Appropriate Tools and Frameworks: — Choose the tools and frameworks that best suit your needs based on the complexity of the task, available resources, and technical expertise. — Example: For natural language processing tasks, frameworks like Hugging Face Transformers or spaCy might be appropriate. 3. Data Collection and Preprocessing: — Gather and preprocess the data required for training the AI agent. Ensure that the data is clean, labeled (if necessary), and representative of the problem domain. — Example: Collect customer interaction logs and preprocess them to remove noise and irrelevant information for training a customer service chatbot. 4. Model Selection and Training: — Select the appropriate machine learning or deep learning model for your AI agent. Train the model using the preprocessed data, and fine-tune it to achieve optimal performance. — Example: Use a pre-trained transformer model like BERT for fine-tuning on a specific NLP task. Example Code for Training a Model with Hugging Face Transformers: from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments from datasets import load_dataset # Load dataset dataset = load_dataset('csv', data_files={'train': 'train.csv', 'test': 'test.csv'}) # Load tokenizer and model tokenizer = BertTokenizer.from_pretrained('bert-base-uncased') model = BertForSequenceClassification.from_pretrained('bert-base-uncased') # Tokenize data def tokenize_function(examples): return tokenizer(examples['text'], padding='max_length', truncation=True) tokenized_datasets = dataset.map(tokenize_function, batched=True) # Define training arguments training_args = TrainingArguments( output_dir='./results', evaluation_strategy='epoch', learning_rate=2e-5, per_device_train_batch_size=8, per_device_eval_batch_size=8, num_train_epochs=3, weight_decay=0.01, ) # Initialize Trainer trainer = Trainer( model=model, args=training_args, train_dataset=tokenized_datasets['train'], eval_dataset=tokenized_datasets['test'], ) # Train model trainer.train() 5. Evaluation and Testing: — Evaluate the AI agent’s performance using appropriate metrics and test it in various scenarios to ensure robustness and reliability. — Example: Evaluate a recommendation system using metrics like precision, recall, and F1-score on a validation dataset. 6. Deployment: — Deploy the AI agent in the target environment, ensuring it integrates smoothly with existing systems and can operate at scale. — Example: Deploy a trained chatbot on a cloud platform like AWS Lambda for scalable, serverless execution. Example Code for Deploying a Model on AWS Lambda: import json import boto3 from transformers import BertTokenizer, BertForSequenceClassification # Initialize AWS Lambda client client = boto3.client('lambda') # Define the Lambda function def lambda_handler(event, context): # Load tokenizer and model tokenizer = BertTokenizer.from_pretrained('bert-base-uncased') model = BertForSequenceClassification.from_pretrained('bert-base-uncased') # Parse input input_text = event['text'] inputs = tokenizer(input_text, return_tensors='pt') # Perform inference outputs = model(**inputs) predictions = outputs.logits.argmax(dim=-1).item() # Return response return { 'statusCode': 200, 'body': json.dumps({'prediction': predictions}) } # Deploy the Lambda function response = client.create_function( FunctionName='AIChatbot', Runtime='python3.8', Role='your-aws-role', Handler='lambda_function.lambda_handler', Code={'ZipFile': open('function.zip', 'rb').read()}, Timeout=15, MemorySize=128, ) 7. Monitoring and Maintenance: — Continuously monitor the AI agent’s performance and make necessary updates or retrain the model to maintain its effectiveness. — Example: Regularly update the training data and retrain a sentiment analysis model to adapt to changing trends and language usage. Tools and Frameworks 1. Machine Learning Frameworks: — TensorFlow: Widely used for building and deploying machine learning models, with extensive support for neural networks and deep learning. — PyTorch: Popular for", "tokens": 665, "chunk_type": "original-large", "url": "https://medium.com/@williamwarley/understanding-ai-agents-how-they-work-types-and-practical-applications-bd261845f7c3", "anchor": "#body:part-10", "type": "blog", "title": "", "section": "Body", "source": "blog:medium.com", "published": "", "sha256": "80bee5fb3d1b45677f42423e475133e3e4191c2bb25f85154395fc97edea9f27"}
{"doc_id": "blog:medium.com", "chunk_id": "body:part-10-o1", "text": "specific tasks and performance metrics. — Example: An e-commerce platform might aim to implement an AI agent for personalized product recommendations, with the objective of increasing sales and customer satisfaction. 2. Select Appropriate Tools and Frameworks: — Choose the tools and frameworks that best suit your needs based on the complexity of the task, available resources, and technical expertise. — Example: For natural language processing tasks, frameworks like Hugging Face Transformers or spaCy might be appropriate. 3. Data Collection and Preprocessing: — Gather and preprocess the data required for training the AI agent. Ensure that the data is clean, labeled (if necessary), and representative of the problem domain. — Example: Collect customer interaction logs and preprocess them to remove noise and irrelevant information for training a customer service chatbot. 4. Model Selection and Training: — Select the appropriate machine learning or deep learning model for your AI agent. Train the model using the preprocessed data, and fine-tune it to achieve optimal performance. — Example: Use a pre-trained transformer model like BERT for fine-tuning on a specific NLP task. Example Code for Training a Model with Hugging Face Transformers: from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments from datasets import load_dataset # Load dataset dataset = load_dataset('csv', data_files={'train': 'train.csv', 'test': 'test.csv'}) # Load tokenizer and model tokenizer = BertTokenizer.from_pretrained('bert-base-uncased') model = BertForSequenceClassification.from_pretrained('bert-base-uncased') # Tokenize data def tokenize_function(examples): return tokenizer(examples['text'], padding='max_length', truncation=True) tokenized_datasets = dataset.map(tokenize_function, batched=True) # Define training arguments training_args = TrainingArguments( output_dir='./results', evaluation_strategy='epoch', learning_rate=2e-5, per_device_train_batch_size=8, per_device_eval_batch_size=8, num_train_epochs=3, weight_decay=0.01, ) # Initialize Trainer trainer = Trainer( model=model, args=training_args, train_dataset=tokenized_datasets['train'], eval_dataset=tokenized_datasets['test'], ) # Train model trainer.train() 5. Evaluation and Testing: — Evaluate the AI agent’s performance using appropriate metrics and test it in various scenarios to ensure robustness and reliability. — Example: Evaluate a recommendation system using metrics like precision, recall, and F1-score on a validation dataset. 6.", "tokens": 396, "chunk_type": "overlap-1", "url": "https://medium.com/@williamwarley/understanding-ai-agents-how-they-work-types-and-practical-applications-bd261845f7c3", "anchor": "#body:part-10", "type": "blog", "title": "", "section": "Body", "source": "blog:medium.com", "published": "", "sha256": "80bee5fb3d1b45677f42423e475133e3e4191c2bb25f85154395fc97edea9f27"}
{"doc_id": "blog:medium.com", "chunk_id": "body:part-10-o2", "text": "Example Code for Training a Model with Hugging Face Transformers: from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments from datasets import load_dataset # Load dataset dataset = load_dataset('csv', data_files={'train': 'train.csv', 'test': 'test.csv'}) # Load tokenizer and model tokenizer = BertTokenizer.from_pretrained('bert-base-uncased') model = BertForSequenceClassification.from_pretrained('bert-base-uncased') # Tokenize data def tokenize_function(examples): return tokenizer(examples['text'], padding='max_length', truncation=True) tokenized_datasets = dataset.map(tokenize_function, batched=True) # Define training arguments training_args = TrainingArguments( output_dir='./results', evaluation_strategy='epoch', learning_rate=2e-5, per_device_train_batch_size=8, per_device_eval_batch_size=8, num_train_epochs=3, weight_decay=0.01, ) # Initialize Trainer trainer = Trainer( model=model, args=training_args, train_dataset=tokenized_datasets['train'], eval_dataset=tokenized_datasets['test'], ) # Train model trainer.train() 5. Evaluation and Testing: — Evaluate the AI agent’s performance using appropriate metrics and test it in various scenarios to ensure robustness and reliability. — Example: Evaluate a recommendation system using metrics like precision, recall, and F1-score on a validation dataset. 6. Deployment: — Deploy the AI agent in the target environment, ensuring it integrates smoothly with existing systems and can operate at scale. — Example: Deploy a trained chatbot on a cloud platform like AWS Lambda for scalable, serverless execution. Example Code for Deploying a Model on AWS Lambda: import json import boto3 from transformers import BertTokenizer, BertForSequenceClassification # Initialize AWS Lambda client client = boto3.client('lambda') # Define the Lambda function def lambda_handler(event, context): # Load tokenizer and model tokenizer = BertTokenizer.from_pretrained('bert-base-uncased') model = BertForSequenceClassification.from_pretrained('bert-base-uncased') # Parse input input_text = event['text'] inputs = tokenizer(input_text, return_tensors='pt') # Perform inference outputs = model(**inputs) predictions = outputs.logits.argmax(dim=-1).item() # Return response return { 'statusCode': 200, 'body': json.dumps({'prediction': predictions}) } # Deploy the Lambda function response = client.create_function( FunctionName='AIChatbot', Runtime='python3.8', Role='your-aws-role', Handler='lambda_function.lambda_handler', Code={'ZipFile': open('function.zip', 'rb').read()}, Timeout=15, MemorySize=128, ) 7. Monitoring and Maintenance: — Continuously monitor the AI agent’s performance and make necessary updates or retrain the model to maintain its effectiveness. — Example: Regularly update the training data and retrain a sentiment analysis model to adapt to changing trends and language usage. Tools and Frameworks 1.", "tokens": 399, "chunk_type": "overlap-2", "url": "https://medium.com/@williamwarley/understanding-ai-agents-how-they-work-types-and-practical-applications-bd261845f7c3", "anchor": "#body:part-10", "type": "blog", "title": "", "section": "Body", "source": "blog:medium.com", "published": "", "sha256": "80bee5fb3d1b45677f42423e475133e3e4191c2bb25f85154395fc97edea9f27"}
{"doc_id": "blog:medium.com", "chunk_id": "body:part-10-o3", "text": "Example Code for Deploying a Model on AWS Lambda: import json import boto3 from transformers import BertTokenizer, BertForSequenceClassification # Initialize AWS Lambda client client = boto3.client('lambda') # Define the Lambda function def lambda_handler(event, context): # Load tokenizer and model tokenizer = BertTokenizer.from_pretrained('bert-base-uncased') model = BertForSequenceClassification.from_pretrained('bert-base-uncased') # Parse input input_text = event['text'] inputs = tokenizer(input_text, return_tensors='pt') # Perform inference outputs = model(**inputs) predictions = outputs.logits.argmax(dim=-1).item() # Return response return { 'statusCode': 200, 'body': json.dumps({'prediction': predictions}) } # Deploy the Lambda function response = client.create_function( FunctionName='AIChatbot', Runtime='python3.8', Role='your-aws-role', Handler='lambda_function.lambda_handler', Code={'ZipFile': open('function.zip', 'rb').read()}, Timeout=15, MemorySize=128, ) 7. Monitoring and Maintenance: — Continuously monitor the AI agent’s performance and make necessary updates or retrain the model to maintain its effectiveness. — Example: Regularly update the training data and retrain a sentiment analysis model to adapt to changing trends and language usage. Tools and Frameworks 1. Machine Learning Frameworks: — TensorFlow: Widely used for building and deploying machine learning models, with extensive support for neural networks and deep learning. — PyTorch: Popular for", "tokens": 218, "chunk_type": "overlap-3", "url": "https://medium.com/@williamwarley/understanding-ai-agents-how-they-work-types-and-practical-applications-bd261845f7c3", "anchor": "#body:part-10", "type": "blog", "title": "", "section": "Body", "source": "blog:medium.com", "published": "", "sha256": "80bee5fb3d1b45677f42423e475133e3e4191c2bb25f85154395fc97edea9f27"}
{"doc_id": "blog:medium.com", "chunk_id": "body:part-11", "text": "open('function.zip', 'rb').read()}, Timeout=15, MemorySize=128, ) 7. Monitoring and Maintenance: — Continuously monitor the AI agent’s performance and make necessary updates or retrain the model to maintain its effectiveness. — Example: Regularly update the training data and retrain a sentiment analysis model to adapt to changing trends and language usage. Tools and Frameworks 1. Machine Learning Frameworks: — TensorFlow: Widely used for building and deploying machine learning models, with extensive support for neural networks and deep learning. — PyTorch: Popular for its dynamic computation graph and ease of use in research and development of deep learning models. — Scikit-Learn: Ideal for implementing traditional machine learning algorithms and preprocessing data. 2. Natural Language Processing (NLP) Frameworks: — Hugging Face Transformers: Provides pre-trained models and tools for NLP tasks such as text classification, question answering, and language translation. — spaCy: Efficient and scalable library for NLP tasks, including tokenization, named entity recognition, and dependency parsing. 3. Deployment Platforms: — AWS SageMaker: Comprehensive platform for building, training, and deploying machine learning models at scale. — Google Cloud AI Platform: Managed services for training and deploying machine learning models on Google Cloud. — Azure Machine Learning: End-to-end platform for training, deploying, and managing machine learning models on Azure. Best Practices 1. Data Quality: — Ensure high-quality data by cleaning, preprocessing, and labeling it accurately. Good data is crucial for training effective AI agents. — Example: Remove duplicates and outliers from your dataset to improve model accuracy. 2. Model Evaluation: — Use appropriate metrics to evaluate model performance and ensure it meets the desired objectives. — Example: Evaluate a classification model using metrics like accuracy, precision, recall, and F1-score. 3. Scalability and Efficiency: — Design AI agents to scale efficiently, ensuring they can handle increasing workloads and data volumes. — Example: Use distributed training and inference techniques to scale your AI agent across multiple machines. 4. Security and Privacy: — Implement security measures to protect data and ensure privacy, especially when dealing with sensitive information. — Example: Encrypt data at rest and in transit, and implement access controls to protect user data. Common Challenges and Solutions 1. Data Availability: — Challenge: Lack of sufficient labeled data for training. — Solution: Use data augmentation techniques, transfer learning, or synthetic data generation to augment your dataset. 2. Model Overfitting: — Challenge: The model performs well on training data but poorly on unseen data. — Solution: Implement regularization techniques, such as dropout and L2 regularization, and use cross-validation to assess model performance. 3. Integration Complexity: — Challenge: Integrating the AI agent with existing systems and workflows. — Solution: Use APIs and modular design principles to ensure smooth integration and compatibility with other systems. 4. Computational Resources: — Challenge: High computational requirements for training and inference. — Solution: Use cloud-based platforms and distributed computing to leverage scalable resources and reduce costs. This chapter provides a comprehensive guide to implementing AI agents, covering practical steps, tools and frameworks, best practices, and common challenges. Future Trends and Developments in AI Agents The field of AI agents is rapidly evolving, with continuous advancements that promise", "tokens": 665, "chunk_type": "original-large", "url": "https://medium.com/@williamwarley/understanding-ai-agents-how-they-work-types-and-practical-applications-bd261845f7c3", "anchor": "#body:part-11", "type": "blog", "title": "", "section": "Body", "source": "blog:medium.com", "published": "", "sha256": "82c268770d0f29bd77ef0ae2cb6477b22177d3a8fdda5826231a3de61df5f370"}
{"doc_id": "blog:medium.com", "chunk_id": "body:part-11-o1", "text": "open('function.zip', 'rb').read()}, Timeout=15, MemorySize=128, ) 7. Monitoring and Maintenance: — Continuously monitor the AI agent’s performance and make necessary updates or retrain the model to maintain its effectiveness. — Example: Regularly update the training data and retrain a sentiment analysis model to adapt to changing trends and language usage. Tools and Frameworks 1. Machine Learning Frameworks: — TensorFlow: Widely used for building and deploying machine learning models, with extensive support for neural networks and deep learning. — PyTorch: Popular for its dynamic computation graph and ease of use in research and development of deep learning models. — Scikit-Learn: Ideal for implementing traditional machine learning algorithms and preprocessing data. 2. Natural Language Processing (NLP) Frameworks: — Hugging Face Transformers: Provides pre-trained models and tools for NLP tasks such as text classification, question answering, and language translation. — spaCy: Efficient and scalable library for NLP tasks, including tokenization, named entity recognition, and dependency parsing. 3. Deployment Platforms: — AWS SageMaker: Comprehensive platform for building, training, and deploying machine learning models at scale. — Google Cloud AI Platform: Managed services for training and deploying machine learning models on Google Cloud. — Azure Machine Learning: End-to-end platform for training, deploying, and managing machine learning models on Azure. Best Practices 1. Data Quality: — Ensure high-quality data by cleaning, preprocessing, and labeling it accurately. Good data is crucial for training effective AI agents. — Example: Remove duplicates and outliers from your dataset to improve model accuracy. 2. Model Evaluation: — Use appropriate metrics to evaluate model performance and ensure it meets the desired objectives. — Example: Evaluate a classification model using metrics like accuracy, precision, recall, and F1-score. 3. Scalability and Efficiency: — Design AI agents to scale efficiently, ensuring they can handle increasing workloads and data volumes. — Example: Use distributed training and inference techniques to scale your AI agent across multiple machines. 4.", "tokens": 403, "chunk_type": "overlap-1", "url": "https://medium.com/@williamwarley/understanding-ai-agents-how-they-work-types-and-practical-applications-bd261845f7c3", "anchor": "#body:part-11", "type": "blog", "title": "", "section": "Body", "source": "blog:medium.com", "published": "", "sha256": "82c268770d0f29bd77ef0ae2cb6477b22177d3a8fdda5826231a3de61df5f370"}
{"doc_id": "blog:medium.com", "chunk_id": "body:part-11-o2", "text": "Model Evaluation: — Use appropriate metrics to evaluate model performance and ensure it meets the desired objectives. — Example: Evaluate a classification model using metrics like accuracy, precision, recall, and F1-score. 3. Scalability and Efficiency: — Design AI agents to scale efficiently, ensuring they can handle increasing workloads and data volumes. — Example: Use distributed training and inference techniques to scale your AI agent across multiple machines. 4. Security and Privacy: — Implement security measures to protect data and ensure privacy, especially when dealing with sensitive information. — Example: Encrypt data at rest and in transit, and implement access controls to protect user data. Common Challenges and Solutions 1. Data Availability: — Challenge: Lack of sufficient labeled data for training. — Solution: Use data augmentation techniques, transfer learning, or synthetic data generation to augment your dataset. 2. Model Overfitting: — Challenge: The model performs well on training data but poorly on unseen data. — Solution: Implement regularization techniques, such as dropout and L2 regularization, and use cross-validation to assess model performance. 3. Integration Complexity: — Challenge: Integrating the AI agent with existing systems and workflows. — Solution: Use APIs and modular design principles to ensure smooth integration and compatibility with other systems. 4. Computational Resources: — Challenge: High computational requirements for training and inference. — Solution: Use cloud-based platforms and distributed computing to leverage scalable resources and reduce costs. This chapter provides a comprehensive guide to implementing AI agents, covering practical steps, tools and frameworks, best practices, and common challenges. Future Trends and Developments in AI Agents The field of AI agents is rapidly evolving, with continuous advancements that promise", "tokens": 351, "chunk_type": "overlap-2", "url": "https://medium.com/@williamwarley/understanding-ai-agents-how-they-work-types-and-practical-applications-bd261845f7c3", "anchor": "#body:part-11", "type": "blog", "title": "", "section": "Body", "source": "blog:medium.com", "published": "", "sha256": "82c268770d0f29bd77ef0ae2cb6477b22177d3a8fdda5826231a3de61df5f370"}
{"doc_id": "blog:medium.com", "chunk_id": "body:part-12", "text": "modular design principles to ensure smooth integration and compatibility with other systems. 4. Computational Resources: — Challenge: High computational requirements for training and inference. — Solution: Use cloud-based platforms and distributed computing to leverage scalable resources and reduce costs. This chapter provides a comprehensive guide to implementing AI agents, covering practical steps, tools and frameworks, best practices, and common challenges. Future Trends and Developments in AI Agents The field of AI agents is rapidly evolving, with continuous advancements that promise to enhance their capabilities and applications. This chapter explores some of the key future trends and developments in AI agents, including advances in reinforcement learning, integration with the Internet of Things (IoT), ethical considerations, and human-agent collaboration. Advances in Reinforcement Learning 1. Deep Reinforcement Learning (DRL): — Combining deep learning with reinforcement learning has led to significant breakthroughs in creating more capable and sophisticated AI agents. DRL algorithms enable agents to learn complex behaviors in high-dimensional environments. — Future Trend: Development of more efficient DRL algorithms that can learn faster and require less computational power, making them accessible for a broader range of applications. 2. Meta-Learning: — Meta-learning, or “learning to learn,” involves training AI agents to adapt quickly to new tasks with minimal data. This approach enhances the flexibility and generalization of AI agents. — Future Trend: Increased focus on meta-learning techniques to create AI agents that can efficiently transfer knowledge across different tasks and domains. 3. Multi-Agent Systems: — Multi-agent reinforcement learning (MARL) involves multiple AI agents interacting and learning within the same environment. This approach is useful for tasks requiring coordination and collaboration. — Future Trend: Advancements in MARL will enable more complex and realistic simulations, such as autonomous traffic management and collaborative robotics. Integration with IoT 1. Edge AI: — Edge AI involves deploying AI agents on edge devices, allowing for real-time data processing and decision-making closer to the source. This reduces latency and bandwidth usage. — Future Trend: Greater integration of AI agents with IoT devices to enable intelligent and autonomous operations in smart homes, industrial automation, and healthcare. 2. Distributed AI Systems: — Distributed AI systems leverage multiple connected devices to share computational loads and improve overall system performance and reliability. — Future Trend: Development of robust distributed AI frameworks that facilitate seamless collaboration between AI agents and IoT devices. 3. Predictive Maintenance: — AI agents can analyze data from IoT sensors to predict equipment failures and schedule maintenance proactively, reducing downtime and costs. — Future Trend: Enhanced predictive maintenance solutions using AI agents to improve efficiency and reliability in various industries, including manufacturing and energy. Ethical and Responsible AI 1. Bias Mitigation: — Addressing biases in AI agents is crucial to ensure fairness and equity. Researchers are developing techniques to detect and mitigate biases in training data and algorithms. — Future Trend: Implementation of more advanced bias detection and mitigation methods to promote ethical AI practices. 2. Transparency and Explainability: — As AI agents become more complex, understanding and explaining their decision-making processes is essential for building trust and accountability. — Future Trend: Development of", "tokens": 665, "chunk_type": "original-large", "url": "https://medium.com/@williamwarley/understanding-ai-agents-how-they-work-types-and-practical-applications-bd261845f7c3", "anchor": "#body:part-12", "type": "blog", "title": "", "section": "Body", "source": "blog:medium.com", "published": "", "sha256": "14a559968231d021077028b43db24b5569a3e4fca439208f43ab4cf754cb8087"}
{"doc_id": "blog:medium.com", "chunk_id": "body:part-12-o1", "text": "modular design principles to ensure smooth integration and compatibility with other systems. 4. Computational Resources: — Challenge: High computational requirements for training and inference. — Solution: Use cloud-based platforms and distributed computing to leverage scalable resources and reduce costs. This chapter provides a comprehensive guide to implementing AI agents, covering practical steps, tools and frameworks, best practices, and common challenges. Future Trends and Developments in AI Agents The field of AI agents is rapidly evolving, with continuous advancements that promise to enhance their capabilities and applications. This chapter explores some of the key future trends and developments in AI agents, including advances in reinforcement learning, integration with the Internet of Things (IoT), ethical considerations, and human-agent collaboration. Advances in Reinforcement Learning 1. Deep Reinforcement Learning (DRL): — Combining deep learning with reinforcement learning has led to significant breakthroughs in creating more capable and sophisticated AI agents. DRL algorithms enable agents to learn complex behaviors in high-dimensional environments. — Future Trend: Development of more efficient DRL algorithms that can learn faster and require less computational power, making them accessible for a broader range of applications. 2. Meta-Learning: — Meta-learning, or “learning to learn,” involves training AI agents to adapt quickly to new tasks with minimal data. This approach enhances the flexibility and generalization of AI agents. — Future Trend: Increased focus on meta-learning techniques to create AI agents that can efficiently transfer knowledge across different tasks and domains. 3. Multi-Agent Systems: — Multi-agent reinforcement learning (MARL) involves multiple AI agents interacting and learning within the same environment. This approach is useful for tasks requiring coordination and collaboration. — Future Trend: Advancements in MARL will enable more complex and realistic simulations, such as autonomous traffic management and collaborative robotics. Integration with IoT 1.", "tokens": 378, "chunk_type": "overlap-1", "url": "https://medium.com/@williamwarley/understanding-ai-agents-how-they-work-types-and-practical-applications-bd261845f7c3", "anchor": "#body:part-12", "type": "blog", "title": "", "section": "Body", "source": "blog:medium.com", "published": "", "sha256": "14a559968231d021077028b43db24b5569a3e4fca439208f43ab4cf754cb8087"}
{"doc_id": "blog:medium.com", "chunk_id": "body:part-12-o2", "text": "This approach enhances the flexibility and generalization of AI agents. — Future Trend: Increased focus on meta-learning techniques to create AI agents that can efficiently transfer knowledge across different tasks and domains. 3. Multi-Agent Systems: — Multi-agent reinforcement learning (MARL) involves multiple AI agents interacting and learning within the same environment. This approach is useful for tasks requiring coordination and collaboration. — Future Trend: Advancements in MARL will enable more complex and realistic simulations, such as autonomous traffic management and collaborative robotics. Integration with IoT 1. Edge AI: — Edge AI involves deploying AI agents on edge devices, allowing for real-time data processing and decision-making closer to the source. This reduces latency and bandwidth usage. — Future Trend: Greater integration of AI agents with IoT devices to enable intelligent and autonomous operations in smart homes, industrial automation, and healthcare. 2. Distributed AI Systems: — Distributed AI systems leverage multiple connected devices to share computational loads and improve overall system performance and reliability. — Future Trend: Development of robust distributed AI frameworks that facilitate seamless collaboration between AI agents and IoT devices. 3. Predictive Maintenance: — AI agents can analyze data from IoT sensors to predict equipment failures and schedule maintenance proactively, reducing downtime and costs. — Future Trend: Enhanced predictive maintenance solutions using AI agents to improve efficiency and reliability in various industries, including manufacturing and energy. Ethical and Responsible AI 1. Bias Mitigation: — Addressing biases in AI agents is crucial to ensure fairness and equity. Researchers are developing techniques to detect and mitigate biases in training data and algorithms. — Future Trend: Implementation of more advanced bias detection and mitigation methods to promote ethical AI practices. 2. Transparency and Explainability: — As AI agents become more complex, understanding and explaining their decision-making processes is essential for building trust and accountability. — Future Trend: Development of", "tokens": 399, "chunk_type": "overlap-2", "url": "https://medium.com/@williamwarley/understanding-ai-agents-how-they-work-types-and-practical-applications-bd261845f7c3", "anchor": "#body:part-12", "type": "blog", "title": "", "section": "Body", "source": "blog:medium.com", "published": "", "sha256": "14a559968231d021077028b43db24b5569a3e4fca439208f43ab4cf754cb8087"}
{"doc_id": "blog:medium.com", "chunk_id": "body:part-13", "text": "Ethical and Responsible AI 1. Bias Mitigation: — Addressing biases in AI agents is crucial to ensure fairness and equity. Researchers are developing techniques to detect and mitigate biases in training data and algorithms. — Future Trend: Implementation of more advanced bias detection and mitigation methods to promote ethical AI practices. 2. Transparency and Explainability: — As AI agents become more complex, understanding and explaining their decision-making processes is essential for building trust and accountability. — Future Trend: Development of explainable AI (XAI) techniques that provide insights into how AI agents make decisions, enhancing transparency and user confidence. 3. Regulatory Compliance: — Ensuring that AI agents comply with regulatory standards and guidelines is vital for their safe and ethical deployment. — Future Trend: Establishment of comprehensive AI regulations and standards that guide the development and deployment of responsible AI agents. Human-Agent Collaboration 1. Human-in-the-Loop Systems: — Human-in-the-loop (HITL) systems involve human oversight and interaction with AI agents, combining human expertise with AI efficiency. — Future Trend: Increased adoption of HITL systems in critical applications such as healthcare, finance, and autonomous systems to ensure safe and effective operation. 2. Augmented Intelligence: — Augmented intelligence focuses on enhancing human capabilities with AI agents, rather than replacing humans. This approach leverages the strengths of both humans and AI. — Future Trend: Development of collaborative tools and platforms that empower humans to work alongside AI agents, improving productivity and decision-making. 3. Interactive Learning: — Interactive learning involves AI agents learning from direct interactions with humans, receiving feedback, and improving their performance. — Future Trend: Enhanced interactive learning frameworks that facilitate seamless and intuitive human-agent interactions, leading to more personalized and adaptive AI systems. Conclusion The future of AI agents is filled with exciting possibilities and challenges. Advances in reinforcement learning, integration with IoT, ethical considerations, and human-agent collaboration will drive the next wave of innovations in AI. By staying informed about these trends and developments, organizations and developers can harness the full potential of AI agents to create smarter, more efficient, and ethical solutions. This chapter explores the future trends and developments in AI agents, highlighting key advancements and their potential impact. Case Studies and Real-World Examples AI agents have made significant strides in various industries, solving complex problems and enhancing operational efficiency. This chapter presents several case studies and real-world examples to illustrate the successful application of AI agents in different domains. Case Study 1: Autonomous Vehicles Company: Waymo Challenge: Developing self-driving cars that can safely navigate complex urban environments and interact with other road users. Solution: Waymo uses AI agents to process data from sensors (LIDAR, cameras, radar) and make real-time driving decisions. Implementation: - Perception: AI agents use sensor data to detect and classify objects such as pedestrians, vehicles, and traffic signals. - Decision-Making: Agents apply machine learning algorithms to predict the behavior of other road users and plan safe driving maneuvers. - Action: The AI system controls the vehicle’s acceleration, braking, and steering to navigate through traffic. Outcome: Waymo’s self-driving cars have successfully logged millions of miles on public roads, demonstrating the", "tokens": 665, "chunk_type": "original-large", "url": "https://medium.com/@williamwarley/understanding-ai-agents-how-they-work-types-and-practical-applications-bd261845f7c3", "anchor": "#body:part-13", "type": "blog", "title": "", "section": "Body", "source": "blog:medium.com", "published": "", "sha256": "710e7f0c045800c3fd24fbdc5e64995fad77bfcefa411fe865b25e936cc7400b"}
{"doc_id": "blog:medium.com", "chunk_id": "body:part-13-o1", "text": "Ethical and Responsible AI 1. Bias Mitigation: — Addressing biases in AI agents is crucial to ensure fairness and equity. Researchers are developing techniques to detect and mitigate biases in training data and algorithms. — Future Trend: Implementation of more advanced bias detection and mitigation methods to promote ethical AI practices. 2. Transparency and Explainability: — As AI agents become more complex, understanding and explaining their decision-making processes is essential for building trust and accountability. — Future Trend: Development of explainable AI (XAI) techniques that provide insights into how AI agents make decisions, enhancing transparency and user confidence. 3. Regulatory Compliance: — Ensuring that AI agents comply with regulatory standards and guidelines is vital for their safe and ethical deployment. — Future Trend: Establishment of comprehensive AI regulations and standards that guide the development and deployment of responsible AI agents. Human-Agent Collaboration 1. Human-in-the-Loop Systems: — Human-in-the-loop (HITL) systems involve human oversight and interaction with AI agents, combining human expertise with AI efficiency. — Future Trend: Increased adoption of HITL systems in critical applications such as healthcare, finance, and autonomous systems to ensure safe and effective operation. 2. Augmented Intelligence: — Augmented intelligence focuses on enhancing human capabilities with AI agents, rather than replacing humans. This approach leverages the strengths of both humans and AI. — Future Trend: Development of collaborative tools and platforms that empower humans to work alongside AI agents, improving productivity and decision-making. 3. Interactive Learning: — Interactive learning involves AI agents learning from direct interactions with humans, receiving feedback, and improving their performance. — Future Trend: Enhanced interactive learning frameworks that facilitate seamless and intuitive human-agent interactions, leading to more personalized and adaptive AI systems. Conclusion The future of AI agents is filled with exciting possibilities and challenges.", "tokens": 379, "chunk_type": "overlap-1", "url": "https://medium.com/@williamwarley/understanding-ai-agents-how-they-work-types-and-practical-applications-bd261845f7c3", "anchor": "#body:part-13", "type": "blog", "title": "", "section": "Body", "source": "blog:medium.com", "published": "", "sha256": "710e7f0c045800c3fd24fbdc5e64995fad77bfcefa411fe865b25e936cc7400b"}
{"doc_id": "blog:medium.com", "chunk_id": "body:part-13-o2", "text": "This approach leverages the strengths of both humans and AI. — Future Trend: Development of collaborative tools and platforms that empower humans to work alongside AI agents, improving productivity and decision-making. 3. Interactive Learning: — Interactive learning involves AI agents learning from direct interactions with humans, receiving feedback, and improving their performance. — Future Trend: Enhanced interactive learning frameworks that facilitate seamless and intuitive human-agent interactions, leading to more personalized and adaptive AI systems. Conclusion The future of AI agents is filled with exciting possibilities and challenges. Advances in reinforcement learning, integration with IoT, ethical considerations, and human-agent collaboration will drive the next wave of innovations in AI. By staying informed about these trends and developments, organizations and developers can harness the full potential of AI agents to create smarter, more efficient, and ethical solutions. This chapter explores the future trends and developments in AI agents, highlighting key advancements and their potential impact. Case Studies and Real-World Examples AI agents have made significant strides in various industries, solving complex problems and enhancing operational efficiency. This chapter presents several case studies and real-world examples to illustrate the successful application of AI agents in different domains. Case Study 1: Autonomous Vehicles Company: Waymo Challenge: Developing self-driving cars that can safely navigate complex urban environments and interact with other road users. Solution: Waymo uses AI agents to process data from sensors (LIDAR, cameras, radar) and make real-time driving decisions. Implementation: - Perception: AI agents use sensor data to detect and classify objects such as pedestrians, vehicles, and traffic signals. - Decision-Making: Agents apply machine learning algorithms to predict the behavior of other road users and plan safe driving maneuvers. - Action: The AI system controls the vehicle’s acceleration, braking, and steering to navigate through traffic. Outcome: Waymo’s self-driving cars have successfully logged millions of miles on public roads, demonstrating the", "tokens": 399, "chunk_type": "overlap-2", "url": "https://medium.com/@williamwarley/understanding-ai-agents-how-they-work-types-and-practical-applications-bd261845f7c3", "anchor": "#body:part-13", "type": "blog", "title": "", "section": "Body", "source": "blog:medium.com", "published": "", "sha256": "710e7f0c045800c3fd24fbdc5e64995fad77bfcefa411fe865b25e936cc7400b"}
{"doc_id": "blog:medium.com", "chunk_id": "body:part-14", "text": "sensors (LIDAR, cameras, radar) and make real-time driving decisions. Implementation: - Perception: AI agents use sensor data to detect and classify objects such as pedestrians, vehicles, and traffic signals. - Decision-Making: Agents apply machine learning algorithms to predict the behavior of other road users and plan safe driving maneuvers. - Action: The AI system controls the vehicle’s acceleration, braking, and steering to navigate through traffic. Outcome: Waymo’s self-driving cars have successfully logged millions of miles on public roads, demonstrating the potential of AI agents to enhance transportation safety and efficiency. Case Study 2: Healthcare Diagnostics Company: IBM Watson Health Challenge: Assisting doctors in diagnosing diseases and recommending personalized treatment plans based on vast amounts of medical data. Solution: IBM Watson for Oncology uses AI agents to analyze medical records, research papers, and clinical guidelines to provide evidence-based recommendations. Implementation: - Data Integration: AI agents aggregate and analyze data from electronic health records (EHRs), medical literature, and clinical trial results. - Natural Language Processing (NLP): Agents use NLP to interpret unstructured medical texts and extract relevant information. - Decision Support: The AI system suggests potential diagnoses and treatment options based on the latest medical evidence and patient-specific factors. Outcome: Watson for Oncology has been deployed in several hospitals worldwide, aiding oncologists in developing effective and personalized treatment plans, thus improving patient outcomes. Case Study 3: Financial Trading Company: BlackRock Challenge: Optimizing investment strategies and managing large portfolios with real-time market analysis and trading decisions. Solution: BlackRock’s Aladdin platform employs AI agents to analyze market data, assess risks, and execute trades autonomously. Implementation: - Market Analysis: AI agents continuously monitor and analyze financial news, market trends, and economic indicators. - Risk Management: Agents assess portfolio risks and suggest adjustments to optimize performance. - Automated Trading: The AI system executes trades based on predefined strategies and real-time market conditions. Outcome: Aladdin has enhanced BlackRock’s ability to manage assets efficiently, providing clients with optimized investment strategies and improved financial returns. Case Study 4: E-commerce Personalization Company: Amazon Challenge: Providing personalized shopping experiences to millions of customers by recommending relevant products. Solution: Amazon uses AI agents in its recommendation engine to analyze customer behavior and suggest products tailored to individual preferences. Implementation: - Data Collection: AI agents gather data on customer browsing history, purchase behavior, and product interactions. - Machine Learning: Agents use collaborative filtering and deep learning algorithms to identify patterns and preferences. - Personalized Recommendations: The AI system generates real-time product recommendations for each customer based on their unique profile. Outcome: Amazon’s recommendation engine significantly boosts customer engagement and sales, contributing to its status as a leading e-commerce platform. Case Study 5: Customer Service Automation Company: Bank of America Challenge: Enhancing customer service by providing instant, accurate, and personalized responses to customer inquiries. Solution: Bank of America implemented Erica, an AI-powered virtual assistant that helps customers with banking tasks and queries. Implementation: - Natural Language Understanding (NLU): Erica uses NLU to understand customer queries expressed in natural language. - Machine Learning: The virtual assistant learns from interactions to improve its responses and provide personalized assistance.", "tokens": 665, "chunk_type": "original-large", "url": "https://medium.com/@williamwarley/understanding-ai-agents-how-they-work-types-and-practical-applications-bd261845f7c3", "anchor": "#body:part-14", "type": "blog", "title": "", "section": "Body", "source": "blog:medium.com", "published": "", "sha256": "87dfd521726c29e844071fef5a75c297aaf935faef6fdf236856f173130490ca"}
{"doc_id": "blog:medium.com", "chunk_id": "body:part-14-o1", "text": "sensors (LIDAR, cameras, radar) and make real-time driving decisions. Implementation: - Perception: AI agents use sensor data to detect and classify objects such as pedestrians, vehicles, and traffic signals. - Decision-Making: Agents apply machine learning algorithms to predict the behavior of other road users and plan safe driving maneuvers. - Action: The AI system controls the vehicle’s acceleration, braking, and steering to navigate through traffic. Outcome: Waymo’s self-driving cars have successfully logged millions of miles on public roads, demonstrating the potential of AI agents to enhance transportation safety and efficiency. Case Study 2: Healthcare Diagnostics Company: IBM Watson Health Challenge: Assisting doctors in diagnosing diseases and recommending personalized treatment plans based on vast amounts of medical data. Solution: IBM Watson for Oncology uses AI agents to analyze medical records, research papers, and clinical guidelines to provide evidence-based recommendations. Implementation: - Data Integration: AI agents aggregate and analyze data from electronic health records (EHRs), medical literature, and clinical trial results. - Natural Language Processing (NLP): Agents use NLP to interpret unstructured medical texts and extract relevant information. - Decision Support: The AI system suggests potential diagnoses and treatment options based on the latest medical evidence and patient-specific factors. Outcome: Watson for Oncology has been deployed in several hospitals worldwide, aiding oncologists in developing effective and personalized treatment plans, thus improving patient outcomes. Case Study 3: Financial Trading Company: BlackRock Challenge: Optimizing investment strategies and managing large portfolios with real-time market analysis and trading decisions. Solution: BlackRock’s Aladdin platform employs AI agents to analyze market data, assess risks, and execute trades autonomously. Implementation: - Market Analysis: AI agents continuously monitor and analyze financial news, market trends, and economic indicators. - Risk Management: Agents assess portfolio risks and suggest adjustments to optimize performance. - Automated Trading: The AI system executes trades based on predefined strategies and real-time market conditions.", "tokens": 397, "chunk_type": "overlap-1", "url": "https://medium.com/@williamwarley/understanding-ai-agents-how-they-work-types-and-practical-applications-bd261845f7c3", "anchor": "#body:part-14", "type": "blog", "title": "", "section": "Body", "source": "blog:medium.com", "published": "", "sha256": "87dfd521726c29e844071fef5a75c297aaf935faef6fdf236856f173130490ca"}
{"doc_id": "blog:medium.com", "chunk_id": "body:part-14-o2", "text": "Solution: BlackRock’s Aladdin platform employs AI agents to analyze market data, assess risks, and execute trades autonomously. Implementation: - Market Analysis: AI agents continuously monitor and analyze financial news, market trends, and economic indicators. - Risk Management: Agents assess portfolio risks and suggest adjustments to optimize performance. - Automated Trading: The AI system executes trades based on predefined strategies and real-time market conditions. Outcome: Aladdin has enhanced BlackRock’s ability to manage assets efficiently, providing clients with optimized investment strategies and improved financial returns. Case Study 4: E-commerce Personalization Company: Amazon Challenge: Providing personalized shopping experiences to millions of customers by recommending relevant products. Solution: Amazon uses AI agents in its recommendation engine to analyze customer behavior and suggest products tailored to individual preferences. Implementation: - Data Collection: AI agents gather data on customer browsing history, purchase behavior, and product interactions. - Machine Learning: Agents use collaborative filtering and deep learning algorithms to identify patterns and preferences. - Personalized Recommendations: The AI system generates real-time product recommendations for each customer based on their unique profile. Outcome: Amazon’s recommendation engine significantly boosts customer engagement and sales, contributing to its status as a leading e-commerce platform. Case Study 5: Customer Service Automation Company: Bank of America Challenge: Enhancing customer service by providing instant, accurate, and personalized responses to customer inquiries. Solution: Bank of America implemented Erica, an AI-powered virtual assistant that helps customers with banking tasks and queries. Implementation: - Natural Language Understanding (NLU): Erica uses NLU to understand customer queries expressed in natural language. - Machine Learning: The virtual assistant learns from interactions to improve its responses and provide personalized assistance.", "tokens": 349, "chunk_type": "overlap-2", "url": "https://medium.com/@williamwarley/understanding-ai-agents-how-they-work-types-and-practical-applications-bd261845f7c3", "anchor": "#body:part-14", "type": "blog", "title": "", "section": "Body", "source": "blog:medium.com", "published": "", "sha256": "87dfd521726c29e844071fef5a75c297aaf935faef6fdf236856f173130490ca"}
{"doc_id": "blog:medium.com", "chunk_id": "body:part-15", "text": "a leading e-commerce platform. Case Study 5: Customer Service Automation Company: Bank of America Challenge: Enhancing customer service by providing instant, accurate, and personalized responses to customer inquiries. Solution: Bank of America implemented Erica, an AI-powered virtual assistant that helps customers with banking tasks and queries. Implementation: - Natural Language Understanding (NLU): Erica uses NLU to understand customer queries expressed in natural language. - Machine Learning: The virtual assistant learns from interactions to improve its responses and provide personalized assistance. - Integration: Erica integrates with the bank’s systems to access account information, perform transactions, and provide financial advice. Outcome: Erica handles millions of customer interactions, improving response times, reducing workload for human agents, and enhancing customer satisfaction. Case Study 6: Smart Home Management Company: Google Challenge: Creating a smart home ecosystem that automates household tasks and enhances convenience for users. Solution: Google Assistant uses AI agents to control smart home devices, manage schedules, and provide information. Implementation: - Voice Recognition: AI agents use speech recognition to understand voice commands from users. - Device Control: The assistant interacts with smart home devices (e.g., lights, thermostats, security systems) to execute commands. - Personalization: The AI system learns user preferences and routines to automate tasks and provide relevant information. Outcome: Google Assistant enhances the smart home experience, making it easier for users to manage their homes efficiently and conveniently. This chapter showcases successful applications of AI agents across various industries, demonstrating their versatility and impact. Key Insights and Final Recommendations As we conclude this comprehensive guide on AI agents, it is important to summarize the key insights and provide final recommendations for effectively leveraging AI agents in various applications and industries. Summary of Key Insights 1. Definition and Importance: — AI agents are autonomous entities that use AI to perceive their environment, make decisions, and perform actions to achieve specific goals. — They play a crucial role in automating tasks, enhancing decision-making, and improving operational efficiency across various domains. 2. How AI Agents Work: — AI agents operate based on core concepts such as perception, reasoning, action, and learning. — They can be categorized into different types, including simple reflex agents, model-based reflex agents, goal-based agents, utility-based agents, and learning agents. 3. Types of AI Agents: — Simple Reflex Agents: Operate based on predefined rules and immediate perception. — Model-Based Reflex Agents: Maintain an internal model of the environment to make informed decisions. — Goal-Based Agents: Make decisions based on predefined goals and desired outcomes. — Utility-Based Agents: Evaluate the utility of different actions to maximize overall satisfaction or performance. — Learning Agents: Continuously learn from experiences and adapt to new situations. 4. Applications of AI Agents: — AI agents are used in various industries, including autonomous vehicles, robotics, personal assistants, game AI, financial trading, and healthcare. — Their ability to automate tasks, make real-time decisions, and adapt to changing conditions makes them valuable assets in these fields. 5. Advantages of AI Agents: — Efficiency and Automation: Automate repetitive tasks, increase productivity, and reduce errors. — Scalability: Handle large-scale operations and adapt to varying workloads.", "tokens": 665, "chunk_type": "original-large", "url": "https://medium.com/@williamwarley/understanding-ai-agents-how-they-work-types-and-practical-applications-bd261845f7c3", "anchor": "#body:part-15", "type": "blog", "title": "", "section": "Body", "source": "blog:medium.com", "published": "", "sha256": "1a01135065758864d0b7b4093740e0b41a2801a4de4dfc6955715649040054ec"}
{"doc_id": "blog:medium.com", "chunk_id": "body:part-15-o1", "text": "a leading e-commerce platform. Case Study 5: Customer Service Automation Company: Bank of America Challenge: Enhancing customer service by providing instant, accurate, and personalized responses to customer inquiries. Solution: Bank of America implemented Erica, an AI-powered virtual assistant that helps customers with banking tasks and queries. Implementation: - Natural Language Understanding (NLU): Erica uses NLU to understand customer queries expressed in natural language. - Machine Learning: The virtual assistant learns from interactions to improve its responses and provide personalized assistance. - Integration: Erica integrates with the bank’s systems to access account information, perform transactions, and provide financial advice. Outcome: Erica handles millions of customer interactions, improving response times, reducing workload for human agents, and enhancing customer satisfaction. Case Study 6: Smart Home Management Company: Google Challenge: Creating a smart home ecosystem that automates household tasks and enhances convenience for users. Solution: Google Assistant uses AI agents to control smart home devices, manage schedules, and provide information. Implementation: - Voice Recognition: AI agents use speech recognition to understand voice commands from users. - Device Control: The assistant interacts with smart home devices (e.g., lights, thermostats, security systems) to execute commands. - Personalization: The AI system learns user preferences and routines to automate tasks and provide relevant information. Outcome: Google Assistant enhances the smart home experience, making it easier for users to manage their homes efficiently and conveniently. This chapter showcases successful applications of AI agents across various industries, demonstrating their versatility and impact. Key Insights and Final Recommendations As we conclude this comprehensive guide on AI agents, it is important to summarize the key insights and provide final recommendations for effectively leveraging AI agents in various applications and industries. Summary of Key Insights 1.", "tokens": 367, "chunk_type": "overlap-1", "url": "https://medium.com/@williamwarley/understanding-ai-agents-how-they-work-types-and-practical-applications-bd261845f7c3", "anchor": "#body:part-15", "type": "blog", "title": "", "section": "Body", "source": "blog:medium.com", "published": "", "sha256": "1a01135065758864d0b7b4093740e0b41a2801a4de4dfc6955715649040054ec"}
{"doc_id": "blog:medium.com", "chunk_id": "body:part-15-o2", "text": "Outcome: Google Assistant enhances the smart home experience, making it easier for users to manage their homes efficiently and conveniently. This chapter showcases successful applications of AI agents across various industries, demonstrating their versatility and impact. Key Insights and Final Recommendations As we conclude this comprehensive guide on AI agents, it is important to summarize the key insights and provide final recommendations for effectively leveraging AI agents in various applications and industries. Summary of Key Insights 1. Definition and Importance: — AI agents are autonomous entities that use AI to perceive their environment, make decisions, and perform actions to achieve specific goals. — They play a crucial role in automating tasks, enhancing decision-making, and improving operational efficiency across various domains. 2. How AI Agents Work: — AI agents operate based on core concepts such as perception, reasoning, action, and learning. — They can be categorized into different types, including simple reflex agents, model-based reflex agents, goal-based agents, utility-based agents, and learning agents. 3. Types of AI Agents: — Simple Reflex Agents: Operate based on predefined rules and immediate perception. — Model-Based Reflex Agents: Maintain an internal model of the environment to make informed decisions. — Goal-Based Agents: Make decisions based on predefined goals and desired outcomes. — Utility-Based Agents: Evaluate the utility of different actions to maximize overall satisfaction or performance. — Learning Agents: Continuously learn from experiences and adapt to new situations. 4. Applications of AI Agents: — AI agents are used in various industries, including autonomous vehicles, robotics, personal assistants, game AI, financial trading, and healthcare. — Their ability to automate tasks, make real-time decisions, and adapt to changing conditions makes them valuable assets in these fields. 5. Advantages of AI Agents: — Efficiency and Automation: Automate repetitive tasks, increase productivity, and reduce errors. — Scalability: Handle large-scale operations and adapt to varying workloads.", "tokens": 397, "chunk_type": "overlap-2", "url": "https://medium.com/@williamwarley/understanding-ai-agents-how-they-work-types-and-practical-applications-bd261845f7c3", "anchor": "#body:part-15", "type": "blog", "title": "", "section": "Body", "source": "blog:medium.com", "published": "", "sha256": "1a01135065758864d0b7b4093740e0b41a2801a4de4dfc6955715649040054ec"}
{"doc_id": "blog:medium.com", "chunk_id": "body:part-16", "text": "from experiences and adapt to new situations. 4. Applications of AI Agents: — AI agents are used in various industries, including autonomous vehicles, robotics, personal assistants, game AI, financial trading, and healthcare. — Their ability to automate tasks, make real-time decisions, and adapt to changing conditions makes them valuable assets in these fields. 5. Advantages of AI Agents: — Efficiency and Automation: Automate repetitive tasks, increase productivity, and reduce errors. — Scalability: Handle large-scale operations and adapt to varying workloads. — Real-Time Decision Making: Provide immediate responses and adapt strategies based on real-time data. — Adaptability and Learning: Improve performance over time and handle complex, dynamic environments. — Cost Efficiency: Reduce operational costs and optimize resource utilization. 6. Implementing AI Agents: — The implementation process involves defining objectives, selecting tools and frameworks, collecting and preprocessing data, training models, evaluating and testing, deploying, and maintaining AI agents. — Best practices include ensuring data quality, using appropriate evaluation metrics, designing for scalability and efficiency, and addressing security and privacy concerns. 7. Future Trends and Developments: — Advances in reinforcement learning, integration with IoT, ethical considerations, and human-agent collaboration are shaping the future of AI agents. — These trends promise to enhance the capabilities, applications, and ethical deployment of AI agents. 8. Case Studies and Real-World Examples: — Successful applications of AI agents in various industries highlight their practical benefits and impact. — Case studies demonstrate the versatility of AI agents in solving complex problems and improving operational efficiency. Final Recommendations 1. Stay Informed and Adaptable: — The field of AI agents is rapidly evolving. Stay informed about the latest developments, research, and best practices to leverage new opportunities and advancements. 2. Invest in Data Quality: — High-quality data is crucial for training effective AI agents. Ensure that your data is clean, representative, and accurately labeled. 3. Select the Right Tools and Frameworks: — Choose tools and frameworks that align with your specific requirements and technical expertise. Consider factors such as scalability, ease of use, and community support. 4. Focus on Ethical and Responsible AI: — Address ethical considerations, including bias mitigation, transparency, and regulatory compliance. Implement robust measures to ensure the responsible deployment of AI agents. 5. Optimize for Scalability and Efficiency: — Design AI agents to scale efficiently and handle varying workloads. Use cloud-based platforms and distributed computing to optimize performance and costs. 6. Continuous Monitoring and Improvement: — Continuously monitor the performance of AI agents and make necessary updates or retrain models to maintain their effectiveness. Stay proactive in addressing any issues that arise. 7. Leverage Human-Agent Collaboration: — Implement human-in-the-loop systems and augmented intelligence approaches to combine human expertise with AI efficiency. Facilitate seamless collaboration between humans and AI agents. 8. Explore Diverse Applications: — Explore various applications of AI agents across different industries. Identify opportunities where AI agents can provide significant value and drive innovation. By following these recommendations and leveraging the insights provided in this guide, you can effectively implement and benefit from AI agents in your projects and applications. Feel free to reach out if you have any", "tokens": 665, "chunk_type": "original-large", "url": "https://medium.com/@williamwarley/understanding-ai-agents-how-they-work-types-and-practical-applications-bd261845f7c3", "anchor": "#body:part-16", "type": "blog", "title": "", "section": "Body", "source": "blog:medium.com", "published": "", "sha256": "2a72235b86b580bb9e7d773e070d2eb8eec239b6d684cb5f07294436c935ed52"}
{"doc_id": "blog:medium.com", "chunk_id": "body:part-16-o1", "text": "from experiences and adapt to new situations. 4. Applications of AI Agents: — AI agents are used in various industries, including autonomous vehicles, robotics, personal assistants, game AI, financial trading, and healthcare. — Their ability to automate tasks, make real-time decisions, and adapt to changing conditions makes them valuable assets in these fields. 5. Advantages of AI Agents: — Efficiency and Automation: Automate repetitive tasks, increase productivity, and reduce errors. — Scalability: Handle large-scale operations and adapt to varying workloads. — Real-Time Decision Making: Provide immediate responses and adapt strategies based on real-time data. — Adaptability and Learning: Improve performance over time and handle complex, dynamic environments. — Cost Efficiency: Reduce operational costs and optimize resource utilization. 6. Implementing AI Agents: — The implementation process involves defining objectives, selecting tools and frameworks, collecting and preprocessing data, training models, evaluating and testing, deploying, and maintaining AI agents. — Best practices include ensuring data quality, using appropriate evaluation metrics, designing for scalability and efficiency, and addressing security and privacy concerns. 7. Future Trends and Developments: — Advances in reinforcement learning, integration with IoT, ethical considerations, and human-agent collaboration are shaping the future of AI agents. — These trends promise to enhance the capabilities, applications, and ethical deployment of AI agents. 8. Case Studies and Real-World Examples: — Successful applications of AI agents in various industries highlight their practical benefits and impact. — Case studies demonstrate the versatility of AI agents in solving complex problems and improving operational efficiency. Final Recommendations 1. Stay Informed and Adaptable: — The field of AI agents is rapidly evolving. Stay informed about the latest developments, research, and best practices to leverage new opportunities and advancements. 2. Invest in Data Quality: — High-quality data is crucial for training effective AI agents. Ensure that your data is clean, representative, and accurately labeled. 3.", "tokens": 395, "chunk_type": "overlap-1", "url": "https://medium.com/@williamwarley/understanding-ai-agents-how-they-work-types-and-practical-applications-bd261845f7c3", "anchor": "#body:part-16", "type": "blog", "title": "", "section": "Body", "source": "blog:medium.com", "published": "", "sha256": "2a72235b86b580bb9e7d773e070d2eb8eec239b6d684cb5f07294436c935ed52"}
{"doc_id": "blog:medium.com", "chunk_id": "body:part-16-o2", "text": "Case Studies and Real-World Examples: — Successful applications of AI agents in various industries highlight their practical benefits and impact. — Case studies demonstrate the versatility of AI agents in solving complex problems and improving operational efficiency. Final Recommendations 1. Stay Informed and Adaptable: — The field of AI agents is rapidly evolving. Stay informed about the latest developments, research, and best practices to leverage new opportunities and advancements. 2. Invest in Data Quality: — High-quality data is crucial for training effective AI agents. Ensure that your data is clean, representative, and accurately labeled. 3. Select the Right Tools and Frameworks: — Choose tools and frameworks that align with your specific requirements and technical expertise. Consider factors such as scalability, ease of use, and community support. 4. Focus on Ethical and Responsible AI: — Address ethical considerations, including bias mitigation, transparency, and regulatory compliance. Implement robust measures to ensure the responsible deployment of AI agents. 5. Optimize for Scalability and Efficiency: — Design AI agents to scale efficiently and handle varying workloads. Use cloud-based platforms and distributed computing to optimize performance and costs. 6. Continuous Monitoring and Improvement: — Continuously monitor the performance of AI agents and make necessary updates or retrain models to maintain their effectiveness. Stay proactive in addressing any issues that arise. 7. Leverage Human-Agent Collaboration: — Implement human-in-the-loop systems and augmented intelligence approaches to combine human expertise with AI efficiency. Facilitate seamless collaboration between humans and AI agents. 8. Explore Diverse Applications: — Explore various applications of AI agents across different industries. Identify opportunities where AI agents can provide significant value and drive innovation. By following these recommendations and leveraging the insights provided in this guide, you can effectively implement and benefit from AI agents in your projects and applications. Feel free to reach out if you have any", "tokens": 393, "chunk_type": "overlap-2", "url": "https://medium.com/@williamwarley/understanding-ai-agents-how-they-work-types-and-practical-applications-bd261845f7c3", "anchor": "#body:part-16", "type": "blog", "title": "", "section": "Body", "source": "blog:medium.com", "published": "", "sha256": "2a72235b86b580bb9e7d773e070d2eb8eec239b6d684cb5f07294436c935ed52"}
{"doc_id": "blog:medium.com", "chunk_id": "body:part-17", "text": "and augmented intelligence approaches to combine human expertise with AI efficiency. Facilitate seamless collaboration between humans and AI agents. 8. Explore Diverse Applications: — Explore various applications of AI agents across different industries. Identify opportunities where AI agents can provide significant value and drive innovation. By following these recommendations and leveraging the insights provided in this guide, you can effectively implement and benefit from AI agents in your projects and applications. Feel free to reach out if you have any questions or need further assistance with specific aspects of AI agents.", "tokens": 118, "chunk_type": "original", "url": "https://medium.com/@williamwarley/understanding-ai-agents-how-they-work-types-and-practical-applications-bd261845f7c3", "anchor": "#body:part-17", "type": "blog", "title": "", "section": "Body", "source": "blog:medium.com", "published": "", "sha256": "adead498c82e56d7bf29fe8393130979553492f46dd1095812179a0bb1a85bd0"}
{"doc_id": "blog:www.amazon.science", "chunk_id": "body", "text": "--- title: Distributed training of large language models on AWS Trainium author: Xinwei Fu; Zhen Zhang; Haozheng Fan; Guangtai Huang; Randy Huang; Rahul Solanki; Fei Wu; Ron Diamant; Yida Wang url: https://www.amazon.science/publications/distributed-training-of-large-language-models-on-aws-trainium hostname: amazon.science description: Large language models (LLMs) are ubiquitously powerful but prohibitively expensive to train, often requiring thousands of compute devices, typically GPUs. To reduce the cost of training LLMs for customers, Amazon Web Services (AWS) launched the Amazon EC2 trn1 instances, powered by AWS Trainium,… sitename: Amazon Science date: 2025-10-16 --- Distributed training of large language models on AWS Trainium 2024 Large language models (LLMs) are ubiquitously powerful but prohibitively expensive to train, often requiring thousands of compute devices, typically GPUs. To reduce the cost of training LLMs for customers, Amazon Web Services (AWS) launched the Amazon EC2 trn1 instances, powered by AWS Trainium, Amazon’s homegrown deep-learning accelerator, as an alternative to distributed LLM training. The trn1 instances provide a high-performance LLM training solution at a lower cost compared to their GPU-based counterpart, the p4d instances, which are powered by Nvidia A100 GPUs. This paper describes the design and development of the Neuron Distributed Training Library, a component of the AWS Neuron SDK, which enables distributed training of large language models on AWS Trainium. Neuron Distributed Training Library supports a variety of existing distributed training techniques with unified interfaces, and provides features to address trn1-specific challenges as well. Our evaluation shows that trn1 instances, specifically the trn1.32xlarge, achieve better or comparable performance (up to 24.6% improvement) while offering significant lower costs (up to 46.3% cost saving) in selected workloads when compared to p4d.24xlarge instances. As a result, AWS Trainium has been adopted for training numerous external and internal models, showcasing its high-performance and cost effectiveness. Several supported open-source LLMs are accessible via HuggingFace Optimum Neuron. Research areas", "tokens": 390, "chunk_type": "original", "url": "https://www.amazon.science/publications/distributed-training-of-large-language-models-on-aws-trainium", "anchor": "#body", "type": "blog", "title": "", "section": "Body", "source": "blog:www.amazon.science", "published": "", "sha256": "a5ec3f94275fee5369b8b92bbd4659e5c2a1d4b9cd05784ed919eafc0ea59d14"}
{"doc_id": "blog:zhihaojia.medium.com", "chunk_id": "body:part-1", "text": "--- title: Compiling LLMs into a MegaKernel: A Path to Low-Latency Inference author: Zhihao Jia url: https://zhihaojia.medium.com/compiling-llms-into-a-megakernel-a-path-to-low-latency-inference-cf7840913c17 hostname: medium.com description: TL;DR: We developed a compiler that automatically transforms LLM inference into a single megakernel — a fused GPU kernel that performs… sitename: Medium date: 2025-06-19 --- Compiling LLMs into a MegaKernel: A Path to Low-Latency Inference TL;DR: We developed a compiler that automatically transforms LLM inference into a single megakernel — a fused GPU kernel that performs all necessary computation and communication in one launch. This end-to-end GPU fusion approach reduces LLM inference latency by 1.2-6.7x. Our compiler is easy to use — you can compile your LLM into a high-performance megakernel with just a few dozen lines of Python. What’s the key idea? Traditional LLM systems often rely on sequences of GPU kernel launches and external communication calls, resulting in underutilized hardware. Our compiler automatically fuses these operations — spanning multiple layers, iterations, and GPUs — into a megakernel. This design eliminates launch overhead, enables fine-grained software pipelining, and overlaps computation with communication across GPUs. Team members: Xinhao Cheng, Bohan Hou, Yingyi Huang, Jianan Ji, Jinchen Jiang, Hongyi Jin, Ruihang Lai, Shengjie Lin, Xupeng Miao, Gabriele Oliaro, Zihao Ye, Zhihao Zhang, Yilong Zhao, Tianqi Chen, Zhihao Jia One of the most effective ways to reduce latency in LLM inference is to fuse all computation and communication into a single megakernel — also known as a persistent kernel. In this design, the system launches just one GPU kernel to execute the entire model — from layer-by-layer computation to inter-GPU communication — without interruption. This approach offers several key performance advantages: - Eliminates kernel launch overhead, even in multi-GPU settings, by avoiding repeated kernel invocations; - Enables software pipelining across layers, allowing the kernel to begin loading data for the next layer while computing the current one; - Overlaps computation and communication, as a megakernel can simultaneously execute compute operations and inter-GPU communication to hide latency. Despite these advantages, compiling an LLM into a megakernel is highly challenging. Existing high-level ML frameworks — such as PyTorch, Triton, and TVM — do not natively support end-to-end megakernel generation. Additionally, modern LLM systems are built from a diverse collection of specialized kernel libraries: NCCL or NVSHMEM for communication, FlashInfer or FlashAttention for efficient attention, and CUDA or Triton for custom computation. This fragmentation makes it difficult to consolidate the entire inference pipeline into a single, unified kernel. Can we automate this process through compilation? Motivated by this question, our team from CMU, UW, Berkeley, NVIDIA, and Tsinghua developed Mirage Persistent Kernel (MPK) — a compiler and runtime system that automatically transforms multi-GPU LLM inference into a high-performance megakernel. MPK unlocks the benefits of end-to-end GPU fusion while requiring minimal manual effort from developers. Why MPK? A key advantage of MPK is extremely low latency for LLM inference by eliminating kernel launch overhead and maximally overlapping computation, data loading, and inter-GPU communication across layers. Figure 1 illustrates a performance comparison between MPK and existing LLM inference systems on both single- and multi-GPU configurations. On a single", "tokens": 665, "chunk_type": "original-large", "url": "https://zhihaojia.medium.com/compiling-llms-into-a-megakernel-a-path-to-low-latency-inference-cf7840913c17", "anchor": "#body:part-1", "type": "blog", "title": "", "section": "Body", "source": "blog:zhihaojia.medium.com", "published": "", "sha256": "aa38190950687c1e82b1cb7a1731f226dcf2489817e072148c639d22b0b0e2da"}
{"doc_id": "blog:zhihaojia.medium.com", "chunk_id": "body:part-1-o1", "text": "--- title: Compiling LLMs into a MegaKernel: A Path to Low-Latency Inference author: Zhihao Jia url: https://zhihaojia.medium.com/compiling-llms-into-a-megakernel-a-path-to-low-latency-inference-cf7840913c17 hostname: medium.com description: TL;DR: We developed a compiler that automatically transforms LLM inference into a single megakernel — a fused GPU kernel that performs… sitename: Medium date: 2025-06-19 --- Compiling LLMs into a MegaKernel: A Path to Low-Latency Inference TL;DR: We developed a compiler that automatically transforms LLM inference into a single megakernel — a fused GPU kernel that performs all necessary computation and communication in one launch. This end-to-end GPU fusion approach reduces LLM inference latency by 1.2-6.7x. Our compiler is easy to use — you can compile your LLM into a high-performance megakernel with just a few dozen lines of Python. What’s the key idea? Traditional LLM systems often rely on sequences of GPU kernel launches and external communication calls, resulting in underutilized hardware. Our compiler automatically fuses these operations — spanning multiple layers, iterations, and GPUs — into a megakernel. This design eliminates launch overhead, enables fine-grained software pipelining, and overlaps computation with communication across GPUs. Team members: Xinhao Cheng, Bohan Hou, Yingyi Huang, Jianan Ji, Jinchen Jiang, Hongyi Jin, Ruihang Lai, Shengjie Lin, Xupeng Miao, Gabriele Oliaro, Zihao Ye, Zhihao Zhang, Yilong Zhao, Tianqi Chen, Zhihao Jia One of the most effective ways to reduce latency in LLM inference is to fuse all computation and communication into a single megakernel — also known as a persistent kernel. In this design, the system launches just one GPU kernel to execute the entire model — from layer-by-layer computation to inter-GPU communication — without interruption.", "tokens": 341, "chunk_type": "overlap-1", "url": "https://zhihaojia.medium.com/compiling-llms-into-a-megakernel-a-path-to-low-latency-inference-cf7840913c17", "anchor": "#body:part-1", "type": "blog", "title": "", "section": "Body", "source": "blog:zhihaojia.medium.com", "published": "", "sha256": "aa38190950687c1e82b1cb7a1731f226dcf2489817e072148c639d22b0b0e2da"}
{"doc_id": "blog:zhihaojia.medium.com", "chunk_id": "body:part-1-o2", "text": "Team members: Xinhao Cheng, Bohan Hou, Yingyi Huang, Jianan Ji, Jinchen Jiang, Hongyi Jin, Ruihang Lai, Shengjie Lin, Xupeng Miao, Gabriele Oliaro, Zihao Ye, Zhihao Zhang, Yilong Zhao, Tianqi Chen, Zhihao Jia One of the most effective ways to reduce latency in LLM inference is to fuse all computation and communication into a single megakernel — also known as a persistent kernel. In this design, the system launches just one GPU kernel to execute the entire model — from layer-by-layer computation to inter-GPU communication — without interruption. This approach offers several key performance advantages: - Eliminates kernel launch overhead, even in multi-GPU settings, by avoiding repeated kernel invocations; - Enables software pipelining across layers, allowing the kernel to begin loading data for the next layer while computing the current one; - Overlaps computation and communication, as a megakernel can simultaneously execute compute operations and inter-GPU communication to hide latency. Despite these advantages, compiling an LLM into a megakernel is highly challenging. Existing high-level ML frameworks — such as PyTorch, Triton, and TVM — do not natively support end-to-end megakernel generation. Additionally, modern LLM systems are built from a diverse collection of specialized kernel libraries: NCCL or NVSHMEM for communication, FlashInfer or FlashAttention for efficient attention, and CUDA or Triton for custom computation. This fragmentation makes it difficult to consolidate the entire inference pipeline into a single, unified kernel. Can we automate this process through compilation? Motivated by this question, our team from CMU, UW, Berkeley, NVIDIA, and Tsinghua developed Mirage Persistent Kernel (MPK) — a compiler and runtime system that automatically transforms multi-GPU LLM inference into a high-performance megakernel. MPK unlocks the benefits of end-to-end GPU fusion while requiring minimal manual effort from developers. Why MPK?", "tokens": 371, "chunk_type": "overlap-2", "url": "https://zhihaojia.medium.com/compiling-llms-into-a-megakernel-a-path-to-low-latency-inference-cf7840913c17", "anchor": "#body:part-1", "type": "blog", "title": "", "section": "Body", "source": "blog:zhihaojia.medium.com", "published": "", "sha256": "aa38190950687c1e82b1cb7a1731f226dcf2489817e072148c639d22b0b0e2da"}
{"doc_id": "blog:zhihaojia.medium.com", "chunk_id": "body:part-1-o3", "text": "This fragmentation makes it difficult to consolidate the entire inference pipeline into a single, unified kernel. Can we automate this process through compilation? Motivated by this question, our team from CMU, UW, Berkeley, NVIDIA, and Tsinghua developed Mirage Persistent Kernel (MPK) — a compiler and runtime system that automatically transforms multi-GPU LLM inference into a high-performance megakernel. MPK unlocks the benefits of end-to-end GPU fusion while requiring minimal manual effort from developers. Why MPK? A key advantage of MPK is extremely low latency for LLM inference by eliminating kernel launch overhead and maximally overlapping computation, data loading, and inter-GPU communication across layers. Figure 1 illustrates a performance comparison between MPK and existing LLM inference systems on both single- and multi-GPU configurations. On a single", "tokens": 161, "chunk_type": "overlap-3", "url": "https://zhihaojia.medium.com/compiling-llms-into-a-megakernel-a-path-to-low-latency-inference-cf7840913c17", "anchor": "#body:part-1", "type": "blog", "title": "", "section": "Body", "source": "blog:zhihaojia.medium.com", "published": "", "sha256": "aa38190950687c1e82b1cb7a1731f226dcf2489817e072148c639d22b0b0e2da"}
{"doc_id": "blog:zhihaojia.medium.com", "chunk_id": "body:part-2", "text": "and runtime system that automatically transforms multi-GPU LLM inference into a high-performance megakernel. MPK unlocks the benefits of end-to-end GPU fusion while requiring minimal manual effort from developers. Why MPK? A key advantage of MPK is extremely low latency for LLM inference by eliminating kernel launch overhead and maximally overlapping computation, data loading, and inter-GPU communication across layers. Figure 1 illustrates a performance comparison between MPK and existing LLM inference systems on both single- and multi-GPU configurations. On a single NVIDIA A100 40GB GPU, MPK reduces per-token decoding latency from 14.5 ms — as achieved by optimized systems like vLLM and SGLang — to 12.5 ms, approaching the theoretical lower bound of 10 ms (based on loading 16 GB of weights with 1.6 TB/s memory bandwidth). Beyond single-GPU optimization, MPK fuses computation and inter-GPU communication into a single megakernel. This design enables MPK to maximally overlap computation and communication. As a result, the performance improvements of MPK over current systems increase with the number of GPUs, making it particularly effective for multi-GPU deployments. What’s Next? The rest of this blog dives deeper into how MPK works: - Part 1 introduces the MPK compiler, which transforms an LLM’s computation graph into an optimized task graph; - Part 2 covers the MPK runtime, which executes this task graph within a megakernel to achieve high throughput and low latency. Part 1. The Compiler: Transforming an LLM into a Fine-Grained Task Graph The computation performed by a large language model (LLM) is typically represented as a computation graph, where each node corresponds to a compute operation (e.g., matrix multiplication, attention) or a collective communication primitive (e.g., all-reduce), and edges denote data dependencies between operations. In existing systems, each operator is generally executed via a dedicated GPU kernel. However, this kernel-per-operator execution model often fails to exploit pipelining opportunities, since dependencies are enforced at a coarse granularity — across entire kernels — rather than the actual data units. Consider a typical example: an allreduce operation following a matrix multiplication. In existing kernel-per-operator systems, the allreduce kernel must wait until the entire matmul kernel completes. In reality, though, each chunk of data for the allreduce only depends on a portion of the matmul output. This mismatch between logical and actual data dependencies limits the potential for overlapping computation and communication. To address this issue, MPK introduces a compiler that automatically transforms the LLM’s computation graph into a fine-grained task graph. This task graph explicitly captures dependencies at the sub-kernel level, enabling more aggressive pipelining across layers. In an MPK task graph: - Each task (shown as a rectangle in Figure 2) represents a unit of computation or communication assigned to a single GPU streaming multiprocessor (SM). - Each event (shown as a circle) represents a synchronization point between tasks. - Each task has an outgoing edge to a triggering event, which is activated once all associated tasks complete. - Each tasks also has an incoming edge from a dependent event, indicating the task can start execution as soon as the event is activated. Task graphs allow MPK to", "tokens": 665, "chunk_type": "original-large", "url": "https://zhihaojia.medium.com/compiling-llms-into-a-megakernel-a-path-to-low-latency-inference-cf7840913c17", "anchor": "#body:part-2", "type": "blog", "title": "", "section": "Body", "source": "blog:zhihaojia.medium.com", "published": "", "sha256": "a7d97b057695ff049a8cb657a271c6cd468ab1f4966fffc7efffe06f3bb20250"}
{"doc_id": "blog:zhihaojia.medium.com", "chunk_id": "body:part-2-o1", "text": "and runtime system that automatically transforms multi-GPU LLM inference into a high-performance megakernel. MPK unlocks the benefits of end-to-end GPU fusion while requiring minimal manual effort from developers. Why MPK? A key advantage of MPK is extremely low latency for LLM inference by eliminating kernel launch overhead and maximally overlapping computation, data loading, and inter-GPU communication across layers. Figure 1 illustrates a performance comparison between MPK and existing LLM inference systems on both single- and multi-GPU configurations. On a single NVIDIA A100 40GB GPU, MPK reduces per-token decoding latency from 14.5 ms — as achieved by optimized systems like vLLM and SGLang — to 12.5 ms, approaching the theoretical lower bound of 10 ms (based on loading 16 GB of weights with 1.6 TB/s memory bandwidth). Beyond single-GPU optimization, MPK fuses computation and inter-GPU communication into a single megakernel. This design enables MPK to maximally overlap computation and communication. As a result, the performance improvements of MPK over current systems increase with the number of GPUs, making it particularly effective for multi-GPU deployments. What’s Next? The rest of this blog dives deeper into how MPK works: - Part 1 introduces the MPK compiler, which transforms an LLM’s computation graph into an optimized task graph; - Part 2 covers the MPK runtime, which executes this task graph within a megakernel to achieve high throughput and low latency. Part 1. The Compiler: Transforming an LLM into a Fine-Grained Task Graph The computation performed by a large language model (LLM) is typically represented as a computation graph, where each node corresponds to a compute operation (e.g., matrix multiplication, attention) or a collective communication primitive (e.g., all-reduce), and edges denote data dependencies between operations. In existing systems, each operator is generally executed via a dedicated GPU kernel.", "tokens": 380, "chunk_type": "overlap-1", "url": "https://zhihaojia.medium.com/compiling-llms-into-a-megakernel-a-path-to-low-latency-inference-cf7840913c17", "anchor": "#body:part-2", "type": "blog", "title": "", "section": "Body", "source": "blog:zhihaojia.medium.com", "published": "", "sha256": "a7d97b057695ff049a8cb657a271c6cd468ab1f4966fffc7efffe06f3bb20250"}
{"doc_id": "blog:zhihaojia.medium.com", "chunk_id": "body:part-2-o2", "text": "The Compiler: Transforming an LLM into a Fine-Grained Task Graph The computation performed by a large language model (LLM) is typically represented as a computation graph, where each node corresponds to a compute operation (e.g., matrix multiplication, attention) or a collective communication primitive (e.g., all-reduce), and edges denote data dependencies between operations. In existing systems, each operator is generally executed via a dedicated GPU kernel. However, this kernel-per-operator execution model often fails to exploit pipelining opportunities, since dependencies are enforced at a coarse granularity — across entire kernels — rather than the actual data units. Consider a typical example: an allreduce operation following a matrix multiplication. In existing kernel-per-operator systems, the allreduce kernel must wait until the entire matmul kernel completes. In reality, though, each chunk of data for the allreduce only depends on a portion of the matmul output. This mismatch between logical and actual data dependencies limits the potential for overlapping computation and communication. To address this issue, MPK introduces a compiler that automatically transforms the LLM’s computation graph into a fine-grained task graph. This task graph explicitly captures dependencies at the sub-kernel level, enabling more aggressive pipelining across layers. In an MPK task graph: - Each task (shown as a rectangle in Figure 2) represents a unit of computation or communication assigned to a single GPU streaming multiprocessor (SM). - Each event (shown as a circle) represents a synchronization point between tasks. - Each task has an outgoing edge to a triggering event, which is activated once all associated tasks complete. - Each tasks also has an incoming edge from a dependent event, indicating the task can start execution as soon as the event is activated. Task graphs allow MPK to", "tokens": 369, "chunk_type": "overlap-2", "url": "https://zhihaojia.medium.com/compiling-llms-into-a-megakernel-a-path-to-low-latency-inference-cf7840913c17", "anchor": "#body:part-2", "type": "blog", "title": "", "section": "Body", "source": "blog:zhihaojia.medium.com", "published": "", "sha256": "a7d97b057695ff049a8cb657a271c6cd468ab1f4966fffc7efffe06f3bb20250"}
{"doc_id": "blog:zhihaojia.medium.com", "chunk_id": "body:part-3", "text": "in Figure 2) represents a unit of computation or communication assigned to a single GPU streaming multiprocessor (SM). - Each event (shown as a circle) represents a synchronization point between tasks. - Each task has an outgoing edge to a triggering event, which is activated once all associated tasks complete. - Each tasks also has an incoming edge from a dependent event, indicating the task can start execution as soon as the event is activated. Task graphs allow MPK to uncover pipelining opportunities that would be missed in computation graphs. For example, MPK can construct an optimized task graph where each allreduce task depends only on the corresponding matmul task that produces its input — enabling partial execution and overlap. In addition to generating an optimized task graph, MPK also automatically generates high-performance CUDA implementations for each task using the Mirage kernel superoptimizer. This ensures that each task runs efficiently on a GPU SM. (For more about the kernel superoptimizer, see this post.) Part 2. The Runtime: Executing a Task Graph in a MegaKernel MPK includes an on-GPU runtime system that executes the task graph entirely within a single GPU megakernel, allowing for fine-grained control over task execution and scheduling without any kernel launches during inference. To achieve this, MPK statically partitions all streaming multiprocessors (SMs) on a GPU into two roles: workers and schedulers. The number of worker and scheduler SMs is fixed at kernel launch time and matches the total number of physical SMs, avoiding any dynamic context switching overhead. Workers Each worker operates on an SM and maintains a dedicated task queue. It follows a simple but efficient execution loop: - Fetch the next task from its queue. - Execute the task (e.g., matrix multiplication, attention, or inter-GPU data transfers). - Notify the triggering event upon task completion. - Repeat. This design ensures that workers remain fully utilized while enabling task execution to proceed asynchronously across layers and operations. Schedulers Scheduling decisions are handled by MPK’s distributed schedulers, each of which runs on a single warp. Because each SM can accommodate multiple warps, up to four schedulers can run concurrently per SM. Each scheduler maintains a queue of activated events. It continuously: - Dequeues activated events whose dependencies are satisfied (i.e., all prerequisite tasks have completed). - Launches the set of tasks that depend on the activated event. This decentralized scheduling mechanism minimizes coordination overhead while enabling scalable execution across SMs. Event-Driven Execution Figure 3 illustrates MPK’s execution timeline. Each rectangle represents a task running on a worker; each circle represents an event. As a task completes, it increments the counter for its corresponding triggering event. When the event counter reaches a pre-defined threshold, the event is considered activated and is enqueued into a scheduler’s event queue. The scheduler then launches any downstream tasks that depend on this event. This design allows for fine-grained software pipelining and overlap between computation and communication. For example: - Matmul tasks can execute in parallel with attention tasks from different layers. - Allreduce communication can begin as soon as partial matmul results are", "tokens": 665, "chunk_type": "original-large", "url": "https://zhihaojia.medium.com/compiling-llms-into-a-megakernel-a-path-to-low-latency-inference-cf7840913c17", "anchor": "#body:part-3", "type": "blog", "title": "", "section": "Body", "source": "blog:zhihaojia.medium.com", "published": "", "sha256": "506b645a092d930507e1e4d2a5dc35423610c386b06ef44a8beb15a6ef4d1ed7"}
{"doc_id": "blog:zhihaojia.medium.com", "chunk_id": "body:part-3-o1", "text": "in Figure 2) represents a unit of computation or communication assigned to a single GPU streaming multiprocessor (SM). - Each event (shown as a circle) represents a synchronization point between tasks. - Each task has an outgoing edge to a triggering event, which is activated once all associated tasks complete. - Each tasks also has an incoming edge from a dependent event, indicating the task can start execution as soon as the event is activated. Task graphs allow MPK to uncover pipelining opportunities that would be missed in computation graphs. For example, MPK can construct an optimized task graph where each allreduce task depends only on the corresponding matmul task that produces its input — enabling partial execution and overlap. In addition to generating an optimized task graph, MPK also automatically generates high-performance CUDA implementations for each task using the Mirage kernel superoptimizer. This ensures that each task runs efficiently on a GPU SM. (For more about the kernel superoptimizer, see this post.) Part 2. The Runtime: Executing a Task Graph in a MegaKernel MPK includes an on-GPU runtime system that executes the task graph entirely within a single GPU megakernel, allowing for fine-grained control over task execution and scheduling without any kernel launches during inference. To achieve this, MPK statically partitions all streaming multiprocessors (SMs) on a GPU into two roles: workers and schedulers. The number of worker and scheduler SMs is fixed at kernel launch time and matches the total number of physical SMs, avoiding any dynamic context switching overhead. Workers Each worker operates on an SM and maintains a dedicated task queue. It follows a simple but efficient execution loop: - Fetch the next task from its queue. - Execute the task (e.g., matrix multiplication, attention, or inter-GPU data transfers). - Notify the triggering event upon task completion. - Repeat.", "tokens": 393, "chunk_type": "overlap-1", "url": "https://zhihaojia.medium.com/compiling-llms-into-a-megakernel-a-path-to-low-latency-inference-cf7840913c17", "anchor": "#body:part-3", "type": "blog", "title": "", "section": "Body", "source": "blog:zhihaojia.medium.com", "published": "", "sha256": "506b645a092d930507e1e4d2a5dc35423610c386b06ef44a8beb15a6ef4d1ed7"}
{"doc_id": "blog:zhihaojia.medium.com", "chunk_id": "body:part-3-o2", "text": "The number of worker and scheduler SMs is fixed at kernel launch time and matches the total number of physical SMs, avoiding any dynamic context switching overhead. Workers Each worker operates on an SM and maintains a dedicated task queue. It follows a simple but efficient execution loop: - Fetch the next task from its queue. - Execute the task (e.g., matrix multiplication, attention, or inter-GPU data transfers). - Notify the triggering event upon task completion. - Repeat. This design ensures that workers remain fully utilized while enabling task execution to proceed asynchronously across layers and operations. Schedulers Scheduling decisions are handled by MPK’s distributed schedulers, each of which runs on a single warp. Because each SM can accommodate multiple warps, up to four schedulers can run concurrently per SM. Each scheduler maintains a queue of activated events. It continuously: - Dequeues activated events whose dependencies are satisfied (i.e., all prerequisite tasks have completed). - Launches the set of tasks that depend on the activated event. This decentralized scheduling mechanism minimizes coordination overhead while enabling scalable execution across SMs. Event-Driven Execution Figure 3 illustrates MPK’s execution timeline. Each rectangle represents a task running on a worker; each circle represents an event. As a task completes, it increments the counter for its corresponding triggering event. When the event counter reaches a pre-defined threshold, the event is considered activated and is enqueued into a scheduler’s event queue. The scheduler then launches any downstream tasks that depend on this event. This design allows for fine-grained software pipelining and overlap between computation and communication. For example: - Matmul tasks can execute in parallel with attention tasks from different layers. - Allreduce communication can begin as soon as partial matmul results are", "tokens": 373, "chunk_type": "overlap-2", "url": "https://zhihaojia.medium.com/compiling-llms-into-a-megakernel-a-path-to-low-latency-inference-cf7840913c17", "anchor": "#body:part-3", "type": "blog", "title": "", "section": "Body", "source": "blog:zhihaojia.medium.com", "published": "", "sha256": "506b645a092d930507e1e4d2a5dc35423610c386b06ef44a8beb15a6ef4d1ed7"}
{"doc_id": "blog:zhihaojia.medium.com", "chunk_id": "body:part-4", "text": "the counter for its corresponding triggering event. When the event counter reaches a pre-defined threshold, the event is considered activated and is enqueued into a scheduler’s event queue. The scheduler then launches any downstream tasks that depend on this event. This design allows for fine-grained software pipelining and overlap between computation and communication. For example: - Matmul tasks can execute in parallel with attention tasks from different layers. - Allreduce communication can begin as soon as partial matmul results are available. Because all scheduling and task transitions occur within a single kernel context, the overhead between tasks is extremely low — typically just 1–2 microseconds — enabling efficient execution of multi-layer, multi-GPU LLM workloads. Looking Forward Our vision for MPK is to make megakernel compilation both easy to use and highly performant. Currently you can compile an LLM into a megakernel with just a few dozen lines of Python code — mainly to specify the megakernel’s inputs and outputs. We’re excited about this direction, and there’s still much more to explore. Some of the key areas we’re actively working on include: - Support for modern GPU architectures. One of our next milestones is extending MPK to support next-generation architectures such as NVIDIA Blackwell. A major challenge lies in integrating warp specialization — a key optimization for newer GPUs — with MPK’s megakernel execution model. - Handling workload dynamism. MPK currently builds a static task graph, which limits its ability to handle dynamic workloads such as mixture-of-experts (MoE) models. We’re developing new compilation strategies that allow MPK to support dynamic control flow and conditional execution inside megakernels. - Advanced scheduling and task assignment: MPK unlocks a new level of fine-grained scheduling at the task level. While our current implementation uses simple round-robin scheduling to distribute tasks across SMs, we see exciting opportunities in advanced scheduling policies — such as priority-aware or throughput-optimized strategies — for use cases like latency-SLO-driven serving or hybrid batching. We believe MPK represents a foundational shift in how LLM inference workloads are compiled and executed on GPUs, and we’re eager to collaborate with the community to push this vision forward. Want to Learn More? To learn more about MPK and explore our code and documentation, please visit our project website: https://github.com/mirage-project/mirage. We welcome feedback, contributions, and collaborations from the community!", "tokens": 497, "chunk_type": "original", "url": "https://zhihaojia.medium.com/compiling-llms-into-a-megakernel-a-path-to-low-latency-inference-cf7840913c17", "anchor": "#body:part-4", "type": "blog", "title": "", "section": "Body", "source": "blog:zhihaojia.medium.com", "published": "", "sha256": "46a386ef010ce6d11b54aac6ddcb9c3755d6855cc20c8ee3c34c304d34322605"}
{"doc_id": "blog:hazyresearch.stanford.edu", "chunk_id": "body:part-1", "text": "--- title: We Bought the Whole GPU, So We're Damn Well Going to Use the Whole GPU url: https://hazyresearch.stanford.edu/blog/2025-09-28-tp-llama-main hostname: stanford.edu sitename: hazyresearch.stanford.edu date: 2025-09-28 --- Sep 28, 2025 · 24 min read We Bought the Whole GPU, So We're Damn Well Going to Use the Whole GPU Benjamin Spector*, Jordan Juravsky*, Stuart Sul*, Dylan Lim, Owen Dugan, Simran Arora, Chris Ré Intro Post | Code | Low-Latency Megakernels | Brr TLDR: We're releasing a throughput-optimized megakernel for tensor-parallel inference with Llama-70B on H100s. Our kernel can aggressively overlap compute, memory, and communication ops in order to simultaneously use the different hardware resources available on a GPU. When integrated into the Tokasaurus inference engine, our megakernel can outperform SGLang by >22% on end-to-end throughput (measured as time to finish 65,536 prompts from the ShareGPT benchmark). We're releasing the code here; please be warned that this really is research code; it is sensitive to compiler versions, GPU setup, and sometimes even being looked at the wrong way, and we have no intention whatsoever of supporting it. We hope you'll find the ideas and results interesting nonetheless! Figure 1: Zoooommmm A few months ago, we showed how we could fuse an entire model forward pass into a single \"megakernel\" in order to deliver low-latency inference with Llama-1B. In that post, we teased that many of the same concepts we introduced would also be useful for optimizing for throughput. We're now excited to bring receipts and release a new megakernel optimized for high-throughput inference with Llama-70B. The inference workloads targeted by our low-latency and high-throughput megakernels are quite different and require distinct optimizations. Our low-latency megakernel targeted inference using Llama-1B when running on a single GPU with batch size one. This workload was entirely memory bound, and our focus was therefore on eliminating stalls that delayed loading model weights from global memory. With large-batch Llama-70B inference, our workload is much more heterogeneous. Large portions of it (e.g. matrix multiplies, attention prefill) are compute-bound. Other parts (e.g. attention decode, RMS norm) are still bottlenecked by global memory bandwidth. Additionally, by distributing our model across multiple GPUs, we now need to perform cross-GPU communication that throttles the NVLink connections between devices. By running these components sequentially, we've paid for the whole GPU, but are only using little bits and pieces of it at a time. :( Overall, these different operations in our model each make use of different resources available on the GPU (e.g. tensor cores, non-matmul compute units, HBM bandwidth, NVLink bandwidth) in unique ways. Therefore, a key area for optimizing this high-throughput workload is to overlap multiple kinds of work in order to simultaneously use more of the GPU's resources. We want to do this across many levels of the GPU -- within an individual SM, across multiple SMs, and even across GPUs. Existing approaches to overlapping include assigning different SMs to different ops, developing custom kernels to run prefill and decode simultaneously, and running kernels in parallel with cross-gpu memory copying operations. Here, we show that the same simple, interpreter-based megakernel patterns we previously", "tokens": 665, "chunk_type": "original-large", "url": "https://hazyresearch.stanford.edu/blog/2025-09-28-tp-llama-main", "anchor": "#body:part-1", "type": "blog", "title": "", "section": "Body", "source": "blog:hazyresearch.stanford.edu", "published": "", "sha256": "1dbe7437f7df8a8a62be2418f7044545666d592f5024b8e7de7b28fc03412043"}
{"doc_id": "blog:hazyresearch.stanford.edu", "chunk_id": "body:part-1-o1", "text": "--- title: We Bought the Whole GPU, So We're Damn Well Going to Use the Whole GPU url: https://hazyresearch.stanford.edu/blog/2025-09-28-tp-llama-main hostname: stanford.edu sitename: hazyresearch.stanford.edu date: 2025-09-28 --- Sep 28, 2025 · 24 min read We Bought the Whole GPU, So We're Damn Well Going to Use the Whole GPU Benjamin Spector*, Jordan Juravsky*, Stuart Sul*, Dylan Lim, Owen Dugan, Simran Arora, Chris Ré Intro Post | Code | Low-Latency Megakernels | Brr TLDR: We're releasing a throughput-optimized megakernel for tensor-parallel inference with Llama-70B on H100s. Our kernel can aggressively overlap compute, memory, and communication ops in order to simultaneously use the different hardware resources available on a GPU. When integrated into the Tokasaurus inference engine, our megakernel can outperform SGLang by >22% on end-to-end throughput (measured as time to finish 65,536 prompts from the ShareGPT benchmark). We're releasing the code here; please be warned that this really is research code; it is sensitive to compiler versions, GPU setup, and sometimes even being looked at the wrong way, and we have no intention whatsoever of supporting it. We hope you'll find the ideas and results interesting nonetheless! Figure 1: Zoooommmm A few months ago, we showed how we could fuse an entire model forward pass into a single \"megakernel\" in order to deliver low-latency inference with Llama-1B. In that post, we teased that many of the same concepts we introduced would also be useful for optimizing for throughput. We're now excited to bring receipts and release a new megakernel optimized for high-throughput inference with Llama-70B. The inference workloads targeted by our low-latency and high-throughput megakernels are quite different and require distinct optimizations. Our low-latency megakernel targeted inference using Llama-1B when running on a single GPU with batch size one. This workload was entirely memory bound, and our focus was therefore on eliminating stalls that delayed loading model weights from global memory.", "tokens": 401, "chunk_type": "overlap-1", "url": "https://hazyresearch.stanford.edu/blog/2025-09-28-tp-llama-main", "anchor": "#body:part-1", "type": "blog", "title": "", "section": "Body", "source": "blog:hazyresearch.stanford.edu", "published": "", "sha256": "1dbe7437f7df8a8a62be2418f7044545666d592f5024b8e7de7b28fc03412043"}
{"doc_id": "blog:hazyresearch.stanford.edu", "chunk_id": "body:part-1-o2", "text": "We're now excited to bring receipts and release a new megakernel optimized for high-throughput inference with Llama-70B. The inference workloads targeted by our low-latency and high-throughput megakernels are quite different and require distinct optimizations. Our low-latency megakernel targeted inference using Llama-1B when running on a single GPU with batch size one. This workload was entirely memory bound, and our focus was therefore on eliminating stalls that delayed loading model weights from global memory. With large-batch Llama-70B inference, our workload is much more heterogeneous. Large portions of it (e.g. matrix multiplies, attention prefill) are compute-bound. Other parts (e.g. attention decode, RMS norm) are still bottlenecked by global memory bandwidth. Additionally, by distributing our model across multiple GPUs, we now need to perform cross-GPU communication that throttles the NVLink connections between devices. By running these components sequentially, we've paid for the whole GPU, but are only using little bits and pieces of it at a time. :( Overall, these different operations in our model each make use of different resources available on the GPU (e.g. tensor cores, non-matmul compute units, HBM bandwidth, NVLink bandwidth) in unique ways. Therefore, a key area for optimizing this high-throughput workload is to overlap multiple kinds of work in order to simultaneously use more of the GPU's resources. We want to do this across many levels of the GPU -- within an individual SM, across multiple SMs, and even across GPUs. Existing approaches to overlapping include assigning different SMs to different ops, developing custom kernels to run prefill and decode simultaneously, and running kernels in parallel with cross-gpu memory copying operations. Here, we show that the same simple, interpreter-based megakernel patterns we previously", "tokens": 358, "chunk_type": "overlap-2", "url": "https://hazyresearch.stanford.edu/blog/2025-09-28-tp-llama-main", "anchor": "#body:part-1", "type": "blog", "title": "", "section": "Body", "source": "blog:hazyresearch.stanford.edu", "published": "", "sha256": "1dbe7437f7df8a8a62be2418f7044545666d592f5024b8e7de7b28fc03412043"}
{"doc_id": "blog:hazyresearch.stanford.edu", "chunk_id": "body:part-2", "text": "overlap multiple kinds of work in order to simultaneously use more of the GPU's resources. We want to do this across many levels of the GPU -- within an individual SM, across multiple SMs, and even across GPUs. Existing approaches to overlapping include assigning different SMs to different ops, developing custom kernels to run prefill and decode simultaneously, and running kernels in parallel with cross-gpu memory copying operations. Here, we show that the same simple, interpreter-based megakernel patterns we previously introduced can also achieve all of these fine-grained overlapping patterns -- and more! Most excitingly, despite the significant differences between our low-latency and high-throughput workloads, our core megakernel abstraction (a pipelined instruction interpreter that runs on each SM) is highly transferable across both domains. In the rest of this blog, we will: - Give a brief recap on the design of our megakernels from our last, low-latency post. - Walk through the details of the tensor-parallel Llama forward pass that we map into our megakernel, including a novel approach to communicating intermediate results across GPUs right after running attention. This new operation requires a complicated multi-GPU transpose not efficiently expressable with standard communication patterns, but is trivial to implement within the megakernel! - Show how megakernels can achieve fine-grained resource overlapping at multiple levels of the GPU hierarchy: within individual SMs, across multiple SMs, and across multiple GPUs! - Within individual SMs, the same inter-instruction pipelining we used in low-latency llama can also help keep overlap memory movement and compute across instructions, thereby keeping the tensor cores running. - Across multiple SMs, careful scheduling of instructions can overlap both compute-intensive (e.g. matrix multiply) and memory-intensive (e.g. RMS norm) kinds of work at once, on an individual GPU. - Across GPUs, we can hide communication costs within special \"storer\" threads, leaving other threads free to do work on the next instruction while communication happens in the background. - Finally, we put it all together by benchmarking our megakernel against vLLM and SGLang. Megakernels: A Brief Recap In our last post, we wrote our first full-model megakernel in order to optimize a low-latency scenario: running inference with Llama-3.2-1B and batch size one. We discovered that popular inference engines like vLLM and SGLang were only using about half of the available GPU bandwidth on an H100. The problem is that traditional systems break down model forward passes into dozens or hundreds of separate kernels, each with setup and teardown periods where no useful work gets done. These overhead periods create \"memory pipeline bubbles\" where an SM (i.e. a streaming multiprocessor, one of the compute subunits on a GPU) sits idle instead of loading model weights. Our solution to this was to merge the entire Llama-1B forward pass into a single fused \"megakernel\" that eliminates kernel boundaries altogether. We found that on small models, our megakernel could provide per-user throughput around 50% higher than inference frameworks like SGLang and vLLM. The core abstraction behind our megakernel lays in an instruction-and-interpreter model. - Instructions: Instead of decomposing a model forward pass into a series of coarse-grained kernels,", "tokens": 665, "chunk_type": "original-large", "url": "https://hazyresearch.stanford.edu/blog/2025-09-28-tp-llama-main", "anchor": "#body:part-2", "type": "blog", "title": "", "section": "Body", "source": "blog:hazyresearch.stanford.edu", "published": "", "sha256": "a45c8729d48f3fe1d877360134a66a2f1fd2069d3e12c3475bce13dcb32fcd68"}
{"doc_id": "blog:hazyresearch.stanford.edu", "chunk_id": "body:part-2-o1", "text": "overlap multiple kinds of work in order to simultaneously use more of the GPU's resources. We want to do this across many levels of the GPU -- within an individual SM, across multiple SMs, and even across GPUs. Existing approaches to overlapping include assigning different SMs to different ops, developing custom kernels to run prefill and decode simultaneously, and running kernels in parallel with cross-gpu memory copying operations. Here, we show that the same simple, interpreter-based megakernel patterns we previously introduced can also achieve all of these fine-grained overlapping patterns -- and more! Most excitingly, despite the significant differences between our low-latency and high-throughput workloads, our core megakernel abstraction (a pipelined instruction interpreter that runs on each SM) is highly transferable across both domains. In the rest of this blog, we will: - Give a brief recap on the design of our megakernels from our last, low-latency post. - Walk through the details of the tensor-parallel Llama forward pass that we map into our megakernel, including a novel approach to communicating intermediate results across GPUs right after running attention. This new operation requires a complicated multi-GPU transpose not efficiently expressable with standard communication patterns, but is trivial to implement within the megakernel! - Show how megakernels can achieve fine-grained resource overlapping at multiple levels of the GPU hierarchy: within individual SMs, across multiple SMs, and across multiple GPUs! - Within individual SMs, the same inter-instruction pipelining we used in low-latency llama can also help keep overlap memory movement and compute across instructions, thereby keeping the tensor cores running. - Across multiple SMs, careful scheduling of instructions can overlap both compute-intensive (e.g. matrix multiply) and memory-intensive (e.g.", "tokens": 358, "chunk_type": "overlap-1", "url": "https://hazyresearch.stanford.edu/blog/2025-09-28-tp-llama-main", "anchor": "#body:part-2", "type": "blog", "title": "", "section": "Body", "source": "blog:hazyresearch.stanford.edu", "published": "", "sha256": "a45c8729d48f3fe1d877360134a66a2f1fd2069d3e12c3475bce13dcb32fcd68"}
{"doc_id": "blog:hazyresearch.stanford.edu", "chunk_id": "body:part-2-o2", "text": "This new operation requires a complicated multi-GPU transpose not efficiently expressable with standard communication patterns, but is trivial to implement within the megakernel! - Show how megakernels can achieve fine-grained resource overlapping at multiple levels of the GPU hierarchy: within individual SMs, across multiple SMs, and across multiple GPUs! - Within individual SMs, the same inter-instruction pipelining we used in low-latency llama can also help keep overlap memory movement and compute across instructions, thereby keeping the tensor cores running. - Across multiple SMs, careful scheduling of instructions can overlap both compute-intensive (e.g. matrix multiply) and memory-intensive (e.g. RMS norm) kinds of work at once, on an individual GPU. - Across GPUs, we can hide communication costs within special \"storer\" threads, leaving other threads free to do work on the next instruction while communication happens in the background. - Finally, we put it all together by benchmarking our megakernel against vLLM and SGLang. Megakernels: A Brief Recap In our last post, we wrote our first full-model megakernel in order to optimize a low-latency scenario: running inference with Llama-3.2-1B and batch size one. We discovered that popular inference engines like vLLM and SGLang were only using about half of the available GPU bandwidth on an H100. The problem is that traditional systems break down model forward passes into dozens or hundreds of separate kernels, each with setup and teardown periods where no useful work gets done. These overhead periods create \"memory pipeline bubbles\" where an SM (i.e. a streaming multiprocessor, one of the compute subunits on a GPU) sits idle instead of loading model weights. Our solution to this was to merge the entire Llama-1B forward pass into a single fused \"megakernel\" that eliminates kernel boundaries altogether. We found that on small models, our megakernel could provide per-user throughput around 50% higher than inference frameworks like SGLang and vLLM.", "tokens": 399, "chunk_type": "overlap-2", "url": "https://hazyresearch.stanford.edu/blog/2025-09-28-tp-llama-main", "anchor": "#body:part-2", "type": "blog", "title": "", "section": "Body", "source": "blog:hazyresearch.stanford.edu", "published": "", "sha256": "a45c8729d48f3fe1d877360134a66a2f1fd2069d3e12c3475bce13dcb32fcd68"}
{"doc_id": "blog:hazyresearch.stanford.edu", "chunk_id": "body:part-2-o3", "text": "These overhead periods create \"memory pipeline bubbles\" where an SM (i.e. a streaming multiprocessor, one of the compute subunits on a GPU) sits idle instead of loading model weights. Our solution to this was to merge the entire Llama-1B forward pass into a single fused \"megakernel\" that eliminates kernel boundaries altogether. We found that on small models, our megakernel could provide per-user throughput around 50% higher than inference frameworks like SGLang and vLLM. The core abstraction behind our megakernel lays in an instruction-and-interpreter model. - Instructions: Instead of decomposing a model forward pass into a series of coarse-grained kernels,", "tokens": 128, "chunk_type": "overlap-3", "url": "https://hazyresearch.stanford.edu/blog/2025-09-28-tp-llama-main", "anchor": "#body:part-2", "type": "blog", "title": "", "section": "Body", "source": "blog:hazyresearch.stanford.edu", "published": "", "sha256": "a45c8729d48f3fe1d877360134a66a2f1fd2069d3e12c3475bce13dcb32fcd68"}
{"doc_id": "blog:hazyresearch.stanford.edu", "chunk_id": "body:part-3", "text": "on a GPU) sits idle instead of loading model weights. Our solution to this was to merge the entire Llama-1B forward pass into a single fused \"megakernel\" that eliminates kernel boundaries altogether. We found that on small models, our megakernel could provide per-user throughput around 50% higher than inference frameworks like SGLang and vLLM. The core abstraction behind our megakernel lays in an instruction-and-interpreter model. - Instructions: Instead of decomposing a model forward pass into a series of coarse-grained kernels, we instead decomposed it into a sequence of fine-grained instructions. Instructions can have distinct types, loosely corresponding to the kinds of kernels one would use in conventional implementations (e.g. matrix multiply, attention prefill, RMS norm). Each instruction specifies a unit of work that would traditionally be performed by a thread block, e.g. compute an output tile for a matrix multiplication. Furthermore, each instruction is organized into dedicated sections, e.g. a load function that reads from global memory, a compute function, and a store function that writes out results. - Interpreter: We execute these instructions using an on-GPU interpreter. When the megakernel launches, each SM initializes an interpreter and starts executing a sequence of instructions (these sequences are scheduled into per-SM queues ahead of time). A key feature of these interpreters is that they can aggressively pipeline across instruction boundaries, starting tasks for the next instruction (e.g. loading model weights) while the current instruction finishes. For our low-latency megakernel, this let us eliminate most of the memory bubbles between operations. For more details on the interpreter design (e.g. how we manage shared memory across instructions, how we synchronize different SMs), see the original blog post and the codebase. In this blog, we'll focus on a high-throughput workload with very different performance considerations than our previous Llama-1B target. However, as we'll describe below, this same core instruction/interpreter abstraction will be extremely helpful for achieving high throughput. THE WORKLOAD Anatomy of a Llama First, we'll start with a brief walkthrough of the operations needed to perform a large-batch forwards pass using tensor-parallel Llama-70B. Specifically, we implement the \"sequence parallel\" variant of TP, where some operations are performed data-parallel (i.e. each GPU holds full activation vectors for a slice of the tokens in a batch) and some operations are performed tensor-parallel (i.e. each GPU holds a slice of the activation vectors for all tokens). Concretely, with sequence parallelism each transformer block receives a data-parallel chunk of the hidden states (i.e. the full hidden states for a subset of tokens) as input and performs the following operations: - Data-parallel pre-attention RMS norm. - All-gather (i.e. each GPU collects the activations from all other GPUs, so that each GPU now has the activations from all tokens). - Tensor-parallel QKV projections, attention, and O projection (each GPU is responsible for a subset of attention heads). - Reduce-scatter. - Data-parallel post-attention residual connection and pre-MLP RMS norm. - All-gather again. - Tensor-parallel MLP. - Reduce-scatter. - Post-MLP residual connection. However, we've made one important change to this formulation. One of our targets for operation overlapping is to overlap the O", "tokens": 665, "chunk_type": "original-large", "url": "https://hazyresearch.stanford.edu/blog/2025-09-28-tp-llama-main", "anchor": "#body:part-3", "type": "blog", "title": "", "section": "Body", "source": "blog:hazyresearch.stanford.edu", "published": "", "sha256": "c3610180da8142a86c6da8fa64158b3111dd241530c346247f43b476451127f8"}
{"doc_id": "blog:hazyresearch.stanford.edu", "chunk_id": "body:part-3-o1", "text": "on a GPU) sits idle instead of loading model weights. Our solution to this was to merge the entire Llama-1B forward pass into a single fused \"megakernel\" that eliminates kernel boundaries altogether. We found that on small models, our megakernel could provide per-user throughput around 50% higher than inference frameworks like SGLang and vLLM. The core abstraction behind our megakernel lays in an instruction-and-interpreter model. - Instructions: Instead of decomposing a model forward pass into a series of coarse-grained kernels, we instead decomposed it into a sequence of fine-grained instructions. Instructions can have distinct types, loosely corresponding to the kinds of kernels one would use in conventional implementations (e.g. matrix multiply, attention prefill, RMS norm). Each instruction specifies a unit of work that would traditionally be performed by a thread block, e.g. compute an output tile for a matrix multiplication. Furthermore, each instruction is organized into dedicated sections, e.g. a load function that reads from global memory, a compute function, and a store function that writes out results. - Interpreter: We execute these instructions using an on-GPU interpreter. When the megakernel launches, each SM initializes an interpreter and starts executing a sequence of instructions (these sequences are scheduled into per-SM queues ahead of time). A key feature of these interpreters is that they can aggressively pipeline across instruction boundaries, starting tasks for the next instruction (e.g. loading model weights) while the current instruction finishes. For our low-latency megakernel, this let us eliminate most of the memory bubbles between operations. For more details on the interpreter design (e.g. how we manage shared memory across instructions, how we synchronize different SMs), see the original blog post and the codebase. In this blog, we'll focus on a high-throughput workload with very different performance considerations than our previous Llama-1B target.", "tokens": 384, "chunk_type": "overlap-1", "url": "https://hazyresearch.stanford.edu/blog/2025-09-28-tp-llama-main", "anchor": "#body:part-3", "type": "blog", "title": "", "section": "Body", "source": "blog:hazyresearch.stanford.edu", "published": "", "sha256": "c3610180da8142a86c6da8fa64158b3111dd241530c346247f43b476451127f8"}
{"doc_id": "blog:hazyresearch.stanford.edu", "chunk_id": "body:part-3-o2", "text": "For our low-latency megakernel, this let us eliminate most of the memory bubbles between operations. For more details on the interpreter design (e.g. how we manage shared memory across instructions, how we synchronize different SMs), see the original blog post and the codebase. In this blog, we'll focus on a high-throughput workload with very different performance considerations than our previous Llama-1B target. However, as we'll describe below, this same core instruction/interpreter abstraction will be extremely helpful for achieving high throughput. THE WORKLOAD Anatomy of a Llama First, we'll start with a brief walkthrough of the operations needed to perform a large-batch forwards pass using tensor-parallel Llama-70B. Specifically, we implement the \"sequence parallel\" variant of TP, where some operations are performed data-parallel (i.e. each GPU holds full activation vectors for a slice of the tokens in a batch) and some operations are performed tensor-parallel (i.e. each GPU holds a slice of the activation vectors for all tokens). Concretely, with sequence parallelism each transformer block receives a data-parallel chunk of the hidden states (i.e. the full hidden states for a subset of tokens) as input and performs the following operations: - Data-parallel pre-attention RMS norm. - All-gather (i.e. each GPU collects the activations from all other GPUs, so that each GPU now has the activations from all tokens). - Tensor-parallel QKV projections, attention, and O projection (each GPU is responsible for a subset of attention heads). - Reduce-scatter. - Data-parallel post-attention residual connection and pre-MLP RMS norm. - All-gather again. - Tensor-parallel MLP. - Reduce-scatter. - Post-MLP residual connection. However, we've made one important change to this formulation. One of our targets for operation overlapping is to overlap the O", "tokens": 361, "chunk_type": "overlap-2", "url": "https://hazyresearch.stanford.edu/blog/2025-09-28-tp-llama-main", "anchor": "#body:part-3", "type": "blog", "title": "", "section": "Body", "source": "blog:hazyresearch.stanford.edu", "published": "", "sha256": "c3610180da8142a86c6da8fa64158b3111dd241530c346247f43b476451127f8"}
{"doc_id": "blog:hazyresearch.stanford.edu", "chunk_id": "body:part-4", "text": "collects the activations from all other GPUs, so that each GPU now has the activations from all tokens). - Tensor-parallel QKV projections, attention, and O projection (each GPU is responsible for a subset of attention heads). - Reduce-scatter. - Data-parallel post-attention residual connection and pre-MLP RMS norm. - All-gather again. - Tensor-parallel MLP. - Reduce-scatter. - Post-MLP residual connection. However, we've made one important change to this formulation. One of our targets for operation overlapping is to overlap the O projection matrix multiplication with the subsequent reduce-scatter. However, the tensor-parallel sharded O matrix is too small for us to effectively hide the reduce-scatter communication cost. To solve this, we instead choose to replicate the O projection matrix across each GPU and run the O projection with data parallelism instead of with tensor parallelism. Alongside this change, we eliminate the post-attention reduce-scatter and replace it with a \"distributed transpose\" operation after attention that repartitions our data from a tensor-parallel configuration to a data-parallel configuration. When using 8 GPUs, this reduces the network traffic by a factor of 8, which makes it much easier to hide the cost of communication cost by overlapping it with matrix multiplications. Note that the downside of this approach is it reduces the maximum batch size by about 15%, because replicating the O projection weights consumes an additional 9 GB of memory per GPU. Defining our Megakernel Instruction Set With our parallelism scheme decided on, we are able to construct the instruction set for our high-throughput megakernel. We partition our workload into the following fused instructions: - RMS norm + all-gather. - A QKV matrix multiply + RoPE. - Attention + distributed transpose. - O-projection matrix multiply + residual. - Gate matrix multiply + SiLU. - Up-projection matrix multiply + elementwise multiplication with the output of the gate. - Down matrix multiply + reduce-scatter + residual. - RMS norm without the all-gather (for the LM head) - LM head matrix multiply. Relative to our latency-focused Llama-1B megakernel, this instruction set contains several high-level changes in our approach: - Most of our instructions for low-latency centered around matrix-vector multiplication, rather than the matrix-matrix multiplications we do here. The optimal work partitioning for these two operations is generally completely different. For matrix-vector products, each instruction computes several complete columns of the output vector. However, in matrix-matrix products, each instruction computes a tile of the output matrix instead. - To avoid extra trips to global memory, for our low-latency megakernel we frequently recomputed results across the GPU rather than communicating them through memory. This allowed us to fuse operations more aggressively than usual. For example, our QKV matrix-vector product fused the RMS norm into its beginning, saving an instruction boundary, but performing identical RMS computations over a hundred times. When focusing on throughput, this recomputation is not worth it. - Putting many tokens in our batch requires us to expand our inter-instruction signalling scheme to track data dependencies across tokens. This signalling can vary by instruction -- for some operations (e.g. attention, RMS norms), sometimes synchronization is at the granularity of 128", "tokens": 665, "chunk_type": "original-large", "url": "https://hazyresearch.stanford.edu/blog/2025-09-28-tp-llama-main", "anchor": "#body:part-4", "type": "blog", "title": "", "section": "Body", "source": "blog:hazyresearch.stanford.edu", "published": "", "sha256": "a10b1f9797f169a4f85905861936f137ca67c6976a707ef8888ceb4d2ce351e4"}
{"doc_id": "blog:hazyresearch.stanford.edu", "chunk_id": "body:part-4-o1", "text": "collects the activations from all other GPUs, so that each GPU now has the activations from all tokens). - Tensor-parallel QKV projections, attention, and O projection (each GPU is responsible for a subset of attention heads). - Reduce-scatter. - Data-parallel post-attention residual connection and pre-MLP RMS norm. - All-gather again. - Tensor-parallel MLP. - Reduce-scatter. - Post-MLP residual connection. However, we've made one important change to this formulation. One of our targets for operation overlapping is to overlap the O projection matrix multiplication with the subsequent reduce-scatter. However, the tensor-parallel sharded O matrix is too small for us to effectively hide the reduce-scatter communication cost. To solve this, we instead choose to replicate the O projection matrix across each GPU and run the O projection with data parallelism instead of with tensor parallelism. Alongside this change, we eliminate the post-attention reduce-scatter and replace it with a \"distributed transpose\" operation after attention that repartitions our data from a tensor-parallel configuration to a data-parallel configuration. When using 8 GPUs, this reduces the network traffic by a factor of 8, which makes it much easier to hide the cost of communication cost by overlapping it with matrix multiplications. Note that the downside of this approach is it reduces the maximum batch size by about 15%, because replicating the O projection weights consumes an additional 9 GB of memory per GPU. Defining our Megakernel Instruction Set With our parallelism scheme decided on, we are able to construct the instruction set for our high-throughput megakernel.", "tokens": 325, "chunk_type": "overlap-1", "url": "https://hazyresearch.stanford.edu/blog/2025-09-28-tp-llama-main", "anchor": "#body:part-4", "type": "blog", "title": "", "section": "Body", "source": "blog:hazyresearch.stanford.edu", "published": "", "sha256": "a10b1f9797f169a4f85905861936f137ca67c6976a707ef8888ceb4d2ce351e4"}
{"doc_id": "blog:hazyresearch.stanford.edu", "chunk_id": "body:part-4-o2", "text": "When using 8 GPUs, this reduces the network traffic by a factor of 8, which makes it much easier to hide the cost of communication cost by overlapping it with matrix multiplications. Note that the downside of this approach is it reduces the maximum batch size by about 15%, because replicating the O projection weights consumes an additional 9 GB of memory per GPU. Defining our Megakernel Instruction Set With our parallelism scheme decided on, we are able to construct the instruction set for our high-throughput megakernel. We partition our workload into the following fused instructions: - RMS norm + all-gather. - A QKV matrix multiply + RoPE. - Attention + distributed transpose. - O-projection matrix multiply + residual. - Gate matrix multiply + SiLU. - Up-projection matrix multiply + elementwise multiplication with the output of the gate. - Down matrix multiply + reduce-scatter + residual. - RMS norm without the all-gather (for the LM head) - LM head matrix multiply. Relative to our latency-focused Llama-1B megakernel, this instruction set contains several high-level changes in our approach: - Most of our instructions for low-latency centered around matrix-vector multiplication, rather than the matrix-matrix multiplications we do here. The optimal work partitioning for these two operations is generally completely different. For matrix-vector products, each instruction computes several complete columns of the output vector. However, in matrix-matrix products, each instruction computes a tile of the output matrix instead. - To avoid extra trips to global memory, for our low-latency megakernel we frequently recomputed results across the GPU rather than communicating them through memory. This allowed us to fuse operations more aggressively than usual. For example, our QKV matrix-vector product fused the RMS norm into its beginning, saving an instruction boundary, but performing identical RMS computations over a hundred times.", "tokens": 384, "chunk_type": "overlap-2", "url": "https://hazyresearch.stanford.edu/blog/2025-09-28-tp-llama-main", "anchor": "#body:part-4", "type": "blog", "title": "", "section": "Body", "source": "blog:hazyresearch.stanford.edu", "published": "", "sha256": "a10b1f9797f169a4f85905861936f137ca67c6976a707ef8888ceb4d2ce351e4"}
{"doc_id": "blog:hazyresearch.stanford.edu", "chunk_id": "body:part-4-o3", "text": "However, in matrix-matrix products, each instruction computes a tile of the output matrix instead. - To avoid extra trips to global memory, for our low-latency megakernel we frequently recomputed results across the GPU rather than communicating them through memory. This allowed us to fuse operations more aggressively than usual. For example, our QKV matrix-vector product fused the RMS norm into its beginning, saving an instruction boundary, but performing identical RMS computations over a hundred times. When focusing on throughput, this recomputation is not worth it. - Putting many tokens in our batch requires us to expand our inter-instruction signalling scheme to track data dependencies across tokens. This signalling can vary by instruction -- for some operations (e.g. attention, RMS norms), sometimes synchronization is at the granularity of 128", "tokens": 166, "chunk_type": "overlap-3", "url": "https://hazyresearch.stanford.edu/blog/2025-09-28-tp-llama-main", "anchor": "#body:part-4", "type": "blog", "title": "", "section": "Body", "source": "blog:hazyresearch.stanford.edu", "published": "", "sha256": "a10b1f9797f169a4f85905861936f137ca67c6976a707ef8888ceb4d2ce351e4"}
{"doc_id": "blog:hazyresearch.stanford.edu", "chunk_id": "body:part-5", "text": "usual. For example, our QKV matrix-vector product fused the RMS norm into its beginning, saving an instruction boundary, but performing identical RMS computations over a hundred times. When focusing on throughput, this recomputation is not worth it. - Putting many tokens in our batch requires us to expand our inter-instruction signalling scheme to track data dependencies across tokens. This signalling can vary by instruction -- for some operations (e.g. attention, RMS norms), sometimes synchronization is at the granularity of 128 output rows of a matrix; other times it's at the granularity of an individual attention head for an individual token. Overlapping Resources Within a Megakernel Our primary goal when optimizing forward pass for throughput is to overlap hardware resources (e.g. use as much GPU memory bandwidth, compute units, and interconnect bandwidth as possible). Below, we show that our megakernel allows us to do this at three levels of hierarchy: within an SM by overlapping the stages of different instructions, across SMs by running different instructions, and across GPUs by overlapping communication with other work. Overlapping within the SM Within individual SMs, we make use of our megakernel template's instruction pipeline (previously described here) to overlap loading weights and activations for the next instruction with performing compute for the previous instruction. Even though our objective is now throughput, it's still useful to be able to start loading the next data in advance -- to keep matrix multiplies running as quickly as possible. Within the SM, our interpreter specializes threads to different functions: each of the load, compute, and store functions within the instruction template is executed by its own, independent set of threads. This means that even while a compute or store is running, the loads for the next matrix multiplies can start as soon as possible. To help understand this, we've written profiling tools that make it easier to see what's going on here. Figure 2, below, shows the brief transition between two different kinds of instructions on a single SM -- the Gate SiLU instruction (brown) and the Up matmul instruction (pink). If you'd like to build a better intuition for the profiler, you can access it here, alongside an example profile to download and play with. Figure 2: A zoomed-in snapshot of a single SM across about 15 microseconds, as it transitions from a Gate SiLU instruction to an Up matmul instruction. First, a quick tutorial on how to read what's going on in this zoomed-in profile snapshot: - The three horizontal tracks represent different kinds of threads within the interpreter that runs instructions. - At the top are the loader threads, which pull data from global memory into shared memory. - The thick band in the middle represents consumer threads that perform the main work -- in this case, running the tensor cores. - The bottom row tracks storer threads, which store results from shared memory back up to global memory. - Finally, in the background are controller threads that help coordinate the interpreter, although they're not programmed by the user. - Different colored bars represent different kinds of", "tokens": 665, "chunk_type": "original-large", "url": "https://hazyresearch.stanford.edu/blog/2025-09-28-tp-llama-main", "anchor": "#body:part-5", "type": "blog", "title": "", "section": "Body", "source": "blog:hazyresearch.stanford.edu", "published": "", "sha256": "ebce6ed8d9840b353cb6122ee61e39db3bd086285e87adb650fe482513f5ae09"}
{"doc_id": "blog:hazyresearch.stanford.edu", "chunk_id": "body:part-5-o1", "text": "usual. For example, our QKV matrix-vector product fused the RMS norm into its beginning, saving an instruction boundary, but performing identical RMS computations over a hundred times. When focusing on throughput, this recomputation is not worth it. - Putting many tokens in our batch requires us to expand our inter-instruction signalling scheme to track data dependencies across tokens. This signalling can vary by instruction -- for some operations (e.g. attention, RMS norms), sometimes synchronization is at the granularity of 128 output rows of a matrix; other times it's at the granularity of an individual attention head for an individual token. Overlapping Resources Within a Megakernel Our primary goal when optimizing forward pass for throughput is to overlap hardware resources (e.g. use as much GPU memory bandwidth, compute units, and interconnect bandwidth as possible). Below, we show that our megakernel allows us to do this at three levels of hierarchy: within an SM by overlapping the stages of different instructions, across SMs by running different instructions, and across GPUs by overlapping communication with other work. Overlapping within the SM Within individual SMs, we make use of our megakernel template's instruction pipeline (previously described here) to overlap loading weights and activations for the next instruction with performing compute for the previous instruction. Even though our objective is now throughput, it's still useful to be able to start loading the next data in advance -- to keep matrix multiplies running as quickly as possible. Within the SM, our interpreter specializes threads to different functions: each of the load, compute, and store functions within the instruction template is executed by its own, independent set of threads. This means that even while a compute or store is running, the loads for the next matrix multiplies can start as soon as possible.", "tokens": 384, "chunk_type": "overlap-1", "url": "https://hazyresearch.stanford.edu/blog/2025-09-28-tp-llama-main", "anchor": "#body:part-5", "type": "blog", "title": "", "section": "Body", "source": "blog:hazyresearch.stanford.edu", "published": "", "sha256": "ebce6ed8d9840b353cb6122ee61e39db3bd086285e87adb650fe482513f5ae09"}
{"doc_id": "blog:hazyresearch.stanford.edu", "chunk_id": "body:part-5-o2", "text": "Even though our objective is now throughput, it's still useful to be able to start loading the next data in advance -- to keep matrix multiplies running as quickly as possible. Within the SM, our interpreter specializes threads to different functions: each of the load, compute, and store functions within the instruction template is executed by its own, independent set of threads. This means that even while a compute or store is running, the loads for the next matrix multiplies can start as soon as possible. To help understand this, we've written profiling tools that make it easier to see what's going on here. Figure 2, below, shows the brief transition between two different kinds of instructions on a single SM -- the Gate SiLU instruction (brown) and the Up matmul instruction (pink). If you'd like to build a better intuition for the profiler, you can access it here, alongside an example profile to download and play with. Figure 2: A zoomed-in snapshot of a single SM across about 15 microseconds, as it transitions from a Gate SiLU instruction to an Up matmul instruction. First, a quick tutorial on how to read what's going on in this zoomed-in profile snapshot: - The three horizontal tracks represent different kinds of threads within the interpreter that runs instructions. - At the top are the loader threads, which pull data from global memory into shared memory. - The thick band in the middle represents consumer threads that perform the main work -- in this case, running the tensor cores. - The bottom row tracks storer threads, which store results from shared memory back up to global memory. - Finally, in the background are controller threads that help coordinate the interpreter, although they're not programmed by the user. - Different colored bars represent different kinds of", "tokens": 392, "chunk_type": "overlap-2", "url": "https://hazyresearch.stanford.edu/blog/2025-09-28-tp-llama-main", "anchor": "#body:part-5", "type": "blog", "title": "", "section": "Body", "source": "blog:hazyresearch.stanford.edu", "published": "", "sha256": "ebce6ed8d9840b353cb6122ee61e39db3bd086285e87adb650fe482513f5ae09"}
{"doc_id": "blog:hazyresearch.stanford.edu", "chunk_id": "body:part-6", "text": "loader threads, which pull data from global memory into shared memory. - The thick band in the middle represents consumer threads that perform the main work -- in this case, running the tensor cores. - The bottom row tracks storer threads, which store results from shared memory back up to global memory. - Finally, in the background are controller threads that help coordinate the interpreter, although they're not programmed by the user. - Different colored bars represent different kinds of instructions. In this case, the brown bars on the left correspond to the Gate SiLU instruction, and the pink bars on the right correspond to the Up matmul instruction. - Thin vertical lines represent different kinds of events. For example, blue and cyan lines correspond to different kinds of loads being issued. Purple lines represent the beginning of a compute phase, and yellow and orange lines correspond to different kinds of stores. Finally, red lines represent wait events, and green lines represent ready states. In general, we have instructions only report events from the first 8 and last 4 stages, since we have limited timing slots. - Tall vertical lines in the background represent events happening within the controller warp of the interpreter. The salmon line tells the SM to fetch the next instruction, the pale green line indicates that the next instruction is set up, and the white line indicates the last instruction has finished and can now be torn down. Here's a complete timeline of this little snapshot. | Time | Threads | Action | |---|---|---| | 4234.78 μs | Loader | Starts issuing loads for last four stages of matrix multiply pipeline (128 x 64 x 256). Dark blue lines = A matrix loads, cyan lines = B matrix loads (300ns later) | | + 0.51 μs | Loader | Signals controller to begin setting up next instruction (after fourth-to-last load) | | + 2.53 μs | Consumer | Begins running fourth-to-last matrix multiply (associated with first load) | | + 3.23 μs | Controller | Finishes setting up next Up matmul instruction, entailing fetching and decoding instruction, setting up semaphores, and remapping shared memory pages. | | + 3.68 μs | Loader | Begin running dependency check before loading inputs. | | + 5.25 μs | Consumer | Finish running matrix multiplies, begin storing results into two unreleased shared memory pages | | + 5.31 μs | Loader | Resolves dependency check, and issues loads for first 2.5 stages, before stalling due to lack of available shared memory pages. | | + 8.16 μs | Storer | Receives results, launches asynchronous store to global memory. Consumer threads start Up matmul matrix multiplies while shared memory still in use | | + 8.64 μs | Consumer | Begins running matrix multiplies on Up matmul instruction. | | + 9.09 μs | Storer | First store finishes reading from shared memory, releases one page. Loader threads restart load pipeline. | | + 10.18 μs | Storer | Final store finishes reading from shared memory, releases last page to Up matmul", "tokens": 665, "chunk_type": "original-large", "url": "https://hazyresearch.stanford.edu/blog/2025-09-28-tp-llama-main", "anchor": "#body:part-6", "type": "blog", "title": "", "section": "Body", "source": "blog:hazyresearch.stanford.edu", "published": "", "sha256": "705296739f5582d9fcc0be34bb6817d99fee77d7ce1285ae41b897a294df231c"}
{"doc_id": "blog:hazyresearch.stanford.edu", "chunk_id": "body:part-6-o1", "text": "loader threads, which pull data from global memory into shared memory. - The thick band in the middle represents consumer threads that perform the main work -- in this case, running the tensor cores. - The bottom row tracks storer threads, which store results from shared memory back up to global memory. - Finally, in the background are controller threads that help coordinate the interpreter, although they're not programmed by the user. - Different colored bars represent different kinds of instructions. In this case, the brown bars on the left correspond to the Gate SiLU instruction, and the pink bars on the right correspond to the Up matmul instruction. - Thin vertical lines represent different kinds of events. For example, blue and cyan lines correspond to different kinds of loads being issued. Purple lines represent the beginning of a compute phase, and yellow and orange lines correspond to different kinds of stores. Finally, red lines represent wait events, and green lines represent ready states. In general, we have instructions only report events from the first 8 and last 4 stages, since we have limited timing slots. - Tall vertical lines in the background represent events happening within the controller warp of the interpreter. The salmon line tells the SM to fetch the next instruction, the pale green line indicates that the next instruction is set up, and the white line indicates the last instruction has finished and can now be torn down. Here's a complete timeline of this little snapshot. | Time | Threads | Action | |---|---|---| | 4234.78 μs | Loader | Starts issuing loads for last four stages of matrix multiply pipeline (128 x 64 x 256).", "tokens": 364, "chunk_type": "overlap-1", "url": "https://hazyresearch.stanford.edu/blog/2025-09-28-tp-llama-main", "anchor": "#body:part-6", "type": "blog", "title": "", "section": "Body", "source": "blog:hazyresearch.stanford.edu", "published": "", "sha256": "705296739f5582d9fcc0be34bb6817d99fee77d7ce1285ae41b897a294df231c"}
{"doc_id": "blog:hazyresearch.stanford.edu", "chunk_id": "body:part-6-o2", "text": "The salmon line tells the SM to fetch the next instruction, the pale green line indicates that the next instruction is set up, and the white line indicates the last instruction has finished and can now be torn down. Here's a complete timeline of this little snapshot. | Time | Threads | Action | |---|---|---| | 4234.78 μs | Loader | Starts issuing loads for last four stages of matrix multiply pipeline (128 x 64 x 256). Dark blue lines = A matrix loads, cyan lines = B matrix loads (300ns later) | | + 0.51 μs | Loader | Signals controller to begin setting up next instruction (after fourth-to-last load) | | + 2.53 μs | Consumer | Begins running fourth-to-last matrix multiply (associated with first load) | | + 3.23 μs | Controller | Finishes setting up next Up matmul instruction, entailing fetching and decoding instruction, setting up semaphores, and remapping shared memory pages. | | + 3.68 μs | Loader | Begin running dependency check before loading inputs. | | + 5.25 μs | Consumer | Finish running matrix multiplies, begin storing results into two unreleased shared memory pages | | + 5.31 μs | Loader | Resolves dependency check, and issues loads for first 2.5 stages, before stalling due to lack of available shared memory pages. | | + 8.16 μs | Storer | Receives results, launches asynchronous store to global memory. Consumer threads start Up matmul matrix multiplies while shared memory still in use | | + 8.64 μs | Consumer | Begins running matrix multiplies on Up matmul instruction. | | + 9.09 μs | Storer | First store finishes reading from shared memory, releases one page. Loader threads restart load pipeline. | | + 10.18 μs | Storer | Final store finishes reading from shared memory, releases last page to Up matmul", "tokens": 401, "chunk_type": "overlap-2", "url": "https://hazyresearch.stanford.edu/blog/2025-09-28-tp-llama-main", "anchor": "#body:part-6", "type": "blog", "title": "", "section": "Body", "source": "blog:hazyresearch.stanford.edu", "published": "", "sha256": "705296739f5582d9fcc0be34bb6817d99fee77d7ce1285ae41b897a294df231c"}
{"doc_id": "blog:hazyresearch.stanford.edu", "chunk_id": "body:part-7", "text": "results, launches asynchronous store to global memory. Consumer threads start Up matmul matrix multiplies while shared memory still in use | | + 8.64 μs | Consumer | Begins running matrix multiplies on Up matmul instruction. | | + 9.09 μs | Storer | First store finishes reading from shared memory, releases one page. Loader threads restart load pipeline. | | + 10.18 μs | Storer | Final store finishes reading from shared memory, releases last page to Up matmul instruction. Up matrix multiply pipeline completely unblocked for the rest of the instruction. | | + 12.13 μs | Storer | Asynchronous stores to global memory complete. Atomically increments flag in global memory signifying instruction completion. | | + 12.38 μs | Controller | Notified all threads completed instruction work. Begins teardown, invalidating previous instruction semaphores, and writing timing data to global memory. | Now let's contrast this with a snapshot with the instruction pipeline disabled. Figure 3: The exact same profile, but with no inter-instruction pipelining. In this ablated profile, the store must finish, instruction be torn down, next instruction set up, and memory loaded, before matrix multiplies can begin again. Whereas with instruction pipelining enabled, the extra gap between consecutive matrix multiply stages is just 3.4 microseconds, without the pipeline, this gap jumps to 10.2 microseconds -- meaning this optimization alone reduces runtime by over 7% on these instructions. Nor is this effect isolated to the boundary of these two particular instructions; it shows up everywhere. In Figure 4, we take a look at some zoomed out profiles showing all 8 GPUs, and we'll use similar profiles for other ablations in the rest of this post, so it's worth understanding this profile well. Figure 4: Block profiles of pipelined versus serial instruction execution. Serial execution has a lot more gaps! What we're looking at here represents a little over two full transformer blocks of a Llama-70B forward pass with a batch size of 8,192, across all 8 GPUs. Unlike the zoomed-in view, where each SM separates into three separate bars, each horizontal bar here just represents the activity of the consumers for that SM. And, as before, each different color represents a different kind of instruction. From left to right: - Blue represents the Attention RMS norm and all-gather. - Orange represents the QKV matrix multiply. - Green represents the attention and inter-GPU transpose. - Red represents the O-projection matrix multiply. - Purple represents the MLP RMS norm and all-gather. - Brown represents the Gate SiLU. - Pink represents the Up matrix multiply. - Grey represents the Down projection matrix multiply. We also lightly shade the background to make it easier to distinguish SMs on different GPUs: if you look closely you should see light color bands to help make this visible. In the case of figure 4, the pipelined profile runs at 31,516 decoding TPS for this particular test workload (Prefill \"tell me a funny joke about cookies\", 30 decode tokens -- we have absolutely cornered the market on efficiently generating short cookie jokes), whereas the serial profile runs", "tokens": 665, "chunk_type": "original-large", "url": "https://hazyresearch.stanford.edu/blog/2025-09-28-tp-llama-main", "anchor": "#body:part-7", "type": "blog", "title": "", "section": "Body", "source": "blog:hazyresearch.stanford.edu", "published": "", "sha256": "ac397fd353dfbfe7774fa8c6ec86c27791d082973aec2416f557ef74f0a5d5d8"}
{"doc_id": "blog:hazyresearch.stanford.edu", "chunk_id": "body:part-7-o1", "text": "results, launches asynchronous store to global memory. Consumer threads start Up matmul matrix multiplies while shared memory still in use | | + 8.64 μs | Consumer | Begins running matrix multiplies on Up matmul instruction. | | + 9.09 μs | Storer | First store finishes reading from shared memory, releases one page. Loader threads restart load pipeline. | | + 10.18 μs | Storer | Final store finishes reading from shared memory, releases last page to Up matmul instruction. Up matrix multiply pipeline completely unblocked for the rest of the instruction. | | + 12.13 μs | Storer | Asynchronous stores to global memory complete. Atomically increments flag in global memory signifying instruction completion. | | + 12.38 μs | Controller | Notified all threads completed instruction work. Begins teardown, invalidating previous instruction semaphores, and writing timing data to global memory. | Now let's contrast this with a snapshot with the instruction pipeline disabled. Figure 3: The exact same profile, but with no inter-instruction pipelining. In this ablated profile, the store must finish, instruction be torn down, next instruction set up, and memory loaded, before matrix multiplies can begin again. Whereas with instruction pipelining enabled, the extra gap between consecutive matrix multiply stages is just 3.4 microseconds, without the pipeline, this gap jumps to 10.2 microseconds -- meaning this optimization alone reduces runtime by over 7% on these instructions. Nor is this effect isolated to the boundary of these two particular instructions; it shows up everywhere. In Figure 4, we take a look at some zoomed out profiles showing all 8 GPUs, and we'll use similar profiles for other ablations in the rest of this post, so it's worth understanding this profile well. Figure 4: Block profiles of pipelined versus serial instruction execution. Serial execution has a lot more gaps!", "tokens": 392, "chunk_type": "overlap-1", "url": "https://hazyresearch.stanford.edu/blog/2025-09-28-tp-llama-main", "anchor": "#body:part-7", "type": "blog", "title": "", "section": "Body", "source": "blog:hazyresearch.stanford.edu", "published": "", "sha256": "ac397fd353dfbfe7774fa8c6ec86c27791d082973aec2416f557ef74f0a5d5d8"}
{"doc_id": "blog:hazyresearch.stanford.edu", "chunk_id": "body:part-7-o2", "text": "Nor is this effect isolated to the boundary of these two particular instructions; it shows up everywhere. In Figure 4, we take a look at some zoomed out profiles showing all 8 GPUs, and we'll use similar profiles for other ablations in the rest of this post, so it's worth understanding this profile well. Figure 4: Block profiles of pipelined versus serial instruction execution. Serial execution has a lot more gaps! What we're looking at here represents a little over two full transformer blocks of a Llama-70B forward pass with a batch size of 8,192, across all 8 GPUs. Unlike the zoomed-in view, where each SM separates into three separate bars, each horizontal bar here just represents the activity of the consumers for that SM. And, as before, each different color represents a different kind of instruction. From left to right: - Blue represents the Attention RMS norm and all-gather. - Orange represents the QKV matrix multiply. - Green represents the attention and inter-GPU transpose. - Red represents the O-projection matrix multiply. - Purple represents the MLP RMS norm and all-gather. - Brown represents the Gate SiLU. - Pink represents the Up matrix multiply. - Grey represents the Down projection matrix multiply. We also lightly shade the background to make it easier to distinguish SMs on different GPUs: if you look closely you should see light color bands to help make this visible. In the case of figure 4, the pipelined profile runs at 31,516 decoding TPS for this particular test workload (Prefill \"tell me a funny joke about cookies\", 30 decode tokens -- we have absolutely cornered the market on efficiently generating short cookie jokes), whereas the serial profile runs", "tokens": 365, "chunk_type": "overlap-2", "url": "https://hazyresearch.stanford.edu/blog/2025-09-28-tp-llama-main", "anchor": "#body:part-7", "type": "blog", "title": "", "section": "Body", "source": "blog:hazyresearch.stanford.edu", "published": "", "sha256": "ac397fd353dfbfe7774fa8c6ec86c27791d082973aec2416f557ef74f0a5d5d8"}
{"doc_id": "blog:hazyresearch.stanford.edu", "chunk_id": "body:part-8", "text": "multiply. We also lightly shade the background to make it easier to distinguish SMs on different GPUs: if you look closely you should see light color bands to help make this visible. In the case of figure 4, the pipelined profile runs at 31,516 decoding TPS for this particular test workload (Prefill \"tell me a funny joke about cookies\", 30 decode tokens -- we have absolutely cornered the market on efficiently generating short cookie jokes), whereas the serial profile runs at just 29,607 TPS, corresponding to a difference of over 6%. This difference turns out to persist across different batch sizes, and generally provides around 2-6% end to end MFU, as shown in the table below. | Batch size | Best config | Best config minus pipelining | % Difference | |---|---|---|---| | 1024 | 18,676 | 18,201 | 2.5% | | 2048 | 26,388 | 24,641 | 6.6% | | 4096 | 29,214 | 28,426 | 2.7% | | 8192 | 31,516 | 29,607 | 6.1% | Table 1: Decoding TPS Generating Short Cookie Jokes at Various Batch Sizes, with and without Pipelining As a final note of this section, relating to the profiling itself, one might reasonably ask what overhead generating these plots incurs. It turns out to be very little: across 32 separate experiments we ran in the course of writing this post, we measured each runtime with and without generating timing data, each of which is a separate compilation path. We found the average difference to be just 0.39%, with a maximum of 1.07%. So, although timing may introduce a small amount of distortion, we think that this data is overall quite reliable. All TPS numbers are reported without timing recording enabled. Overlapping across SMs With our low-latency megakernel, each SM was assigned its own queue of instructions that are scheduled in advance. Instead, for our high-throughput megakernel, we create a global work queue -- a single list of instructions that defines all the work that needs to run on the GPU. When an SM needs to fetch a new instruction to run, it atomically increments a global instruction counter that keeps track of the next instruction to be assigned. This approach is automatically robust to jitter in the execution across different SMs; if one SM is slow to finish its instruction relative to others, it will simply delay its request for new work, allowing other SMs to pick up the slack. This solution wasn't possible for our low-latency megakernel, because the runtime of each instruction was so fast that the latency of this atomic increment would be prohibitive. But with a throughput-oriented megakernel -- where individual instructions frequently take 100 microseconds or more -- this cost can be entirely hidden as part of our instruction pipeline. Figure 5: Ablating the global work queue. On top, we use the global work queue. On the bottom, we use a simple round robin scheduler to assign work. The global work queue effectively smooths out variances in runtime that are present in the round-robin scheduler. In figure 5, we ablate the", "tokens": 665, "chunk_type": "original-large", "url": "https://hazyresearch.stanford.edu/blog/2025-09-28-tp-llama-main", "anchor": "#body:part-8", "type": "blog", "title": "", "section": "Body", "source": "blog:hazyresearch.stanford.edu", "published": "", "sha256": "f630e4e15cd9ac52b223a16b75de6631b5d8229967ceeb4fda1e5bc8428c3300"}
{"doc_id": "blog:hazyresearch.stanford.edu", "chunk_id": "body:part-8-o1", "text": "multiply. We also lightly shade the background to make it easier to distinguish SMs on different GPUs: if you look closely you should see light color bands to help make this visible. In the case of figure 4, the pipelined profile runs at 31,516 decoding TPS for this particular test workload (Prefill \"tell me a funny joke about cookies\", 30 decode tokens -- we have absolutely cornered the market on efficiently generating short cookie jokes), whereas the serial profile runs at just 29,607 TPS, corresponding to a difference of over 6%. This difference turns out to persist across different batch sizes, and generally provides around 2-6% end to end MFU, as shown in the table below. | Batch size | Best config | Best config minus pipelining | % Difference | |---|---|---|---| | 1024 | 18,676 | 18,201 | 2.5% | | 2048 | 26,388 | 24,641 | 6.6% | | 4096 | 29,214 | 28,426 | 2.7% | | 8192 | 31,516 | 29,607 | 6.1% | Table 1: Decoding TPS Generating Short Cookie Jokes at Various Batch Sizes, with and without Pipelining As a final note of this section, relating to the profiling itself, one might reasonably ask what overhead generating these plots incurs. It turns out to be very little: across 32 separate experiments we ran in the course of writing this post, we measured each runtime with and without generating timing data, each of which is a separate compilation path. We found the average difference to be just 0.39%, with a maximum of 1.07%. So, although timing may introduce a small amount of distortion, we think that this data is overall quite reliable. All TPS numbers are reported without timing recording enabled. Overlapping across SMs With our low-latency megakernel, each SM was assigned its own queue of instructions that are scheduled in advance.", "tokens": 399, "chunk_type": "overlap-1", "url": "https://hazyresearch.stanford.edu/blog/2025-09-28-tp-llama-main", "anchor": "#body:part-8", "type": "blog", "title": "", "section": "Body", "source": "blog:hazyresearch.stanford.edu", "published": "", "sha256": "f630e4e15cd9ac52b223a16b75de6631b5d8229967ceeb4fda1e5bc8428c3300"}
{"doc_id": "blog:hazyresearch.stanford.edu", "chunk_id": "body:part-8-o2", "text": "We found the average difference to be just 0.39%, with a maximum of 1.07%. So, although timing may introduce a small amount of distortion, we think that this data is overall quite reliable. All TPS numbers are reported without timing recording enabled. Overlapping across SMs With our low-latency megakernel, each SM was assigned its own queue of instructions that are scheduled in advance. Instead, for our high-throughput megakernel, we create a global work queue -- a single list of instructions that defines all the work that needs to run on the GPU. When an SM needs to fetch a new instruction to run, it atomically increments a global instruction counter that keeps track of the next instruction to be assigned. This approach is automatically robust to jitter in the execution across different SMs; if one SM is slow to finish its instruction relative to others, it will simply delay its request for new work, allowing other SMs to pick up the slack. This solution wasn't possible for our low-latency megakernel, because the runtime of each instruction was so fast that the latency of this atomic increment would be prohibitive. But with a throughput-oriented megakernel -- where individual instructions frequently take 100 microseconds or more -- this cost can be entirely hidden as part of our instruction pipeline. Figure 5: Ablating the global work queue. On top, we use the global work queue. On the bottom, we use a simple round robin scheduler to assign work. The global work queue effectively smooths out variances in runtime that are present in the round-robin scheduler. In figure 5, we ablate the", "tokens": 348, "chunk_type": "overlap-2", "url": "https://hazyresearch.stanford.edu/blog/2025-09-28-tp-llama-main", "anchor": "#body:part-8", "type": "blog", "title": "", "section": "Body", "source": "blog:hazyresearch.stanford.edu", "published": "", "sha256": "f630e4e15cd9ac52b223a16b75de6631b5d8229967ceeb4fda1e5bc8428c3300"}
{"doc_id": "blog:hazyresearch.stanford.edu", "chunk_id": "body:part-9", "text": "prohibitive. But with a throughput-oriented megakernel -- where individual instructions frequently take 100 microseconds or more -- this cost can be entirely hidden as part of our instruction pipeline. Figure 5: Ablating the global work queue. On top, we use the global work queue. On the bottom, we use a simple round robin scheduler to assign work. The global work queue effectively smooths out variances in runtime that are present in the round-robin scheduler. In figure 5, we ablate the global work queue by replacing it with a simple round-robin scheduler, and find a 14.2% end-to-end reduction in performance at a batch size of 8,192. A broader report is provided in table 2. | Batch size | Best config | Best config without GWQ | % Difference | |---|---|---|---| | 1024 | 18,320 | 18,676 | -1.9% | | 2048 | 26,388 | 25,518 | 3.3% | | 4096 | 29,214 | 27,372 | 6.3% | | 8192 | 31,516 | 27,033 | 14.2% | Table 2: Decoding TPS Generating Short Cookie Jokes at Various Batch Sizes, with and without the Global Work Queue As one can see from table 2, the global work queue becomes useful at large batch sizes, where there is enough that work that jitter across SM's becomes important, and eliminating that jitter with dynamic scheduling makes a big difference. At very small batch sizes, the overhead in the global work queue actually outweighs its benefit. Finally, it's worth noting that the global work queue is not the only way to improve scheduling over a naive round-robin scheduler; many other schedulers might work well. However, static schedulers cannot adapt to runtime jitter in the same way that the global work queue does; the variance in runtime across GPUs in figure 5 (despite them having nearly identical schedules) suggests that this jitter is a major factor. Overlapping across GPUs Networking Background In order to implement our tensor-parallel Llama, we need to be able to exchange data between GPUs. In general, NVIDIA gives us two ways to do this. One common approach is to use the GPU's copy engines -- dedicated hardware for copying big, contiguous chunks of memory within or across devices. One advantage of using the copy engines is that these copies don't need to run within a kernel, freeing up the GPU's SMs to do other useful work! By using multiple cuda streams, we can launch copy engine operations that overlap with kernel computations (e.g. as is done in PyTorch's AsyncTP). The other way to transfer data between GPUs is to do so within a kernel, using the GPU's SMs to write data into remote memory on other GPUs via CUDA's unified memory architecture (thanks Bill!) We've added a corresponding new abstraction to ThunderKittens called the Parallel Global Layout (PGL). With PGLs, we perform asynchronous loads and stores directly to global memory on other devices, overlapping them with compute and local memory operations to achieve near-zero cost. We also leverage NVSwitch's central accelerator to offload collective operations to hardware outside the GPU. Read more about PGLs and", "tokens": 665, "chunk_type": "original-large", "url": "https://hazyresearch.stanford.edu/blog/2025-09-28-tp-llama-main", "anchor": "#body:part-9", "type": "blog", "title": "", "section": "Body", "source": "blog:hazyresearch.stanford.edu", "published": "", "sha256": "fe4ce6c4036261b88a5875baacecd4e1efc77b0dff65110630f1a5c81e935015"}
{"doc_id": "blog:hazyresearch.stanford.edu", "chunk_id": "body:part-9-o1", "text": "prohibitive. But with a throughput-oriented megakernel -- where individual instructions frequently take 100 microseconds or more -- this cost can be entirely hidden as part of our instruction pipeline. Figure 5: Ablating the global work queue. On top, we use the global work queue. On the bottom, we use a simple round robin scheduler to assign work. The global work queue effectively smooths out variances in runtime that are present in the round-robin scheduler. In figure 5, we ablate the global work queue by replacing it with a simple round-robin scheduler, and find a 14.2% end-to-end reduction in performance at a batch size of 8,192. A broader report is provided in table 2. | Batch size | Best config | Best config without GWQ | % Difference | |---|---|---|---| | 1024 | 18,320 | 18,676 | -1.9% | | 2048 | 26,388 | 25,518 | 3.3% | | 4096 | 29,214 | 27,372 | 6.3% | | 8192 | 31,516 | 27,033 | 14.2% | Table 2: Decoding TPS Generating Short Cookie Jokes at Various Batch Sizes, with and without the Global Work Queue As one can see from table 2, the global work queue becomes useful at large batch sizes, where there is enough that work that jitter across SM's becomes important, and eliminating that jitter with dynamic scheduling makes a big difference. At very small batch sizes, the overhead in the global work queue actually outweighs its benefit. Finally, it's worth noting that the global work queue is not the only way to improve scheduling over a naive round-robin scheduler; many other schedulers might work well. However, static schedulers cannot adapt to runtime jitter in the same way that the global work queue does; the variance in runtime across GPUs in figure 5 (despite them having nearly identical schedules) suggests that this jitter is a major factor.", "tokens": 401, "chunk_type": "overlap-1", "url": "https://hazyresearch.stanford.edu/blog/2025-09-28-tp-llama-main", "anchor": "#body:part-9", "type": "blog", "title": "", "section": "Body", "source": "blog:hazyresearch.stanford.edu", "published": "", "sha256": "fe4ce6c4036261b88a5875baacecd4e1efc77b0dff65110630f1a5c81e935015"}
{"doc_id": "blog:hazyresearch.stanford.edu", "chunk_id": "body:part-9-o2", "text": "Finally, it's worth noting that the global work queue is not the only way to improve scheduling over a naive round-robin scheduler; many other schedulers might work well. However, static schedulers cannot adapt to runtime jitter in the same way that the global work queue does; the variance in runtime across GPUs in figure 5 (despite them having nearly identical schedules) suggests that this jitter is a major factor. Overlapping across GPUs Networking Background In order to implement our tensor-parallel Llama, we need to be able to exchange data between GPUs. In general, NVIDIA gives us two ways to do this. One common approach is to use the GPU's copy engines -- dedicated hardware for copying big, contiguous chunks of memory within or across devices. One advantage of using the copy engines is that these copies don't need to run within a kernel, freeing up the GPU's SMs to do other useful work! By using multiple cuda streams, we can launch copy engine operations that overlap with kernel computations (e.g. as is done in PyTorch's AsyncTP). The other way to transfer data between GPUs is to do so within a kernel, using the GPU's SMs to write data into remote memory on other GPUs via CUDA's unified memory architecture (thanks Bill!) We've added a corresponding new abstraction to ThunderKittens called the Parallel Global Layout (PGL). With PGLs, we perform asynchronous loads and stores directly to global memory on other devices, overlapping them with compute and local memory operations to achieve near-zero cost. We also leverage NVSwitch's central accelerator to offload collective operations to hardware outside the GPU. Read more about PGLs and", "tokens": 353, "chunk_type": "overlap-2", "url": "https://hazyresearch.stanford.edu/blog/2025-09-28-tp-llama-main", "anchor": "#body:part-9", "type": "blog", "title": "", "section": "Body", "source": "blog:hazyresearch.stanford.edu", "published": "", "sha256": "fe4ce6c4036261b88a5875baacecd4e1efc77b0dff65110630f1a5c81e935015"}
{"doc_id": "blog:hazyresearch.stanford.edu", "chunk_id": "body:part-10", "text": "the GPU's SMs to write data into remote memory on other GPUs via CUDA's unified memory architecture (thanks Bill!) We've added a corresponding new abstraction to ThunderKittens called the Parallel Global Layout (PGL). With PGLs, we perform asynchronous loads and stores directly to global memory on other devices, overlapping them with compute and local memory operations to achieve near-zero cost. We also leverage NVSwitch's central accelerator to offload collective operations to hardware outside the GPU. Read more about PGLs and our multi-GPU approach in our earlier blog post. In our megakernel we use the second approach, because it gives us the control we need to perform all-gathers, reduce-scatters, and our post-attention distributed transpose (which allows us to do the O-projection in data-parallel form). We perform all communication from our dedicated storer threads, allowing loader and compute threads to move onto future work while inter-GPU communication is performed in the background on the same SM. Interleaving Warp specialization, instruction pipelining, and the global work queue now give us a way to overlap different hardware resources within an SM. However, we can further benefit from GPU resource overlapping if we can assign different types of instructions to different SMs. For example, with a large batch size, tokens at the beginning of the batch will have their compute-bound Down projections completed earlier than tokens at the end of the batch. This makes these early tokens ready to start the next instruction, which is the network-bound pre-attention RMS norm and all-gather. If we run that RMS norm and all-gather for these early tokens on some SMs, while computing the Down projection for the later tokens on different SMs, we can reduce peak network bandwidth and better exploit the hardware resources available on the device. Some prior work, like NanoFlow, implemented this technique on A100s by constructing a schedule ahead-of-time that assigns SMs to different groups, namely compute-focused SMs that compute matmul-heavy ops, memory-focused instructions that compute attention decoding, and comms-focused SMs that communicate results across devices. With our megakernel, we can perform this overlapping at a much finer granularity by interleaving instructions from different ops into our global work queue. Once we have scheduled enough Down projection instructions for some tokens to be ready for attention, we can start interleaving RMS norm instructions while we add the remaining Down projection instructions into our schedule. This interleaving lets different SMs run different kinds of work without needing to explicitly assign SMs to groups like in NanoFlow. Figure 6: Ablating interleaving. On top, we use our standard interleaved schedule. On the bottom, we disable interleaving. Notice how the two RMS norms (purple and blue) are no longer quilted the same way, but instead take up 200-300 microseconds each time they come up. (Note that although QKV and Gate SiLU instructions may begin early, they rely on many RMS norms to satisfy their dependencies, and usually start after all of the RMS norm instructions are finished.) The effect of the interleaving is easy to see in our profiles, since it creates \"quilts\" (i.e. dense multi-colored regions where different SMs", "tokens": 665, "chunk_type": "original-large", "url": "https://hazyresearch.stanford.edu/blog/2025-09-28-tp-llama-main", "anchor": "#body:part-10", "type": "blog", "title": "", "section": "Body", "source": "blog:hazyresearch.stanford.edu", "published": "", "sha256": "3252547c7a8cbe43a8dd436e2156bdf669ffc0bc152efc034f3648ce9c21f039"}
{"doc_id": "blog:hazyresearch.stanford.edu", "chunk_id": "body:part-10-o1", "text": "the GPU's SMs to write data into remote memory on other GPUs via CUDA's unified memory architecture (thanks Bill!) We've added a corresponding new abstraction to ThunderKittens called the Parallel Global Layout (PGL). With PGLs, we perform asynchronous loads and stores directly to global memory on other devices, overlapping them with compute and local memory operations to achieve near-zero cost. We also leverage NVSwitch's central accelerator to offload collective operations to hardware outside the GPU. Read more about PGLs and our multi-GPU approach in our earlier blog post. In our megakernel we use the second approach, because it gives us the control we need to perform all-gathers, reduce-scatters, and our post-attention distributed transpose (which allows us to do the O-projection in data-parallel form). We perform all communication from our dedicated storer threads, allowing loader and compute threads to move onto future work while inter-GPU communication is performed in the background on the same SM. Interleaving Warp specialization, instruction pipelining, and the global work queue now give us a way to overlap different hardware resources within an SM. However, we can further benefit from GPU resource overlapping if we can assign different types of instructions to different SMs. For example, with a large batch size, tokens at the beginning of the batch will have their compute-bound Down projections completed earlier than tokens at the end of the batch. This makes these early tokens ready to start the next instruction, which is the network-bound pre-attention RMS norm and all-gather. If we run that RMS norm and all-gather for these early tokens on some SMs, while computing the Down projection for the later tokens on different SMs, we can reduce peak network bandwidth and better exploit the hardware resources available on the device.", "tokens": 377, "chunk_type": "overlap-1", "url": "https://hazyresearch.stanford.edu/blog/2025-09-28-tp-llama-main", "anchor": "#body:part-10", "type": "blog", "title": "", "section": "Body", "source": "blog:hazyresearch.stanford.edu", "published": "", "sha256": "3252547c7a8cbe43a8dd436e2156bdf669ffc0bc152efc034f3648ce9c21f039"}
{"doc_id": "blog:hazyresearch.stanford.edu", "chunk_id": "body:part-10-o2", "text": "This makes these early tokens ready to start the next instruction, which is the network-bound pre-attention RMS norm and all-gather. If we run that RMS norm and all-gather for these early tokens on some SMs, while computing the Down projection for the later tokens on different SMs, we can reduce peak network bandwidth and better exploit the hardware resources available on the device. Some prior work, like NanoFlow, implemented this technique on A100s by constructing a schedule ahead-of-time that assigns SMs to different groups, namely compute-focused SMs that compute matmul-heavy ops, memory-focused instructions that compute attention decoding, and comms-focused SMs that communicate results across devices. With our megakernel, we can perform this overlapping at a much finer granularity by interleaving instructions from different ops into our global work queue. Once we have scheduled enough Down projection instructions for some tokens to be ready for attention, we can start interleaving RMS norm instructions while we add the remaining Down projection instructions into our schedule. This interleaving lets different SMs run different kinds of work without needing to explicitly assign SMs to groups like in NanoFlow. Figure 6: Ablating interleaving. On top, we use our standard interleaved schedule. On the bottom, we disable interleaving. Notice how the two RMS norms (purple and blue) are no longer quilted the same way, but instead take up 200-300 microseconds each time they come up. (Note that although QKV and Gate SiLU instructions may begin early, they rely on many RMS norms to satisfy their dependencies, and usually start after all of the RMS norm instructions are finished.) The effect of the interleaving is easy to see in our profiles, since it creates \"quilts\" (i.e. dense multi-colored regions where different SMs", "tokens": 370, "chunk_type": "overlap-2", "url": "https://hazyresearch.stanford.edu/blog/2025-09-28-tp-llama-main", "anchor": "#body:part-10", "type": "blog", "title": "", "section": "Body", "source": "blog:hazyresearch.stanford.edu", "published": "", "sha256": "3252547c7a8cbe43a8dd436e2156bdf669ffc0bc152efc034f3648ce9c21f039"}
{"doc_id": "blog:hazyresearch.stanford.edu", "chunk_id": "body:part-11", "text": "two RMS norms (purple and blue) are no longer quilted the same way, but instead take up 200-300 microseconds each time they come up. (Note that although QKV and Gate SiLU instructions may begin early, they rely on many RMS norms to satisfy their dependencies, and usually start after all of the RMS norm instructions are finished.) The effect of the interleaving is easy to see in our profiles, since it creates \"quilts\" (i.e. dense multi-colored regions where different SMs are running different kinds of instructions at a given point in time). | Batch size | Best config | Best config minus interleaving | % Difference | |---|---|---|---| | 1024 | 18,676 | 18,663 | 0.1% | | 2048 | 26,388 | 26,388 | 0.0% | | 4096 | 29,214 | 28,520 | 2.4% | | 8192 | 31,516 | 29,492 | 6.4% | Table 3: Decoding TPS Generating Short Cookie Jokes at Various Batch Sizes, with and without Interleaving As one can see from table 3, interleaving also kicks in at large batch sizes (like the global work queue), where there is enough work on the GPU to allow several waves of instructions of each type, and therefore the effective interleaving of those waves. At these large batch sizes, it's just as important of an optimization as intra-SM pipelining. The key advantage of our approach is that it eliminates the overhead of additional kernel launches and cross-stream synchronization. We also note that we've removed our reliance on NCCL entirely, continuing in our march towards Obadiah Stane's paragon of minimal dependencies. Of course, there's been an astounding amount of work attempting to hide communication overhead alongside compute -- it was surprising to us how straightforward it was to overlap communication within the megakernel framework! REEEEE(sults) To evaluate our megakernel, we integrated it into Tokasaurus, which helps schedule batches of prefill and decode, alongside KV pages. This also allows us to schedule megakernel instructions on the CPU while the previous batch is running on the GPUs; with 64 threads generating instructions in C++, we generally find >90% CPU idle time. For our benchmark, we sampled a set of 65,536 prompts + completion lengths from the ShareGPT dataset, and ran them through both SGLang and our Megakernel. We reproduced SGLang using their recommended benchmarking settings; nonetheless, we recognize that expert tuning is sensitive and important. We report the input, output, and total throughputs in Table 4 (a more precise restatement of the results from Figure 1). | System | Input Throughput (Tokens/s) | Output Throughput (Tokens/s) | Total Throughput (Tokens/s) | |---|---|---|---| | SGLang | 11,783 | 7,387 | 19,170 | | Megakernel | 14,425 | 9,043 | 23,468 | Even despite these promising initial results, we suspect there's still considerable room for optimization within megakernels. Our scheduling heuristics are quite simple, our instructions frequently stall on synchronizations that could likely be hidden better, and our megakernel still has register spills and other low-level problems. All of these point towards this being an exciting direction for future work! Conclusion: Megakernels are Cool In this", "tokens": 665, "chunk_type": "original-large", "url": "https://hazyresearch.stanford.edu/blog/2025-09-28-tp-llama-main", "anchor": "#body:part-11", "type": "blog", "title": "", "section": "Body", "source": "blog:hazyresearch.stanford.edu", "published": "", "sha256": "c07569c90b2230192a1a02b9cd70eb08c0820b3377a3c96939f889002972e241"}
{"doc_id": "blog:hazyresearch.stanford.edu", "chunk_id": "body:part-11-o1", "text": "two RMS norms (purple and blue) are no longer quilted the same way, but instead take up 200-300 microseconds each time they come up. (Note that although QKV and Gate SiLU instructions may begin early, they rely on many RMS norms to satisfy their dependencies, and usually start after all of the RMS norm instructions are finished.) The effect of the interleaving is easy to see in our profiles, since it creates \"quilts\" (i.e. dense multi-colored regions where different SMs are running different kinds of instructions at a given point in time). | Batch size | Best config | Best config minus interleaving | % Difference | |---|---|---|---| | 1024 | 18,676 | 18,663 | 0.1% | | 2048 | 26,388 | 26,388 | 0.0% | | 4096 | 29,214 | 28,520 | 2.4% | | 8192 | 31,516 | 29,492 | 6.4% | Table 3: Decoding TPS Generating Short Cookie Jokes at Various Batch Sizes, with and without Interleaving As one can see from table 3, interleaving also kicks in at large batch sizes (like the global work queue), where there is enough work on the GPU to allow several waves of instructions of each type, and therefore the effective interleaving of those waves. At these large batch sizes, it's just as important of an optimization as intra-SM pipelining. The key advantage of our approach is that it eliminates the overhead of additional kernel launches and cross-stream synchronization. We also note that we've removed our reliance on NCCL entirely, continuing in our march towards Obadiah Stane's paragon of minimal dependencies. Of course, there's been an astounding amount of work attempting to hide communication overhead alongside compute -- it was surprising to us how straightforward it was to overlap communication within the megakernel framework!", "tokens": 382, "chunk_type": "overlap-1", "url": "https://hazyresearch.stanford.edu/blog/2025-09-28-tp-llama-main", "anchor": "#body:part-11", "type": "blog", "title": "", "section": "Body", "source": "blog:hazyresearch.stanford.edu", "published": "", "sha256": "c07569c90b2230192a1a02b9cd70eb08c0820b3377a3c96939f889002972e241"}
{"doc_id": "blog:hazyresearch.stanford.edu", "chunk_id": "body:part-11-o2", "text": "The key advantage of our approach is that it eliminates the overhead of additional kernel launches and cross-stream synchronization. We also note that we've removed our reliance on NCCL entirely, continuing in our march towards Obadiah Stane's paragon of minimal dependencies. Of course, there's been an astounding amount of work attempting to hide communication overhead alongside compute -- it was surprising to us how straightforward it was to overlap communication within the megakernel framework! REEEEE(sults) To evaluate our megakernel, we integrated it into Tokasaurus, which helps schedule batches of prefill and decode, alongside KV pages. This also allows us to schedule megakernel instructions on the CPU while the previous batch is running on the GPUs; with 64 threads generating instructions in C++, we generally find >90% CPU idle time. For our benchmark, we sampled a set of 65,536 prompts + completion lengths from the ShareGPT dataset, and ran them through both SGLang and our Megakernel. We reproduced SGLang using their recommended benchmarking settings; nonetheless, we recognize that expert tuning is sensitive and important. We report the input, output, and total throughputs in Table 4 (a more precise restatement of the results from Figure 1). | System | Input Throughput (Tokens/s) | Output Throughput (Tokens/s) | Total Throughput (Tokens/s) | |---|---|---|---| | SGLang | 11,783 | 7,387 | 19,170 | | Megakernel | 14,425 | 9,043 | 23,468 | Even despite these promising initial results, we suspect there's still considerable room for optimization within megakernels. Our scheduling heuristics are quite simple, our instructions frequently stall on synchronizations that could likely be hidden better, and our megakernel still has register spills and other low-level problems. All of these point towards this being an exciting direction for future work! Conclusion: Megakernels are Cool In this", "tokens": 379, "chunk_type": "overlap-2", "url": "https://hazyresearch.stanford.edu/blog/2025-09-28-tp-llama-main", "anchor": "#body:part-11", "type": "blog", "title": "", "section": "Body", "source": "blog:hazyresearch.stanford.edu", "published": "", "sha256": "c07569c90b2230192a1a02b9cd70eb08c0820b3377a3c96939f889002972e241"}
{"doc_id": "blog:hazyresearch.stanford.edu", "chunk_id": "body:part-12", "text": "| 11,783 | 7,387 | 19,170 | | Megakernel | 14,425 | 9,043 | 23,468 | Even despite these promising initial results, we suspect there's still considerable room for optimization within megakernels. Our scheduling heuristics are quite simple, our instructions frequently stall on synchronizations that could likely be hidden better, and our megakernel still has register spills and other low-level problems. All of these point towards this being an exciting direction for future work! Conclusion: Megakernels are Cool In this post, we introduced a tensor-parallel Llama-70B megakernel focused on maximizing decoding throughput. We designed a custom instruction set for this megakernel within our megakernel interpreter framework, and ablated several key scheduling decisions including pipelining across instruction boundaries, choosing processors for each instruction, and interleaving communication with compute. Finally, we integrated our megakernel into Tokasaurus, and found it outperformed SGLang by 22% on ShareGPT prompts. A direction for future work: a key challenge of writing megakernels is that there is tremendous complexity in both designing these custom instruction sets, and coordinating (and especially debugging) synchronization patterns across GPUs. A corresponding learning from this work is that, going forward, we'd like to design a more general megakernel instruction set and abstract these decisions into the host-side scheduler, in order to simplify the process of designing high-performance megakernels. We think this might make megakernels for training viable, alongside megakernels for inference. *Work by Jordan done while at Stanford.", "tokens": 305, "chunk_type": "original", "url": "https://hazyresearch.stanford.edu/blog/2025-09-28-tp-llama-main", "anchor": "#body:part-12", "type": "blog", "title": "", "section": "Body", "source": "blog:hazyresearch.stanford.edu", "published": "", "sha256": "504893c08c7c38a337733f291c45a376262dccdaf2183fd18db0278c826f75db"}
{"doc_id": "blog:www.usenix.org", "chunk_id": "body", "text": "--- title: {ARK}: {GPU-driven} Code Execution for Distributed Deep Learning author: Changho Hwang; KyoungSoo Park; Ran Shu; Xinyuan Qu; Peng Cheng; Yongqiang Xiong url: https://www.usenix.org/conference/nsdi23/presentation/hwang hostname: usenix.org sitename: usenix.org date: 2023-01-01 --- Changho Hwang, KAIST, Microsoft Research; KyoungSoo Park, KAIST; Ran Shu, Xinyuan Qu, Peng Cheng, and Yongqiang Xiong, Microsoft Research Modern state-of-the-art deep learning (DL) applications tend to scale out to a large number of parallel GPUs. Unfortunately, we observe that the collective communication overhead across GPUs is often the key limiting factor of performance for distributed DL. It under-utilizes the networking bandwidth by frequent transfers of small data chunks, which also incurs a substantial I/O overhead on GPU that interferes with computation on GPU. The root cause lies in the inefficiency of CPU-based communication event handling as well as the inability to control the GPU's internal DMA engine with GPU threads. To address the problem, we propose a GPU-driven code execution system that leverages a GPU-controlled hardware DMA engine for I/O offloading. Our custom DMA engine pipelines multiple DMA requests to support efficient small data transfer while it eliminates the I/O overhead on GPU cores. Unlike existing GPU DMA engines initiated only by CPU, we let GPU threads to directly control DMA operations, which leads to a highly efficient system where GPUs drive their own execution flow and handle communication events autonomously without CPU intervention. Our prototype DMA engine achieves a line-rate from a message size as small as 8KB (3.9x better throughput) with only 4.3us of communication latency (9.1x faster) while it incurs little interference with computation on GPU, achieving 1.8x higher all-reduce throughput in a real training workload. NSDI '23 Open Access Sponsored by King Abdullah University of Science and Technology (KAUST) Open Access Media USENIX is committed to Open Access to the research presented at our events. Papers and proceedings are freely available to everyone once the event begins. Any video, audio, and/or slides that are posted after the event are also free and open to everyone. Support USENIX and our commitment to Open Access. This content is available to: author = {Changho Hwang and KyoungSoo Park and Ran Shu and Xinyuan Qu and Peng Cheng and Yongqiang Xiong}, title = {{ARK}: {GPU-driven} Code Execution for Distributed Deep Learning}, booktitle = {20th USENIX Symposium on Networked Systems Design and Implementation (NSDI 23)}, year = {2023}, isbn = {978-1-939133-33-5}, address = {Boston, MA}, pages = {87--101}, url = {https://www.usenix.org/conference/nsdi23/presentation/hwang}, publisher = {USENIX Association}, month = apr }", "tokens": 534, "chunk_type": "original", "url": "https://www.usenix.org/conference/nsdi23/presentation/hwang", "anchor": "#body", "type": "blog", "title": "", "section": "Body", "source": "blog:www.usenix.org", "published": "", "sha256": "a3c760e06f1f56d284830c34b6e5a82c240ea18b78ab8b71f3bc21a5846a63de"}
{"doc_id": "arxiv:2503.01840", "chunk_id": "abstract", "text": "The sequential nature of modern LLMs makes them expensive and slow, and speculative sam- pling has proven to be an effective solution to this problem. Methods like EAGLE perform autoregression at the feature level, reusing top- layer features from the target model to achieve better results than vanilla speculative sampling. A growing trend in the LLM community is scaling up training data to improve model in- telligence without increasing inference costs. However, we observe that scaling up data pro- vides limited improvements for EAGLE. We identify that this limitation arises from EA- GLE’s feature prediction constraints. In this paper, we introduce EAGLE-3, which aban- dons feature prediction in favor of direct token prediction and replaces reliance on top-layer features with multi-layer feature fusion via a technique named training-time test. These im- provements significantly enhance performance and enable the draft model to fully benefit from scaling up training data. Our experiments in- clude both chat models and reasoning mod- els, evaluated on five tasks. The results show that EAGLE-3 achieves a speedup ratio up to 6.5x, with about 1.4x improvement over EAGLE-2. In the SGLang framework, EAGLE- 3 achieves a 1.38x throughput improvement at a batch size of 64. The code is available at https://github.com/SafeAILab/EAGLE. 1", "tokens": 266, "chunk_type": "original", "url": "https://arxiv.org/abs/2503.01840", "anchor": "#abstract", "type": "paper", "title": "", "section": "Abstract", "source": "arxiv_pdf", "published": "", "sha256": "5e36dcbba95d702d3487322b07dc27e96194651fd5c0bcdb87627bec857753f3"}
{"doc_id": "arxiv:2503.01840", "chunk_id": "introduction:part-1", "text": "Modern Large Language Models (LLMs) are being applied to more domains, with their improved capa- bilities driven by scaling model parameters—some LLMs now exceed hundreds of billions of param- eters. In autoregressive generation, each token requires accessing all model parameters, making LLM inference slow and costly. Recently, test-time scaling up has gained sig- nificant attention. Models like ChatGPT o1 and 1 2 4 8 3.2 3.4 3.6 3.8 4.0 4.2 4.4 Speedup EAGLE-2 EAGLE-3 1 2 4 8 4.0 4.5 5.0 5.5 6.0 Accept length EAGLE-2 EAGLE-3 Figure 1: Scaling law evaluated on the MT-bench using LLaMA-Instruct 3.1 8B as the target model, with the x-axis representing the data scale relative to ShareGPT. The new architectural designs in EAGLE-3 enable an increasing scaling curve, which was never observed in the previous works. DeepSeek-R1 (Guo et al., 2025) engage in delib- erate reasoning before responding, pushing the boundaries of LLM capabilities at the cost of longer inference time. However, these models often re- quire lengthy reasoning processes, making them extremely costly, while the increased response time severely impacts user satisfaction. These reasoning models significantly increase the proportion of in- ference costs in the overall LLM pipeline, driving researchers to explore cheaper and faster inference optimization methods. Speculative sampling methods can reduce LLM latency by partially parallelizing the generation pro- cess. These methods rapidly generate draft tokens and then verify them in parallel. This allows multi- ple tokens to be produced in a single forward pass, significantly reducing inference latency. In vanilla speculative sampling, the draft model is a separate, smaller LLM, typically a lower-parameter version from the same series as the target model. This arXiv:2503.01840v3 [cs.CL] 23 Apr 2025 Vicuna 13B LLaMA-Instruct 3.1 8B LLaMA-Instruct 3.3 70B DeepSeek R1 LLaMA 8B 0 1 2 3 4 5 Speedup 1.0x 1.9x 2.1x 3.1x 4.1x 5.6x 1.0x 3.6x 3.2x 4.4x 1.0x 2.8x 4.1x 1.0x 3.4x 5.0x Vanilla Speculative sampling Medusa HASS EAGLE EAGLE-2 EAGLE-3 Figure 2: Speedup ratios of different methods at temperature=0. For the standard speculative sampling, Vicuna-13B uses Vicuna-68M as the draft model. In Table 1, we present comparisons with additional methods, but this figure only showcases a subset. Chat model’s evaluation dataset is MT-bench, and the reasoning model’s evaluation dataset is GSM8K. DeepSeek R1 LLaMA 8B refers to DeepSeek-R1-Distill-LLaMA 8B. draft model operates independently of the target model. Unlike the vanilla speculative sampling, EAGLE (Li et al., 2024c) reuses the top-layer fea- tures of the target model (the features before the LM head). It trains the draft model to autoregres- sively predict the next feature and then uses the target model’s LM head to obtain the draft token. By leveraging the rich information from the target model, EAGLE achieves significantly better accel- eration compared to vanilla speculative sampling. Subsequent methods such as HASS (Zhang et al., 2024) and Falcon (Gao et al., 2024) also adopt the approach of predicting the next feature using the current feature sequence. Recent LLMs have increasingly relied on larger training datasets to achieve better performance. For example, LLaMA series models with sizes of 7B (8B) have used", "tokens": 665, "chunk_type": "original-large", "url": "https://arxiv.org/abs/2503.01840", "anchor": "#introduction:part-1", "type": "paper", "title": "", "section": "Introduction", "source": "arxiv_pdf", "published": "", "sha256": "e0a2ca243aabb579911a6320c407fd7f7df9fc3c5f2cc8e648a3e8b4e89ebc1d"}
{"doc_id": "arxiv:2503.01840", "chunk_id": "introduction:part-1-o1", "text": "Modern Large Language Models (LLMs) are being applied to more domains, with their improved capa- bilities driven by scaling model parameters—some LLMs now exceed hundreds of billions of param- eters. In autoregressive generation, each token requires accessing all model parameters, making LLM inference slow and costly. Recently, test-time scaling up has gained sig- nificant attention. Models like ChatGPT o1 and 1 2 4 8 3.2 3.4 3.6 3.8 4.0 4.2 4.4 Speedup EAGLE-2 EAGLE-3 1 2 4 8 4.0 4.5 5.0 5.5 6.0 Accept length EAGLE-2 EAGLE-3 Figure 1: Scaling law evaluated on the MT-bench using LLaMA-Instruct 3.1 8B as the target model, with the x-axis representing the data scale relative to ShareGPT. The new architectural designs in EAGLE-3 enable an increasing scaling curve, which was never observed in the previous works. DeepSeek-R1 (Guo et al., 2025) engage in delib- erate reasoning before responding, pushing the boundaries of LLM capabilities at the cost of longer inference time. However, these models often re- quire lengthy reasoning processes, making them extremely costly, while the increased response time severely impacts user satisfaction. These reasoning models significantly increase the proportion of in- ference costs in the overall LLM pipeline, driving researchers to explore cheaper and faster inference optimization methods. Speculative sampling methods can reduce LLM latency by partially parallelizing the generation pro- cess. These methods rapidly generate draft tokens and then verify them in parallel. This allows multi- ple tokens to be produced in a single forward pass, significantly reducing inference latency. In vanilla speculative sampling, the draft model is a separate, smaller LLM, typically a lower-parameter version from the same series as the target model.", "tokens": 353, "chunk_type": "overlap-1", "url": "https://arxiv.org/abs/2503.01840", "anchor": "#introduction:part-1", "type": "paper", "title": "", "section": "Introduction", "source": "arxiv_pdf", "published": "", "sha256": "e0a2ca243aabb579911a6320c407fd7f7df9fc3c5f2cc8e648a3e8b4e89ebc1d"}
{"doc_id": "arxiv:2503.01840", "chunk_id": "introduction:part-1-o2", "text": "Speculative sampling methods can reduce LLM latency by partially parallelizing the generation pro- cess. These methods rapidly generate draft tokens and then verify them in parallel. This allows multi- ple tokens to be produced in a single forward pass, significantly reducing inference latency. In vanilla speculative sampling, the draft model is a separate, smaller LLM, typically a lower-parameter version from the same series as the target model. This arXiv:2503.01840v3 [cs.CL] 23 Apr 2025 Vicuna 13B LLaMA-Instruct 3.1 8B LLaMA-Instruct 3.3 70B DeepSeek R1 LLaMA 8B 0 1 2 3 4 5 Speedup 1.0x 1.9x 2.1x 3.1x 4.1x 5.6x 1.0x 3.6x 3.2x 4.4x 1.0x 2.8x 4.1x 1.0x 3.4x 5.0x Vanilla Speculative sampling Medusa HASS EAGLE EAGLE-2 EAGLE-3 Figure 2: Speedup ratios of different methods at temperature=0. For the standard speculative sampling, Vicuna-13B uses Vicuna-68M as the draft model. In Table 1, we present comparisons with additional methods, but this figure only showcases a subset. Chat model’s evaluation dataset is MT-bench, and the reasoning model’s evaluation dataset is GSM8K. DeepSeek R1 LLaMA 8B refers to DeepSeek-R1-Distill-LLaMA 8B. draft model operates independently of the target model. Unlike the vanilla speculative sampling, EAGLE (Li et al., 2024c) reuses the top-layer fea- tures of the target model (the features before the LM head). It trains the draft model to autoregres- sively predict the next feature and then uses the target model’s LM head to obtain the draft token. By leveraging the rich information from the target model, EAGLE achieves significantly better accel- eration compared to vanilla speculative sampling. Subsequent methods such as HASS (Zhang et al., 2024) and Falcon (Gao et al., 2024) also adopt the approach of predicting the next feature using the current feature sequence. Recent LLMs have increasingly relied on larger training datasets to achieve better performance. For example, LLaMA series models with sizes of 7B (8B) have used", "tokens": 399, "chunk_type": "overlap-2", "url": "https://arxiv.org/abs/2503.01840", "anchor": "#introduction:part-1", "type": "paper", "title": "", "section": "Introduction", "source": "arxiv_pdf", "published": "", "sha256": "e0a2ca243aabb579911a6320c407fd7f7df9fc3c5f2cc8e648a3e8b4e89ebc1d"}
{"doc_id": "arxiv:2503.01840", "chunk_id": "introduction:part-2", "text": "head to obtain the draft token. By leveraging the rich information from the target model, EAGLE achieves significantly better accel- eration compared to vanilla speculative sampling. Subsequent methods such as HASS (Zhang et al., 2024) and Falcon (Gao et al., 2024) also adopt the approach of predicting the next feature using the current feature sequence. Recent LLMs have increasingly relied on larger training datasets to achieve better performance. For example, LLaMA series models with sizes of 7B (8B) have used 1T, 2T, and 15T tokens of training data for LLaMA 1 (Touvron et al., 2023a), LLaMA 2 (Touvron et al., 2023b), and LLaMA 3 (Dubey et al., 2024), respectively, resulting in significant improvements across various metrics while keeping the model architecture and inference cost largely unchanged. Similarly, we aim to improve the ac- ceptance rate and acceleration ratio of EAGLE by increasing its training data. Unfortunately, we ob- serve that the gains from additional training data for EAGLE are limited. We analyze the reasons behind this phenomenon. As shown in the upper part of Figure 3, EAGLE performs autoregressive prediction at the feature level, predicting the next feature and then feeding the feature into the LM head of the target model to obtain the token dis- tribution. EAGLE’s loss function consists of two components: the feature prediction loss lfea and the token prediction loss ltoken. Thanks to the feature prediction loss, the draft model trained only at Step 1 can adapt to Step 2 and acquire multi-step predic- tion capabilities. However, with token prediction as the ultimate goal, feature prediction can be seen as an additional constraint, which limits the expres- siveness of the draft model and makes it difficult to benefit from increased data. After removing the feature constraint and expanding the training data (the middle part of Figure 3), as shown in Figure 4, the acceptance rate 0-α of the first draft token improves significantly. However, the output of the draft model in Step 1, denoted as ˆat+1, is far away from the ground-truth ft+1, causing the input sequence f1, f2, · · · , ft, ˆat+1 in Step 2 to de- viate significantly from the training distribution, resulting in a very low acceptance rate 1-α for the second draft token, as shown in Figure 4. We can address this issue by incorporating Step 1 into the training process (the bottom of Figure 3). Using this method, the benefits of increasing training data become more pronounced. We name this technique as training-time test. EAGLE and speculative sampling methods such as Medusa (Cai et al., 2024) reuse the top-layer fea- tures of the target model, specifically the features immediately before the LM head. For an LM head with a full-rank weight matrix, the top-layer fea- tures corresponding to the logits of the next token are unique, ensuring that the information contained in these features aligns directly with the logits of the next token. However, predicting the next-next token based solely on top-layer features—which are inherently limited to the next token—poses a significant challenge. Fortunately, the training-time test technique described above enables", "tokens": 665, "chunk_type": "original-large", "url": "https://arxiv.org/abs/2503.01840", "anchor": "#introduction:part-2", "type": "paper", "title": "", "section": "Introduction", "source": "arxiv_pdf", "published": "", "sha256": "75460e749b6f86d5369463ee6ce1a0ddfc7666e5d956f60659f203fa9589ff16"}
{"doc_id": "arxiv:2503.01840", "chunk_id": "introduction:part-2-o1", "text": "head to obtain the draft token. By leveraging the rich information from the target model, EAGLE achieves significantly better accel- eration compared to vanilla speculative sampling. Subsequent methods such as HASS (Zhang et al., 2024) and Falcon (Gao et al., 2024) also adopt the approach of predicting the next feature using the current feature sequence. Recent LLMs have increasingly relied on larger training datasets to achieve better performance. For example, LLaMA series models with sizes of 7B (8B) have used 1T, 2T, and 15T tokens of training data for LLaMA 1 (Touvron et al., 2023a), LLaMA 2 (Touvron et al., 2023b), and LLaMA 3 (Dubey et al., 2024), respectively, resulting in significant improvements across various metrics while keeping the model architecture and inference cost largely unchanged. Similarly, we aim to improve the ac- ceptance rate and acceleration ratio of EAGLE by increasing its training data. Unfortunately, we ob- serve that the gains from additional training data for EAGLE are limited. We analyze the reasons behind this phenomenon. As shown in the upper part of Figure 3, EAGLE performs autoregressive prediction at the feature level, predicting the next feature and then feeding the feature into the LM head of the target model to obtain the token dis- tribution. EAGLE’s loss function consists of two components: the feature prediction loss lfea and the token prediction loss ltoken. Thanks to the feature prediction loss, the draft model trained only at Step 1 can adapt to Step 2 and acquire multi-step predic- tion capabilities. However, with token prediction as the ultimate goal, feature prediction can be seen as an additional constraint, which limits the expres- siveness of the draft model and makes it difficult to benefit from increased data.", "tokens": 370, "chunk_type": "overlap-1", "url": "https://arxiv.org/abs/2503.01840", "anchor": "#introduction:part-2", "type": "paper", "title": "", "section": "Introduction", "source": "arxiv_pdf", "published": "", "sha256": "75460e749b6f86d5369463ee6ce1a0ddfc7666e5d956f60659f203fa9589ff16"}
{"doc_id": "arxiv:2503.01840", "chunk_id": "introduction:part-2-o2", "text": "Thanks to the feature prediction loss, the draft model trained only at Step 1 can adapt to Step 2 and acquire multi-step predic- tion capabilities. However, with token prediction as the ultimate goal, feature prediction can be seen as an additional constraint, which limits the expres- siveness of the draft model and makes it difficult to benefit from increased data. After removing the feature constraint and expanding the training data (the middle part of Figure 3), as shown in Figure 4, the acceptance rate 0-α of the first draft token improves significantly. However, the output of the draft model in Step 1, denoted as ˆat+1, is far away from the ground-truth ft+1, causing the input sequence f1, f2, · · · , ft, ˆat+1 in Step 2 to de- viate significantly from the training distribution, resulting in a very low acceptance rate 1-α for the second draft token, as shown in Figure 4. We can address this issue by incorporating Step 1 into the training process (the bottom of Figure 3). Using this method, the benefits of increasing training data become more pronounced. We name this technique as training-time test. EAGLE and speculative sampling methods such as Medusa (Cai et al., 2024) reuse the top-layer fea- tures of the target model, specifically the features immediately before the LM head. For an LM head with a full-rank weight matrix, the top-layer fea- tures corresponding to the logits of the next token are unique, ensuring that the information contained in these features aligns directly with the logits of the next token. However, predicting the next-next token based solely on top-layer features—which are inherently limited to the next token—poses a significant challenge. Fortunately, the training-time test technique described above enables", "tokens": 373, "chunk_type": "overlap-2", "url": "https://arxiv.org/abs/2503.01840", "anchor": "#introduction:part-2", "type": "paper", "title": "", "section": "Introduction", "source": "arxiv_pdf", "published": "", "sha256": "75460e749b6f86d5369463ee6ce1a0ddfc7666e5d956f60659f203fa9589ff16"}
{"doc_id": "arxiv:2503.01840", "chunk_id": "introduction:part-3", "text": "of the target model, specifically the features immediately before the LM head. For an LM head with a full-rank weight matrix, the top-layer fea- tures corresponding to the logits of the next token are unique, ensuring that the information contained in these features aligns directly with the logits of the next token. However, predicting the next-next token based solely on top-layer features—which are inherently limited to the next token—poses a significant challenge. Fortunately, the training-time test technique described above enables the use of features from intermediate layers instead of relying solely on the top layer, as the feature prediction loss lfea has been removed during training. Draft model 𝑓\"#$% ≈𝑓#$% LM head 𝑡̂#$( 𝑡#$( 𝑙*+, 𝑙#-.+/ Draft model 𝑓\"#$( 𝑓#$( LM head 𝑡̂#$0 𝑡#$0 Step 1 Step 2 Draft model 𝑎2#$% LM head 𝑡̂#$( 𝑡#$( 𝑙#-.+/ Draft model 𝑎2#$( LM head 𝑡̂#$0 𝑡#$0 𝑙#-.+/ Step 1 Step 2 𝑓# … 𝑓% 𝑓#3% 𝑎2#$% … 𝑓% 𝑓# Training/Test ≈ ≈ ≈ ≈ ≈ Draft model 𝑎2#$% LM head 𝑡̂#$( 𝑡#$( 𝑙#-.+/ Draft model 𝑎2#$( LM head 𝑡̂#$0 𝑡#$0 𝑎2#$% … 𝑓% 𝑓# Step 1 Step 2 Training ≈ ≈ Test 𝑓# … 𝑓% 𝑓#3% 𝑓\"#$% … 𝑓% 𝑓# Training 𝑓# 𝑓% 𝑓#3% Test … EAGLE EAGLE-3 EAGLE + 𝑙*+, removal Training/Test Training-time test Figure 3: Illustration of training-time test (the bottom part) and its comparison with other draft methods (the upper and middle parts). f denotes the feature, t denotes the token, and a represents the unconstrained vectors. We use the hat to denote the predictions from models. All the methods shown in the figure use the token se- quence from the previous time step, but for simplicity, this is not depicted in the figure. The input to EAGLE-3 is not actually f, but it is not shown in this figure. We will provide a detailed explanation in the following sec- tion. To summarize, this paper introduces EAGLE- 3, an enhanced version of EAGLE that achieves a significant speedup. EAGLE-3 is parallelized and fully compatible with the drafting tree tech- nique from EAGLE-2 (Li et al., 2024b). Our key contributions include: • A novel training-time test architecture for the draft model: We remove the feature pre- diction constraint and directly predict tokens while simulating multi-step generation during training. This direct token prediction provides complete flexibility in the draft model’s input. Instead of reusing only the top-layer features, we integrate and leverage low-, mid-, and high- level features from the target model, capturing rich semantic information from different lay- 1 2 4 8 0.72 0.74 0.76 0.78 0.80 0- EAGLE EAGLE without fea pred EAGLE-3 1 2 4 8 0.2 0.3 0.4 0.5 0.6 0.7 0.8 1- EAGLE EAGLE without fea pred EAGLE-3 Figure 4: Comparison of acceptance rates across differ- ent methods, with the x-axis representing the data scale relative to ShareGPT. ers. • Discovery of a scaling law for inference ac- celeration in large language models: With the new architecture, we observe that increas- ing the amount of training data for the draft model leads to a proportional increase in the speedup", "tokens": 665, "chunk_type": "original-large", "url": "https://arxiv.org/abs/2503.01840", "anchor": "#introduction:part-3", "type": "paper", "title": "", "section": "Introduction", "source": "arxiv_pdf", "published": "", "sha256": "8f28a3e031200fc313cbe18579219d3cc7c4b7914256ccb3a9baf8f3acec0796"}
{"doc_id": "arxiv:2503.01840", "chunk_id": "introduction:part-3-o1", "text": "of the target model, specifically the features immediately before the LM head. For an LM head with a full-rank weight matrix, the top-layer fea- tures corresponding to the logits of the next token are unique, ensuring that the information contained in these features aligns directly with the logits of the next token. However, predicting the next-next token based solely on top-layer features—which are inherently limited to the next token—poses a significant challenge. Fortunately, the training-time test technique described above enables the use of features from intermediate layers instead of relying solely on the top layer, as the feature prediction loss lfea has been removed during training. Draft model 𝑓\"#$% ≈𝑓#$% LM head 𝑡̂#$( 𝑡#$( 𝑙*+, 𝑙#-.+/ Draft model 𝑓\"#$( 𝑓#$( LM head 𝑡̂#$0 𝑡#$0 Step 1 Step 2 Draft model 𝑎2#$% LM head 𝑡̂#$( 𝑡#$( 𝑙#-.+/ Draft model 𝑎2#$( LM head 𝑡̂#$0 𝑡#$0 𝑙#-.+/ Step 1 Step 2 𝑓# … 𝑓% 𝑓#3% 𝑎2#$% … 𝑓% 𝑓# Training/Test ≈ ≈ ≈ ≈ ≈ Draft model 𝑎2#$% LM head 𝑡̂#$( 𝑡#$( 𝑙#-.+/ Draft model 𝑎2#$( LM head 𝑡̂#$0 𝑡#$0 𝑎2#$% … 𝑓% 𝑓# Step 1 Step 2 Training ≈ ≈ Test 𝑓# … 𝑓% 𝑓#3% 𝑓\"#$% … 𝑓% 𝑓# Training 𝑓# 𝑓% 𝑓#3% Test … EAGLE EAGLE-3 EAGLE + 𝑙*+, removal Training/Test Training-time test Figure 3: Illustration of training-time test (the bottom part) and its comparison with other draft methods (the upper and middle parts). f denotes the feature, t denotes the token, and a represents the unconstrained vectors. We use the hat to denote the predictions from models. All the methods shown in the figure use the token se- quence from the previous time step, but for simplicity, this is not depicted in the figure. The input to EAGLE-3 is not actually f, but it is not shown in this figure.", "tokens": 390, "chunk_type": "overlap-1", "url": "https://arxiv.org/abs/2503.01840", "anchor": "#introduction:part-3", "type": "paper", "title": "", "section": "Introduction", "source": "arxiv_pdf", "published": "", "sha256": "8f28a3e031200fc313cbe18579219d3cc7c4b7914256ccb3a9baf8f3acec0796"}
{"doc_id": "arxiv:2503.01840", "chunk_id": "introduction:part-3-o2", "text": "Draft model 𝑓\"#$% ≈𝑓#$% LM head 𝑡̂#$( 𝑡#$( 𝑙*+, 𝑙#-.+/ Draft model 𝑓\"#$( 𝑓#$( LM head 𝑡̂#$0 𝑡#$0 Step 1 Step 2 Draft model 𝑎2#$% LM head 𝑡̂#$( 𝑡#$( 𝑙#-.+/ Draft model 𝑎2#$( LM head 𝑡̂#$0 𝑡#$0 𝑙#-.+/ Step 1 Step 2 𝑓# … 𝑓% 𝑓#3% 𝑎2#$% … 𝑓% 𝑓# Training/Test ≈ ≈ ≈ ≈ ≈ Draft model 𝑎2#$% LM head 𝑡̂#$( 𝑡#$( 𝑙#-.+/ Draft model 𝑎2#$( LM head 𝑡̂#$0 𝑡#$0 𝑎2#$% … 𝑓% 𝑓# Step 1 Step 2 Training ≈ ≈ Test 𝑓# … 𝑓% 𝑓#3% 𝑓\"#$% … 𝑓% 𝑓# Training 𝑓# 𝑓% 𝑓#3% Test … EAGLE EAGLE-3 EAGLE + 𝑙*+, removal Training/Test Training-time test Figure 3: Illustration of training-time test (the bottom part) and its comparison with other draft methods (the upper and middle parts). f denotes the feature, t denotes the token, and a represents the unconstrained vectors. We use the hat to denote the predictions from models. All the methods shown in the figure use the token se- quence from the previous time step, but for simplicity, this is not depicted in the figure. The input to EAGLE-3 is not actually f, but it is not shown in this figure. We will provide a detailed explanation in the following sec- tion. To summarize, this paper introduces EAGLE- 3, an enhanced version of EAGLE that achieves a significant speedup. EAGLE-3 is parallelized and fully compatible with the drafting tree tech- nique from EAGLE-2 (Li et al., 2024b). Our key contributions include: • A novel training-time test architecture for the draft model: We remove the feature pre- diction constraint and directly predict tokens while simulating multi-step generation during training. This direct token prediction provides complete flexibility in the draft model’s input.", "tokens": 367, "chunk_type": "overlap-2", "url": "https://arxiv.org/abs/2503.01840", "anchor": "#introduction:part-3", "type": "paper", "title": "", "section": "Introduction", "source": "arxiv_pdf", "published": "", "sha256": "8f28a3e031200fc313cbe18579219d3cc7c4b7914256ccb3a9baf8f3acec0796"}
{"doc_id": "arxiv:2503.01840", "chunk_id": "introduction:part-3-o3", "text": "EAGLE-3 is parallelized and fully compatible with the drafting tree tech- nique from EAGLE-2 (Li et al., 2024b). Our key contributions include: • A novel training-time test architecture for the draft model: We remove the feature pre- diction constraint and directly predict tokens while simulating multi-step generation during training. This direct token prediction provides complete flexibility in the draft model’s input. Instead of reusing only the top-layer features, we integrate and leverage low-, mid-, and high- level features from the target model, capturing rich semantic information from different lay- 1 2 4 8 0.72 0.74 0.76 0.78 0.80 0- EAGLE EAGLE without fea pred EAGLE-3 1 2 4 8 0.2 0.3 0.4 0.5 0.6 0.7 0.8 1- EAGLE EAGLE without fea pred EAGLE-3 Figure 4: Comparison of acceptance rates across differ- ent methods, with the x-axis representing the data scale relative to ShareGPT. ers. • Discovery of a scaling law for inference ac- celeration in large language models: With the new architecture, we observe that increas- ing the amount of training data for the draft model leads to a proportional increase in the speedup", "tokens": 239, "chunk_type": "overlap-3", "url": "https://arxiv.org/abs/2503.01840", "anchor": "#introduction:part-3", "type": "paper", "title": "", "section": "Introduction", "source": "arxiv_pdf", "published": "", "sha256": "8f28a3e031200fc313cbe18579219d3cc7c4b7914256ccb3a9baf8f3acec0796"}
{"doc_id": "arxiv:2503.01840", "chunk_id": "introduction:part-4", "text": "EAGLE-3 1 2 4 8 0.2 0.3 0.4 0.5 0.6 0.7 0.8 1- EAGLE EAGLE without fea pred EAGLE-3 Figure 4: Comparison of acceptance rates across differ- ent methods, with the x-axis representing the data scale relative to ShareGPT. ers. • Discovery of a scaling law for inference ac- celeration in large language models: With the new architecture, we observe that increas- ing the amount of training data for the draft model leads to a proportional increase in the speedup ratio of EAGLE-3. This scaling be- havior was not observed in the original EA- GLE architecture, as shown in Figure 1 • Improved inference acceleration: EAGLE- 3, trained with approximately 8x more data than EAGLE, achieves a 1.4x latency speedup over EAGLE-2 at batch size 1. Specula- tive sampling is often thought to reduce throughput at large batch sizes. How- ever, in SGLang (Zheng et al., 2024), a production-grade framework, EAGLE-3 im- proves throughput by 40% at a batch size of 64. We expect larger data size would lead to further improved speedup ratio. 2 Preliminaries 2.1 Speculative Sampling Speculative sampling (Leviathan et al., 2023; Chen et al., 2023; Sun et al., 2024c,b) is a lossless LLM acceleration technique that alternates between draft- ing and verification, where drafting is performed at low cost and verification is parallelized, corre- sponding to the generation of drafts and the veri- fication process, respectively. We use ti to denote the i-th token and Ta:b to represent the token se- quence ta, ta+1, · · · , tb. When T1:j is used as the prefix, the two stages of speculative sampling are as follows. In the drafting stage, speculative sampling uti- lizes a draft model (a smaller version from the same series as the target model) to autoregressively gen- erate k tokens to form the draft. ˆTj+1:j+k, while also recording the probability ˆp for each token. In the verification stage, speculative sampling invokes the target model to evaluate the draft ˆTj+1:j+k and records its probability p. Specula- tive sampling then determines the acceptance of draft tokens sequentially, from front to back. For token ˆtj+i, the probability of acceptance is given by min(1, pj+i(ˆtj+i)/ˆpj+i(ˆtj+i)). If the token is accepted, the process moves to the next token. Oth- erwise, a token is sampled from the distribution norm(max(0, pj+i −ˆpj+i)) to replace ˆtj+i, and the remaining tokens in the draft are discarded. Ap- pendix A.1 of (Leviathan et al., 2023) proves that speculative sampling is consistent with the distri- bution of vanilla autoregressive decoding. 2.2 EAGLE and EAGLE-2 The draft model with limited capacity struggles to precisely approximate the large-scale target model. EAGLE leverages the top-layer features of the tar- get model as additional information and performs autoregression at the feature level, simplifying the drafting process. EAGLE performs autoregression at the feature level and then uses the LM head of the target model to obtain the draft token. Due to the sampling results at the token layer being hidden, feature-level autoregression introduces un- certainty. EAGLE addresses this issue by feeding the token sequence from the previous time step, i.e., the", "tokens": 665, "chunk_type": "original-large", "url": "https://arxiv.org/abs/2503.01840", "anchor": "#introduction:part-4", "type": "paper", "title": "", "section": "Introduction", "source": "arxiv_pdf", "published": "", "sha256": "13b9d0cdd8d52f9d1a3d2688a47d9f5cb6b7f2588e6b184d964294f411ae60b5"}
{"doc_id": "arxiv:2503.01840", "chunk_id": "introduction:part-4-o1", "text": "EAGLE-3 1 2 4 8 0.2 0.3 0.4 0.5 0.6 0.7 0.8 1- EAGLE EAGLE without fea pred EAGLE-3 Figure 4: Comparison of acceptance rates across differ- ent methods, with the x-axis representing the data scale relative to ShareGPT. ers. • Discovery of a scaling law for inference ac- celeration in large language models: With the new architecture, we observe that increas- ing the amount of training data for the draft model leads to a proportional increase in the speedup ratio of EAGLE-3. This scaling be- havior was not observed in the original EA- GLE architecture, as shown in Figure 1 • Improved inference acceleration: EAGLE- 3, trained with approximately 8x more data than EAGLE, achieves a 1.4x latency speedup over EAGLE-2 at batch size 1. Specula- tive sampling is often thought to reduce throughput at large batch sizes. How- ever, in SGLang (Zheng et al., 2024), a production-grade framework, EAGLE-3 im- proves throughput by 40% at a batch size of 64. We expect larger data size would lead to further improved speedup ratio. 2 Preliminaries 2.1 Speculative Sampling Speculative sampling (Leviathan et al., 2023; Chen et al., 2023; Sun et al., 2024c,b) is a lossless LLM acceleration technique that alternates between draft- ing and verification, where drafting is performed at low cost and verification is parallelized, corre- sponding to the generation of drafts and the veri- fication process, respectively. We use ti to denote the i-th token and Ta:b to represent the token se- quence ta, ta+1, · · · , tb. When T1:j is used as the prefix, the two stages of speculative sampling are as follows. In the drafting stage, speculative sampling uti- lizes a draft model (a smaller version from the same series as the target model) to autoregressively gen- erate k tokens to form the draft. ˆTj+1:j+k, while also recording the probability ˆp for each token.", "tokens": 404, "chunk_type": "overlap-1", "url": "https://arxiv.org/abs/2503.01840", "anchor": "#introduction:part-4", "type": "paper", "title": "", "section": "Introduction", "source": "arxiv_pdf", "published": "", "sha256": "13b9d0cdd8d52f9d1a3d2688a47d9f5cb6b7f2588e6b184d964294f411ae60b5"}
{"doc_id": "arxiv:2503.01840", "chunk_id": "introduction:part-4-o2", "text": "We use ti to denote the i-th token and Ta:b to represent the token se- quence ta, ta+1, · · · , tb. When T1:j is used as the prefix, the two stages of speculative sampling are as follows. In the drafting stage, speculative sampling uti- lizes a draft model (a smaller version from the same series as the target model) to autoregressively gen- erate k tokens to form the draft. ˆTj+1:j+k, while also recording the probability ˆp for each token. In the verification stage, speculative sampling invokes the target model to evaluate the draft ˆTj+1:j+k and records its probability p. Specula- tive sampling then determines the acceptance of draft tokens sequentially, from front to back. For token ˆtj+i, the probability of acceptance is given by min(1, pj+i(ˆtj+i)/ˆpj+i(ˆtj+i)). If the token is accepted, the process moves to the next token. Oth- erwise, a token is sampled from the distribution norm(max(0, pj+i −ˆpj+i)) to replace ˆtj+i, and the remaining tokens in the draft are discarded. Ap- pendix A.1 of (Leviathan et al., 2023) proves that speculative sampling is consistent with the distri- bution of vanilla autoregressive decoding. 2.2 EAGLE and EAGLE-2 The draft model with limited capacity struggles to precisely approximate the large-scale target model. EAGLE leverages the top-layer features of the tar- get model as additional information and performs autoregression at the feature level, simplifying the drafting process. EAGLE performs autoregression at the feature level and then uses the LM head of the target model to obtain the draft token. Due to the sampling results at the token layer being hidden, feature-level autoregression introduces un- certainty. EAGLE addresses this issue by feeding the token sequence from the previous time step, i.e., the", "tokens": 366, "chunk_type": "overlap-2", "url": "https://arxiv.org/abs/2503.01840", "anchor": "#introduction:part-4", "type": "paper", "title": "", "section": "Introduction", "source": "arxiv_pdf", "published": "", "sha256": "13b9d0cdd8d52f9d1a3d2688a47d9f5cb6b7f2588e6b184d964294f411ae60b5"}
{"doc_id": "arxiv:2503.01840", "chunk_id": "introduction:part-5", "text": "target model. EAGLE leverages the top-layer features of the tar- get model as additional information and performs autoregression at the feature level, simplifying the drafting process. EAGLE performs autoregression at the feature level and then uses the LM head of the target model to obtain the draft token. Due to the sampling results at the token layer being hidden, feature-level autoregression introduces un- certainty. EAGLE addresses this issue by feeding the token sequence from the previous time step, i.e., the sampling results, into the draft model. Unlike the chain-like drafts of Vanilla speculative sam- pling, EAGLE generates multiple draft tokens at the same position, resulting in a tree-like draft. In the verification stage, EAGLE uses tree attention to parallelize the verification of the draft tree. Interest- ingly, EAGLE inspired the multi-token prediction technique used in the pre-training of DeepSeek- v3 (Liu et al., 2024a), which in turn inspired new architectural designs in EAGLE-3. EAGLE (Li et al., 2024c) and Medusa (Cai et al., 2024), among others, use tree-shaped drafts, where the structure of the draft tree is predefined, static, and context-independent. The difficulty of draft- ing is closely related to the context, and a static draft tree can lead to resource wastage. EAGLE- 2 (Li et al., 2024b) approximates the acceptance rate using the confidence of the draft model and dy- namically generates the draft tree based on this, per- forming pruning of the draft tree at the end of the drafting stage. EAGLE-3 also adopts the context- aware dynamic draft tree proposed in EAGLE-2. How can LM Head I Target Model 𝑙how 𝑙can Decoder Layers Embedding 𝑚how 𝑚can Decoder Layers ℎhow ℎcan Decoder Layers Decoder Layers ℎhow ℎcan Concat 𝑚how 𝑚can 𝑙how 𝑙can FC Layer 𝑔how 𝑔can I Decoder Layer FC Layer can 𝑔can 𝑔how 𝑒can 𝑒I 𝑎I LM Head I can do do 𝑔can 𝑔how 𝑒can 𝑒I 𝑎I 𝑒do Decoder Layer FC Layer 𝑎do LM Head it I do 𝑔can 𝑔how 𝑒can 𝑒I 𝑎I 𝑒do it 𝑎do 𝑒it ... ① ② ③ can 𝑎can 𝑎I 𝑎can Figure 5: Diagram of the EAGLE-3 inference pipeline, illustrating the three steps of the draft model. l, m, and h represent the low, middle, and high-level features of the target model, respectively. e denotes the embedding. 3 EAGLE-3 In this section, we provide a detailed description of the implementation of EAGLE-3. 3.1 Inference Pipeline Consistent with other speculative sampling meth- ods, EAGLE-3 alternates between the drafting and verification stages. The difference between EAGLE-3 and EAGLE lies in the drafting stage, which we introduce with an example, as shown in Figure 5. Consider the prefix “How can”. Dur- ing the prefill phase or the previous verification stage, the target model performs a forward pass to generate the next token, “I”. We record the low, middle, and high-level feature sequences from the target model’s forward pass, denoted as l, m, and h, respectively. We concatenate the k-dimensional vectors l, m, and h to form a 3k-dimensional vector, then pass it through a fully connected (FC) layer to reduce it to k-dimensions, obtaining a feature g that", "tokens": 665, "chunk_type": "original-large", "url": "https://arxiv.org/abs/2503.01840", "anchor": "#introduction:part-5", "type": "paper", "title": "", "section": "Introduction", "source": "arxiv_pdf", "published": "", "sha256": "576e5c91d93c7e7071d87654997e3a7d6b812864f9c790dbe4eaf889e9f9f3f5"}
{"doc_id": "arxiv:2503.01840", "chunk_id": "introduction:part-5-o1", "text": "target model. EAGLE leverages the top-layer features of the tar- get model as additional information and performs autoregression at the feature level, simplifying the drafting process. EAGLE performs autoregression at the feature level and then uses the LM head of the target model to obtain the draft token. Due to the sampling results at the token layer being hidden, feature-level autoregression introduces un- certainty. EAGLE addresses this issue by feeding the token sequence from the previous time step, i.e., the sampling results, into the draft model. Unlike the chain-like drafts of Vanilla speculative sam- pling, EAGLE generates multiple draft tokens at the same position, resulting in a tree-like draft. In the verification stage, EAGLE uses tree attention to parallelize the verification of the draft tree. Interest- ingly, EAGLE inspired the multi-token prediction technique used in the pre-training of DeepSeek- v3 (Liu et al., 2024a), which in turn inspired new architectural designs in EAGLE-3. EAGLE (Li et al., 2024c) and Medusa (Cai et al., 2024), among others, use tree-shaped drafts, where the structure of the draft tree is predefined, static, and context-independent. The difficulty of draft- ing is closely related to the context, and a static draft tree can lead to resource wastage. EAGLE- 2 (Li et al., 2024b) approximates the acceptance rate using the confidence of the draft model and dy- namically generates the draft tree based on this, per- forming pruning of the draft tree at the end of the drafting stage. EAGLE-3 also adopts the context- aware dynamic draft tree proposed in EAGLE-2.", "tokens": 331, "chunk_type": "overlap-1", "url": "https://arxiv.org/abs/2503.01840", "anchor": "#introduction:part-5", "type": "paper", "title": "", "section": "Introduction", "source": "arxiv_pdf", "published": "", "sha256": "576e5c91d93c7e7071d87654997e3a7d6b812864f9c790dbe4eaf889e9f9f3f5"}
{"doc_id": "arxiv:2503.01840", "chunk_id": "introduction:part-5-o2", "text": "The difficulty of draft- ing is closely related to the context, and a static draft tree can lead to resource wastage. EAGLE- 2 (Li et al., 2024b) approximates the acceptance rate using the confidence of the draft model and dy- namically generates the draft tree based on this, per- forming pruning of the draft tree at the end of the drafting stage. EAGLE-3 also adopts the context- aware dynamic draft tree proposed in EAGLE-2. How can LM Head I Target Model 𝑙how 𝑙can Decoder Layers Embedding 𝑚how 𝑚can Decoder Layers ℎhow ℎcan Decoder Layers Decoder Layers ℎhow ℎcan Concat 𝑚how 𝑚can 𝑙how 𝑙can FC Layer 𝑔how 𝑔can I Decoder Layer FC Layer can 𝑔can 𝑔how 𝑒can 𝑒I 𝑎I LM Head I can do do 𝑔can 𝑔how 𝑒can 𝑒I 𝑎I 𝑒do Decoder Layer FC Layer 𝑎do LM Head it I do 𝑔can 𝑔how 𝑒can 𝑒I 𝑎I 𝑒do it 𝑎do 𝑒it ... ① ② ③ can 𝑎can 𝑎I 𝑎can Figure 5: Diagram of the EAGLE-3 inference pipeline, illustrating the three steps of the draft model. l, m, and h represent the low, middle, and high-level features of the target model, respectively. e denotes the embedding. 3 EAGLE-3 In this section, we provide a detailed description of the implementation of EAGLE-3. 3.1 Inference Pipeline Consistent with other speculative sampling meth- ods, EAGLE-3 alternates between the drafting and verification stages. The difference between EAGLE-3 and EAGLE lies in the drafting stage, which we introduce with an example, as shown in Figure 5. Consider the prefix “How can”. Dur- ing the prefill phase or the previous verification stage, the target model performs a forward pass to generate the next token, “I”. We record the low, middle, and high-level feature sequences from the target model’s forward pass, denoted as l, m, and h, respectively.", "tokens": 387, "chunk_type": "overlap-2", "url": "https://arxiv.org/abs/2503.01840", "anchor": "#introduction:part-5", "type": "paper", "title": "", "section": "Introduction", "source": "arxiv_pdf", "published": "", "sha256": "576e5c91d93c7e7071d87654997e3a7d6b812864f9c790dbe4eaf889e9f9f3f5"}
{"doc_id": "arxiv:2503.01840", "chunk_id": "introduction:part-5-o3", "text": "The difference between EAGLE-3 and EAGLE lies in the drafting stage, which we introduce with an example, as shown in Figure 5. Consider the prefix “How can”. Dur- ing the prefill phase or the previous verification stage, the target model performs a forward pass to generate the next token, “I”. We record the low, middle, and high-level feature sequences from the target model’s forward pass, denoted as l, m, and h, respectively. We concatenate the k-dimensional vectors l, m, and h to form a 3k-dimensional vector, then pass it through a fully connected (FC) layer to reduce it to k-dimensions, obtaining a feature g that", "tokens": 136, "chunk_type": "overlap-3", "url": "https://arxiv.org/abs/2503.01840", "anchor": "#introduction:part-5", "type": "paper", "title": "", "section": "Introduction", "source": "arxiv_pdf", "published": "", "sha256": "576e5c91d93c7e7071d87654997e3a7d6b812864f9c790dbe4eaf889e9f9f3f5"}
{"doc_id": "arxiv:2503.01840", "chunk_id": "introduction:part-6", "text": "“How can”. Dur- ing the prefill phase or the previous verification stage, the target model performs a forward pass to generate the next token, “I”. We record the low, middle, and high-level feature sequences from the target model’s forward pass, denoted as l, m, and h, respectively. We concatenate the k-dimensional vectors l, m, and h to form a 3k-dimensional vector, then pass it through a fully connected (FC) layer to reduce it to k-dimensions, obtaining a feature g that integrates information from different layers. Here, k refers to the hidden size of the target model. Our goal is to generate a draft token sequence with the prefix “How can I”. By inputting only ghow and gcan, the draft model cannot access the random sampling process. Therefore, similar to EAGLE (Li et al., 2024c), we introduce the embed- ding eI of the sampled token “I”. The concatenated vector is then passed through an FC layer to reduce its dimensionality to k, and subsequently inputted into a single layer decoder, producing the output a. Finally, we input aI into the LM head and sample to obtain the draft token “do”. In Step 1, with the prefix “How can”, we reuse ghow and gcan from the target model. In Step 2, the prefix becomes “How can I”. Ideally, we would reuse ghow, gcan, and gI from the target model. How- ever, this is not possible because the token “I” has not yet been checked by the target model, and we cannot obtain gI. Instead, we use the output aI from the draft model in the previous step to replace gI, and concatenate aI with the embedding edo of the sampled result “do” as the input to the draft model in Step 1. In Step 3, we similarly cannot obtain gdo, so we use ado as a replacement, concatenating ado with eit as the input to the draft model. The same approach is followed for subsequent steps. 3.2 Draft Model Training The input to the draft model in EAGLE is either, or at least approximately, the top-layer features f1, f2, · · · , ft of the target model. In contrast, the input to the draft model in EAGLE-3 may include the features g1, g2, · · · , gt from the target model, or it may include the output at+1, at+2 · · · , at+j from the draft model. Therefore, we need to train the draft model to adapt to different inputs. During training, we perform test steps, where we generate a and feed it back into the draft model for further training. The core of the draft model in EAGLE-3 is a Transformer decoder layer. Aside from the self- attention operation, no other components interact with the context, so no further modifications are required during training or testing. The only com- ponent that requires slight modification is the self- attention, which we will describe in detail below. Although the actual input consists of features, for clarity, we describe the process using tokens as input. As shown in Figure 6, the original train-", "tokens": 665, "chunk_type": "original-large", "url": "https://arxiv.org/abs/2503.01840", "anchor": "#introduction:part-6", "type": "paper", "title": "", "section": "Introduction", "source": "arxiv_pdf", "published": "", "sha256": "8c675fa9fc0eab08d6d370bc346e1e6df3e14da2466f69013d8027e6515d7804"}
{"doc_id": "arxiv:2503.01840", "chunk_id": "introduction:part-6-o1", "text": "“How can”. Dur- ing the prefill phase or the previous verification stage, the target model performs a forward pass to generate the next token, “I”. We record the low, middle, and high-level feature sequences from the target model’s forward pass, denoted as l, m, and h, respectively. We concatenate the k-dimensional vectors l, m, and h to form a 3k-dimensional vector, then pass it through a fully connected (FC) layer to reduce it to k-dimensions, obtaining a feature g that integrates information from different layers. Here, k refers to the hidden size of the target model. Our goal is to generate a draft token sequence with the prefix “How can I”. By inputting only ghow and gcan, the draft model cannot access the random sampling process. Therefore, similar to EAGLE (Li et al., 2024c), we introduce the embed- ding eI of the sampled token “I”. The concatenated vector is then passed through an FC layer to reduce its dimensionality to k, and subsequently inputted into a single layer decoder, producing the output a. Finally, we input aI into the LM head and sample to obtain the draft token “do”. In Step 1, with the prefix “How can”, we reuse ghow and gcan from the target model. In Step 2, the prefix becomes “How can I”. Ideally, we would reuse ghow, gcan, and gI from the target model. How- ever, this is not possible because the token “I” has not yet been checked by the target model, and we cannot obtain gI. Instead, we use the output aI from the draft model in the previous step to replace gI, and concatenate aI with the embedding edo of the sampled result “do” as the input to the draft model in Step 1.", "tokens": 377, "chunk_type": "overlap-1", "url": "https://arxiv.org/abs/2503.01840", "anchor": "#introduction:part-6", "type": "paper", "title": "", "section": "Introduction", "source": "arxiv_pdf", "published": "", "sha256": "8c675fa9fc0eab08d6d370bc346e1e6df3e14da2466f69013d8027e6515d7804"}
{"doc_id": "arxiv:2503.01840", "chunk_id": "introduction:part-6-o2", "text": "How- ever, this is not possible because the token “I” has not yet been checked by the target model, and we cannot obtain gI. Instead, we use the output aI from the draft model in the previous step to replace gI, and concatenate aI with the embedding edo of the sampled result “do” as the input to the draft model in Step 1. In Step 3, we similarly cannot obtain gdo, so we use ado as a replacement, concatenating ado with eit as the input to the draft model. The same approach is followed for subsequent steps. 3.2 Draft Model Training The input to the draft model in EAGLE is either, or at least approximately, the top-layer features f1, f2, · · · , ft of the target model. In contrast, the input to the draft model in EAGLE-3 may include the features g1, g2, · · · , gt from the target model, or it may include the output at+1, at+2 · · · , at+j from the draft model. Therefore, we need to train the draft model to adapt to different inputs. During training, we perform test steps, where we generate a and feed it back into the draft model for further training. The core of the draft model in EAGLE-3 is a Transformer decoder layer. Aside from the self- attention operation, no other components interact with the context, so no further modifications are required during training or testing. The only com- ponent that requires slight modification is the self- attention, which we will describe in detail below. Although the actual input consists of features, for clarity, we describe the process using tokens as input. As shown in Figure 6, the original train-", "tokens": 370, "chunk_type": "overlap-2", "url": "https://arxiv.org/abs/2503.01840", "anchor": "#introduction:part-6", "type": "paper", "title": "", "section": "Introduction", "source": "arxiv_pdf", "published": "", "sha256": "8c675fa9fc0eab08d6d370bc346e1e6df3e14da2466f69013d8027e6515d7804"}
{"doc_id": "arxiv:2503.01840", "chunk_id": "introduction:part-7", "text": "The core of the draft model in EAGLE-3 is a Transformer decoder layer. Aside from the self- attention operation, no other components interact with the context, so no further modifications are required during training or testing. The only com- ponent that requires slight modification is the self- attention, which we will describe in detail below. Although the actual input consists of features, for clarity, we describe the process using tokens as input. As shown in Figure 6, the original train- ing data is a sequence of length 3, “How can I”, with a normal sequential dependency in the context. Therefore, the attention mask is a standard lower triangular matrix. The outputs at the three positions are “are”, “we”, and “do”, which have a tree-like contextual relationship with “how”, “can”, and “I”. As a result, when the input “are”, “we”, and “do” is fed into Step 2, the attention mask needs to be adjusted accordingly, as shown in the top-right cor- ner of Figure 6. All attention masks are diagonal, except when the original training data is used as the key. Using matrix multiplication in this case would result in significant computational waste, so we can use vector dot products to calculate the attention score only for the corresponding positions. HASS (Zhang et al., 2024) and EAGLE-3 both make similar modifications to the attention mecha- nism to simulate the testing process during training, but this is not the main focus of EAGLE-3. The motivations, methods, and outcomes of the two ap- proaches are distinctly different. The motivation behind HASS is to mitigate the error accumula- tion caused by inaccurate feature predictions in EAGLE. HASS still performs feature prediction, includes a feature prediction loss lfea, and the in- put to the draft model must be the top-layer fea- tures. In contrast, the motivation behind EAGLE-3 is to remove unnecessary constraints to enhance the model’s expressive power. EAGLE-3 no longer requires the draft model’s output to fit the top-layer features of the target model, thus avoiding error accumulation. After removing feature prediction, the input to EAGLE-3 is completely free, and it is replaced by a fusion of features from different layers of semantic information. The removal of the feature prediction loss also enables us to discover a new scaling law for inference acceleration which was never found before. Figure 2 also shows the speedup of EAGLE-3 and HASS, with EAGLE-3 demonstrating significantly better performance. 4", "tokens": 522, "chunk_type": "original", "url": "https://arxiv.org/abs/2503.01840", "anchor": "#introduction:part-7", "type": "paper", "title": "", "section": "Introduction", "source": "arxiv_pdf", "published": "", "sha256": "3a67daebea8414f6ac623db14d476791273e6f7703f8f424e6a88d3b39bc4fed"}
{"doc_id": "arxiv:2503.01840", "chunk_id": "experiments:part-1", "text": "Models. We conduct experiments with state-of- the-art open-source chat and reasoning models, in- cluding Vicuna 13B (Chiang et al., 2023), LLaMA- Instruct 3.1 8B, LLaMA-Instruct 3.3 70B (Dubey et al., 2024), and DeepSeek-R1-Distill-LLaMA 8B (DeepSeek-AI et al., 2025). Due to the GPU constraint, we are unable to test EAGLE-3 on the 405B and 671B models. Tasks. Following EAGLE (Li et al., 2024c) and Spec-Bench (Xia et al., 2024), we evaluate on five common tasks, using the same weights for all tasks without fine-tuning on the respective tasks. For multi-turn conversation, code genera- How can I How can I ✓ ✓✓ ✓✓✓ How can I How can I are we do How can I ✓ ✓ ✓✓ ✓ ✓✓✓ ✓ are we do are we do How can I are we do you help it How can I ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ you help it are we do you help it Query Key Query Key Query Key Figure 6: Diagram of the attention causal masks during training-time test. It sequentially shows a native training step (the first step) and two simulated training steps (the second and third steps). The arrows between tokens represent contextual relationships. The gray tokens represent the training data while the blue and yellow tokens represent the first- and second-round predictions by the draft model, respectively. tion, mathematical reasoning, instruction follow- ing, and summarization„ we chose the MT-bench (Zheng et al., 2023), HumanEval (Chen et al., 2021), GSM8K (Cobbe et al., 2021), Alpaca (Taori et al., 2023), and CNN/Daily Mail (Nallapati et al., 2016) datasets, respectively. Metrics. EAGLE-3 does not modify the target model’s weights and uses strict speculative sam- pling acceptance conditions, ensuring no loss in performance. Therefore, we do not evaluate gener- ation quality. Instead, we use the following metrics to assess the acceleration performance: • Speedup Ratio: The actual test speedup ratio relative to vanilla autoregressive decoding. • Average Acceptance Length τ: The aver- age number of tokens generated per drafting- verification cycle, which corresponds to the number of tokens accepted from the draft. • Acceptance Rate n-α: The proportion of draft tokens accepted, which directly reflects the draft model’s approximation to the tar- get model. Following EAGLE’s setup, we use a chain-like draft rather than a tree-like draft when testing acceptance rates. EA- GLE suffers from error accumulation, mean- ing that the input to the draft model may be its own estimates rather than the exact val- ues from the target model. Therefore, EA- GLE uses n-α to represent the acceptance rate when the input contains n estimated fea- tures, under the condition that the previous estimated tokens are all accepted by the tar- get model. In other words, the acceptance rate for inputs f1, f2, · · · , fi, ˆfi+1, · · · , ˆfi+n, where f is the exact value and ˆf is the draft model’s estimate. Similarly, we use n-α to represent the acceptance rate in EAGLE-3 when the input contains n self-predicted val- ues a, i.e., the acceptance rate for inputs g1, g2,", "tokens": 665, "chunk_type": "original-large", "url": "https://arxiv.org/abs/2503.01840", "anchor": "#experiments:part-1", "type": "paper", "title": "", "section": "Experiments", "source": "arxiv_pdf", "published": "", "sha256": "dcd110f89e71a32c016e5de604676833e802d79c1cda4e2dba6c6dcb98a45a13"}
