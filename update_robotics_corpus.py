import requests
from bs4 import BeautifulSoup
import json
import time
import hashlib
from pathlib import Path
from datetime import datetime
from urllib.parse import urlparse

def download_and_extract(url):
    """ä¸‹è¼‰ä¸¦æå–æ–‡ç« å…§å®¹"""
    try:
        print(f"  ä¸‹è¼‰ä¸­...")
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        }
        
        response = requests.get(url, headers=headers, timeout=30)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # æå–æ¨™é¡Œ
        title = soup.find('h1')
        if not title:
            title = soup.find('title')
        title_text = title.get_text().strip() if title else ""
        
        # æå–å…§å®¹
        content = None
        for selector in ['article', 'main', '[class*="post-content"]', '[class*="entry-content"]', '[class*="article-content"]', 'body']:
            content = soup.select_one(selector)
            if content:
                break
        
        # æ¸…ç†ä¸éœ€è¦çš„æ¨™ç±¤
        if content:
            for tag in content(["script", "style", "nav", "footer", "header", "aside", "iframe", "noscript"]):
                tag.decompose()
        
        # æå–æ–‡å­—
        content_text = content.get_text(separator=' ', strip=True) if content else soup.get_text(separator=' ', strip=True)
        
        # æ¸…ç†å¤šé¤˜ç©ºç™½
        content_text = ' '.join(content_text.split())
        
        return title_text, content_text
        
    except Exception as e:
        print(f"  âœ— éŒ¯èª¤: {e}")
        return None, None

def create_doc_id(url, index):
    """ç”Ÿæˆæ–‡æª” ID"""
    domain = urlparse(url).netloc
    return f"blog:{domain}#robotics-{index:03d}"

def update_corpus():
    """æ›´æ–° corpus.jsonl å’Œ corpus.stats.json"""
    
    urls_file = Path('data/raw/blogs/urls.txt')
    corpus_file = Path('data/processed/corpus.jsonl')
    stats_file = Path('data/processed/corpus.stats.json')
    
    # æª¢æŸ¥æª”æ¡ˆ
    if not urls_file.exists():
        print(f"âŒ éŒ¯èª¤: æ‰¾ä¸åˆ° {urls_file}")
        return
    
    # è®€å–æ–°çš„ URLs
    with open(urls_file, 'r', encoding='utf-8') as f:
        new_urls = [line.strip() for line in f if line.strip() and not line.startswith('#')]
    
    print(f"ğŸ“‹ æ‰¾åˆ° {len(new_urls)} å€‹æ–°çš„ URLs\n")
    
    # è®€å–ç¾æœ‰çš„ corpus
    existing_data = []
    existing_urls = set()
    
    if corpus_file.exists():
        with open(corpus_file, 'r', encoding='utf-8') as f:
            for line in f:
                if line.strip():
                    entry = json.loads(line)
                    existing_data.append(entry)
                    existing_urls.add(entry.get('url', ''))
        print(f"ğŸ“š ç¾æœ‰æ–‡ç« æ•¸: {len(existing_data)}\n")
    
    # è™•ç†æ–°çš„ URLs
    new_entries = []
    success_count = 0
    
    print("é–‹å§‹è™•ç†æ–°æ–‡ç« ...")
    print("=" * 80)
    
    for i, url in enumerate(new_urls, 1):
        print(f"\n[{i}/{len(new_urls)}] {url}")
        
        # æª¢æŸ¥æ˜¯å¦å·²å­˜åœ¨
        if url in existing_urls:
            print("  â­ï¸  è·³éï¼ˆå·²å­˜åœ¨ï¼‰")
            continue
        
        # ä¸‹è¼‰å…§å®¹
        title, content = download_and_extract(url)
        
        if title and content and len(content) > 100:
            # è¨ˆç®— token æ•¸ï¼ˆç²—ç•¥ä¼°è¨ˆï¼š1 token â‰ˆ 4 å­—å…ƒï¼‰
            tokens = len(content) // 4
            
            # è¨ˆç®— SHA256
            sha256 = hashlib.sha256(content.encode('utf-8')).hexdigest()
            
            # ç”Ÿæˆ doc_id
            doc_id = create_doc_id(url, len(existing_data) + success_count + 1)
            
            # å»ºç«‹æ¢ç›®ï¼ˆåŒ¹é…ç¾æœ‰æ ¼å¼ï¼‰
            entry = {
                'doc_id': doc_id,
                'url': url,
                'anchor': '',
                'type': 'blog',
                'title': title,
                'section': 'Body',
                'text': content,
                'source': f"blog:{urlparse(url).netloc}",
                'published': '',
                'authors': '',
                'tokens': tokens,
                'sha256': sha256
            }
            
            new_entries.append(entry)
            success_count += 1
            
            print(f"  âœ… æˆåŠŸï¼")
            print(f"     æ¨™é¡Œ: {title[:60]}...")
            print(f"     å­—æ•¸: {len(content.split()):,} words")
            print(f"     Tokens: {tokens:,}")
        else:
            print(f"  âš ï¸  å…§å®¹å¤ªçŸ­æˆ–æå–å¤±æ•—ï¼Œè·³é")
        
        # ç¦®è²Œå»¶é²
        if i < len(new_urls):
            time.sleep(2)
    
    print("\n" + "=" * 80)
    
    # åˆä½µä¸¦å¯«å…¥ corpus.jsonl
    all_data = existing_data + new_entries
    
    with open(corpus_file, 'w', encoding='utf-8') as f:
        for entry in all_data:
            f.write(json.dumps(entry, ensure_ascii=False) + '\n')
    
    print(f"\nâœ… å·²æ›´æ–° {corpus_file}")
    print(f"   åŸæœ‰æ–‡ç« : {len(existing_data)}")
    print(f"   æ–°å¢æ–‡ç« : {len(new_entries)}")
    print(f"   ç¸½è¨ˆæ–‡ç« : {len(all_data)}")
    
    # æ›´æ–°çµ±è¨ˆæª”æ¡ˆ
    total_tokens = sum(entry.get('tokens', 0) for entry in all_data)
    
    # çµ±è¨ˆä¾†æºåˆ†å¸ƒ
    sources = {}
    for entry in all_data:
        source = entry.get('source', 'unknown')
        sources[source] = sources.get(source, 0) + 1
    
    stats = {
        'total_documents': len(all_data),
        'total_tokens': total_tokens,
        'sources': sources,
        'robotics_articles': len(new_entries),
        'last_updated': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    }
    
    with open(stats_file, 'w', encoding='utf-8') as f:
        json.dump(stats, f, ensure_ascii=False, indent=2)
    
    print(f"\nâœ… å·²æ›´æ–° {stats_file}")
    print(f"   ç¸½æ–‡ä»¶æ•¸: {stats['total_documents']:,}")
    print(f"   ç¸½ Tokens: {stats['total_tokens']:,}")
    print(f"   ä¾†æºåˆ†å¸ƒ: {len(sources)} å€‹ä¸åŒä¾†æº")
    
    print("\n" + "=" * 80)
    print("ğŸ‰ å®Œæˆï¼")

if __name__ == '__main__':
    update_corpus()
