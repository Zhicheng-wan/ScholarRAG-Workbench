[
  {
    "id": "2510.13794v1",
    "title": "MimicKit: A Reinforcement Learning Framework for Motion Imitation and Control",
    "authors": [
      "Xue Bin Peng"
    ],
    "abstract": "MimicKit is an open-source framework for training motion controllers using\nmotion imitation and reinforcement learning. The codebase provides\nimplementations of commonly-used motion-imitation techniques and RL algorithms.\nThis framework is intended to support research and applications in computer\ngraphics and robotics by providing a unified training framework, along with\nstandardized environment, agent, and data structures. The codebase is designed\nto be modular and easily configurable, enabling convenient modification and\nextension to new characters and tasks. The open-source codebase is available\nat: https://github.com/xbpeng/MimicKit.",
    "published": "2025-10-15 17:51:42+00:00",
    "pdf_url": "http://arxiv.org/pdf/2510.13794v1"
  },
  {
    "id": "2510.13778v1",
    "title": "InternVLA-M1: A Spatially Guided Vision-Language-Action Framework for Generalist Robot Policy",
    "authors": [
      "Xinyi Chen",
      "Yilun Chen",
      "Yanwei Fu",
      "Ning Gao",
      "Jiaya Jia",
      "Weiyang Jin",
      "Hao Li",
      "Yao Mu",
      "Jiangmiao Pang",
      "Yu Qiao",
      "Yang Tian",
      "Bin Wang",
      "Bolun Wang",
      "Fangjing Wang",
      "Hanqing Wang",
      "Tai Wang",
      "Ziqin Wang",
      "Xueyuan Wei",
      "Chao Wu",
      "Shuai Yang",
      "Jinhui Ye",
      "Junqiu Yu",
      "Jia Zeng",
      "Jingjing Zhang",
      "Jinyu Zhang",
      "Shi Zhang",
      "Feng Zheng",
      "Bowen Zhou",
      "Yangkun Zhu"
    ],
    "abstract": "We introduce InternVLA-M1, a unified framework for spatial grounding and\nrobot control that advances instruction-following robots toward scalable,\ngeneral-purpose intelligence. Its core idea is spatially guided\nvision-language-action training, where spatial grounding serves as the critical\nlink between instructions and robot actions. InternVLA-M1 employs a two-stage\npipeline: (i) spatial grounding pre-training on over 2.3M spatial reasoning\ndata to determine ``where to act'' by aligning instructions with visual,\nembodiment-agnostic positions, and (ii) spatially guided action post-training\nto decide ``how to act'' by generating embodiment-aware actions through\nplug-and-play spatial prompting. This spatially guided training recipe yields\nconsistent gains: InternVLA-M1 outperforms its variant without spatial guidance\nby +14.6% on SimplerEnv Google Robot, +17% on WidowX, and +4.3% on LIBERO\nFranka, while demonstrating stronger spatial reasoning capability in box,\npoint, and trace prediction. To further scale instruction following, we built a\nsimulation engine to collect 244K generalizable pick-and-place episodes,\nenabling a 6.2% average improvement across 200 tasks and 3K+ objects. In\nreal-world clustered pick-and-place, InternVLA-M1 improved by 7.3%, and with\nsynthetic co-training, achieved +20.6% on unseen objects and novel\nconfigurations. Moreover, in long-horizon reasoning-intensive scenarios, it\nsurpassed existing works by over 10%. These results highlight spatially guided\ntraining as a unifying principle for scalable and resilient generalist robots.\nCode and models are available at\nhttps://github.com/InternRobotics/InternVLA-M1.",
    "published": "2025-10-15 17:30:05+00:00",
    "pdf_url": "http://arxiv.org/pdf/2510.13778v1"
  },
  {
    "id": "2510.13704v1",
    "title": "Simplicial Embeddings Improve Sample Efficiency in Actor-Critic Agents",
    "authors": [
      "Johan Obando-Ceron",
      "Walter Mayor",
      "Samuel Lavoie",
      "Scott Fujimoto",
      "Aaron Courville",
      "Pablo Samuel Castro"
    ],
    "abstract": "Recent works have proposed accelerating the wall-clock training time of\nactor-critic methods via the use of large-scale environment parallelization;\nunfortunately, these can sometimes still require large number of environment\ninteractions to achieve a desired level of performance. Noting that\nwell-structured representations can improve the generalization and sample\nefficiency of deep reinforcement learning (RL) agents, we propose the use of\nsimplicial embeddings: lightweight representation layers that constrain\nembeddings to simplicial structures. This geometric inductive bias results in\nsparse and discrete features that stabilize critic bootstrapping and strengthen\npolicy gradients. When applied to FastTD3, FastSAC, and PPO, simplicial\nembeddings consistently improve sample efficiency and final performance across\na variety of continuous- and discrete-control environments, without any loss in\nruntime speed.",
    "published": "2025-10-15 16:01:30+00:00",
    "pdf_url": "http://arxiv.org/pdf/2510.13704v1"
  },
  {
    "id": "2510.13686v1",
    "title": "Hierarchical Discrete Lattice Assembly: An Approach for the Digital Fabrication of Scalable Macroscale Structures",
    "authors": [
      "Miana Smith",
      "Paul Arthur Richard",
      "Alexander Htet Kyaw",
      "Neil Gershenfeld"
    ],
    "abstract": "Although digital fabrication processes at the desktop scale have become\nproficient and prolific, systems aimed at producing larger-scale structures are\nstill typically complex, expensive, and unreliable. In this work, we present an\napproach for the fabrication of scalable macroscale structures using simple\nrobots and interlocking lattice building blocks. A target structure is first\nvoxelized so that it can be populated with an architected lattice. These voxels\nare then grouped into larger interconnected blocks, which are produced using\nstandard digital fabrication processes, leveraging their capability to produce\nhighly complex geometries at a small scale. These blocks, on the size scale of\ntens of centimeters, are then fed to mobile relative robots that are able to\ntraverse over the structure and place new blocks to form structures on the\nmeter scale. To facilitate the assembly of large structures, we introduce a\nlive digital twin simulation tool for controlling and coordinating assembly\nrobots that enables both global planning for a target structure and live user\ndesign, interaction, or intervention. To improve assembly throughput, we\nintroduce a new modular assembly robot, designed for hierarchical voxel\nhandling. We validate this system by demonstrating the voxelization,\nhierarchical blocking, path planning, and robotic fabrication of a set of\nmeter-scale objects.",
    "published": "2025-10-15 15:43:43+00:00",
    "pdf_url": "http://arxiv.org/pdf/2510.13686v1"
  },
  {
    "id": "2510.13644v1",
    "title": "On Your Own: Pro-level Autonomous Drone Racing in Uninstrumented Arenas",
    "authors": [
      "Michael Bosello",
      "Flavio Pinzarrone",
      "Sara Kiade",
      "Davide Aguiari",
      "Yvo Keuter",
      "Aaesha AlShehhi",
      "Gyordan Caminati",
      "Kei Long Wong",
      "Ka Seng Chou",
      "Junaid Halepota",
      "Fares Alneyadi",
      "Jacopo Panerati",
      "Giovanni Pau"
    ],
    "abstract": "Drone technology is proliferating in many industries, including agriculture,\nlogistics, defense, infrastructure, and environmental monitoring. Vision-based\nautonomy is one of its key enablers, particularly for real-world applications.\nThis is essential for operating in novel, unstructured environments where\ntraditional navigation methods may be unavailable. Autonomous drone racing has\nbecome the de facto benchmark for such systems. State-of-the-art research has\nshown that autonomous systems can surpass human-level performance in racing\narenas. However, direct applicability to commercial and field operations is\nstill limited as current systems are often trained and evaluated in highly\ncontrolled environments. In our contribution, the system's capabilities are\nanalyzed within a controlled environment -- where external tracking is\navailable for ground-truth comparison -- but also demonstrated in a\nchallenging, uninstrumented environment -- where ground-truth measurements were\nnever available. We show that our approach can match the performance of\nprofessional human pilots in both scenarios. We also publicly release the data\nfrom the flights carried out by our approach and a world-class human pilot.",
    "published": "2025-10-15 15:06:47+00:00",
    "pdf_url": "http://arxiv.org/pdf/2510.13644v1"
  },
  {
    "id": "2510.13626v1",
    "title": "LIBERO-Plus: In-depth Robustness Analysis of Vision-Language-Action Models",
    "authors": [
      "Senyu Fei",
      "Siyin Wang",
      "Junhao Shi",
      "Zihao Dai",
      "Jikun Cai",
      "Pengfang Qian",
      "Li Ji",
      "Xinzhe He",
      "Shiduo Zhang",
      "Zhaoye Fei",
      "Jinlan Fu",
      "Jingjing Gong",
      "Xipeng Qiu"
    ],
    "abstract": "Visual-Language-Action (VLA) models report impressive success rates on\nrobotic manipulation benchmarks, yet these results may mask fundamental\nweaknesses in robustness. We perform a systematic vulnerability analysis by\nintroducing controlled perturbations across seven dimensions: objects layout,\ncamera viewpoints, robot initial states, language instructions, light\nconditions, background textures and sensor noise. We comprehensively analyzed\nmultiple state-of-the-art models and revealed consistent brittleness beneath\napparent competence. Our analysis exposes critical weaknesses: models exhibit\nextreme sensitivity to perturbation factors, including camera viewpoints and\nrobot initial states, with performance dropping from 95% to below 30% under\nmodest perturbations. Surprisingly, models are largely insensitive to language\nvariations, with further experiments revealing that models tend to ignore\nlanguage instructions completely. Our findings challenge the assumption that\nhigh benchmark scores equate to true competency and highlight the need for\nevaluation practices that assess reliability under realistic variation.",
    "published": "2025-10-15 14:51:36+00:00",
    "pdf_url": "http://arxiv.org/pdf/2510.13626v1"
  },
  {
    "id": "2510.13625v1",
    "title": "A Modular Object Detection System for Humanoid Robots Using YOLO",
    "authors": [
      "Nicolas Pottier",
      "Meng Cheng Lau"
    ],
    "abstract": "Within the field of robotics, computer vision remains a significant barrier\nto progress, with many tasks hindered by inefficient vision systems. This\nresearch proposes a generalized vision module leveraging YOLOv9, a\nstate-of-the-art framework optimized for computationally constrained\nenvironments like robots. The model is trained on a dataset tailored to the\nFIRA robotics Hurocup. A new vision module is implemented in ROS1 using a\nvirtual environment to enable YOLO compatibility. Performance is evaluated\nusing metrics such as frames per second (FPS) and Mean Average Precision (mAP).\nPerformance is then compared to the existing geometric framework in static and\ndynamic contexts. The YOLO model achieved comparable precision at a higher\ncomputational cost then the geometric model, while providing improved\nrobustness.",
    "published": "2025-10-15 14:51:31+00:00",
    "pdf_url": "http://arxiv.org/pdf/2510.13625v1"
  },
  {
    "id": "2510.13619v1",
    "title": "Characterizing Lidar Point-Cloud Adversities Using a Vector Field Visualization",
    "authors": [
      "Daniel Choate",
      "Jason Rife"
    ],
    "abstract": "In this paper we introduce a visualization methodology to aid a human analyst\nin classifying adversity modes that impact lidar scan matching. Our methodology\nis intended for offline rather than real-time analysis. The method generates a\nvector-field plot that characterizes local discrepancies between a pair of\nregistered point clouds. The vector field plot reveals patterns that would be\ndifficult for the analyst to extract from raw point-cloud data. After\nintroducing our methodology, we apply the process to two proof-of-concept\nexamples: one a simulation study and the other a field experiment. For both\ndata sets, a human analyst was able to reason about a series of adversity\nmechanisms and iteratively remove those mechanisms from the raw data, to help\nfocus attention on progressively smaller discrepancies.",
    "published": "2025-10-15 14:49:27+00:00",
    "pdf_url": "http://arxiv.org/pdf/2510.13619v1"
  },
  {
    "id": "2510.13616v1",
    "title": "Efficient Force and Stiffness Prediction in Robotic Produce Handling with a Piezoresistive Pressure Sensor",
    "authors": [
      "Preston Fairchild",
      "Claudia Chen",
      "Xiaobo Tan"
    ],
    "abstract": "Properly handling delicate produce with robotic manipulators is a major part\nof the future role of automation in agricultural harvesting and processing.\nGrasping with the correct amount of force is crucial in not only ensuring\nproper grip on the object, but also to avoid damaging or bruising the product.\nIn this work, a flexible pressure sensor that is both low cost and easy to\nfabricate is integrated with robotic grippers for working with produce of\nvarying shapes, sizes, and stiffnesses. The sensor is successfully integrated\nwith both a rigid robotic gripper, as well as a pneumatically actuated soft\nfinger. Furthermore, an algorithm is proposed for accelerated estimation of the\nsteady-state value of the sensor output based on the transient response data,\nto enable real-time applications. The sensor is shown to be effective in\nincorporating feedback to correctly grasp objects of unknown sizes and\nstiffnesses. At the same time, the sensor provides estimates for these values\nwhich can be utilized for identification of qualities such as ripeness levels\nand bruising. It is also shown to be able to provide force feedback for objects\nof variable stiffnesses. This enables future use not only for produce\nidentification, but also for tasks such as quality control and selective\ndistribution based on ripeness levels.",
    "published": "2025-10-15 14:45:37+00:00",
    "pdf_url": "http://arxiv.org/pdf/2510.13616v1"
  },
  {
    "id": "2510.13599v1",
    "title": "PlanarMesh: Building Compact 3D Meshes from LiDAR using Incremental Adaptive Resolution Reconstruction",
    "authors": [
      "Jiahao Wang",
      "Nived Chebrolu",
      "Yifu Tao",
      "Lintong Zhang",
      "Ayoung Kim",
      "Maurice Fallon"
    ],
    "abstract": "Building an online 3D LiDAR mapping system that produces a detailed surface\nreconstruction while remaining computationally efficient is a challenging task.\nIn this paper, we present PlanarMesh, a novel incremental, mesh-based LiDAR\nreconstruction system that adaptively adjusts mesh resolution to achieve\ncompact, detailed reconstructions in real-time. It introduces a new\nrepresentation, planar-mesh, which combines plane modeling and meshing to\ncapture both large surfaces and detailed geometry. The planar-mesh can be\nincrementally updated considering both local surface curvature and free-space\ninformation from sensor measurements. We employ a multi-threaded architecture\nwith a Bounding Volume Hierarchy (BVH) for efficient data storage and fast\nsearch operations, enabling real-time performance. Experimental results show\nthat our method achieves reconstruction accuracy on par with, or exceeding,\nstate-of-the-art techniques-including truncated signed distance functions,\noccupancy mapping, and voxel-based meshing-while producing smaller output file\nsizes (10 times smaller than raw input and more than 5 times smaller than\nmesh-based methods) and maintaining real-time performance (around 2 Hz for a\n64-beam sensor).",
    "published": "2025-10-15 14:33:00+00:00",
    "pdf_url": "http://arxiv.org/pdf/2510.13599v1"
  },
  {
    "id": "2510.13595v1",
    "title": "Active Tactile Exploration for Rigid Body Pose and Shape Estimation",
    "authors": [
      "Ethan K. Gordon",
      "Bruke Baraki",
      "Hien Bui",
      "Michael Posa"
    ],
    "abstract": "General robot manipulation requires the handling of previously unseen\nobjects. Learning a physically accurate model at test time can provide\nsignificant benefits in data efficiency, predictability, and reuse between\ntasks. Tactile sensing can compliment vision with its robustness to occlusion,\nbut its temporal sparsity necessitates careful online exploration to maintain\ndata efficiency. Direct contact can also cause an unrestrained object to move,\nrequiring both shape and location estimation. In this work, we propose a\nlearning and exploration framework that uses only tactile data to\nsimultaneously determine the shape and location of rigid objects with minimal\nrobot motion. We build on recent advances in contact-rich system identification\nto formulate a loss function that penalizes physical constraint violation\nwithout introducing the numerical stiffness inherent in rigid-body contact.\nOptimizing this loss, we can learn cuboid and convex polyhedral geometries with\nless than 10s of randomly collected data after first contact. Our exploration\nscheme seeks to maximize Expected Information Gain and results in significantly\nfaster learning in both simulated and real-robot experiments. More information\ncan be found at https://dairlab.github.io/activetactile",
    "published": "2025-10-15 14:24:57+00:00",
    "pdf_url": "http://arxiv.org/pdf/2510.13595v1"
  },
  {
    "id": "2510.13594v1",
    "title": "Development of an Intuitive GUI for Non-Expert Teleoperation of Humanoid Robots",
    "authors": [
      "Austin Barret",
      "Meng Cheng Lau"
    ],
    "abstract": "The operation of humanoid robotics is an essential field of research with\nmany practical and competitive applications. Many of these systems, however, do\nnot invest heavily in developing a non-expert-centered graphical user interface\n(GUI) for operation. The focus of this research is to develop a scalable GUI\nthat is tailored to be simple and intuitive so non-expert operators can control\nthe robot through a FIRA-regulated obstacle course. Using common practices from\nuser interface development (UI) and understanding concepts described in\nhuman-robot interaction (HRI) and other related concepts, we will develop a new\ninterface with the goal of a non-expert teleoperation system.",
    "published": "2025-10-15 14:24:48+00:00",
    "pdf_url": "http://arxiv.org/pdf/2510.13594v1"
  },
  {
    "id": "2510.13553v1",
    "title": "Hoecken-D Hand: A Novel Robotic Hand for Linear Parallel Pinching and Self-Adaptive Grasping",
    "authors": [
      "Wentao Guo",
      "Wenzeng Zhang"
    ],
    "abstract": "This paper presents the Hoecken-D Hand, an underactuated robotic gripper that\ncombines a modified Hoecken linkage with a differential spring mechanism to\nachieve both linear parallel pinching and a mid-stroke transition to adaptive\nenvelope. The original Hoecken linkage is reconfigured by replacing one member\nwith differential links, preserving straight-line guidance while enabling\ncontact-triggered reconfiguration without additional actuators. A\ndouble-parallelogram arrangement maintains fingertip parallelism during\nconventional pinching, whereas the differential mechanism allows one finger to\nwrap inward upon encountering an obstacle, improving stability on irregular or\nthin objects. The mechanism can be driven by a single linear actuator,\nminimizing complexity and cost; in our prototype, each finger is driven by its\nown linear actuator for simplicity. We perform kinematic modeling and force\nanalysis to characterize grasp performance, including simulated grasping forces\nand spring-opening behavior under varying geometric parameters. The design was\nprototyped using PLA-based 3D printing, achieving a linear pinching span of\napproximately 200 mm. Preliminary tests demonstrate reliable grasping in both\nmodes across a wide range of object geometries, highlighting the Hoecken-D Hand\nas a compact, adaptable, and cost-effective solution for manipulation in\nunstructured environments.",
    "published": "2025-10-15 13:49:02+00:00",
    "pdf_url": "http://arxiv.org/pdf/2510.13553v1"
  },
  {
    "id": "2510.13546v1",
    "title": "Accelerated Feature Detectors for Visual SLAM: A Comparative Study of FPGA vs GPU",
    "authors": [
      "Ruiqi Ye",
      "Mikel Luján"
    ],
    "abstract": "Feature detection is a common yet time-consuming module in Simultaneous\nLocalization and Mapping (SLAM) implementations, which are increasingly\ndeployed on power-constrained platforms, such as drones. Graphics Processing\nUnits (GPUs) have been a popular accelerator for computer vision in general,\nand feature detection and SLAM in particular.\n  On the other hand, System-on-Chips (SoCs) with integrated Field Programmable\nGate Array (FPGA) are also widely available. This paper presents the first\nstudy of hardware-accelerated feature detectors considering a Visual SLAM\n(V-SLAM) pipeline. We offer new insights by comparing the best GPU-accelerated\nFAST, Harris, and SuperPoint implementations against the FPGA-accelerated\ncounterparts on modern SoCs (Nvidia Jetson Orin and AMD Versal).\n  The evaluation shows that when using a non-learning-based feature detector\nsuch as FAST and Harris, their GPU implementations, and the GPU-accelerated\nV-SLAM can achieve better run-time performance and energy efficiency than the\nFAST and Harris FPGA implementations as well as the FPGA-accelerated V-SLAM.\nHowever, when considering a learning-based detector such as SuperPoint, its\nFPGA implementation can achieve better run-time performance and energy\nefficiency (up to 3.1$\\times$ and 1.4$\\times$ improvements, respectively) than\nthe GPU implementation. The FPGA-accelerated V-SLAM can also achieve comparable\nrun-time performance compared to the GPU-accelerated V-SLAM, with better FPS in\n2 out of 5 dataset sequences. When considering the accuracy, the results show\nthat the GPU-accelerated V-SLAM is more accurate than the FPGA-accelerated\nV-SLAM in general. Last but not least, the use of hardware acceleration for\nfeature detection could further improve the performance of the V-SLAM pipeline\nby having the global bundle adjustment module invoked less frequently without\nsacrificing accuracy.",
    "published": "2025-10-15 13:40:55+00:00",
    "pdf_url": "http://arxiv.org/pdf/2510.13546v1"
  },
  {
    "id": "2510.13535v1",
    "title": "A Novel Robot Hand with Hoeckens Linkages and Soft Phalanges for Scooping and Self-Adaptive Grasping in Environmental Constraints",
    "authors": [
      "Wentao Guo",
      "Yizhou Wang",
      "Wenzeng Zhang"
    ],
    "abstract": "This paper presents a novel underactuated adaptive robotic hand, Hockens-A\nHand, which integrates the Hoeckens mechanism, a double-parallelogram linkage,\nand a specialized four-bar linkage to achieve three adaptive grasping modes:\nparallel pinching, asymmetric scooping, and enveloping grasping. Hockens-A Hand\nrequires only a single linear actuator, leveraging passive mechanical\nintelligence to ensure adaptability and compliance in unstructured\nenvironments. Specifically, the vertical motion of the Hoeckens mechanism\nintroduces compliance, the double-parallelogram linkage ensures line contact at\nthe fingertip, and the four-bar amplification system enables natural\ntransitions between different grasping modes. Additionally, the inclusion of a\nmesh-textured silicone phalanx further enhances the ability to envelop objects\nof various shapes and sizes. This study employs detailed kinematic analysis to\noptimize the push angle and design the linkage lengths for optimal performance.\nSimulations validated the design by analyzing the fingertip motion and ensuring\nsmooth transitions between grasping modes. Furthermore, the grasping force was\nanalyzed using power equations to enhance the understanding of the system's\nperformance.Experimental validation using a 3D-printed prototype demonstrates\nthe three grasping modes of the hand in various scenarios under environmental\nconstraints, verifying its grasping stability and broad applicability.",
    "published": "2025-10-15 13:27:46+00:00",
    "pdf_url": "http://arxiv.org/pdf/2510.13535v1"
  },
  {
    "id": "2510.13488v1",
    "title": "Bridge the Gap: Enhancing Quadruped Locomotion with Vertical Ground Perturbations",
    "authors": [
      "Maximilian Stasica",
      "Arne Bick",
      "Nico Bohlinger",
      "Omid Mohseni",
      "Max Johannes Alois Fritzsche",
      "Clemens Hübler",
      "Jan Peters",
      "André Seyfarth"
    ],
    "abstract": "Legged robots, particularly quadrupeds, excel at navigating rough terrains,\nyet their performance under vertical ground perturbations, such as those from\noscillating surfaces, remains underexplored. This study introduces a novel\napproach to enhance quadruped locomotion robustness by training the Unitree Go2\nrobot on an oscillating bridge - a 13.24-meter steel-and-concrete structure\nwith a 2.0 Hz eigenfrequency designed to perturb locomotion. Using\nReinforcement Learning (RL) with the Proximal Policy Optimization (PPO)\nalgorithm in a MuJoCo simulation, we trained 15 distinct locomotion policies,\ncombining five gaits (trot, pace, bound, free, default) with three training\nconditions: rigid bridge and two oscillating bridge setups with differing\nheight regulation strategies (relative to bridge surface or ground). Domain\nrandomization ensured zero-shot transfer to the real-world bridge. Our results\ndemonstrate that policies trained on the oscillating bridge exhibit superior\nstability and adaptability compared to those trained on rigid surfaces. Our\nframework enables robust gait patterns even without prior bridge exposure.\nThese findings highlight the potential of simulation-based RL to improve\nquadruped locomotion during dynamic ground perturbations, offering insights for\ndesigning robots capable of traversing vibrating environments.",
    "published": "2025-10-15 12:37:55+00:00",
    "pdf_url": "http://arxiv.org/pdf/2510.13488v1"
  },
  {
    "id": "2510.13464v1",
    "title": "Through the Lens of Doubt: Robust and Efficient Uncertainty Estimation for Visual Place Recognition",
    "authors": [
      "Emily Miller",
      "Michael Milford",
      "Muhammad Burhan Hafez",
      "SD Ramchurn",
      "Shoaib Ehsan"
    ],
    "abstract": "Visual Place Recognition (VPR) enables robots and autonomous vehicles to\nidentify previously visited locations by matching current observations against\na database of known places. However, VPR systems face significant challenges\nwhen deployed across varying visual environments, lighting conditions, seasonal\nchanges, and viewpoints changes. Failure-critical VPR applications, such as\nloop closure detection in simultaneous localization and mapping (SLAM)\npipelines, require robust estimation of place matching uncertainty. We propose\nthree training-free uncertainty metrics that estimate prediction confidence by\nanalyzing inherent statistical patterns in similarity scores from any existing\nVPR method. Similarity Distribution (SD) quantifies match distinctiveness by\nmeasuring score separation between candidates; Ratio Spread (RS) evaluates\ncompetitive ambiguity among top-scoring locations; and Statistical Uncertainty\n(SU) is a combination of SD and RS that provides a unified metric that\ngeneralizes across datasets and VPR methods without requiring validation data\nto select the optimal metric. All three metrics operate without additional\nmodel training, architectural modifications, or computationally expensive\ngeometric verification. Comprehensive evaluation across nine state-of-the-art\nVPR methods and six benchmark datasets confirms that our metrics excel at\ndiscriminating between correct and incorrect VPR matches, and consistently\noutperform existing approaches while maintaining negligible computational\noverhead, making it deployable for real-time robotic applications across varied\nenvironmental conditions with improved precision-recall performance.",
    "published": "2025-10-15 12:12:55+00:00",
    "pdf_url": "http://arxiv.org/pdf/2510.13464v1"
  },
  {
    "id": "2510.13461v1",
    "title": "Physics-Informed Neural Network Modeling of Vehicle Collision Dynamics in Precision Immobilization Technique Maneuvers",
    "authors": [
      "Yangye Jiang",
      "Jiachen Wang",
      "Daofei Li"
    ],
    "abstract": "Accurate prediction of vehicle collision dynamics is crucial for advanced\nsafety systems and post-impact control applications, yet existing methods face\ninherent trade-offs among computational efficiency, prediction accuracy, and\ndata requirements. This paper proposes a dual Physics-Informed Neural Network\nframework addressing these challenges through two complementary networks. The\nfirst network integrates Gaussian Mixture Models with PINN architecture to\nlearn impact force distributions from finite element analysis data while\nenforcing momentum conservation and energy consistency constraints. The second\nnetwork employs an adaptive PINN with dynamic constraint weighting to predict\npost-collision vehicle dynamics, featuring an adaptive physics guard layer that\nprevents unrealistic predictions whil e preserving data-driven learning\ncapabilities. The framework incorporates uncertainty quantification through\ntime-varying parameters and enables rapid adaptation via fine-tuning\nstrategies. Validation demonstrates significant improvements: the impact force\nmodel achieves relative errors below 15.0% for force prediction on finite\nelement analysis (FEA) datasets, while the vehicle dynamics model reduces\naverage trajectory prediction error by 63.6% compared to traditional\nfour-degree-of-freedom models in scaled vehicle experiments. The integrated\nsystem maintains millisecond-level computational efficiency suitable for\nreal-time applications while providing probabilistic confidence bounds\nessential for safety-critical control. Comprehensive validation through FEA\nsimulation, dynamic modeling, and scaled vehicle experiments confirms the\nframework's effectiveness for Precision Immobilization Technique scenarios and\ngeneral collision dynamics prediction.",
    "published": "2025-10-15 12:08:55+00:00",
    "pdf_url": "http://arxiv.org/pdf/2510.13461v1"
  },
  {
    "id": "2510.13443v1",
    "title": "Real-Time Knee Angle Prediction Using EMG and Kinematic Data with an Attention-Based CNN-LSTM Network and Transfer Learning Across Multiple Datasets",
    "authors": [
      "Mojtaba Mollahossein",
      "Gholamreza Vossoughi",
      "Mohammad Hossein Rohban"
    ],
    "abstract": "Electromyography (EMG) signals are widely used for predicting body joint\nangles through machine learning (ML) and deep learning (DL) methods. However,\nthese approaches often face challenges such as limited real-time applicability,\nnon-representative test conditions, and the need for large datasets to achieve\noptimal performance. This paper presents a transfer-learning framework for knee\njoint angle prediction that requires only a few gait cycles from new subjects.\nThree datasets - Georgia Tech, the University of California Irvine (UCI), and\nthe Sharif Mechatronic Lab Exoskeleton (SMLE) - containing four EMG channels\nrelevant to knee motion were utilized. A lightweight attention-based CNN-LSTM\nmodel was developed and pre-trained on the Georgia Tech dataset, then\ntransferred to the UCI and SMLE datasets. The proposed model achieved\nNormalized Mean Absolute Errors (NMAE) of 6.8 percent and 13.7 percent for\none-step and 50-step predictions on abnormal subjects using EMG inputs alone.\nIncorporating historical knee angles reduced the NMAE to 3.1 percent and 3.5\npercent for normal subjects, and to 2.8 percent and 7.5 percent for abnormal\nsubjects. When further adapted to the SMLE exoskeleton with EMG, kinematic, and\ninteraction force inputs, the model achieved 1.09 percent and 3.1 percent NMAE\nfor one- and 50-step predictions, respectively. These results demonstrate\nrobust performance and strong generalization for both short- and long-term\nrehabilitation scenarios.",
    "published": "2025-10-15 11:41:40+00:00",
    "pdf_url": "http://arxiv.org/pdf/2510.13443v1"
  },
  {
    "id": "2510.13367v1",
    "title": "A New Perspective on Transformers in Online Reinforcement Learning for Continuous Control",
    "authors": [
      "Nikita Kachaev",
      "Daniil Zelezetsky",
      "Egor Cherepanov",
      "Alexey K. Kovelev",
      "Aleksandr I. Panov"
    ],
    "abstract": "Despite their effectiveness and popularity in offline or model-based\nreinforcement learning (RL), transformers remain underexplored in online\nmodel-free RL due to their sensitivity to training setups and model design\ndecisions such as how to structure the policy and value networks, share\ncomponents, or handle temporal information. In this paper, we show that\ntransformers can be strong baselines for continuous control in online\nmodel-free RL. We investigate key design questions: how to condition inputs,\nshare components between actor and critic, and slice sequential data for\ntraining. Our experiments reveal stable architectural and training strategies\nenabling competitive performance across fully and partially observable tasks,\nand in both vector- and image-based settings. These findings offer practical\nguidance for applying transformers in online RL.",
    "published": "2025-10-15 09:58:54+00:00",
    "pdf_url": "http://arxiv.org/pdf/2510.13367v1"
  },
  {
    "id": "2510.13358v1",
    "title": "Adversarial Fine-tuning in Offline-to-Online Reinforcement Learning for Robust Robot Control",
    "authors": [
      "Shingo Ayabe",
      "Hiroshi Kera",
      "Kazuhiko Kawamoto"
    ],
    "abstract": "Offline reinforcement learning enables sample-efficient policy acquisition\nwithout risky online interaction, yet policies trained on static datasets\nremain brittle under action-space perturbations such as actuator faults. This\nstudy introduces an offline-to-online framework that trains policies on clean\ndata and then performs adversarial fine-tuning, where perturbations are\ninjected into executed actions to induce compensatory behavior and improve\nresilience. A performance-aware curriculum further adjusts the perturbation\nprobability during training via an exponential-moving-average signal, balancing\nrobustness and stability throughout the learning process. Experiments on\ncontinuous-control locomotion tasks demonstrate that the proposed method\nconsistently improves robustness over offline-only baselines and converges\nfaster than training from scratch. Matching the fine-tuning and evaluation\nconditions yields the strongest robustness to action-space perturbations, while\nthe adaptive curriculum strategy mitigates the degradation of nominal\nperformance observed with the linear curriculum strategy. Overall, the results\nshow that adversarial fine-tuning enables adaptive and robust control under\nuncertain environments, bridging the gap between offline efficiency and online\nadaptability.",
    "published": "2025-10-15 09:45:24+00:00",
    "pdf_url": "http://arxiv.org/pdf/2510.13358v1"
  },
  {
    "id": "2510.13356v1",
    "title": "MODUR: A Modular Dual-reconfigurable Robot",
    "authors": [
      "Jie Gu",
      "Tin Lun Lam",
      "Chunxu Tian",
      "Zhihao Xia",
      "Yongheng Xing",
      "Dan Zhang"
    ],
    "abstract": "Modular Self-Reconfigurable Robot (MSRR) systems are a class of robots\ncapable of forming higher-level robotic systems by altering the topological\nrelationships between modules, offering enhanced adaptability and robustness in\nvarious environments. This paper presents a novel MSRR called MODUR, featuring\ndual-level reconfiguration capabilities designed to integrate reconfigurable\nmechanisms into MSRR. Specifically, MODUR can perform high-level\nself-reconfiguration among modules to create different configurations, while\neach module is also able to change its shape to execute basic motions. The\ndesign of MODUR primarily includes a compact connector and scissor linkage\ngroups that provide actuation, forming a parallel mechanism capable of\nachieving both connector motion decoupling and adjacent position migration\ncapabilities. Furthermore, the workspace, considering the interdependent\nconnectors, is comprehensively analyzed, laying a theoretical foundation for\nthe design of the module's basic motion. Finally, the motion of MODUR is\nvalidated through a series of experiments.",
    "published": "2025-10-15 09:42:56+00:00",
    "pdf_url": "http://arxiv.org/pdf/2510.13356v1"
  },
  {
    "id": "2510.13324v1",
    "title": "Tactile-Conditioned Diffusion Policy for Force-Aware Robotic Manipulation",
    "authors": [
      "Erik Helmut",
      "Niklas Funk",
      "Tim Schneider",
      "Cristiana de Farias",
      "Jan Peters"
    ],
    "abstract": "Contact-rich manipulation depends on applying the correct grasp forces\nthroughout the manipulation task, especially when handling fragile or\ndeformable objects. Most existing imitation learning approaches often treat\nvisuotactile feedback only as an additional observation, leaving applied forces\nas an uncontrolled consequence of gripper commands. In this work, we present\nForce-Aware Robotic Manipulation (FARM), an imitation learning framework that\nintegrates high-dimensional tactile data to infer tactile-conditioned force\nsignals, which in turn define a matching force-based action space. We collect\nhuman demonstrations using a modified version of the handheld Universal\nManipulation Interface (UMI) gripper that integrates a GelSight Mini visual\ntactile sensor. For deploying the learned policies, we developed an actuated\nvariant of the UMI gripper with geometry matching our handheld version. During\npolicy rollouts, the proposed FARM diffusion policy jointly predicts robot\npose, grip width, and grip force. FARM outperforms several baselines across\nthree tasks with distinct force requirements -- high-force, low-force, and\ndynamic force adaptation -- demonstrating the advantages of its two key\ncomponents: leveraging force-grounded, high-dimensional tactile observations\nand a force-based control space. The codebase and design files are open-sourced\nand available at https://tactile-farm.github.io .",
    "published": "2025-10-15 09:11:14+00:00",
    "pdf_url": "http://arxiv.org/pdf/2510.13324v1"
  },
  {
    "id": "2510.13287v1",
    "title": "DAMM-LOAM: Degeneracy Aware Multi-Metric LiDAR Odometry and Mapping",
    "authors": [
      "Nishant Chandna",
      "Akshat Kaushal"
    ],
    "abstract": "LiDAR Simultaneous Localization and Mapping (SLAM) systems are essential for\nenabling precise navigation and environmental reconstruction across various\napplications. Although current point-to-plane ICP algorithms perform effec-\ntively in structured, feature-rich environments, they struggle in scenarios\nwith sparse features, repetitive geometric structures, and high-frequency\nmotion. This leads to degeneracy in 6- DOF pose estimation. Most\nstate-of-the-art algorithms address these challenges by incorporating\nadditional sensing modalities, but LiDAR-only solutions continue to face\nlimitations under such conditions. To address these issues, we propose a novel\nDegeneracy-Aware Multi-Metric LiDAR Odometry and Map- ping (DAMM-LOAM) module.\nOur system improves mapping accuracy through point cloud classification based\non surface normals and neighborhood analysis. Points are classified into\nground, walls, roof, edges, and non-planar points, enabling accurate\ncorrespondences. A Degeneracy-based weighted least squares-based ICP algorithm\nis then applied for accurate odom- etry estimation. Additionally, a Scan\nContext based back-end is implemented to support robust loop closures.\nDAMM-LOAM demonstrates significant improvements in odometry accuracy,\nespecially in indoor environments such as long corridors",
    "published": "2025-10-15 08:32:47+00:00",
    "pdf_url": "http://arxiv.org/pdf/2510.13287v1"
  },
  {
    "id": "2510.13284v1",
    "title": "ALOHA2 Robot Kitchen Application Scenario Reproduction Report",
    "authors": [
      "Haoyang Wu",
      "Siheng Wu",
      "William X. Liu",
      "Fangui Zeng"
    ],
    "abstract": "ALOHA2 is an enhanced version of the dual-arm teleoperated robot ALOHA,\nfeaturing higher performance and robustness compared to the original design,\nwhile also being more ergonomic. Like ALOHA, ALOHA2 consists of two grippers\nand two ViperX 6-DoF arms, as well as two smaller WidowX arms. Users control\nthe follower mechanical arms by operating the leader mechanical arms through\nback-driving. The device also includes cameras that generate images from\nmultiple viewpoints, allowing for RGB data collection during teleoperation. The\nrobot is mounted on a 48-inch x 30-inch table, equipped with an aluminum frame\nthat provides additional mounting points for cameras and gravity compensation\nsystems.",
    "published": "2025-10-15 08:31:36+00:00",
    "pdf_url": "http://arxiv.org/pdf/2510.13284v1"
  },
  {
    "id": "2510.13149v1",
    "title": "RoboHiMan: A Hierarchical Evaluation Paradigm for Compositional Generalization in Long-Horizon Manipulation",
    "authors": [
      "Yangtao Chen",
      "Zixuan Chen",
      "Nga Teng Chan",
      "Junting Chen",
      "Junhui Yin",
      "Jieqi Shi",
      "Yang Gao",
      "Yong-Lu Li",
      "Jing Huo"
    ],
    "abstract": "Enabling robots to flexibly schedule and compose learned skills for novel\nlong-horizon manipulation under diverse perturbations remains a core challenge.\nEarly explorations with end-to-end VLA models show limited success, as these\nmodels struggle to generalize beyond the training distribution. Hierarchical\napproaches, where high-level planners generate subgoals for low-level policies,\nbring certain improvements but still suffer under complex perturbations,\nrevealing limited capability in skill composition. However, existing benchmarks\nprimarily emphasize task completion in long-horizon settings, offering little\ninsight into compositional generalization, robustness, and the interplay\nbetween planning and execution. To systematically investigate these gaps, we\npropose RoboHiMan, a hierarchical evaluation paradigm for compositional\ngeneralization in long-horizon manipulation. RoboHiMan introduces HiMan-Bench,\na benchmark of atomic and compositional tasks under diverse perturbations,\nsupported by a multi-level training dataset for analyzing progressive data\nscaling, and proposes three evaluation paradigms (vanilla, decoupled, coupled)\nthat probe the necessity of skill composition and reveal bottlenecks in\nhierarchical architectures. Experiments highlight clear capability gaps across\nrepresentative models and architectures, pointing to directions for advancing\nmodels better suited to real-world long-horizon manipulation tasks. Videos and\nopen-source code can be found on our project website:\nhttps://chenyt31.github.io/robo-himan.github.io/.",
    "published": "2025-10-15 04:58:13+00:00",
    "pdf_url": "http://arxiv.org/pdf/2510.13149v1"
  },
  {
    "id": "2510.13114v1",
    "title": "Safe Driving in Occluded Environments",
    "authors": [
      "Zhuoyuan Wang",
      "Tongyao Jia",
      "Pharuj Rajborirug",
      "Neeraj Ramesh",
      "Hiroyuki Okuda",
      "Tatsuya Suzuki",
      "Soummya Kar",
      "Yorie Nakahira"
    ],
    "abstract": "Ensuring safe autonomous driving in the presence of occlusions poses a\nsignificant challenge in its policy design. While existing model-driven control\ntechniques based on set invariance can handle visible risks, occlusions create\nlatent risks in which safety-critical states are not observable. Data-driven\ntechniques also struggle to handle latent risks because direct mappings from\nrisk-critical objects in sensor inputs to safe actions cannot be learned\nwithout visible risk-critical objects. Motivated by these challenges, in this\npaper, we propose a probabilistic safety certificate for latent risk. Our key\ntechnical enabler is the application of probabilistic invariance: It relaxes\nthe strict observability requirements imposed by set-invariance methods that\ndemand the knowledge of risk-critical states. The proposed techniques provide\nlinear action constraints that confine the latent risk probability within\ntolerance. Such constraints can be integrated into model predictive controllers\nor embedded in data-driven policies to mitigate latent risks. The proposed\nmethod is tested using the CARLA simulator and compared with a few existing\ntechniques. The theoretical and empirical analysis jointly demonstrate that the\nproposed methods assure long-term safety in real-time control in occluded\nenvironments without being overly conservative and with transparency to exposed\nrisks.",
    "published": "2025-10-15 03:20:03+00:00",
    "pdf_url": "http://arxiv.org/pdf/2510.13114v1"
  },
  {
    "id": "2510.13108v1",
    "title": "DriveCritic: Towards Context-Aware, Human-Aligned Evaluation for Autonomous Driving with Vision-Language Models",
    "authors": [
      "Jingyu Song",
      "Zhenxin Li",
      "Shiyi Lan",
      "Xinglong Sun",
      "Nadine Chang",
      "Maying Shen",
      "Joshua Chen",
      "Katherine A. Skinner",
      "Jose M. Alvarez"
    ],
    "abstract": "Benchmarking autonomous driving planners to align with human judgment remains\na critical challenge, as state-of-the-art metrics like the Extended Predictive\nDriver Model Score (EPDMS) lack context awareness in nuanced scenarios. To\naddress this, we introduce DriveCritic, a novel framework featuring two key\ncontributions: the DriveCritic dataset, a curated collection of challenging\nscenarios where context is critical for correct judgment and annotated with\npairwise human preferences, and the DriveCritic model, a Vision-Language Model\n(VLM) based evaluator. Fine-tuned using a two-stage supervised and\nreinforcement learning pipeline, the DriveCritic model learns to adjudicate\nbetween trajectory pairs by integrating visual and symbolic context.\nExperiments show DriveCritic significantly outperforms existing metrics and\nbaselines in matching human preferences and demonstrates strong context\nawareness. Overall, our work provides a more reliable, human-aligned foundation\nto evaluating autonomous driving systems.",
    "published": "2025-10-15 03:00:38+00:00",
    "pdf_url": "http://arxiv.org/pdf/2510.13108v1"
  },
  {
    "id": "2510.13054v1",
    "title": "VLA-0: Building State-of-the-Art VLAs with Zero Modification",
    "authors": [
      "Ankit Goyal",
      "Hugo Hadfield",
      "Xuning Yang",
      "Valts Blukis",
      "Fabio Ramos"
    ],
    "abstract": "Vision-Language-Action models (VLAs) hold immense promise for enabling\ngeneralist robot manipulation. However, the best way to build them remains an\nopen question. Current approaches often add complexity, such as modifying the\nexisting vocabulary of a Vision-Language Model (VLM) with action tokens or\nintroducing special action heads. Curiously, the simplest strategy of\nrepresenting actions directly as text has remained largely unexplored. This\nwork introduces VLA-0 to investigate this idea. We find that VLA-0 is not only\neffective; it is surprisingly powerful. With the right design, VLA-0\noutperforms more involved models. On LIBERO, a popular benchmark for evaluating\nVLAs, VLA-0 outperforms all existing methods trained on the same robotic data,\nincluding $\\pi_0.5$-KI, OpenVLA-OFT and SmolVLA. Furthermore, without\nlarge-scale robotics-specific training, it outperforms methods trained on\nlarge-scale robotic data, like $\\pi_0.5$-KI, $\\pi_0$, GR00T-N1 and MolmoAct.\nThese findings also translate to the real world, where VLA-0 outperforms\nSmolVLA, a VLA model pre-trained on large-scale real data. This paper\nsummarizes our unexpected findings and spells out the specific techniques\nrequired to unlock the high performance of this simple yet potent VLA design.\nVisual results, code, and trained models are provided here:\nhttps://vla0.github.io/.",
    "published": "2025-10-15 00:31:10+00:00",
    "pdf_url": "http://arxiv.org/pdf/2510.13054v1"
  },
  {
    "id": "2510.13048v1",
    "title": "Kinematic Kitbashing for Modeling Functional Articulated Objects",
    "authors": [
      "Minghao Guo",
      "Victor Zordan",
      "Sheldon Andrews",
      "Wojciech Matusik",
      "Maneesh Agrawala",
      "Hsueh-Ti Derek Liu"
    ],
    "abstract": "We introduce Kinematic Kitbashing, an automatic framework that synthesizes\nfunctionality-aware articulated objects by reusing parts from existing models.\nGiven a kinematic graph with a small collection of articulated parts, our\noptimizer jointly solves for the spatial placement of every part so that (i)\nattachments remain geometrically sound over the entire range of motion and (ii)\nthe assembled object satisfies user-specified functional goals such as\ncollision-free actuation, reachability, or trajectory following. At its core is\na kinematics-aware attachment energy that aligns vector distance function\nfeatures sampled across multiple articulation snapshots. We embed this\nattachment term within an annealed Riemannian Langevin dynamics sampler that\ntreats functionality objectives as additional energies, enabling robust global\nexploration while accommodating non-differentiable functionality objectives and\nconstraints. Our framework produces a wide spectrum of assembled articulated\nshapes, from trash-can wheels grafted onto car bodies to multi-segment lamps,\ngear-driven paddlers, and reconfigurable furniture, and delivers strong\nquantitative improvements over state-of-the-art baselines across geometric,\nkinematic, and functional metrics. By tightly coupling articulation-aware\ngeometry matching with functionality-driven optimization, Kinematic Kitbashing\nbridges part-based shape modeling and functional assembly design, empowering\nrapid creation of interactive articulated assets.",
    "published": "2025-10-14 23:54:58+00:00",
    "pdf_url": "http://arxiv.org/pdf/2510.13048v1"
  }
]